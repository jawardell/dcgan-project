{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "782ac4d2-dc04-4ded-aa30-32586482a06f",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa2db3a8-9db2-49b1-a58a-f8cb17e96201",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import STL10\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2762880-1c03-4ec5-95b1-db4724364359",
   "metadata": {},
   "source": [
    "# Seed Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "212fd277-50e4-4ca4-9457-ba39c1ad1612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  999\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "manualSeed = 999\n",
    "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "torch.use_deterministic_algorithms(True) # Needed for reproducible results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f9b4ba-8879-448f-8887-5b13cf1da0fb",
   "metadata": {},
   "source": [
    "# Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "624d937c-52c5-455e-aa37-a9fc6fb2da3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of channels in the training images. For color images this is 3\n",
    "nc = 3\n",
    "\n",
    "# Size of z latent vector (i.e. size of generator input)\n",
    "nz = 64\n",
    "\n",
    "# Size of feature maps in discriminator\n",
    "ndf = 96\n",
    "\n",
    "# Size of feature maps in generator\n",
    "ngf = 96\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 100\n",
    "\n",
    "# Learning rate for optimizers\n",
    "lr=0.001\n",
    "\n",
    "# Beta1 hyperparameter for Adam optimizers\n",
    "beta1 = 0.5\n",
    "\n",
    "# Number of GPUs available. Use 0 for CPU mode.\n",
    "ngpu = 1\n",
    "\n",
    "\n",
    "# Decide which device we want to run on\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993ac5a4-7da8-497c-9f89-f95eadaa38b6",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7495082-8dc9-4bee-b7e4-13d76d25f0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105000\n",
      "411\n",
      "8000\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "image_size = 96\n",
    "\n",
    "# Create a new transformation that resizes the images\n",
    "transform=transforms.Compose([\n",
    "                               transforms.Resize(image_size),\n",
    "                               transforms.CenterCrop(image_size),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "\n",
    "# Load STL-10 dataset\n",
    "train_dataset = STL10(root='./data', split='train+unlabeled', transform=transform, download=False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "print(len(train_dataset))\n",
    "print(len(train_loader))\n",
    "\n",
    "test_dataset = STL10(root='./data', split='test', transform=transform, download=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "print(len(test_dataset))\n",
    "print(len(test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ad2b20-36d3-4f7b-9cd4-e04e4e386104",
   "metadata": {},
   "source": [
    "# Weight Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fa4d62d-c72b-4b33-9efb-abe6769302fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom weights initialization called on ``netG`` and ``netD``\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff863710-ccde-4367-bd0e-9e1ec02da31d",
   "metadata": {},
   "source": [
    "# Encoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "827e05c6-4dda-4bb7-9451-7a3fcce12b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(96, 192, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (5): Conv2d(192, 384, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (6): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (8): Conv2d(384, 768, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (11): Conv2d(768, 64, kernel_size=(6, 6), stride=(1, 1), bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, ngpu, dim_z):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        nc = 3  # Number of input channels for the 96x96x3 image\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 96 x 96\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size: (ndf) x 48 x 48\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size: (ndf*2) x 24 x 24\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size: (ndf*4) x 12 x 12\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size: (ndf*8) x 6 x 6\n",
    "            nn.Conv2d(ndf * 8, dim_z, 6, 1, 0, bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        z = self.main(input)\n",
    "        return z\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate the encoder\n",
    "netD = Encoder(ngpu=0, dim_z=64).to(device)\n",
    "\n",
    "# Handle multi-GPU\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    netD = nn.DataParallel(netD, list(range(ngpu)))\n",
    "\n",
    "# Randomly initialize all weights\n",
    "netD.apply(weights_init)\n",
    "\n",
    "# Print the model\n",
    "print(netD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ec28a7-86d3-476e-b08f-6ca47ae3118a",
   "metadata": {},
   "source": [
    "# Decoder (Generator) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "918d2451-71fc-42cf-bae2-190e7a96b8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder(\n",
      "  (main): Sequential(\n",
      "    (0): ConvTranspose2d(64, 768, kernel_size=(6, 6), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): ConvTranspose2d(768, 384, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): ConvTranspose2d(384, 192, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (7): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): ConvTranspose2d(192, 96, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (10): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): ConvTranspose2d(96, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (13): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Generator Code\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, ngpu, dim_z):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z 64x1x1, going into a convolution\n",
    "            nn.ConvTranspose2d( dim_z, ngf * 8, 6, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size: (ndf*8) x 6 x 6\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size: (ndf*4) x 12 x 12\n",
    "            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size: (ndf*2) x 24 x 24\n",
    "            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size: (ndf) x 48 x 48\n",
    "            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # input is (nc) x 96 x 96\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "\n",
    "# Instantiate the decoder\n",
    "netG = Decoder(ngpu=0, dim_z=64).to(device)\n",
    "\n",
    "# Handle multi-GPU\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    netG = nn.DataParallel(netG, list(range(ngpu)))\n",
    "\n",
    "# Randomly initialize all weights\n",
    "netG.apply(weights_init)\n",
    "\n",
    "# Print the model\n",
    "print(netG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7914c706-f792-41fa-916f-cc348ae7029b",
   "metadata": {},
   "source": [
    "# Criterion / Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b054e06-ad10-443c-9b63-ecfd5747ffd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5d171d-1372-48a4-8424-76c50b58f12c",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02d1ed52-80d3-4fd7-a0b2-e328948d4b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(netD.parameters()) + list(netG.parameters())\n",
    "\n",
    "optimizer = torch.optim.Adam(params, lr=lr, weight_decay=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f01e6b-2c72-436f-a206-2d9a63c95912",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54113c88-1c89-4463-8a1c-98e96bb56935",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Loop...\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.38199812173843384\n",
      "iteration 1 current loss: 0.3522549867630005\n",
      "iteration 2 current loss: 0.3098854422569275\n",
      "iteration 3 current loss: 0.28222528100013733\n",
      "iteration 4 current loss: 0.27860334515571594\n",
      "iteration 5 current loss: 0.25685155391693115\n",
      "iteration 6 current loss: 0.26178795099258423\n",
      "iteration 7 current loss: 0.25403350591659546\n",
      "iteration 8 current loss: 0.2521161437034607\n",
      "iteration 9 current loss: 0.24196098744869232\n",
      "iteration 10 current loss: 0.25181105732917786\n",
      "iteration 11 current loss: 0.23587338626384735\n",
      "iteration 12 current loss: 0.22920696437358856\n",
      "iteration 13 current loss: 0.23533661663532257\n",
      "iteration 14 current loss: 0.23220700025558472\n",
      "iteration 15 current loss: 0.22536370158195496\n",
      "iteration 16 current loss: 0.2169862985610962\n",
      "iteration 17 current loss: 0.2127988636493683\n",
      "iteration 18 current loss: 0.20925939083099365\n",
      "iteration 19 current loss: 0.1953972429037094\n",
      "iteration 20 current loss: 0.1972280740737915\n",
      "iteration 21 current loss: 0.20108182728290558\n",
      "iteration 22 current loss: 0.18368063867092133\n",
      "iteration 23 current loss: 0.18951384723186493\n",
      "iteration 24 current loss: 0.19211670756340027\n",
      "iteration 25 current loss: 0.1881575882434845\n",
      "iteration 26 current loss: 0.191395565867424\n",
      "iteration 27 current loss: 0.1783970147371292\n",
      "iteration 28 current loss: 0.17578446865081787\n",
      "iteration 29 current loss: 0.1797192245721817\n",
      "iteration 30 current loss: 0.18115168809890747\n",
      "iteration 31 current loss: 0.16788330674171448\n",
      "iteration 32 current loss: 0.17537686228752136\n",
      "iteration 33 current loss: 0.16766980290412903\n",
      "iteration 34 current loss: 0.17895497381687164\n",
      "iteration 35 current loss: 0.16107940673828125\n",
      "iteration 36 current loss: 0.15838557481765747\n",
      "iteration 37 current loss: 0.16380426287651062\n",
      "iteration 38 current loss: 0.1779898852109909\n",
      "iteration 39 current loss: 0.16137726604938507\n",
      "iteration 40 current loss: 0.1548614501953125\n",
      "iteration 41 current loss: 0.15094736218452454\n",
      "iteration 42 current loss: 0.15413376688957214\n",
      "iteration 43 current loss: 0.15034739673137665\n",
      "iteration 44 current loss: 0.15720058977603912\n",
      "iteration 45 current loss: 0.1569182425737381\n",
      "iteration 46 current loss: 0.1479460746049881\n",
      "iteration 47 current loss: 0.1526343673467636\n",
      "iteration 48 current loss: 0.1486300379037857\n",
      "iteration 49 current loss: 0.1455303281545639\n",
      "iteration 50 current loss: 0.14640486240386963\n",
      "iteration 51 current loss: 0.1479494273662567\n",
      "iteration 52 current loss: 0.14987638592720032\n",
      "iteration 53 current loss: 0.14076873660087585\n",
      "iteration 54 current loss: 0.1467185914516449\n",
      "iteration 55 current loss: 0.14513170719146729\n",
      "iteration 56 current loss: 0.14586398005485535\n",
      "iteration 57 current loss: 0.13954629004001617\n",
      "iteration 58 current loss: 0.1339789628982544\n",
      "iteration 59 current loss: 0.13379746675491333\n",
      "iteration 60 current loss: 0.13218653202056885\n",
      "iteration 61 current loss: 0.14375466108322144\n",
      "iteration 62 current loss: 0.1352836638689041\n",
      "iteration 63 current loss: 0.1388196051120758\n",
      "iteration 64 current loss: 0.13290764391422272\n",
      "iteration 65 current loss: 0.136623814702034\n",
      "iteration 66 current loss: 0.13863027095794678\n",
      "iteration 67 current loss: 0.13037893176078796\n",
      "iteration 68 current loss: 0.13807816803455353\n",
      "iteration 69 current loss: 0.1373741328716278\n",
      "iteration 70 current loss: 0.13369503617286682\n",
      "iteration 71 current loss: 0.134465754032135\n",
      "iteration 72 current loss: 0.12935924530029297\n",
      "iteration 73 current loss: 0.13050530850887299\n",
      "iteration 74 current loss: 0.13310258090496063\n",
      "iteration 75 current loss: 0.12520265579223633\n",
      "iteration 76 current loss: 0.13327500224113464\n",
      "iteration 77 current loss: 0.13188645243644714\n",
      "iteration 78 current loss: 0.1274937242269516\n",
      "iteration 79 current loss: 0.12897054851055145\n",
      "iteration 80 current loss: 0.12961770594120026\n",
      "iteration 81 current loss: 0.1305556297302246\n",
      "iteration 82 current loss: 0.13205593824386597\n",
      "iteration 83 current loss: 0.1269163191318512\n",
      "iteration 84 current loss: 0.12873941659927368\n",
      "iteration 85 current loss: 0.12205217778682709\n",
      "iteration 86 current loss: 0.12413789331912994\n",
      "iteration 87 current loss: 0.12212900817394257\n",
      "iteration 88 current loss: 0.12598180770874023\n",
      "iteration 89 current loss: 0.12410460412502289\n",
      "iteration 90 current loss: 0.12500496208667755\n",
      "iteration 91 current loss: 0.12065599858760834\n",
      "iteration 92 current loss: 0.12517733871936798\n",
      "iteration 93 current loss: 0.11728604882955551\n",
      "iteration 94 current loss: 0.11531762778759003\n",
      "iteration 95 current loss: 0.11336831003427505\n",
      "iteration 96 current loss: 0.11674924939870834\n",
      "iteration 97 current loss: 0.12119463086128235\n",
      "iteration 98 current loss: 0.11250413209199905\n",
      "iteration 99 current loss: 0.12063070386648178\n",
      "iteration 100 current loss: 0.11595679819583893\n",
      "iteration 101 current loss: 0.11276359856128693\n",
      "iteration 102 current loss: 0.11214551329612732\n",
      "iteration 103 current loss: 0.11326203495264053\n",
      "iteration 104 current loss: 0.11973647773265839\n",
      "iteration 105 current loss: 0.11357946693897247\n",
      "iteration 106 current loss: 0.1174820065498352\n",
      "iteration 107 current loss: 0.11429735273122787\n",
      "iteration 108 current loss: 0.11704441159963608\n",
      "iteration 109 current loss: 0.11794014275074005\n",
      "iteration 110 current loss: 0.11414209753274918\n",
      "iteration 111 current loss: 0.11999697238206863\n",
      "iteration 112 current loss: 0.11988089978694916\n",
      "iteration 113 current loss: 0.11101359128952026\n",
      "iteration 114 current loss: 0.1091647744178772\n",
      "iteration 115 current loss: 0.1097981259226799\n",
      "iteration 116 current loss: 0.11403774470090866\n",
      "iteration 117 current loss: 0.11782923340797424\n",
      "iteration 118 current loss: 0.10378388315439224\n",
      "iteration 119 current loss: 0.10372062772512436\n",
      "iteration 120 current loss: 0.11123086512088776\n",
      "iteration 121 current loss: 0.1082763522863388\n",
      "iteration 122 current loss: 0.11016678065061569\n",
      "iteration 123 current loss: 0.10453107953071594\n",
      "iteration 124 current loss: 0.10856207460165024\n",
      "iteration 125 current loss: 0.10358089953660965\n",
      "iteration 126 current loss: 0.11069359630346298\n",
      "iteration 127 current loss: 0.11047854274511337\n",
      "iteration 128 current loss: 0.11481525003910065\n",
      "iteration 129 current loss: 0.10416489094495773\n",
      "iteration 130 current loss: 0.10624191910028458\n",
      "iteration 131 current loss: 0.11141461133956909\n",
      "iteration 132 current loss: 0.10546232759952545\n",
      "iteration 133 current loss: 0.113577701151371\n",
      "iteration 134 current loss: 0.10881443321704865\n",
      "iteration 135 current loss: 0.10803667455911636\n",
      "iteration 136 current loss: 0.10337306559085846\n",
      "iteration 137 current loss: 0.10501743108034134\n",
      "iteration 138 current loss: 0.10355004668235779\n",
      "iteration 139 current loss: 0.10931342095136642\n",
      "iteration 140 current loss: 0.10312459617853165\n",
      "iteration 141 current loss: 0.10060422867536545\n",
      "iteration 142 current loss: 0.11050122231245041\n",
      "iteration 143 current loss: 0.10597323626279831\n",
      "iteration 144 current loss: 0.10731782019138336\n",
      "iteration 145 current loss: 0.10547824949026108\n",
      "iteration 146 current loss: 0.10809067636728287\n",
      "iteration 147 current loss: 0.1097525879740715\n",
      "iteration 148 current loss: 0.10594548285007477\n",
      "iteration 149 current loss: 0.10158515721559525\n",
      "iteration 150 current loss: 0.10393919795751572\n",
      "iteration 151 current loss: 0.10869882255792618\n",
      "iteration 152 current loss: 0.1046995222568512\n",
      "iteration 153 current loss: 0.10438250005245209\n",
      "iteration 154 current loss: 0.10844086110591888\n",
      "iteration 155 current loss: 0.0994410589337349\n",
      "iteration 156 current loss: 0.09899705648422241\n",
      "iteration 157 current loss: 0.10684537887573242\n",
      "iteration 158 current loss: 0.10543777048587799\n",
      "iteration 159 current loss: 0.09812738001346588\n",
      "iteration 160 current loss: 0.1032780185341835\n",
      "iteration 161 current loss: 0.10014539957046509\n",
      "iteration 162 current loss: 0.10691173374652863\n",
      "iteration 163 current loss: 0.11026015877723694\n",
      "iteration 164 current loss: 0.10494822263717651\n",
      "iteration 165 current loss: 0.10444517433643341\n",
      "iteration 166 current loss: 0.10063537210226059\n",
      "iteration 167 current loss: 0.10738086700439453\n",
      "iteration 168 current loss: 0.10254757851362228\n",
      "iteration 169 current loss: 0.10346341878175735\n",
      "iteration 170 current loss: 0.09922929108142853\n",
      "iteration 171 current loss: 0.10332473367452621\n",
      "iteration 172 current loss: 0.09856586903333664\n",
      "iteration 173 current loss: 0.10009237378835678\n",
      "iteration 174 current loss: 0.10187576711177826\n",
      "iteration 175 current loss: 0.10095944255590439\n",
      "iteration 176 current loss: 0.09783448278903961\n",
      "iteration 177 current loss: 0.10340861231088638\n",
      "iteration 178 current loss: 0.10370277613401413\n",
      "iteration 179 current loss: 0.09897075593471527\n",
      "iteration 180 current loss: 0.09726700186729431\n",
      "iteration 181 current loss: 0.10210805386304855\n",
      "iteration 182 current loss: 0.10111870616674423\n",
      "iteration 183 current loss: 0.10278292000293732\n",
      "iteration 184 current loss: 0.09956388175487518\n",
      "iteration 185 current loss: 0.10038227587938309\n",
      "iteration 186 current loss: 0.09768623858690262\n",
      "iteration 187 current loss: 0.10011489689350128\n",
      "iteration 188 current loss: 0.10050228238105774\n",
      "iteration 189 current loss: 0.09538747370243073\n",
      "iteration 190 current loss: 0.0998699739575386\n",
      "iteration 191 current loss: 0.09477118402719498\n",
      "iteration 192 current loss: 0.09642117470502853\n",
      "iteration 193 current loss: 0.09830711781978607\n",
      "iteration 194 current loss: 0.10057000070810318\n",
      "iteration 195 current loss: 0.09409689158201218\n",
      "iteration 196 current loss: 0.09712737053632736\n",
      "iteration 197 current loss: 0.09497354924678802\n",
      "iteration 198 current loss: 0.10226774960756302\n",
      "iteration 199 current loss: 0.09615545719861984\n",
      "iteration 200 current loss: 0.091628298163414\n",
      "iteration 201 current loss: 0.09686846286058426\n",
      "iteration 202 current loss: 0.09629648923873901\n",
      "iteration 203 current loss: 0.09518660604953766\n",
      "iteration 204 current loss: 0.09660671651363373\n",
      "iteration 205 current loss: 0.09527773410081863\n",
      "iteration 206 current loss: 0.09623593091964722\n",
      "iteration 207 current loss: 0.09654887765645981\n",
      "iteration 208 current loss: 0.08992794156074524\n",
      "iteration 209 current loss: 0.09489546716213226\n",
      "iteration 210 current loss: 0.09507442265748978\n",
      "iteration 211 current loss: 0.09819532185792923\n",
      "iteration 212 current loss: 0.08952231705188751\n",
      "iteration 213 current loss: 0.09475113451480865\n",
      "iteration 214 current loss: 0.09427113085985184\n",
      "iteration 215 current loss: 0.10040856897830963\n",
      "iteration 216 current loss: 0.09506300091743469\n",
      "iteration 217 current loss: 0.09413378685712814\n",
      "iteration 218 current loss: 0.09439253062009811\n",
      "iteration 219 current loss: 0.09557358175516129\n",
      "iteration 220 current loss: 0.09446574747562408\n",
      "iteration 221 current loss: 0.08990266919136047\n",
      "iteration 222 current loss: 0.09247952699661255\n",
      "iteration 223 current loss: 0.09327203035354614\n",
      "iteration 224 current loss: 0.09449918568134308\n",
      "iteration 225 current loss: 0.09027966111898422\n",
      "iteration 226 current loss: 0.09642376005649567\n",
      "iteration 227 current loss: 0.09944033622741699\n",
      "iteration 228 current loss: 0.09520106762647629\n",
      "iteration 229 current loss: 0.09197573363780975\n",
      "iteration 230 current loss: 0.09197214990854263\n",
      "iteration 231 current loss: 0.09032745659351349\n",
      "iteration 232 current loss: 0.08813778311014175\n",
      "iteration 233 current loss: 0.0845566838979721\n",
      "iteration 234 current loss: 0.09199462085962296\n",
      "iteration 235 current loss: 0.0892818346619606\n",
      "iteration 236 current loss: 0.09089483320713043\n",
      "iteration 237 current loss: 0.09212879836559296\n",
      "iteration 238 current loss: 0.0958222821354866\n",
      "iteration 239 current loss: 0.0918208435177803\n",
      "iteration 240 current loss: 0.09691397100687027\n",
      "iteration 241 current loss: 0.0911099836230278\n",
      "iteration 242 current loss: 0.08812160789966583\n",
      "iteration 243 current loss: 0.09276852756738663\n",
      "iteration 244 current loss: 0.09196184575557709\n",
      "iteration 245 current loss: 0.08839301764965057\n",
      "iteration 246 current loss: 0.09214278310537338\n",
      "iteration 247 current loss: 0.0905143991112709\n",
      "iteration 248 current loss: 0.08569465577602386\n",
      "iteration 249 current loss: 0.08878268301486969\n",
      "iteration 250 current loss: 0.09518056362867355\n",
      "iteration 251 current loss: 0.09378495812416077\n",
      "iteration 252 current loss: 0.08865080028772354\n",
      "iteration 253 current loss: 0.09326644986867905\n",
      "iteration 254 current loss: 0.08728969842195511\n",
      "iteration 255 current loss: 0.09295016527175903\n",
      "iteration 256 current loss: 0.08721475303173065\n",
      "iteration 257 current loss: 0.08955036848783493\n",
      "iteration 258 current loss: 0.08839961141347885\n",
      "iteration 259 current loss: 0.0871923565864563\n",
      "iteration 260 current loss: 0.0900690108537674\n",
      "iteration 261 current loss: 0.08874792605638504\n",
      "iteration 262 current loss: 0.09445226192474365\n",
      "iteration 263 current loss: 0.09067478775978088\n",
      "iteration 264 current loss: 0.09341510385274887\n",
      "iteration 265 current loss: 0.09092945605516434\n",
      "iteration 266 current loss: 0.0916169136762619\n",
      "iteration 267 current loss: 0.08606614172458649\n",
      "iteration 268 current loss: 0.08917414397001266\n",
      "iteration 269 current loss: 0.08989571034908295\n",
      "iteration 270 current loss: 0.0874686986207962\n",
      "iteration 271 current loss: 0.08771632611751556\n",
      "iteration 272 current loss: 0.08890854567289352\n",
      "iteration 273 current loss: 0.08875041455030441\n",
      "iteration 274 current loss: 0.08824696391820908\n",
      "iteration 275 current loss: 0.0910220742225647\n",
      "iteration 276 current loss: 0.09043378382921219\n",
      "iteration 277 current loss: 0.08690635114908218\n",
      "iteration 278 current loss: 0.08884185552597046\n",
      "iteration 279 current loss: 0.08745851367712021\n",
      "iteration 280 current loss: 0.0865921825170517\n",
      "iteration 281 current loss: 0.08535974472761154\n",
      "iteration 282 current loss: 0.09252700954675674\n",
      "iteration 283 current loss: 0.08547885715961456\n",
      "iteration 284 current loss: 0.09004921466112137\n",
      "iteration 285 current loss: 0.08730749785900116\n",
      "iteration 286 current loss: 0.08719075471162796\n",
      "iteration 287 current loss: 0.08936170488595963\n",
      "iteration 288 current loss: 0.09044189751148224\n",
      "iteration 289 current loss: 0.08671942353248596\n",
      "iteration 290 current loss: 0.08752261102199554\n",
      "iteration 291 current loss: 0.08652644604444504\n",
      "iteration 292 current loss: 0.08464150130748749\n",
      "iteration 293 current loss: 0.08621025830507278\n",
      "iteration 294 current loss: 0.08544595539569855\n",
      "iteration 295 current loss: 0.0836048498749733\n",
      "iteration 296 current loss: 0.08488134294748306\n",
      "iteration 297 current loss: 0.08287873864173889\n",
      "iteration 298 current loss: 0.08182096481323242\n",
      "iteration 299 current loss: 0.08443593233823776\n",
      "iteration 300 current loss: 0.08307576924562454\n",
      "iteration 301 current loss: 0.08523474633693695\n",
      "iteration 302 current loss: 0.08458980172872543\n",
      "iteration 303 current loss: 0.08225724846124649\n",
      "iteration 304 current loss: 0.086407370865345\n",
      "iteration 305 current loss: 0.08439213037490845\n",
      "iteration 306 current loss: 0.08467105776071548\n",
      "iteration 307 current loss: 0.07973571866750717\n",
      "iteration 308 current loss: 0.08028538525104523\n",
      "iteration 309 current loss: 0.0810118019580841\n",
      "iteration 310 current loss: 0.07840204238891602\n",
      "iteration 311 current loss: 0.08663629740476608\n",
      "iteration 312 current loss: 0.08308853954076767\n",
      "iteration 313 current loss: 0.07946759462356567\n",
      "iteration 314 current loss: 0.0771702229976654\n",
      "iteration 315 current loss: 0.07732751965522766\n",
      "iteration 316 current loss: 0.08109209686517715\n",
      "iteration 317 current loss: 0.0756068080663681\n",
      "iteration 318 current loss: 0.07960374653339386\n",
      "iteration 319 current loss: 0.08190267533063889\n",
      "iteration 320 current loss: 0.08199568092823029\n",
      "iteration 321 current loss: 0.07981309294700623\n",
      "iteration 322 current loss: 0.07746042311191559\n",
      "iteration 323 current loss: 0.08275371044874191\n",
      "iteration 324 current loss: 0.08074069023132324\n",
      "iteration 325 current loss: 0.07763848453760147\n",
      "iteration 326 current loss: 0.07548083364963531\n",
      "iteration 327 current loss: 0.07594193518161774\n",
      "iteration 328 current loss: 0.07643776386976242\n",
      "iteration 329 current loss: 0.07735496014356613\n",
      "iteration 330 current loss: 0.0747918039560318\n",
      "iteration 331 current loss: 0.08098962903022766\n",
      "iteration 332 current loss: 0.07767440378665924\n",
      "iteration 333 current loss: 0.07530441135168076\n",
      "iteration 334 current loss: 0.07709696888923645\n",
      "iteration 335 current loss: 0.07993100583553314\n",
      "iteration 336 current loss: 0.07770823687314987\n",
      "iteration 337 current loss: 0.08115572482347488\n",
      "iteration 338 current loss: 0.07376448065042496\n",
      "iteration 339 current loss: 0.07789845764636993\n",
      "iteration 340 current loss: 0.07995892316102982\n",
      "iteration 341 current loss: 0.07999496161937714\n",
      "iteration 342 current loss: 0.07850348204374313\n",
      "iteration 343 current loss: 0.07979534566402435\n",
      "iteration 344 current loss: 0.07683908194303513\n",
      "iteration 345 current loss: 0.07407177239656448\n",
      "iteration 346 current loss: 0.08239830285310745\n",
      "iteration 347 current loss: 0.0731525719165802\n",
      "iteration 348 current loss: 0.0775064006447792\n",
      "iteration 349 current loss: 0.07640310376882553\n",
      "iteration 350 current loss: 0.07812042534351349\n",
      "iteration 351 current loss: 0.07432852685451508\n",
      "iteration 352 current loss: 0.07347367703914642\n",
      "iteration 353 current loss: 0.07827004045248032\n",
      "iteration 354 current loss: 0.0747753232717514\n",
      "iteration 355 current loss: 0.07134655117988586\n",
      "iteration 356 current loss: 0.0777379497885704\n",
      "iteration 357 current loss: 0.07645644247531891\n",
      "iteration 358 current loss: 0.07471270859241486\n",
      "iteration 359 current loss: 0.07457626610994339\n",
      "iteration 360 current loss: 0.0750923678278923\n",
      "iteration 361 current loss: 0.07607752084732056\n",
      "iteration 362 current loss: 0.07641898095607758\n",
      "iteration 363 current loss: 0.07404834032058716\n",
      "iteration 364 current loss: 0.07210763543844223\n",
      "iteration 365 current loss: 0.0736553743481636\n",
      "iteration 366 current loss: 0.07010982930660248\n",
      "iteration 367 current loss: 0.07454632222652435\n",
      "iteration 368 current loss: 0.07536250352859497\n",
      "iteration 369 current loss: 0.07168623059988022\n",
      "iteration 370 current loss: 0.07276605069637299\n",
      "iteration 371 current loss: 0.07171066105365753\n",
      "iteration 372 current loss: 0.07267756760120392\n",
      "iteration 373 current loss: 0.07256380468606949\n",
      "iteration 374 current loss: 0.07278703898191452\n",
      "iteration 375 current loss: 0.07240113615989685\n",
      "iteration 376 current loss: 0.0708213821053505\n",
      "iteration 377 current loss: 0.07461611181497574\n",
      "iteration 378 current loss: 0.07608485966920853\n",
      "iteration 379 current loss: 0.07663001120090485\n",
      "iteration 380 current loss: 0.07631171494722366\n",
      "iteration 381 current loss: 0.06898990273475647\n",
      "iteration 382 current loss: 0.07167469710111618\n",
      "iteration 383 current loss: 0.07648573070764542\n",
      "iteration 384 current loss: 0.07750403136014938\n",
      "iteration 385 current loss: 0.0747097060084343\n",
      "iteration 386 current loss: 0.0737328976392746\n",
      "iteration 387 current loss: 0.07324698567390442\n",
      "iteration 388 current loss: 0.07259877771139145\n",
      "iteration 389 current loss: 0.07676253467798233\n",
      "iteration 390 current loss: 0.07285787910223007\n",
      "iteration 391 current loss: 0.07462256401777267\n",
      "iteration 392 current loss: 0.0736713781952858\n",
      "iteration 393 current loss: 0.07304071635007858\n",
      "iteration 394 current loss: 0.06941980123519897\n",
      "iteration 395 current loss: 0.06847940385341644\n",
      "iteration 396 current loss: 0.07429943978786469\n",
      "iteration 397 current loss: 0.0731491968035698\n",
      "iteration 398 current loss: 0.07314854115247726\n",
      "iteration 399 current loss: 0.06933915615081787\n",
      "iteration 400 current loss: 0.06983458250761032\n",
      "iteration 401 current loss: 0.07203365862369537\n",
      "iteration 402 current loss: 0.07058561593294144\n",
      "iteration 403 current loss: 0.07301729917526245\n",
      "iteration 404 current loss: 0.07338029146194458\n",
      "iteration 405 current loss: 0.0738433375954628\n",
      "iteration 406 current loss: 0.073967844247818\n",
      "iteration 407 current loss: 0.07076352834701538\n",
      "iteration 408 current loss: 0.07440434396266937\n",
      "iteration 409 current loss: 0.0745568796992302\n",
      "iteration 410 current loss: 0.07726794481277466\n",
      "\t\tEpoch 0/100 complete. Epoch loss 0.1091566119170827\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 0, Validation Loss: 0.07691907603293657\n",
      "best loss 0.1091566119170827\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.07370656728744507\n",
      "iteration 1 current loss: 0.07854875177145004\n",
      "iteration 2 current loss: 0.0773383229970932\n",
      "iteration 3 current loss: 0.07974972575902939\n",
      "iteration 4 current loss: 0.07597251236438751\n",
      "iteration 5 current loss: 0.07161630690097809\n",
      "iteration 6 current loss: 0.07583300024271011\n",
      "iteration 7 current loss: 0.07398535311222076\n",
      "iteration 8 current loss: 0.07404765486717224\n",
      "iteration 9 current loss: 0.07310616225004196\n",
      "iteration 10 current loss: 0.07225541770458221\n",
      "iteration 11 current loss: 0.07076773792505264\n",
      "iteration 12 current loss: 0.07282543182373047\n",
      "iteration 13 current loss: 0.07298113405704498\n",
      "iteration 14 current loss: 0.07296112179756165\n",
      "iteration 15 current loss: 0.07560192793607712\n",
      "iteration 16 current loss: 0.06772065907716751\n",
      "iteration 17 current loss: 0.07064099609851837\n",
      "iteration 18 current loss: 0.06826043874025345\n",
      "iteration 19 current loss: 0.07334204763174057\n",
      "iteration 20 current loss: 0.07102220505475998\n",
      "iteration 21 current loss: 0.07221879065036774\n",
      "iteration 22 current loss: 0.07376845180988312\n",
      "iteration 23 current loss: 0.06935499608516693\n",
      "iteration 24 current loss: 0.0684313178062439\n",
      "iteration 25 current loss: 0.06996168941259384\n",
      "iteration 26 current loss: 0.0710224062204361\n",
      "iteration 27 current loss: 0.06834208220243454\n",
      "iteration 28 current loss: 0.07108863443136215\n",
      "iteration 29 current loss: 0.06906349956989288\n",
      "iteration 30 current loss: 0.07134956866502762\n",
      "iteration 31 current loss: 0.07149359583854675\n",
      "iteration 32 current loss: 0.07222048193216324\n",
      "iteration 33 current loss: 0.06584671884775162\n",
      "iteration 34 current loss: 0.07201047986745834\n",
      "iteration 35 current loss: 0.06943336874246597\n",
      "iteration 36 current loss: 0.06933978945016861\n",
      "iteration 37 current loss: 0.06443371623754501\n",
      "iteration 38 current loss: 0.07112017273902893\n",
      "iteration 39 current loss: 0.06415971368551254\n",
      "iteration 40 current loss: 0.07187655568122864\n",
      "iteration 41 current loss: 0.07149446755647659\n",
      "iteration 42 current loss: 0.06955794990062714\n",
      "iteration 43 current loss: 0.06987768411636353\n",
      "iteration 44 current loss: 0.06798766553401947\n",
      "iteration 45 current loss: 0.06837740540504456\n",
      "iteration 46 current loss: 0.0679662674665451\n",
      "iteration 47 current loss: 0.07320763170719147\n",
      "iteration 48 current loss: 0.06827329844236374\n",
      "iteration 49 current loss: 0.06680121272802353\n",
      "iteration 50 current loss: 0.06805653870105743\n",
      "iteration 51 current loss: 0.06833501160144806\n",
      "iteration 52 current loss: 0.06728339195251465\n",
      "iteration 53 current loss: 0.06912115961313248\n",
      "iteration 54 current loss: 0.07011788338422775\n",
      "iteration 55 current loss: 0.06932897120714188\n",
      "iteration 56 current loss: 0.0671880841255188\n",
      "iteration 57 current loss: 0.06627769768238068\n",
      "iteration 58 current loss: 0.06680436432361603\n",
      "iteration 59 current loss: 0.0690702572464943\n",
      "iteration 60 current loss: 0.07001455873250961\n",
      "iteration 61 current loss: 0.06373573839664459\n",
      "iteration 62 current loss: 0.06657364219427109\n",
      "iteration 63 current loss: 0.06813862919807434\n",
      "iteration 64 current loss: 0.07247008383274078\n",
      "iteration 65 current loss: 0.06653105467557907\n",
      "iteration 66 current loss: 0.06471279263496399\n",
      "iteration 67 current loss: 0.06473124772310257\n",
      "iteration 68 current loss: 0.06862687319517136\n",
      "iteration 69 current loss: 0.0660935565829277\n",
      "iteration 70 current loss: 0.06696411222219467\n",
      "iteration 71 current loss: 0.06898374110460281\n",
      "iteration 72 current loss: 0.0684046819806099\n",
      "iteration 73 current loss: 0.06527777761220932\n",
      "iteration 74 current loss: 0.0639188140630722\n",
      "iteration 75 current loss: 0.06709567457437515\n",
      "iteration 76 current loss: 0.0703476145863533\n",
      "iteration 77 current loss: 0.06534428149461746\n",
      "iteration 78 current loss: 0.06454881280660629\n",
      "iteration 79 current loss: 0.07130447775125504\n",
      "iteration 80 current loss: 0.06931577622890472\n",
      "iteration 81 current loss: 0.0654616579413414\n",
      "iteration 82 current loss: 0.07020048052072525\n",
      "iteration 83 current loss: 0.06813464313745499\n",
      "iteration 84 current loss: 0.06687066704034805\n",
      "iteration 85 current loss: 0.06709075719118118\n",
      "iteration 86 current loss: 0.06577928364276886\n",
      "iteration 87 current loss: 0.06618752330541611\n",
      "iteration 88 current loss: 0.06621196866035461\n",
      "iteration 89 current loss: 0.06679931282997131\n",
      "iteration 90 current loss: 0.06975172460079193\n",
      "iteration 91 current loss: 0.06713108718395233\n",
      "iteration 92 current loss: 0.06721227616071701\n",
      "iteration 93 current loss: 0.07049234956502914\n",
      "iteration 94 current loss: 0.06624539196491241\n",
      "iteration 95 current loss: 0.06641187518835068\n",
      "iteration 96 current loss: 0.06879566609859467\n",
      "iteration 97 current loss: 0.06639181822538376\n",
      "iteration 98 current loss: 0.0636584684252739\n",
      "iteration 99 current loss: 0.0669928714632988\n",
      "iteration 100 current loss: 0.06604796648025513\n",
      "iteration 101 current loss: 0.06545207649469376\n",
      "iteration 102 current loss: 0.06849246472120285\n",
      "iteration 103 current loss: 0.06399135291576385\n",
      "iteration 104 current loss: 0.0657694861292839\n",
      "iteration 105 current loss: 0.0688864216208458\n",
      "iteration 106 current loss: 0.0664198026061058\n",
      "iteration 107 current loss: 0.06809777021408081\n",
      "iteration 108 current loss: 0.06693152338266373\n",
      "iteration 109 current loss: 0.06459790468215942\n",
      "iteration 110 current loss: 0.06547059118747711\n",
      "iteration 111 current loss: 0.06548766046762466\n",
      "iteration 112 current loss: 0.07007934153079987\n",
      "iteration 113 current loss: 0.06679091602563858\n",
      "iteration 114 current loss: 0.06423433870077133\n",
      "iteration 115 current loss: 0.06450636684894562\n",
      "iteration 116 current loss: 0.07059893012046814\n",
      "iteration 117 current loss: 0.06764208525419235\n",
      "iteration 118 current loss: 0.06770739704370499\n",
      "iteration 119 current loss: 0.07240692526102066\n",
      "iteration 120 current loss: 0.06493432819843292\n",
      "iteration 121 current loss: 0.06469856947660446\n",
      "iteration 122 current loss: 0.06505395472049713\n",
      "iteration 123 current loss: 0.06471091508865356\n",
      "iteration 124 current loss: 0.06760617345571518\n",
      "iteration 125 current loss: 0.06561043858528137\n",
      "iteration 126 current loss: 0.06461001187562943\n",
      "iteration 127 current loss: 0.06880144029855728\n",
      "iteration 128 current loss: 0.06798777729272842\n",
      "iteration 129 current loss: 0.06552281230688095\n",
      "iteration 130 current loss: 0.0649079903960228\n",
      "iteration 131 current loss: 0.07055129110813141\n",
      "iteration 132 current loss: 0.07048030197620392\n",
      "iteration 133 current loss: 0.06707586348056793\n",
      "iteration 134 current loss: 0.0697348415851593\n",
      "iteration 135 current loss: 0.06423266977071762\n",
      "iteration 136 current loss: 0.06461301445960999\n",
      "iteration 137 current loss: 0.06461072713136673\n",
      "iteration 138 current loss: 0.06958137452602386\n",
      "iteration 139 current loss: 0.06486804783344269\n",
      "iteration 140 current loss: 0.06323358416557312\n",
      "iteration 141 current loss: 0.06676065921783447\n",
      "iteration 142 current loss: 0.06522651761770248\n",
      "iteration 143 current loss: 0.0657414123415947\n",
      "iteration 144 current loss: 0.06684549152851105\n",
      "iteration 145 current loss: 0.06363586336374283\n",
      "iteration 146 current loss: 0.06160511448979378\n",
      "iteration 147 current loss: 0.06651177257299423\n",
      "iteration 148 current loss: 0.06590215116739273\n",
      "iteration 149 current loss: 0.0666688084602356\n",
      "iteration 150 current loss: 0.0662645623087883\n",
      "iteration 151 current loss: 0.06845451891422272\n",
      "iteration 152 current loss: 0.07007262855768204\n",
      "iteration 153 current loss: 0.06461313366889954\n",
      "iteration 154 current loss: 0.06268144398927689\n",
      "iteration 155 current loss: 0.06754189729690552\n",
      "iteration 156 current loss: 0.06615716964006424\n",
      "iteration 157 current loss: 0.0649518072605133\n",
      "iteration 158 current loss: 0.06669474393129349\n",
      "iteration 159 current loss: 0.06461869180202484\n",
      "iteration 160 current loss: 0.06622898578643799\n",
      "iteration 161 current loss: 0.06646531820297241\n",
      "iteration 162 current loss: 0.06549827754497528\n",
      "iteration 163 current loss: 0.06533387303352356\n",
      "iteration 164 current loss: 0.0663476511836052\n",
      "iteration 165 current loss: 0.06745748966932297\n",
      "iteration 166 current loss: 0.06625095754861832\n",
      "iteration 167 current loss: 0.06528588384389877\n",
      "iteration 168 current loss: 0.062267228960990906\n",
      "iteration 169 current loss: 0.06301300972700119\n",
      "iteration 170 current loss: 0.06222304701805115\n",
      "iteration 171 current loss: 0.0657120943069458\n",
      "iteration 172 current loss: 0.06618957966566086\n",
      "iteration 173 current loss: 0.06468887627124786\n",
      "iteration 174 current loss: 0.06378994882106781\n",
      "iteration 175 current loss: 0.06556817889213562\n",
      "iteration 176 current loss: 0.0609767809510231\n",
      "iteration 177 current loss: 0.06465505063533783\n",
      "iteration 178 current loss: 0.06355958431959152\n",
      "iteration 179 current loss: 0.06380319595336914\n",
      "iteration 180 current loss: 0.0643896535038948\n",
      "iteration 181 current loss: 0.06347374618053436\n",
      "iteration 182 current loss: 0.06284698843955994\n",
      "iteration 183 current loss: 0.061957623809576035\n",
      "iteration 184 current loss: 0.06115126609802246\n",
      "iteration 185 current loss: 0.06337634474039078\n",
      "iteration 186 current loss: 0.06318832188844681\n",
      "iteration 187 current loss: 0.06573972851037979\n",
      "iteration 188 current loss: 0.06594272702932358\n",
      "iteration 189 current loss: 0.06600002199411392\n",
      "iteration 190 current loss: 0.06622881442308426\n",
      "iteration 191 current loss: 0.06345190107822418\n",
      "iteration 192 current loss: 0.0648704469203949\n",
      "iteration 193 current loss: 0.06264316290616989\n",
      "iteration 194 current loss: 0.06437961012125015\n",
      "iteration 195 current loss: 0.06401604413986206\n",
      "iteration 196 current loss: 0.06551940739154816\n",
      "iteration 197 current loss: 0.06602828949689865\n",
      "iteration 198 current loss: 0.06627795845270157\n",
      "iteration 199 current loss: 0.06818955391645432\n",
      "iteration 200 current loss: 0.06621889770030975\n",
      "iteration 201 current loss: 0.06865917146205902\n",
      "iteration 202 current loss: 0.06618836522102356\n",
      "iteration 203 current loss: 0.0673229992389679\n",
      "iteration 204 current loss: 0.06313585489988327\n",
      "iteration 205 current loss: 0.06649170070886612\n",
      "iteration 206 current loss: 0.06313081085681915\n",
      "iteration 207 current loss: 0.0650913342833519\n",
      "iteration 208 current loss: 0.059786371886730194\n",
      "iteration 209 current loss: 0.06360574066638947\n",
      "iteration 210 current loss: 0.0648128092288971\n",
      "iteration 211 current loss: 0.06501507014036179\n",
      "iteration 212 current loss: 0.06253869831562042\n",
      "iteration 213 current loss: 0.06795002520084381\n",
      "iteration 214 current loss: 0.06770263612270355\n",
      "iteration 215 current loss: 0.06134125217795372\n",
      "iteration 216 current loss: 0.06556902825832367\n",
      "iteration 217 current loss: 0.06652649492025375\n",
      "iteration 218 current loss: 0.06368077546358109\n",
      "iteration 219 current loss: 0.06062350422143936\n",
      "iteration 220 current loss: 0.06404541432857513\n",
      "iteration 221 current loss: 0.06456492841243744\n",
      "iteration 222 current loss: 0.06742103397846222\n",
      "iteration 223 current loss: 0.06523673981428146\n",
      "iteration 224 current loss: 0.06581627577543259\n",
      "iteration 225 current loss: 0.06818971037864685\n",
      "iteration 226 current loss: 0.06505052745342255\n",
      "iteration 227 current loss: 0.06569726020097733\n",
      "iteration 228 current loss: 0.06677912920713425\n",
      "iteration 229 current loss: 0.06610115617513657\n",
      "iteration 230 current loss: 0.06827309727668762\n",
      "iteration 231 current loss: 0.06613985449075699\n",
      "iteration 232 current loss: 0.06615886092185974\n",
      "iteration 233 current loss: 0.06523317098617554\n",
      "iteration 234 current loss: 0.06692564487457275\n",
      "iteration 235 current loss: 0.06348805874586105\n",
      "iteration 236 current loss: 0.06425729393959045\n",
      "iteration 237 current loss: 0.06365706026554108\n",
      "iteration 238 current loss: 0.06672156602144241\n",
      "iteration 239 current loss: 0.06214570999145508\n",
      "iteration 240 current loss: 0.06372411549091339\n",
      "iteration 241 current loss: 0.07044189423322678\n",
      "iteration 242 current loss: 0.06771443784236908\n",
      "iteration 243 current loss: 0.062066495418548584\n",
      "iteration 244 current loss: 0.0662650465965271\n",
      "iteration 245 current loss: 0.06360242515802383\n",
      "iteration 246 current loss: 0.06346889585256577\n",
      "iteration 247 current loss: 0.06433753669261932\n",
      "iteration 248 current loss: 0.06218140572309494\n",
      "iteration 249 current loss: 0.06478507816791534\n",
      "iteration 250 current loss: 0.06701692938804626\n",
      "iteration 251 current loss: 0.06798501312732697\n",
      "iteration 252 current loss: 0.06707071512937546\n",
      "iteration 253 current loss: 0.06515704840421677\n",
      "iteration 254 current loss: 0.06354092806577682\n",
      "iteration 255 current loss: 0.06319188326597214\n",
      "iteration 256 current loss: 0.064570851624012\n",
      "iteration 257 current loss: 0.06501690298318863\n",
      "iteration 258 current loss: 0.06514297425746918\n",
      "iteration 259 current loss: 0.06453658640384674\n",
      "iteration 260 current loss: 0.06674598902463913\n",
      "iteration 261 current loss: 0.06625709682703018\n",
      "iteration 262 current loss: 0.06083657592535019\n",
      "iteration 263 current loss: 0.0637514516711235\n",
      "iteration 264 current loss: 0.06296979635953903\n",
      "iteration 265 current loss: 0.06579668074846268\n",
      "iteration 266 current loss: 0.06387268006801605\n",
      "iteration 267 current loss: 0.06147764250636101\n",
      "iteration 268 current loss: 0.06885045021772385\n",
      "iteration 269 current loss: 0.06449249386787415\n",
      "iteration 270 current loss: 0.06047024577856064\n",
      "iteration 271 current loss: 0.06359899789094925\n",
      "iteration 272 current loss: 0.06333895772695541\n",
      "iteration 273 current loss: 0.06665937602519989\n",
      "iteration 274 current loss: 0.062454912811517715\n",
      "iteration 275 current loss: 0.06668964773416519\n",
      "iteration 276 current loss: 0.06428640335798264\n",
      "iteration 277 current loss: 0.06540850549936295\n",
      "iteration 278 current loss: 0.06354287266731262\n",
      "iteration 279 current loss: 0.06386368721723557\n",
      "iteration 280 current loss: 0.06672392040491104\n",
      "iteration 281 current loss: 0.0602731890976429\n",
      "iteration 282 current loss: 0.06414319574832916\n",
      "iteration 283 current loss: 0.060856156051158905\n",
      "iteration 284 current loss: 0.06363456696271896\n",
      "iteration 285 current loss: 0.06396935880184174\n",
      "iteration 286 current loss: 0.0631989985704422\n",
      "iteration 287 current loss: 0.06867767125368118\n",
      "iteration 288 current loss: 0.06574695557355881\n",
      "iteration 289 current loss: 0.06388214975595474\n",
      "iteration 290 current loss: 0.06352252513170242\n",
      "iteration 291 current loss: 0.06425639241933823\n",
      "iteration 292 current loss: 0.06377824395895004\n",
      "iteration 293 current loss: 0.06423814594745636\n",
      "iteration 294 current loss: 0.06537822633981705\n",
      "iteration 295 current loss: 0.06448356062173843\n",
      "iteration 296 current loss: 0.06338391453027725\n",
      "iteration 297 current loss: 0.06629543751478195\n",
      "iteration 298 current loss: 0.06071142107248306\n",
      "iteration 299 current loss: 0.06533865630626678\n",
      "iteration 300 current loss: 0.06293772906064987\n",
      "iteration 301 current loss: 0.0618777722120285\n",
      "iteration 302 current loss: 0.06566549837589264\n",
      "iteration 303 current loss: 0.06256972998380661\n",
      "iteration 304 current loss: 0.06827359646558762\n",
      "iteration 305 current loss: 0.06466637551784515\n",
      "iteration 306 current loss: 0.06700453907251358\n",
      "iteration 307 current loss: 0.06442528963088989\n",
      "iteration 308 current loss: 0.061944130808115005\n",
      "iteration 309 current loss: 0.06524685770273209\n",
      "iteration 310 current loss: 0.0689602941274643\n",
      "iteration 311 current loss: 0.06217082217335701\n",
      "iteration 312 current loss: 0.06402573734521866\n",
      "iteration 313 current loss: 0.06639709323644638\n",
      "iteration 314 current loss: 0.06590495258569717\n",
      "iteration 315 current loss: 0.06559745222330093\n",
      "iteration 316 current loss: 0.06642593443393707\n",
      "iteration 317 current loss: 0.06715569645166397\n",
      "iteration 318 current loss: 0.06338411569595337\n",
      "iteration 319 current loss: 0.06681951135396957\n",
      "iteration 320 current loss: 0.06486114859580994\n",
      "iteration 321 current loss: 0.06604038178920746\n",
      "iteration 322 current loss: 0.06399592012166977\n",
      "iteration 323 current loss: 0.06360764056444168\n",
      "iteration 324 current loss: 0.06648014485836029\n",
      "iteration 325 current loss: 0.06494088470935822\n",
      "iteration 326 current loss: 0.06138712540268898\n",
      "iteration 327 current loss: 0.062044285237789154\n",
      "iteration 328 current loss: 0.06342356652021408\n",
      "iteration 329 current loss: 0.06395267695188522\n",
      "iteration 330 current loss: 0.06639442592859268\n",
      "iteration 331 current loss: 0.0645347610116005\n",
      "iteration 332 current loss: 0.06280113756656647\n",
      "iteration 333 current loss: 0.06132077798247337\n",
      "iteration 334 current loss: 0.06517818570137024\n",
      "iteration 335 current loss: 0.06018468365073204\n",
      "iteration 336 current loss: 0.06289200484752655\n",
      "iteration 337 current loss: 0.062409065663814545\n",
      "iteration 338 current loss: 0.062389228492975235\n",
      "iteration 339 current loss: 0.06326217204332352\n",
      "iteration 340 current loss: 0.06522319465875626\n",
      "iteration 341 current loss: 0.06579719483852386\n",
      "iteration 342 current loss: 0.06562285125255585\n",
      "iteration 343 current loss: 0.065145343542099\n",
      "iteration 344 current loss: 0.06692687422037125\n",
      "iteration 345 current loss: 0.06667376309633255\n",
      "iteration 346 current loss: 0.06193533167243004\n",
      "iteration 347 current loss: 0.06302958726882935\n",
      "iteration 348 current loss: 0.06704752892255783\n",
      "iteration 349 current loss: 0.06325847655534744\n",
      "iteration 350 current loss: 0.06455571949481964\n",
      "iteration 351 current loss: 0.06780872493982315\n",
      "iteration 352 current loss: 0.06442003697156906\n",
      "iteration 353 current loss: 0.06673194468021393\n",
      "iteration 354 current loss: 0.06216562166810036\n",
      "iteration 355 current loss: 0.06371181458234787\n",
      "iteration 356 current loss: 0.0678628534078598\n",
      "iteration 357 current loss: 0.06432627886533737\n",
      "iteration 358 current loss: 0.06499113887548447\n",
      "iteration 359 current loss: 0.06746333092451096\n",
      "iteration 360 current loss: 0.0681457594037056\n",
      "iteration 361 current loss: 0.06109858304262161\n",
      "iteration 362 current loss: 0.06407542526721954\n",
      "iteration 363 current loss: 0.06597712635993958\n",
      "iteration 364 current loss: 0.06318838894367218\n",
      "iteration 365 current loss: 0.05894019082188606\n",
      "iteration 366 current loss: 0.06540055572986603\n",
      "iteration 367 current loss: 0.06554301828145981\n",
      "iteration 368 current loss: 0.06583739817142487\n",
      "iteration 369 current loss: 0.06436756998300552\n",
      "iteration 370 current loss: 0.06522269546985626\n",
      "iteration 371 current loss: 0.060677748173475266\n",
      "iteration 372 current loss: 0.06259527802467346\n",
      "iteration 373 current loss: 0.06457539647817612\n",
      "iteration 374 current loss: 0.06456966698169708\n",
      "iteration 375 current loss: 0.06388843804597855\n",
      "iteration 376 current loss: 0.06352411955595016\n",
      "iteration 377 current loss: 0.06356443464756012\n",
      "iteration 378 current loss: 0.06447772681713104\n",
      "iteration 379 current loss: 0.062124766409397125\n",
      "iteration 380 current loss: 0.07029010355472565\n",
      "iteration 381 current loss: 0.05963540077209473\n",
      "iteration 382 current loss: 0.06401734799146652\n",
      "iteration 383 current loss: 0.06495985388755798\n",
      "iteration 384 current loss: 0.061546891927719116\n",
      "iteration 385 current loss: 0.06335746496915817\n",
      "iteration 386 current loss: 0.06685536354780197\n",
      "iteration 387 current loss: 0.06140657886862755\n",
      "iteration 388 current loss: 0.06494040787220001\n",
      "iteration 389 current loss: 0.062812440097332\n",
      "iteration 390 current loss: 0.06332691758871078\n",
      "iteration 391 current loss: 0.0623662956058979\n",
      "iteration 392 current loss: 0.06675340235233307\n",
      "iteration 393 current loss: 0.06229868531227112\n",
      "iteration 394 current loss: 0.06586851179599762\n",
      "iteration 395 current loss: 0.06225302442908287\n",
      "iteration 396 current loss: 0.06402281671762466\n",
      "iteration 397 current loss: 0.06506869196891785\n",
      "iteration 398 current loss: 0.06579966098070145\n",
      "iteration 399 current loss: 0.06777839362621307\n",
      "iteration 400 current loss: 0.06246943771839142\n",
      "iteration 401 current loss: 0.06731944531202316\n",
      "iteration 402 current loss: 0.059955958276987076\n",
      "iteration 403 current loss: 0.067575603723526\n",
      "iteration 404 current loss: 0.06786869466304779\n",
      "iteration 405 current loss: 0.06353580206632614\n",
      "iteration 406 current loss: 0.06411448866128922\n",
      "iteration 407 current loss: 0.06386489421129227\n",
      "iteration 408 current loss: 0.06398630887269974\n",
      "iteration 409 current loss: 0.06465743482112885\n",
      "iteration 410 current loss: 0.07287329435348511\n",
      "\t\tEpoch 1/100 complete. Epoch loss 0.06603033061155147\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 1, Validation Loss: 0.06682920758612454\n",
      "best loss 0.06603033061155147\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.06453006714582443\n",
      "iteration 1 current loss: 0.06177440285682678\n",
      "iteration 2 current loss: 0.0651249811053276\n",
      "iteration 3 current loss: 0.06317315250635147\n",
      "iteration 4 current loss: 0.06380733102560043\n",
      "iteration 5 current loss: 0.0670556128025055\n",
      "iteration 6 current loss: 0.06458315998315811\n",
      "iteration 7 current loss: 0.058580201119184494\n",
      "iteration 8 current loss: 0.06411733478307724\n",
      "iteration 9 current loss: 0.06315922737121582\n",
      "iteration 10 current loss: 0.06372372806072235\n",
      "iteration 11 current loss: 0.06024641543626785\n",
      "iteration 12 current loss: 0.06348519027233124\n",
      "iteration 13 current loss: 0.06672680377960205\n",
      "iteration 14 current loss: 0.061662010848522186\n",
      "iteration 15 current loss: 0.06013897806406021\n",
      "iteration 16 current loss: 0.06453319638967514\n",
      "iteration 17 current loss: 0.06385274231433868\n",
      "iteration 18 current loss: 0.062256842851638794\n",
      "iteration 19 current loss: 0.06076425686478615\n",
      "iteration 20 current loss: 0.06250212341547012\n",
      "iteration 21 current loss: 0.05965011194348335\n",
      "iteration 22 current loss: 0.062126386910676956\n",
      "iteration 23 current loss: 0.062030792236328125\n",
      "iteration 24 current loss: 0.06289719045162201\n",
      "iteration 25 current loss: 0.06179725006222725\n",
      "iteration 26 current loss: 0.06139308586716652\n",
      "iteration 27 current loss: 0.06225986033678055\n",
      "iteration 28 current loss: 0.06415180116891861\n",
      "iteration 29 current loss: 0.060372430831193924\n",
      "iteration 30 current loss: 0.06324537843465805\n",
      "iteration 31 current loss: 0.06095593422651291\n",
      "iteration 32 current loss: 0.059965722262859344\n",
      "iteration 33 current loss: 0.06310628354549408\n",
      "iteration 34 current loss: 0.06258441507816315\n",
      "iteration 35 current loss: 0.06522819399833679\n",
      "iteration 36 current loss: 0.06676172465085983\n",
      "iteration 37 current loss: 0.06378704309463501\n",
      "iteration 38 current loss: 0.06359510123729706\n",
      "iteration 39 current loss: 0.062447577714920044\n",
      "iteration 40 current loss: 0.061407364904880524\n",
      "iteration 41 current loss: 0.06203838810324669\n",
      "iteration 42 current loss: 0.06492151319980621\n",
      "iteration 43 current loss: 0.06404632329940796\n",
      "iteration 44 current loss: 0.06637399643659592\n",
      "iteration 45 current loss: 0.06425058841705322\n",
      "iteration 46 current loss: 0.062213681638240814\n",
      "iteration 47 current loss: 0.06119886040687561\n",
      "iteration 48 current loss: 0.06562502682209015\n",
      "iteration 49 current loss: 0.06289956718683243\n",
      "iteration 50 current loss: 0.0653294175863266\n",
      "iteration 51 current loss: 0.06294074654579163\n",
      "iteration 52 current loss: 0.0632968321442604\n",
      "iteration 53 current loss: 0.0637931302189827\n",
      "iteration 54 current loss: 0.0627724826335907\n",
      "iteration 55 current loss: 0.06396609544754028\n",
      "iteration 56 current loss: 0.06431028991937637\n",
      "iteration 57 current loss: 0.0637538805603981\n",
      "iteration 58 current loss: 0.06264891475439072\n",
      "iteration 59 current loss: 0.0588020384311676\n",
      "iteration 60 current loss: 0.0620613731443882\n",
      "iteration 61 current loss: 0.06453881412744522\n",
      "iteration 62 current loss: 0.06077858805656433\n",
      "iteration 63 current loss: 0.06440757215023041\n",
      "iteration 64 current loss: 0.06465805321931839\n",
      "iteration 65 current loss: 0.0646856352686882\n",
      "iteration 66 current loss: 0.06354017555713654\n",
      "iteration 67 current loss: 0.0619221106171608\n",
      "iteration 68 current loss: 0.06525297462940216\n",
      "iteration 69 current loss: 0.06520580500364304\n",
      "iteration 70 current loss: 0.06153522804379463\n",
      "iteration 71 current loss: 0.06268471479415894\n",
      "iteration 72 current loss: 0.06219053640961647\n",
      "iteration 73 current loss: 0.0613584890961647\n",
      "iteration 74 current loss: 0.062087513506412506\n",
      "iteration 75 current loss: 0.06586716324090958\n",
      "iteration 76 current loss: 0.0601031556725502\n",
      "iteration 77 current loss: 0.06423189491033554\n",
      "iteration 78 current loss: 0.06316028535366058\n",
      "iteration 79 current loss: 0.06610943377017975\n",
      "iteration 80 current loss: 0.06351272016763687\n",
      "iteration 81 current loss: 0.06515773385763168\n",
      "iteration 82 current loss: 0.06767163425683975\n",
      "iteration 83 current loss: 0.0657770186662674\n",
      "iteration 84 current loss: 0.06331154704093933\n",
      "iteration 85 current loss: 0.06552975624799728\n",
      "iteration 86 current loss: 0.06154334172606468\n",
      "iteration 87 current loss: 0.06631220877170563\n",
      "iteration 88 current loss: 0.06395195424556732\n",
      "iteration 89 current loss: 0.06174274906516075\n",
      "iteration 90 current loss: 0.062338750809431076\n",
      "iteration 91 current loss: 0.06493102759122849\n",
      "iteration 92 current loss: 0.06552314013242722\n",
      "iteration 93 current loss: 0.06203150004148483\n",
      "iteration 94 current loss: 0.06549955159425735\n",
      "iteration 95 current loss: 0.0631834864616394\n",
      "iteration 96 current loss: 0.06671988219022751\n",
      "iteration 97 current loss: 0.0645599290728569\n",
      "iteration 98 current loss: 0.06070198491215706\n",
      "iteration 99 current loss: 0.06445660442113876\n",
      "iteration 100 current loss: 0.061321012675762177\n",
      "iteration 101 current loss: 0.06287036836147308\n",
      "iteration 102 current loss: 0.06293748319149017\n",
      "iteration 103 current loss: 0.06262820214033127\n",
      "iteration 104 current loss: 0.06501704454421997\n",
      "iteration 105 current loss: 0.064249187707901\n",
      "iteration 106 current loss: 0.06296831369400024\n",
      "iteration 107 current loss: 0.05904439091682434\n",
      "iteration 108 current loss: 0.06045467033982277\n",
      "iteration 109 current loss: 0.06427901983261108\n",
      "iteration 110 current loss: 0.06507585197687149\n",
      "iteration 111 current loss: 0.06262664496898651\n",
      "iteration 112 current loss: 0.06120668724179268\n",
      "iteration 113 current loss: 0.062165696173906326\n",
      "iteration 114 current loss: 0.06147579848766327\n",
      "iteration 115 current loss: 0.06055133789777756\n",
      "iteration 116 current loss: 0.06300393491983414\n",
      "iteration 117 current loss: 0.06173918396234512\n",
      "iteration 118 current loss: 0.06219474598765373\n",
      "iteration 119 current loss: 0.06293464452028275\n",
      "iteration 120 current loss: 0.062470901757478714\n",
      "iteration 121 current loss: 0.06708353012800217\n",
      "iteration 122 current loss: 0.060820095241069794\n",
      "iteration 123 current loss: 0.06182653829455376\n",
      "iteration 124 current loss: 0.06252224743366241\n",
      "iteration 125 current loss: 0.06160938739776611\n",
      "iteration 126 current loss: 0.060645200312137604\n",
      "iteration 127 current loss: 0.06207901984453201\n",
      "iteration 128 current loss: 0.06293739378452301\n",
      "iteration 129 current loss: 0.06304315477609634\n",
      "iteration 130 current loss: 0.06019414961338043\n",
      "iteration 131 current loss: 0.06118284538388252\n",
      "iteration 132 current loss: 0.06162932142615318\n",
      "iteration 133 current loss: 0.058494336903095245\n",
      "iteration 134 current loss: 0.06338495016098022\n",
      "iteration 135 current loss: 0.06247371435165405\n",
      "iteration 136 current loss: 0.06191449612379074\n",
      "iteration 137 current loss: 0.06357263773679733\n",
      "iteration 138 current loss: 0.06347580999135971\n",
      "iteration 139 current loss: 0.06373346596956253\n",
      "iteration 140 current loss: 0.06260255724191666\n",
      "iteration 141 current loss: 0.064967580139637\n",
      "iteration 142 current loss: 0.06085114926099777\n",
      "iteration 143 current loss: 0.05994006618857384\n",
      "iteration 144 current loss: 0.06176228076219559\n",
      "iteration 145 current loss: 0.06426069885492325\n",
      "iteration 146 current loss: 0.06279434263706207\n",
      "iteration 147 current loss: 0.06226489692926407\n",
      "iteration 148 current loss: 0.06388378888368607\n",
      "iteration 149 current loss: 0.062251050025224686\n",
      "iteration 150 current loss: 0.06265424937009811\n",
      "iteration 151 current loss: 0.062007881700992584\n",
      "iteration 152 current loss: 0.061397816985845566\n",
      "iteration 153 current loss: 0.06457338482141495\n",
      "iteration 154 current loss: 0.06640581041574478\n",
      "iteration 155 current loss: 0.06043911725282669\n",
      "iteration 156 current loss: 0.06450144201517105\n",
      "iteration 157 current loss: 0.06143058091402054\n",
      "iteration 158 current loss: 0.062250491231679916\n",
      "iteration 159 current loss: 0.06277953833341599\n",
      "iteration 160 current loss: 0.06377460062503815\n",
      "iteration 161 current loss: 0.0652860626578331\n",
      "iteration 162 current loss: 0.06350012868642807\n",
      "iteration 163 current loss: 0.06145235523581505\n",
      "iteration 164 current loss: 0.060164377093315125\n",
      "iteration 165 current loss: 0.061540666967630386\n",
      "iteration 166 current loss: 0.06064862012863159\n",
      "iteration 167 current loss: 0.06054939329624176\n",
      "iteration 168 current loss: 0.06009899824857712\n",
      "iteration 169 current loss: 0.05786490440368652\n",
      "iteration 170 current loss: 0.06191125512123108\n",
      "iteration 171 current loss: 0.05876501277089119\n",
      "iteration 172 current loss: 0.06416354328393936\n",
      "iteration 173 current loss: 0.06316208094358444\n",
      "iteration 174 current loss: 0.06041265279054642\n",
      "iteration 175 current loss: 0.06012456864118576\n",
      "iteration 176 current loss: 0.06420869380235672\n",
      "iteration 177 current loss: 0.061518095433712006\n",
      "iteration 178 current loss: 0.0625709667801857\n",
      "iteration 179 current loss: 0.063367560505867\n",
      "iteration 180 current loss: 0.064205102622509\n",
      "iteration 181 current loss: 0.062346044927835464\n",
      "iteration 182 current loss: 0.0615365244448185\n",
      "iteration 183 current loss: 0.058795902878046036\n",
      "iteration 184 current loss: 0.060948554426431656\n",
      "iteration 185 current loss: 0.06351369619369507\n",
      "iteration 186 current loss: 0.0581149086356163\n",
      "iteration 187 current loss: 0.06151782348752022\n",
      "iteration 188 current loss: 0.062309447675943375\n",
      "iteration 189 current loss: 0.06128036603331566\n",
      "iteration 190 current loss: 0.0645604282617569\n",
      "iteration 191 current loss: 0.06487448513507843\n",
      "iteration 192 current loss: 0.06601882725954056\n",
      "iteration 193 current loss: 0.0627211257815361\n",
      "iteration 194 current loss: 0.05914342403411865\n",
      "iteration 195 current loss: 0.0595262311398983\n",
      "iteration 196 current loss: 0.06257399916648865\n",
      "iteration 197 current loss: 0.05974534898996353\n",
      "iteration 198 current loss: 0.06140362098813057\n",
      "iteration 199 current loss: 0.059923138469457626\n",
      "iteration 200 current loss: 0.0594625286757946\n",
      "iteration 201 current loss: 0.06644296646118164\n",
      "iteration 202 current loss: 0.06206027790904045\n",
      "iteration 203 current loss: 0.06322649121284485\n",
      "iteration 204 current loss: 0.06493977457284927\n",
      "iteration 205 current loss: 0.06594337522983551\n",
      "iteration 206 current loss: 0.06555499881505966\n",
      "iteration 207 current loss: 0.06219010800123215\n",
      "iteration 208 current loss: 0.06183779612183571\n",
      "iteration 209 current loss: 0.06296571344137192\n",
      "iteration 210 current loss: 0.05957705155014992\n",
      "iteration 211 current loss: 0.06094851717352867\n",
      "iteration 212 current loss: 0.06131667643785477\n",
      "iteration 213 current loss: 0.06415437906980515\n",
      "iteration 214 current loss: 0.06323236227035522\n",
      "iteration 215 current loss: 0.06588555127382278\n",
      "iteration 216 current loss: 0.06289810687303543\n",
      "iteration 217 current loss: 0.06299547851085663\n",
      "iteration 218 current loss: 0.061699848622083664\n",
      "iteration 219 current loss: 0.061820536851882935\n",
      "iteration 220 current loss: 0.06159931793808937\n",
      "iteration 221 current loss: 0.05972181633114815\n",
      "iteration 222 current loss: 0.06675329059362411\n",
      "iteration 223 current loss: 0.06189214810729027\n",
      "iteration 224 current loss: 0.06032349541783333\n",
      "iteration 225 current loss: 0.06167934462428093\n",
      "iteration 226 current loss: 0.0636453926563263\n",
      "iteration 227 current loss: 0.06327501684427261\n",
      "iteration 228 current loss: 0.06175442039966583\n",
      "iteration 229 current loss: 0.059946127235889435\n",
      "iteration 230 current loss: 0.061715736985206604\n",
      "iteration 231 current loss: 0.06389336287975311\n",
      "iteration 232 current loss: 0.061828553676605225\n",
      "iteration 233 current loss: 0.060215216130018234\n",
      "iteration 234 current loss: 0.06096915900707245\n",
      "iteration 235 current loss: 0.06257696449756622\n",
      "iteration 236 current loss: 0.0629405528306961\n",
      "iteration 237 current loss: 0.06116113439202309\n",
      "iteration 238 current loss: 0.06003003567457199\n",
      "iteration 239 current loss: 0.06098124012351036\n",
      "iteration 240 current loss: 0.05575843155384064\n",
      "iteration 241 current loss: 0.059583645313978195\n",
      "iteration 242 current loss: 0.059365611523389816\n",
      "iteration 243 current loss: 0.0640837773680687\n",
      "iteration 244 current loss: 0.057758573442697525\n",
      "iteration 245 current loss: 0.06201610714197159\n",
      "iteration 246 current loss: 0.06102977320551872\n",
      "iteration 247 current loss: 0.06174046918749809\n",
      "iteration 248 current loss: 0.06062609702348709\n",
      "iteration 249 current loss: 0.06026582419872284\n",
      "iteration 250 current loss: 0.061073895543813705\n",
      "iteration 251 current loss: 0.060336895287036896\n",
      "iteration 252 current loss: 0.056677818298339844\n",
      "iteration 253 current loss: 0.061571065336465836\n",
      "iteration 254 current loss: 0.062470920383930206\n",
      "iteration 255 current loss: 0.06502194702625275\n",
      "iteration 256 current loss: 0.0627373605966568\n",
      "iteration 257 current loss: 0.059687454253435135\n",
      "iteration 258 current loss: 0.062085073441267014\n",
      "iteration 259 current loss: 0.060313016176223755\n",
      "iteration 260 current loss: 0.06309966742992401\n",
      "iteration 261 current loss: 0.06044289469718933\n",
      "iteration 262 current loss: 0.05880602076649666\n",
      "iteration 263 current loss: 0.062172599136829376\n",
      "iteration 264 current loss: 0.06029641255736351\n",
      "iteration 265 current loss: 0.06348171085119247\n",
      "iteration 266 current loss: 0.05996445566415787\n",
      "iteration 267 current loss: 0.06253770738840103\n",
      "iteration 268 current loss: 0.0603901632130146\n",
      "iteration 269 current loss: 0.06382864713668823\n",
      "iteration 270 current loss: 0.05999300256371498\n",
      "iteration 271 current loss: 0.06422204524278641\n",
      "iteration 272 current loss: 0.06515186280012131\n",
      "iteration 273 current loss: 0.06062186136841774\n",
      "iteration 274 current loss: 0.06447570770978928\n",
      "iteration 275 current loss: 0.059346843510866165\n",
      "iteration 276 current loss: 0.0639791488647461\n",
      "iteration 277 current loss: 0.06246936693787575\n",
      "iteration 278 current loss: 0.06319936364889145\n",
      "iteration 279 current loss: 0.06146162748336792\n",
      "iteration 280 current loss: 0.061179548501968384\n",
      "iteration 281 current loss: 0.06229995936155319\n",
      "iteration 282 current loss: 0.060721274465322495\n",
      "iteration 283 current loss: 0.05996563285589218\n",
      "iteration 284 current loss: 0.060972847044467926\n",
      "iteration 285 current loss: 0.06075887009501457\n",
      "iteration 286 current loss: 0.06094853952527046\n",
      "iteration 287 current loss: 0.057562682777643204\n",
      "iteration 288 current loss: 0.06254854798316956\n",
      "iteration 289 current loss: 0.06112692132592201\n",
      "iteration 290 current loss: 0.060718461871147156\n",
      "iteration 291 current loss: 0.06305542588233948\n",
      "iteration 292 current loss: 0.0619964525103569\n",
      "iteration 293 current loss: 0.061266493052244186\n",
      "iteration 294 current loss: 0.060433413833379745\n",
      "iteration 295 current loss: 0.060669686645269394\n",
      "iteration 296 current loss: 0.06261809170246124\n",
      "iteration 297 current loss: 0.059074535965919495\n",
      "iteration 298 current loss: 0.0602378323674202\n",
      "iteration 299 current loss: 0.06536713987588882\n",
      "iteration 300 current loss: 0.06254608929157257\n",
      "iteration 301 current loss: 0.061256684362888336\n",
      "iteration 302 current loss: 0.061864420771598816\n",
      "iteration 303 current loss: 0.06398928910493851\n",
      "iteration 304 current loss: 0.062160465866327286\n",
      "iteration 305 current loss: 0.061048269271850586\n",
      "iteration 306 current loss: 0.0596931129693985\n",
      "iteration 307 current loss: 0.05904660001397133\n",
      "iteration 308 current loss: 0.06328634172677994\n",
      "iteration 309 current loss: 0.062089718878269196\n",
      "iteration 310 current loss: 0.06414603441953659\n",
      "iteration 311 current loss: 0.06358946114778519\n",
      "iteration 312 current loss: 0.0595797523856163\n",
      "iteration 313 current loss: 0.06243894249200821\n",
      "iteration 314 current loss: 0.06291303038597107\n",
      "iteration 315 current loss: 0.06291893869638443\n",
      "iteration 316 current loss: 0.06275700032711029\n",
      "iteration 317 current loss: 0.06281501054763794\n",
      "iteration 318 current loss: 0.06423168629407883\n",
      "iteration 319 current loss: 0.062106698751449585\n",
      "iteration 320 current loss: 0.06127587705850601\n",
      "iteration 321 current loss: 0.06119808927178383\n",
      "iteration 322 current loss: 0.061696555465459824\n",
      "iteration 323 current loss: 0.06066671758890152\n",
      "iteration 324 current loss: 0.06101159006357193\n",
      "iteration 325 current loss: 0.061724018305540085\n",
      "iteration 326 current loss: 0.056250978261232376\n",
      "iteration 327 current loss: 0.059853095561265945\n",
      "iteration 328 current loss: 0.06481494754552841\n",
      "iteration 329 current loss: 0.06251469999551773\n",
      "iteration 330 current loss: 0.05847584456205368\n",
      "iteration 331 current loss: 0.06362593173980713\n",
      "iteration 332 current loss: 0.060723889619112015\n",
      "iteration 333 current loss: 0.05909641459584236\n",
      "iteration 334 current loss: 0.06165575236082077\n",
      "iteration 335 current loss: 0.05989036709070206\n",
      "iteration 336 current loss: 0.06469107419252396\n",
      "iteration 337 current loss: 0.06338430941104889\n",
      "iteration 338 current loss: 0.05989822745323181\n",
      "iteration 339 current loss: 0.057853128761053085\n",
      "iteration 340 current loss: 0.06230946257710457\n",
      "iteration 341 current loss: 0.06038684397935867\n",
      "iteration 342 current loss: 0.0603005588054657\n",
      "iteration 343 current loss: 0.0641418993473053\n",
      "iteration 344 current loss: 0.060534875839948654\n",
      "iteration 345 current loss: 0.06734540313482285\n",
      "iteration 346 current loss: 0.062203265726566315\n",
      "iteration 347 current loss: 0.06299445778131485\n",
      "iteration 348 current loss: 0.06132398173213005\n",
      "iteration 349 current loss: 0.06094775348901749\n",
      "iteration 350 current loss: 0.060097094625234604\n",
      "iteration 351 current loss: 0.06117027997970581\n",
      "iteration 352 current loss: 0.06107981875538826\n",
      "iteration 353 current loss: 0.06351970881223679\n",
      "iteration 354 current loss: 0.0628853365778923\n",
      "iteration 355 current loss: 0.06044436991214752\n",
      "iteration 356 current loss: 0.06151515617966652\n",
      "iteration 357 current loss: 0.06306695193052292\n",
      "iteration 358 current loss: 0.058038532733917236\n",
      "iteration 359 current loss: 0.057840973138809204\n",
      "iteration 360 current loss: 0.06067529320716858\n",
      "iteration 361 current loss: 0.06403445452451706\n",
      "iteration 362 current loss: 0.06029646471142769\n",
      "iteration 363 current loss: 0.06325451284646988\n",
      "iteration 364 current loss: 0.0662468820810318\n",
      "iteration 365 current loss: 0.06494935601949692\n",
      "iteration 366 current loss: 0.059051282703876495\n",
      "iteration 367 current loss: 0.06252316385507584\n",
      "iteration 368 current loss: 0.059801675379276276\n",
      "iteration 369 current loss: 0.061754658818244934\n",
      "iteration 370 current loss: 0.061714377254247665\n",
      "iteration 371 current loss: 0.0640835240483284\n",
      "iteration 372 current loss: 0.061257097870111465\n",
      "iteration 373 current loss: 0.059769392013549805\n",
      "iteration 374 current loss: 0.06428036838769913\n",
      "iteration 375 current loss: 0.06359802931547165\n",
      "iteration 376 current loss: 0.06254921853542328\n",
      "iteration 377 current loss: 0.06240468472242355\n",
      "iteration 378 current loss: 0.06309384852647781\n",
      "iteration 379 current loss: 0.06302738189697266\n",
      "iteration 380 current loss: 0.0617067888379097\n",
      "iteration 381 current loss: 0.06027054414153099\n",
      "iteration 382 current loss: 0.06212988123297691\n",
      "iteration 383 current loss: 0.060465142130851746\n",
      "iteration 384 current loss: 0.061160530894994736\n",
      "iteration 385 current loss: 0.0622648186981678\n",
      "iteration 386 current loss: 0.06231922656297684\n",
      "iteration 387 current loss: 0.059648338705301285\n",
      "iteration 388 current loss: 0.06482600420713425\n",
      "iteration 389 current loss: 0.06135045364499092\n",
      "iteration 390 current loss: 0.05990166962146759\n",
      "iteration 391 current loss: 0.06071019545197487\n",
      "iteration 392 current loss: 0.06166569143533707\n",
      "iteration 393 current loss: 0.0626174584031105\n",
      "iteration 394 current loss: 0.05853545665740967\n",
      "iteration 395 current loss: 0.057942796498537064\n",
      "iteration 396 current loss: 0.06132936477661133\n",
      "iteration 397 current loss: 0.06401170045137405\n",
      "iteration 398 current loss: 0.058561064302921295\n",
      "iteration 399 current loss: 0.06196583807468414\n",
      "iteration 400 current loss: 0.06050558760762215\n",
      "iteration 401 current loss: 0.05993634834885597\n",
      "iteration 402 current loss: 0.06502936035394669\n",
      "iteration 403 current loss: 0.06203749403357506\n",
      "iteration 404 current loss: 0.06006866693496704\n",
      "iteration 405 current loss: 0.06155990809202194\n",
      "iteration 406 current loss: 0.06264306604862213\n",
      "iteration 407 current loss: 0.05940849706530571\n",
      "iteration 408 current loss: 0.06355809420347214\n",
      "iteration 409 current loss: 0.06020922586321831\n",
      "iteration 410 current loss: 0.06724298000335693\n",
      "\t\tEpoch 2/100 complete. Epoch loss 0.06222426916706011\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 2, Validation Loss: 0.0640112953260541\n",
      "best loss 0.06222426916706011\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.06321504712104797\n",
      "iteration 1 current loss: 0.06352480500936508\n",
      "iteration 2 current loss: 0.06340529024600983\n",
      "iteration 3 current loss: 0.0639975443482399\n",
      "iteration 4 current loss: 0.061396125704050064\n",
      "iteration 5 current loss: 0.058546990156173706\n",
      "iteration 6 current loss: 0.06323984265327454\n",
      "iteration 7 current loss: 0.06317676603794098\n",
      "iteration 8 current loss: 0.06154628470540047\n",
      "iteration 9 current loss: 0.06480222195386887\n",
      "iteration 10 current loss: 0.06181233376264572\n",
      "iteration 11 current loss: 0.06357221305370331\n",
      "iteration 12 current loss: 0.059716708958148956\n",
      "iteration 13 current loss: 0.059174757450819016\n",
      "iteration 14 current loss: 0.062205925583839417\n",
      "iteration 15 current loss: 0.06110993027687073\n",
      "iteration 16 current loss: 0.05988020449876785\n",
      "iteration 17 current loss: 0.06235422194004059\n",
      "iteration 18 current loss: 0.06200435012578964\n",
      "iteration 19 current loss: 0.06059974431991577\n",
      "iteration 20 current loss: 0.0630464032292366\n",
      "iteration 21 current loss: 0.05852005258202553\n",
      "iteration 22 current loss: 0.05985814705491066\n",
      "iteration 23 current loss: 0.06025087088346481\n",
      "iteration 24 current loss: 0.06008068472146988\n",
      "iteration 25 current loss: 0.061725370585918427\n",
      "iteration 26 current loss: 0.06124984472990036\n",
      "iteration 27 current loss: 0.06068240851163864\n",
      "iteration 28 current loss: 0.06285524368286133\n",
      "iteration 29 current loss: 0.05958728492259979\n",
      "iteration 30 current loss: 0.05963193252682686\n",
      "iteration 31 current loss: 0.06249850243330002\n",
      "iteration 32 current loss: 0.06301984190940857\n",
      "iteration 33 current loss: 0.06004711613059044\n",
      "iteration 34 current loss: 0.06101074814796448\n",
      "iteration 35 current loss: 0.06067417934536934\n",
      "iteration 36 current loss: 0.059081509709358215\n",
      "iteration 37 current loss: 0.06219036504626274\n",
      "iteration 38 current loss: 0.06206359341740608\n",
      "iteration 39 current loss: 0.05977504327893257\n",
      "iteration 40 current loss: 0.06195497512817383\n",
      "iteration 41 current loss: 0.06557425856590271\n",
      "iteration 42 current loss: 0.058586668223142624\n",
      "iteration 43 current loss: 0.06081907078623772\n",
      "iteration 44 current loss: 0.05967065319418907\n",
      "iteration 45 current loss: 0.06424649804830551\n",
      "iteration 46 current loss: 0.05840238928794861\n",
      "iteration 47 current loss: 0.0636163279414177\n",
      "iteration 48 current loss: 0.06104278191924095\n",
      "iteration 49 current loss: 0.05996791645884514\n",
      "iteration 50 current loss: 0.06081059202551842\n",
      "iteration 51 current loss: 0.06633420288562775\n",
      "iteration 52 current loss: 0.060057539492845535\n",
      "iteration 53 current loss: 0.05979543179273605\n",
      "iteration 54 current loss: 0.06604351103305817\n",
      "iteration 55 current loss: 0.06497973948717117\n",
      "iteration 56 current loss: 0.0637015625834465\n",
      "iteration 57 current loss: 0.06446308642625809\n",
      "iteration 58 current loss: 0.06050678715109825\n",
      "iteration 59 current loss: 0.057374827563762665\n",
      "iteration 60 current loss: 0.06203277036547661\n",
      "iteration 61 current loss: 0.06424709409475327\n",
      "iteration 62 current loss: 0.06142956390976906\n",
      "iteration 63 current loss: 0.06151486188173294\n",
      "iteration 64 current loss: 0.06550178676843643\n",
      "iteration 65 current loss: 0.06477663666009903\n",
      "iteration 66 current loss: 0.06444870680570602\n",
      "iteration 67 current loss: 0.05988502874970436\n",
      "iteration 68 current loss: 0.06107098236680031\n",
      "iteration 69 current loss: 0.06026503071188927\n",
      "iteration 70 current loss: 0.06224073842167854\n",
      "iteration 71 current loss: 0.057602979242801666\n",
      "iteration 72 current loss: 0.06454742699861526\n",
      "iteration 73 current loss: 0.0631711333990097\n",
      "iteration 74 current loss: 0.06341591477394104\n",
      "iteration 75 current loss: 0.06294471025466919\n",
      "iteration 76 current loss: 0.06208738684654236\n",
      "iteration 77 current loss: 0.06330817192792892\n",
      "iteration 78 current loss: 0.06068507954478264\n",
      "iteration 79 current loss: 0.05953960865736008\n",
      "iteration 80 current loss: 0.061932846903800964\n",
      "iteration 81 current loss: 0.06621287763118744\n",
      "iteration 82 current loss: 0.06311393529176712\n",
      "iteration 83 current loss: 0.06292381882667542\n",
      "iteration 84 current loss: 0.058269012719392776\n",
      "iteration 85 current loss: 0.06202214956283569\n",
      "iteration 86 current loss: 0.06132104992866516\n",
      "iteration 87 current loss: 0.061907555907964706\n",
      "iteration 88 current loss: 0.06167205795645714\n",
      "iteration 89 current loss: 0.0635128915309906\n",
      "iteration 90 current loss: 0.06497744470834732\n",
      "iteration 91 current loss: 0.05936766415834427\n",
      "iteration 92 current loss: 0.058952875435352325\n",
      "iteration 93 current loss: 0.06398621946573257\n",
      "iteration 94 current loss: 0.05997336655855179\n",
      "iteration 95 current loss: 0.06754318624734879\n",
      "iteration 96 current loss: 0.061845023185014725\n",
      "iteration 97 current loss: 0.0635341927409172\n",
      "iteration 98 current loss: 0.0638536587357521\n",
      "iteration 99 current loss: 0.06341052800416946\n",
      "iteration 100 current loss: 0.06260178983211517\n",
      "iteration 101 current loss: 0.060806676745414734\n",
      "iteration 102 current loss: 0.061540696769952774\n",
      "iteration 103 current loss: 0.0651262104511261\n",
      "iteration 104 current loss: 0.05690637230873108\n",
      "iteration 105 current loss: 0.060833126306533813\n",
      "iteration 106 current loss: 0.05920850485563278\n",
      "iteration 107 current loss: 0.05841629207134247\n",
      "iteration 108 current loss: 0.06208343058824539\n",
      "iteration 109 current loss: 0.06242460012435913\n",
      "iteration 110 current loss: 0.06275910139083862\n",
      "iteration 111 current loss: 0.060350336134433746\n",
      "iteration 112 current loss: 0.059036705642938614\n",
      "iteration 113 current loss: 0.05951695144176483\n",
      "iteration 114 current loss: 0.060130685567855835\n",
      "iteration 115 current loss: 0.06363023072481155\n",
      "iteration 116 current loss: 0.06050252169370651\n",
      "iteration 117 current loss: 0.05878302454948425\n",
      "iteration 118 current loss: 0.06051070615649223\n",
      "iteration 119 current loss: 0.05915313959121704\n",
      "iteration 120 current loss: 0.05941367894411087\n",
      "iteration 121 current loss: 0.06111803650856018\n",
      "iteration 122 current loss: 0.060510098934173584\n",
      "iteration 123 current loss: 0.06191325560212135\n",
      "iteration 124 current loss: 0.06304346770048141\n",
      "iteration 125 current loss: 0.06236741319298744\n",
      "iteration 126 current loss: 0.06238693743944168\n",
      "iteration 127 current loss: 0.06385550647974014\n",
      "iteration 128 current loss: 0.06185302138328552\n",
      "iteration 129 current loss: 0.05952773243188858\n",
      "iteration 130 current loss: 0.05842819809913635\n",
      "iteration 131 current loss: 0.06042725220322609\n",
      "iteration 132 current loss: 0.057812757790088654\n",
      "iteration 133 current loss: 0.06322820484638214\n",
      "iteration 134 current loss: 0.059955913573503494\n",
      "iteration 135 current loss: 0.06440282613039017\n",
      "iteration 136 current loss: 0.05946122482419014\n",
      "iteration 137 current loss: 0.05867927148938179\n",
      "iteration 138 current loss: 0.05888929218053818\n",
      "iteration 139 current loss: 0.06405526399612427\n",
      "iteration 140 current loss: 0.06255502998828888\n",
      "iteration 141 current loss: 0.058900136500597\n",
      "iteration 142 current loss: 0.055924445390701294\n",
      "iteration 143 current loss: 0.05643846467137337\n",
      "iteration 144 current loss: 0.062467027455568314\n",
      "iteration 145 current loss: 0.061710331588983536\n",
      "iteration 146 current loss: 0.059012271463871\n",
      "iteration 147 current loss: 0.06207026541233063\n",
      "iteration 148 current loss: 0.06134041026234627\n",
      "iteration 149 current loss: 0.06293431669473648\n",
      "iteration 150 current loss: 0.0639706626534462\n",
      "iteration 151 current loss: 0.06105664372444153\n",
      "iteration 152 current loss: 0.058179501444101334\n",
      "iteration 153 current loss: 0.05902275815606117\n",
      "iteration 154 current loss: 0.06435724347829819\n",
      "iteration 155 current loss: 0.05805367976427078\n",
      "iteration 156 current loss: 0.05972571298480034\n",
      "iteration 157 current loss: 0.06117855757474899\n",
      "iteration 158 current loss: 0.06115084886550903\n",
      "iteration 159 current loss: 0.0622793510556221\n",
      "iteration 160 current loss: 0.06001831963658333\n",
      "iteration 161 current loss: 0.059747397899627686\n",
      "iteration 162 current loss: 0.06315139681100845\n",
      "iteration 163 current loss: 0.061920445412397385\n",
      "iteration 164 current loss: 0.055891264230012894\n",
      "iteration 165 current loss: 0.06093888729810715\n",
      "iteration 166 current loss: 0.061010122299194336\n",
      "iteration 167 current loss: 0.06375327706336975\n",
      "iteration 168 current loss: 0.061438366770744324\n",
      "iteration 169 current loss: 0.06033626198768616\n",
      "iteration 170 current loss: 0.060563698410987854\n",
      "iteration 171 current loss: 0.06173796206712723\n",
      "iteration 172 current loss: 0.0600624606013298\n",
      "iteration 173 current loss: 0.06033940613269806\n",
      "iteration 174 current loss: 0.0640011578798294\n",
      "iteration 175 current loss: 0.056585147976875305\n",
      "iteration 176 current loss: 0.0650450810790062\n",
      "iteration 177 current loss: 0.062261682003736496\n",
      "iteration 178 current loss: 0.06013141945004463\n",
      "iteration 179 current loss: 0.06336920708417892\n",
      "iteration 180 current loss: 0.0625452846288681\n",
      "iteration 181 current loss: 0.060345105826854706\n",
      "iteration 182 current loss: 0.061638135462999344\n",
      "iteration 183 current loss: 0.062148332595825195\n",
      "iteration 184 current loss: 0.06255310773849487\n",
      "iteration 185 current loss: 0.06206511706113815\n",
      "iteration 186 current loss: 0.058506790548563004\n",
      "iteration 187 current loss: 0.0618942566215992\n",
      "iteration 188 current loss: 0.0598621740937233\n",
      "iteration 189 current loss: 0.06612659990787506\n",
      "iteration 190 current loss: 0.06007479876279831\n",
      "iteration 191 current loss: 0.05805652216076851\n",
      "iteration 192 current loss: 0.06389268487691879\n",
      "iteration 193 current loss: 0.06048114225268364\n",
      "iteration 194 current loss: 0.05867036059498787\n",
      "iteration 195 current loss: 0.05753858759999275\n",
      "iteration 196 current loss: 0.05974966287612915\n",
      "iteration 197 current loss: 0.062078069895505905\n",
      "iteration 198 current loss: 0.05979355797171593\n",
      "iteration 199 current loss: 0.06443572789430618\n",
      "iteration 200 current loss: 0.06139296293258667\n",
      "iteration 201 current loss: 0.062167998403310776\n",
      "iteration 202 current loss: 0.06149742379784584\n",
      "iteration 203 current loss: 0.05924711003899574\n",
      "iteration 204 current loss: 0.05808514729142189\n",
      "iteration 205 current loss: 0.059768836945295334\n",
      "iteration 206 current loss: 0.05786203593015671\n",
      "iteration 207 current loss: 0.06409285217523575\n",
      "iteration 208 current loss: 0.06246507540345192\n",
      "iteration 209 current loss: 0.06103498861193657\n",
      "iteration 210 current loss: 0.06222756579518318\n",
      "iteration 211 current loss: 0.05978676304221153\n",
      "iteration 212 current loss: 0.05588775873184204\n",
      "iteration 213 current loss: 0.06107131391763687\n",
      "iteration 214 current loss: 0.06289725005626678\n",
      "iteration 215 current loss: 0.0593399740755558\n",
      "iteration 216 current loss: 0.060076016932725906\n",
      "iteration 217 current loss: 0.05897640436887741\n",
      "iteration 218 current loss: 0.060614798218011856\n",
      "iteration 219 current loss: 0.05662617087364197\n",
      "iteration 220 current loss: 0.059308361262083054\n",
      "iteration 221 current loss: 0.05860430374741554\n",
      "iteration 222 current loss: 0.061381977051496506\n",
      "iteration 223 current loss: 0.05880751460790634\n",
      "iteration 224 current loss: 0.060069162398576736\n",
      "iteration 225 current loss: 0.06087924912571907\n",
      "iteration 226 current loss: 0.06123239919543266\n",
      "iteration 227 current loss: 0.059670377522706985\n",
      "iteration 228 current loss: 0.06156723201274872\n",
      "iteration 229 current loss: 0.06423225998878479\n",
      "iteration 230 current loss: 0.06182518228888512\n",
      "iteration 231 current loss: 0.06109602004289627\n",
      "iteration 232 current loss: 0.061130840331315994\n",
      "iteration 233 current loss: 0.05772878974676132\n",
      "iteration 234 current loss: 0.062159255146980286\n",
      "iteration 235 current loss: 0.057907793670892715\n",
      "iteration 236 current loss: 0.06012102589011192\n",
      "iteration 237 current loss: 0.06236708536744118\n",
      "iteration 238 current loss: 0.060622408986091614\n",
      "iteration 239 current loss: 0.0598873570561409\n",
      "iteration 240 current loss: 0.06253078579902649\n",
      "iteration 241 current loss: 0.062224600464105606\n",
      "iteration 242 current loss: 0.0573078989982605\n",
      "iteration 243 current loss: 0.06339706480503082\n",
      "iteration 244 current loss: 0.06153717637062073\n",
      "iteration 245 current loss: 0.059556204825639725\n",
      "iteration 246 current loss: 0.059619929641485214\n",
      "iteration 247 current loss: 0.06091795116662979\n",
      "iteration 248 current loss: 0.059109337627887726\n",
      "iteration 249 current loss: 0.06389640271663666\n",
      "iteration 250 current loss: 0.0603865422308445\n",
      "iteration 251 current loss: 0.06064238399267197\n",
      "iteration 252 current loss: 0.06560701131820679\n",
      "iteration 253 current loss: 0.06138967350125313\n",
      "iteration 254 current loss: 0.059331703931093216\n",
      "iteration 255 current loss: 0.05806681141257286\n",
      "iteration 256 current loss: 0.06084279716014862\n",
      "iteration 257 current loss: 0.06293194741010666\n",
      "iteration 258 current loss: 0.06057954579591751\n",
      "iteration 259 current loss: 0.05877888202667236\n",
      "iteration 260 current loss: 0.059819355607032776\n",
      "iteration 261 current loss: 0.06274820864200592\n",
      "iteration 262 current loss: 0.059107664972543716\n",
      "iteration 263 current loss: 0.0610354021191597\n",
      "iteration 264 current loss: 0.060937680304050446\n",
      "iteration 265 current loss: 0.06207064539194107\n",
      "iteration 266 current loss: 0.05928890034556389\n",
      "iteration 267 current loss: 0.060503799468278885\n",
      "iteration 268 current loss: 0.06212405487895012\n",
      "iteration 269 current loss: 0.05996096506714821\n",
      "iteration 270 current loss: 0.06230749562382698\n",
      "iteration 271 current loss: 0.0631987527012825\n",
      "iteration 272 current loss: 0.05719833821058273\n",
      "iteration 273 current loss: 0.060889773070812225\n",
      "iteration 274 current loss: 0.05856367573142052\n",
      "iteration 275 current loss: 0.06437955796718597\n",
      "iteration 276 current loss: 0.06048395857214928\n",
      "iteration 277 current loss: 0.0613471157848835\n",
      "iteration 278 current loss: 0.06251996010541916\n",
      "iteration 279 current loss: 0.06108556315302849\n",
      "iteration 280 current loss: 0.059395987540483475\n",
      "iteration 281 current loss: 0.0614606998860836\n",
      "iteration 282 current loss: 0.06188115477561951\n",
      "iteration 283 current loss: 0.0618102140724659\n",
      "iteration 284 current loss: 0.06023181974887848\n",
      "iteration 285 current loss: 0.05897820368409157\n",
      "iteration 286 current loss: 0.06110548600554466\n",
      "iteration 287 current loss: 0.05877994745969772\n",
      "iteration 288 current loss: 0.06325878202915192\n",
      "iteration 289 current loss: 0.06072787195444107\n",
      "iteration 290 current loss: 0.06040049344301224\n",
      "iteration 291 current loss: 0.0632244274020195\n",
      "iteration 292 current loss: 0.06020040437579155\n",
      "iteration 293 current loss: 0.06118237227201462\n",
      "iteration 294 current loss: 0.060788996517658234\n",
      "iteration 295 current loss: 0.061810512095689774\n",
      "iteration 296 current loss: 0.06160788983106613\n",
      "iteration 297 current loss: 0.05967283248901367\n",
      "iteration 298 current loss: 0.06130778416991234\n",
      "iteration 299 current loss: 0.05848543718457222\n",
      "iteration 300 current loss: 0.06104480102658272\n",
      "iteration 301 current loss: 0.06028895080089569\n",
      "iteration 302 current loss: 0.06047263741493225\n",
      "iteration 303 current loss: 0.063309445977211\n",
      "iteration 304 current loss: 0.06142710521817207\n",
      "iteration 305 current loss: 0.0608321875333786\n",
      "iteration 306 current loss: 0.05926188826560974\n",
      "iteration 307 current loss: 0.06174411252140999\n",
      "iteration 308 current loss: 0.059612955898046494\n",
      "iteration 309 current loss: 0.06111724302172661\n",
      "iteration 310 current loss: 0.05923056602478027\n",
      "iteration 311 current loss: 0.061592649668455124\n",
      "iteration 312 current loss: 0.05909830331802368\n",
      "iteration 313 current loss: 0.06019510701298714\n",
      "iteration 314 current loss: 0.059423595666885376\n",
      "iteration 315 current loss: 0.06116802990436554\n",
      "iteration 316 current loss: 0.06189074367284775\n",
      "iteration 317 current loss: 0.06280186027288437\n",
      "iteration 318 current loss: 0.061416734009981155\n",
      "iteration 319 current loss: 0.058725278824567795\n",
      "iteration 320 current loss: 0.06195373460650444\n",
      "iteration 321 current loss: 0.06016566604375839\n",
      "iteration 322 current loss: 0.06091749295592308\n",
      "iteration 323 current loss: 0.06335268914699554\n",
      "iteration 324 current loss: 0.05805375799536705\n",
      "iteration 325 current loss: 0.06084000691771507\n",
      "iteration 326 current loss: 0.05764889344573021\n",
      "iteration 327 current loss: 0.059477947652339935\n",
      "iteration 328 current loss: 0.061175134032964706\n",
      "iteration 329 current loss: 0.06232188269495964\n",
      "iteration 330 current loss: 0.06522224098443985\n",
      "iteration 331 current loss: 0.061722997575998306\n",
      "iteration 332 current loss: 0.06368662416934967\n",
      "iteration 333 current loss: 0.05945028364658356\n",
      "iteration 334 current loss: 0.061159439384937286\n",
      "iteration 335 current loss: 0.06018771603703499\n",
      "iteration 336 current loss: 0.05927957594394684\n",
      "iteration 337 current loss: 0.061176467686891556\n",
      "iteration 338 current loss: 0.06408258527517319\n",
      "iteration 339 current loss: 0.05662059038877487\n",
      "iteration 340 current loss: 0.06206948310136795\n",
      "iteration 341 current loss: 0.05914662033319473\n",
      "iteration 342 current loss: 0.05936505272984505\n",
      "iteration 343 current loss: 0.0627070814371109\n",
      "iteration 344 current loss: 0.0626051276922226\n",
      "iteration 345 current loss: 0.05995459854602814\n",
      "iteration 346 current loss: 0.05913164094090462\n",
      "iteration 347 current loss: 0.06261525303125381\n",
      "iteration 348 current loss: 0.061246607452631\n",
      "iteration 349 current loss: 0.06000109016895294\n",
      "iteration 350 current loss: 0.06374809890985489\n",
      "iteration 351 current loss: 0.0638018250465393\n",
      "iteration 352 current loss: 0.06376159191131592\n",
      "iteration 353 current loss: 0.06017675623297691\n",
      "iteration 354 current loss: 0.06143353134393692\n",
      "iteration 355 current loss: 0.06136371195316315\n",
      "iteration 356 current loss: 0.06352482736110687\n",
      "iteration 357 current loss: 0.06344837695360184\n",
      "iteration 358 current loss: 0.060869693756103516\n",
      "iteration 359 current loss: 0.06533687561750412\n",
      "iteration 360 current loss: 0.06048785522580147\n",
      "iteration 361 current loss: 0.05858081579208374\n",
      "iteration 362 current loss: 0.05999235808849335\n",
      "iteration 363 current loss: 0.06004006788134575\n",
      "iteration 364 current loss: 0.05730190873146057\n",
      "iteration 365 current loss: 0.06378098577260971\n",
      "iteration 366 current loss: 0.06229571998119354\n",
      "iteration 367 current loss: 0.06065704673528671\n",
      "iteration 368 current loss: 0.060494083911180496\n",
      "iteration 369 current loss: 0.05889340117573738\n",
      "iteration 370 current loss: 0.06035327911376953\n",
      "iteration 371 current loss: 0.05849349871277809\n",
      "iteration 372 current loss: 0.05655045807361603\n",
      "iteration 373 current loss: 0.05823197960853577\n",
      "iteration 374 current loss: 0.06223340332508087\n",
      "iteration 375 current loss: 0.05908466875553131\n",
      "iteration 376 current loss: 0.06273595243692398\n",
      "iteration 377 current loss: 0.06017950177192688\n",
      "iteration 378 current loss: 0.059945251792669296\n",
      "iteration 379 current loss: 0.05983593687415123\n",
      "iteration 380 current loss: 0.05867289751768112\n",
      "iteration 381 current loss: 0.05715005472302437\n",
      "iteration 382 current loss: 0.06064470484852791\n",
      "iteration 383 current loss: 0.06116402521729469\n",
      "iteration 384 current loss: 0.06264529377222061\n",
      "iteration 385 current loss: 0.057435814291238785\n",
      "iteration 386 current loss: 0.061213165521621704\n",
      "iteration 387 current loss: 0.06326452642679214\n",
      "iteration 388 current loss: 0.05845332890748978\n",
      "iteration 389 current loss: 0.060348812490701675\n",
      "iteration 390 current loss: 0.06019733473658562\n",
      "iteration 391 current loss: 0.06112057715654373\n",
      "iteration 392 current loss: 0.06171194836497307\n",
      "iteration 393 current loss: 0.06174759566783905\n",
      "iteration 394 current loss: 0.06573422998189926\n",
      "iteration 395 current loss: 0.058849990367889404\n",
      "iteration 396 current loss: 0.059449054300785065\n",
      "iteration 397 current loss: 0.061380743980407715\n",
      "iteration 398 current loss: 0.060654330998659134\n",
      "iteration 399 current loss: 0.061449501663446426\n",
      "iteration 400 current loss: 0.062217067927122116\n",
      "iteration 401 current loss: 0.06075834482908249\n",
      "iteration 402 current loss: 0.06021146476268768\n",
      "iteration 403 current loss: 0.06127845123410225\n",
      "iteration 404 current loss: 0.06099969521164894\n",
      "iteration 405 current loss: 0.06236638128757477\n",
      "iteration 406 current loss: 0.06073174998164177\n",
      "iteration 407 current loss: 0.061383772641420364\n",
      "iteration 408 current loss: 0.058705877512693405\n",
      "iteration 409 current loss: 0.05924319475889206\n",
      "iteration 410 current loss: 0.06382691860198975\n",
      "\t\tEpoch 3/100 complete. Epoch loss 0.06108942823688479\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 3, Validation Loss: 0.06223780394066125\n",
      "best loss 0.06108942823688479\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.06237097084522247\n",
      "iteration 1 current loss: 0.06437056511640549\n",
      "iteration 2 current loss: 0.06163664162158966\n",
      "iteration 3 current loss: 0.06167791411280632\n",
      "iteration 4 current loss: 0.06013917550444603\n",
      "iteration 5 current loss: 0.06184441223740578\n",
      "iteration 6 current loss: 0.05651870369911194\n",
      "iteration 7 current loss: 0.059483759105205536\n",
      "iteration 8 current loss: 0.05980847030878067\n",
      "iteration 9 current loss: 0.05976993218064308\n",
      "iteration 10 current loss: 0.06307496130466461\n",
      "iteration 11 current loss: 0.06191857531666756\n",
      "iteration 12 current loss: 0.05836153402924538\n",
      "iteration 13 current loss: 0.05840802937746048\n",
      "iteration 14 current loss: 0.06265629827976227\n",
      "iteration 15 current loss: 0.06210189312696457\n",
      "iteration 16 current loss: 0.06289973109960556\n",
      "iteration 17 current loss: 0.05782148614525795\n",
      "iteration 18 current loss: 0.06174322962760925\n",
      "iteration 19 current loss: 0.05790805444121361\n",
      "iteration 20 current loss: 0.06205293908715248\n",
      "iteration 21 current loss: 0.06205347925424576\n",
      "iteration 22 current loss: 0.05967388674616814\n",
      "iteration 23 current loss: 0.0593363456428051\n",
      "iteration 24 current loss: 0.057246625423431396\n",
      "iteration 25 current loss: 0.05644823983311653\n",
      "iteration 26 current loss: 0.05849018692970276\n",
      "iteration 27 current loss: 0.060965556651353836\n",
      "iteration 28 current loss: 0.060469839721918106\n",
      "iteration 29 current loss: 0.06392621248960495\n",
      "iteration 30 current loss: 0.058154452592134476\n",
      "iteration 31 current loss: 0.06055647134780884\n",
      "iteration 32 current loss: 0.06455618143081665\n",
      "iteration 33 current loss: 0.05652957409620285\n",
      "iteration 34 current loss: 0.06136254966259003\n",
      "iteration 35 current loss: 0.06112823635339737\n",
      "iteration 36 current loss: 0.058109018951654434\n",
      "iteration 37 current loss: 0.059633586555719376\n",
      "iteration 38 current loss: 0.06344440579414368\n",
      "iteration 39 current loss: 0.06064162403345108\n",
      "iteration 40 current loss: 0.060083337128162384\n",
      "iteration 41 current loss: 0.05895628780126572\n",
      "iteration 42 current loss: 0.05659056454896927\n",
      "iteration 43 current loss: 0.059048958122730255\n",
      "iteration 44 current loss: 0.060200124979019165\n",
      "iteration 45 current loss: 0.061431001871824265\n",
      "iteration 46 current loss: 0.059123702347278595\n",
      "iteration 47 current loss: 0.06279081106185913\n",
      "iteration 48 current loss: 0.06049186363816261\n",
      "iteration 49 current loss: 0.06167464703321457\n",
      "iteration 50 current loss: 0.06285524368286133\n",
      "iteration 51 current loss: 0.062029581516981125\n",
      "iteration 52 current loss: 0.06142076477408409\n",
      "iteration 53 current loss: 0.05921642854809761\n",
      "iteration 54 current loss: 0.05972201004624367\n",
      "iteration 55 current loss: 0.06027152016758919\n",
      "iteration 56 current loss: 0.05734625086188316\n",
      "iteration 57 current loss: 0.06373091787099838\n",
      "iteration 58 current loss: 0.05728376656770706\n",
      "iteration 59 current loss: 0.060946036130189896\n",
      "iteration 60 current loss: 0.060264699161052704\n",
      "iteration 61 current loss: 0.06387906521558762\n",
      "iteration 62 current loss: 0.058337803930044174\n",
      "iteration 63 current loss: 0.05951374024152756\n",
      "iteration 64 current loss: 0.05945558100938797\n",
      "iteration 65 current loss: 0.06090938672423363\n",
      "iteration 66 current loss: 0.06097155809402466\n",
      "iteration 67 current loss: 0.05673615261912346\n",
      "iteration 68 current loss: 0.0588560625910759\n",
      "iteration 69 current loss: 0.059796273708343506\n",
      "iteration 70 current loss: 0.05974111706018448\n",
      "iteration 71 current loss: 0.060398269444704056\n",
      "iteration 72 current loss: 0.061579812318086624\n",
      "iteration 73 current loss: 0.06003125384449959\n",
      "iteration 74 current loss: 0.06056830659508705\n",
      "iteration 75 current loss: 0.06029782071709633\n",
      "iteration 76 current loss: 0.061523888260126114\n",
      "iteration 77 current loss: 0.059331562370061874\n",
      "iteration 78 current loss: 0.06389980763196945\n",
      "iteration 79 current loss: 0.06053488701581955\n",
      "iteration 80 current loss: 0.060158807784318924\n",
      "iteration 81 current loss: 0.05973515659570694\n",
      "iteration 82 current loss: 0.05990256369113922\n",
      "iteration 83 current loss: 0.06202882528305054\n",
      "iteration 84 current loss: 0.06162494421005249\n",
      "iteration 85 current loss: 0.0627412423491478\n",
      "iteration 86 current loss: 0.06132893264293671\n",
      "iteration 87 current loss: 0.060730647295713425\n",
      "iteration 88 current loss: 0.060939718037843704\n",
      "iteration 89 current loss: 0.06529311090707779\n",
      "iteration 90 current loss: 0.06091124191880226\n",
      "iteration 91 current loss: 0.06144198402762413\n",
      "iteration 92 current loss: 0.06229454278945923\n",
      "iteration 93 current loss: 0.05779983475804329\n",
      "iteration 94 current loss: 0.06534366309642792\n",
      "iteration 95 current loss: 0.06006205081939697\n",
      "iteration 96 current loss: 0.06413839757442474\n",
      "iteration 97 current loss: 0.059819452464580536\n",
      "iteration 98 current loss: 0.06011229753494263\n",
      "iteration 99 current loss: 0.06129543110728264\n",
      "iteration 100 current loss: 0.0626157596707344\n",
      "iteration 101 current loss: 0.05868835747241974\n",
      "iteration 102 current loss: 0.058913785964250565\n",
      "iteration 103 current loss: 0.05800649896264076\n",
      "iteration 104 current loss: 0.05662054941058159\n",
      "iteration 105 current loss: 0.05946137383580208\n",
      "iteration 106 current loss: 0.06327345967292786\n",
      "iteration 107 current loss: 0.060512248426675797\n",
      "iteration 108 current loss: 0.05996759608387947\n",
      "iteration 109 current loss: 0.060383204370737076\n",
      "iteration 110 current loss: 0.06328247487545013\n",
      "iteration 111 current loss: 0.06412985175848007\n",
      "iteration 112 current loss: 0.06027128919959068\n",
      "iteration 113 current loss: 0.060446880757808685\n",
      "iteration 114 current loss: 0.06052577123045921\n",
      "iteration 115 current loss: 0.05756743252277374\n",
      "iteration 116 current loss: 0.058800291270017624\n",
      "iteration 117 current loss: 0.062195200473070145\n",
      "iteration 118 current loss: 0.05878061801195145\n",
      "iteration 119 current loss: 0.06133000925183296\n",
      "iteration 120 current loss: 0.06233026459813118\n",
      "iteration 121 current loss: 0.05894218757748604\n",
      "iteration 122 current loss: 0.06386832147836685\n",
      "iteration 123 current loss: 0.05936384201049805\n",
      "iteration 124 current loss: 0.0604691319167614\n",
      "iteration 125 current loss: 0.06069576367735863\n",
      "iteration 126 current loss: 0.06739486753940582\n",
      "iteration 127 current loss: 0.058912649750709534\n",
      "iteration 128 current loss: 0.06009690836071968\n",
      "iteration 129 current loss: 0.06279764324426651\n",
      "iteration 130 current loss: 0.05917971208691597\n",
      "iteration 131 current loss: 0.059294551610946655\n",
      "iteration 132 current loss: 0.05720851942896843\n",
      "iteration 133 current loss: 0.056510284543037415\n",
      "iteration 134 current loss: 0.05664853751659393\n",
      "iteration 135 current loss: 0.0594116672873497\n",
      "iteration 136 current loss: 0.05978848412632942\n",
      "iteration 137 current loss: 0.05674528330564499\n",
      "iteration 138 current loss: 0.05878741294145584\n",
      "iteration 139 current loss: 0.059891656041145325\n",
      "iteration 140 current loss: 0.05901358649134636\n",
      "iteration 141 current loss: 0.058332107961177826\n",
      "iteration 142 current loss: 0.061754897236824036\n",
      "iteration 143 current loss: 0.0616583377122879\n",
      "iteration 144 current loss: 0.060320112854242325\n",
      "iteration 145 current loss: 0.059640828520059586\n",
      "iteration 146 current loss: 0.059993356466293335\n",
      "iteration 147 current loss: 0.058432213962078094\n",
      "iteration 148 current loss: 0.060306474566459656\n",
      "iteration 149 current loss: 0.06274828314781189\n",
      "iteration 150 current loss: 0.06118213012814522\n",
      "iteration 151 current loss: 0.05772057920694351\n",
      "iteration 152 current loss: 0.05822497978806496\n",
      "iteration 153 current loss: 0.06173725426197052\n",
      "iteration 154 current loss: 0.05994252860546112\n",
      "iteration 155 current loss: 0.058240070939064026\n",
      "iteration 156 current loss: 0.05920105427503586\n",
      "iteration 157 current loss: 0.05978545546531677\n",
      "iteration 158 current loss: 0.060152318328619\n",
      "iteration 159 current loss: 0.060284413397312164\n",
      "iteration 160 current loss: 0.06005191430449486\n",
      "iteration 161 current loss: 0.06260417401790619\n",
      "iteration 162 current loss: 0.06374596059322357\n",
      "iteration 163 current loss: 0.05892760679125786\n",
      "iteration 164 current loss: 0.059111338108778\n",
      "iteration 165 current loss: 0.06360865384340286\n",
      "iteration 166 current loss: 0.06260738521814346\n",
      "iteration 167 current loss: 0.059759143739938736\n",
      "iteration 168 current loss: 0.05941275879740715\n",
      "iteration 169 current loss: 0.060249604284763336\n",
      "iteration 170 current loss: 0.05900950729846954\n",
      "iteration 171 current loss: 0.05809616297483444\n",
      "iteration 172 current loss: 0.0591961070895195\n",
      "iteration 173 current loss: 0.061954934149980545\n",
      "iteration 174 current loss: 0.05919928103685379\n",
      "iteration 175 current loss: 0.058621134608983994\n",
      "iteration 176 current loss: 0.06430267542600632\n",
      "iteration 177 current loss: 0.061989180743694305\n",
      "iteration 178 current loss: 0.06184733286499977\n",
      "iteration 179 current loss: 0.05952106788754463\n",
      "iteration 180 current loss: 0.05904296785593033\n",
      "iteration 181 current loss: 0.061606213450431824\n",
      "iteration 182 current loss: 0.05935538187623024\n",
      "iteration 183 current loss: 0.05915277078747749\n",
      "iteration 184 current loss: 0.06200532987713814\n",
      "iteration 185 current loss: 0.060109950602054596\n",
      "iteration 186 current loss: 0.056939996778964996\n",
      "iteration 187 current loss: 0.05963674187660217\n",
      "iteration 188 current loss: 0.05907949432730675\n",
      "iteration 189 current loss: 0.05708621069788933\n",
      "iteration 190 current loss: 0.06210872158408165\n",
      "iteration 191 current loss: 0.05926204472780228\n",
      "iteration 192 current loss: 0.0595664456486702\n",
      "iteration 193 current loss: 0.05928604677319527\n",
      "iteration 194 current loss: 0.06093638017773628\n",
      "iteration 195 current loss: 0.06088586524128914\n",
      "iteration 196 current loss: 0.05869286134839058\n",
      "iteration 197 current loss: 0.06028268113732338\n",
      "iteration 198 current loss: 0.05882640555500984\n",
      "iteration 199 current loss: 0.059779901057481766\n",
      "iteration 200 current loss: 0.05974036455154419\n",
      "iteration 201 current loss: 0.059118952602148056\n",
      "iteration 202 current loss: 0.05704520270228386\n",
      "iteration 203 current loss: 0.059161532670259476\n",
      "iteration 204 current loss: 0.06223161146044731\n",
      "iteration 205 current loss: 0.05867014080286026\n",
      "iteration 206 current loss: 0.0635819062590599\n",
      "iteration 207 current loss: 0.060080960392951965\n",
      "iteration 208 current loss: 0.06182487681508064\n",
      "iteration 209 current loss: 0.06250571459531784\n",
      "iteration 210 current loss: 0.062135618180036545\n",
      "iteration 211 current loss: 0.057549163699150085\n",
      "iteration 212 current loss: 0.06117044761776924\n",
      "iteration 213 current loss: 0.05922221392393112\n",
      "iteration 214 current loss: 0.06057782098650932\n",
      "iteration 215 current loss: 0.06086842343211174\n",
      "iteration 216 current loss: 0.06098860502243042\n",
      "iteration 217 current loss: 0.06037602573633194\n",
      "iteration 218 current loss: 0.06304936856031418\n",
      "iteration 219 current loss: 0.05880536139011383\n",
      "iteration 220 current loss: 0.06513658165931702\n",
      "iteration 221 current loss: 0.0542183518409729\n",
      "iteration 222 current loss: 0.05760175362229347\n",
      "iteration 223 current loss: 0.06151898205280304\n",
      "iteration 224 current loss: 0.06120065599679947\n",
      "iteration 225 current loss: 0.05672149360179901\n",
      "iteration 226 current loss: 0.059760693460702896\n",
      "iteration 227 current loss: 0.05991140007972717\n",
      "iteration 228 current loss: 0.06169053539633751\n",
      "iteration 229 current loss: 0.05974314734339714\n",
      "iteration 230 current loss: 0.06179462745785713\n",
      "iteration 231 current loss: 0.05830369144678116\n",
      "iteration 232 current loss: 0.05680754780769348\n",
      "iteration 233 current loss: 0.06342754513025284\n",
      "iteration 234 current loss: 0.06010303646326065\n",
      "iteration 235 current loss: 0.062123846262693405\n",
      "iteration 236 current loss: 0.06106575205922127\n",
      "iteration 237 current loss: 0.058578696101903915\n",
      "iteration 238 current loss: 0.06383885443210602\n",
      "iteration 239 current loss: 0.06254448741674423\n",
      "iteration 240 current loss: 0.060692816972732544\n",
      "iteration 241 current loss: 0.06411522626876831\n",
      "iteration 242 current loss: 0.05858456343412399\n",
      "iteration 243 current loss: 0.05780499428510666\n",
      "iteration 244 current loss: 0.06274572759866714\n",
      "iteration 245 current loss: 0.06492964178323746\n",
      "iteration 246 current loss: 0.05899040028452873\n",
      "iteration 247 current loss: 0.061528004705905914\n",
      "iteration 248 current loss: 0.06106548011302948\n",
      "iteration 249 current loss: 0.05987175181508064\n",
      "iteration 250 current loss: 0.05997992306947708\n",
      "iteration 251 current loss: 0.058757465332746506\n",
      "iteration 252 current loss: 0.059760697185993195\n",
      "iteration 253 current loss: 0.05856313556432724\n",
      "iteration 254 current loss: 0.05816876143217087\n",
      "iteration 255 current loss: 0.05836351588368416\n",
      "iteration 256 current loss: 0.06257561594247818\n",
      "iteration 257 current loss: 0.059598010033369064\n",
      "iteration 258 current loss: 0.06214933842420578\n",
      "iteration 259 current loss: 0.06155941262841225\n",
      "iteration 260 current loss: 0.059091717004776\n",
      "iteration 261 current loss: 0.05569465085864067\n",
      "iteration 262 current loss: 0.06138594076037407\n",
      "iteration 263 current loss: 0.06045503169298172\n",
      "iteration 264 current loss: 0.06387963891029358\n",
      "iteration 265 current loss: 0.05759083107113838\n",
      "iteration 266 current loss: 0.06236280873417854\n",
      "iteration 267 current loss: 0.06058693304657936\n",
      "iteration 268 current loss: 0.05840311199426651\n",
      "iteration 269 current loss: 0.057546406984329224\n",
      "iteration 270 current loss: 0.06107861548662186\n",
      "iteration 271 current loss: 0.058202795684337616\n",
      "iteration 272 current loss: 0.06153998523950577\n",
      "iteration 273 current loss: 0.05929048731923103\n",
      "iteration 274 current loss: 0.06130482628941536\n",
      "iteration 275 current loss: 0.06070678308606148\n",
      "iteration 276 current loss: 0.056984834372997284\n",
      "iteration 277 current loss: 0.0564553402364254\n",
      "iteration 278 current loss: 0.061148740351200104\n",
      "iteration 279 current loss: 0.062165360897779465\n",
      "iteration 280 current loss: 0.06062187999486923\n",
      "iteration 281 current loss: 0.06064770743250847\n",
      "iteration 282 current loss: 0.061995696276426315\n",
      "iteration 283 current loss: 0.057986851781606674\n",
      "iteration 284 current loss: 0.06366905570030212\n",
      "iteration 285 current loss: 0.058716289699077606\n",
      "iteration 286 current loss: 0.058340150862932205\n",
      "iteration 287 current loss: 0.05861234664916992\n",
      "iteration 288 current loss: 0.05928073450922966\n",
      "iteration 289 current loss: 0.05659404397010803\n",
      "iteration 290 current loss: 0.062225475907325745\n",
      "iteration 291 current loss: 0.06172218173742294\n",
      "iteration 292 current loss: 0.06031922996044159\n",
      "iteration 293 current loss: 0.058364883065223694\n",
      "iteration 294 current loss: 0.060196664184331894\n",
      "iteration 295 current loss: 0.058336902409791946\n",
      "iteration 296 current loss: 0.06070033833384514\n",
      "iteration 297 current loss: 0.059235721826553345\n",
      "iteration 298 current loss: 0.062299586832523346\n",
      "iteration 299 current loss: 0.05898699909448624\n",
      "iteration 300 current loss: 0.06102157011628151\n",
      "iteration 301 current loss: 0.06259988993406296\n",
      "iteration 302 current loss: 0.0598442479968071\n",
      "iteration 303 current loss: 0.05812859162688255\n",
      "iteration 304 current loss: 0.06100334972143173\n",
      "iteration 305 current loss: 0.060788922011852264\n",
      "iteration 306 current loss: 0.05910109728574753\n",
      "iteration 307 current loss: 0.056144729256629944\n",
      "iteration 308 current loss: 0.06129623204469681\n",
      "iteration 309 current loss: 0.0578724667429924\n",
      "iteration 310 current loss: 0.05601761117577553\n",
      "iteration 311 current loss: 0.05646080523729324\n",
      "iteration 312 current loss: 0.06035682559013367\n",
      "iteration 313 current loss: 0.06020302698016167\n",
      "iteration 314 current loss: 0.06026122719049454\n",
      "iteration 315 current loss: 0.05800556764006615\n",
      "iteration 316 current loss: 0.061624906957149506\n",
      "iteration 317 current loss: 0.05720438063144684\n",
      "iteration 318 current loss: 0.059328630566596985\n",
      "iteration 319 current loss: 0.061530545353889465\n",
      "iteration 320 current loss: 0.05501687154173851\n",
      "iteration 321 current loss: 0.061107344925403595\n",
      "iteration 322 current loss: 0.05932282656431198\n",
      "iteration 323 current loss: 0.06080738827586174\n",
      "iteration 324 current loss: 0.058568961918354034\n",
      "iteration 325 current loss: 0.05680837854743004\n",
      "iteration 326 current loss: 0.059985749423503876\n",
      "iteration 327 current loss: 0.061139706522226334\n",
      "iteration 328 current loss: 0.06358450651168823\n",
      "iteration 329 current loss: 0.059846628457307816\n",
      "iteration 330 current loss: 0.05935157835483551\n",
      "iteration 331 current loss: 0.06064219772815704\n",
      "iteration 332 current loss: 0.06153923645615578\n",
      "iteration 333 current loss: 0.05601615831255913\n",
      "iteration 334 current loss: 0.06246580183506012\n",
      "iteration 335 current loss: 0.05840453505516052\n",
      "iteration 336 current loss: 0.061112165451049805\n",
      "iteration 337 current loss: 0.05600442364811897\n",
      "iteration 338 current loss: 0.061023686081171036\n",
      "iteration 339 current loss: 0.062133319675922394\n",
      "iteration 340 current loss: 0.06053968146443367\n",
      "iteration 341 current loss: 0.059230953454971313\n",
      "iteration 342 current loss: 0.05964040011167526\n",
      "iteration 343 current loss: 0.05748496577143669\n",
      "iteration 344 current loss: 0.06135457754135132\n",
      "iteration 345 current loss: 0.057922981679439545\n",
      "iteration 346 current loss: 0.062008000910282135\n",
      "iteration 347 current loss: 0.06155563145875931\n",
      "iteration 348 current loss: 0.057963281869888306\n",
      "iteration 349 current loss: 0.0612398162484169\n",
      "iteration 350 current loss: 0.06181731075048447\n",
      "iteration 351 current loss: 0.06119822710752487\n",
      "iteration 352 current loss: 0.06011057645082474\n",
      "iteration 353 current loss: 0.05940551310777664\n",
      "iteration 354 current loss: 0.06056532263755798\n",
      "iteration 355 current loss: 0.05698869004845619\n",
      "iteration 356 current loss: 0.05954817682504654\n",
      "iteration 357 current loss: 0.0567159466445446\n",
      "iteration 358 current loss: 0.06011369824409485\n",
      "iteration 359 current loss: 0.05870291590690613\n",
      "iteration 360 current loss: 0.06247924640774727\n",
      "iteration 361 current loss: 0.06089682877063751\n",
      "iteration 362 current loss: 0.05803869292140007\n",
      "iteration 363 current loss: 0.06202617287635803\n",
      "iteration 364 current loss: 0.05847221612930298\n",
      "iteration 365 current loss: 0.057020463049411774\n",
      "iteration 366 current loss: 0.06100227311253548\n",
      "iteration 367 current loss: 0.059413980692625046\n",
      "iteration 368 current loss: 0.06056644394993782\n",
      "iteration 369 current loss: 0.05781879648566246\n",
      "iteration 370 current loss: 0.059301119297742844\n",
      "iteration 371 current loss: 0.06017196550965309\n",
      "iteration 372 current loss: 0.06059841066598892\n",
      "iteration 373 current loss: 0.06336907297372818\n",
      "iteration 374 current loss: 0.05757436156272888\n",
      "iteration 375 current loss: 0.05940840020775795\n",
      "iteration 376 current loss: 0.05891282483935356\n",
      "iteration 377 current loss: 0.05522901564836502\n",
      "iteration 378 current loss: 0.061014290899038315\n",
      "iteration 379 current loss: 0.05978049337863922\n",
      "iteration 380 current loss: 0.0598502941429615\n",
      "iteration 381 current loss: 0.06480897217988968\n",
      "iteration 382 current loss: 0.062167469412088394\n",
      "iteration 383 current loss: 0.06134941428899765\n",
      "iteration 384 current loss: 0.06238642334938049\n",
      "iteration 385 current loss: 0.061224523931741714\n",
      "iteration 386 current loss: 0.06183358654379845\n",
      "iteration 387 current loss: 0.05624411255121231\n",
      "iteration 388 current loss: 0.06038268283009529\n",
      "iteration 389 current loss: 0.05793425813317299\n",
      "iteration 390 current loss: 0.05923110991716385\n",
      "iteration 391 current loss: 0.060686707496643066\n",
      "iteration 392 current loss: 0.0592895932495594\n",
      "iteration 393 current loss: 0.061601582914590836\n",
      "iteration 394 current loss: 0.059755172580480576\n",
      "iteration 395 current loss: 0.061938948929309845\n",
      "iteration 396 current loss: 0.05846218019723892\n",
      "iteration 397 current loss: 0.06015655770897865\n",
      "iteration 398 current loss: 0.0639914944767952\n",
      "iteration 399 current loss: 0.05799834057688713\n",
      "iteration 400 current loss: 0.05705072358250618\n",
      "iteration 401 current loss: 0.05699256807565689\n",
      "iteration 402 current loss: 0.05692063644528389\n",
      "iteration 403 current loss: 0.05752021446824074\n",
      "iteration 404 current loss: 0.06315256655216217\n",
      "iteration 405 current loss: 0.060976892709732056\n",
      "iteration 406 current loss: 0.05967731401324272\n",
      "iteration 407 current loss: 0.06297951191663742\n",
      "iteration 408 current loss: 0.059349626302719116\n",
      "iteration 409 current loss: 0.060781531035900116\n",
      "iteration 410 current loss: 0.06367680430412292\n",
      "\t\tEpoch 4/100 complete. Epoch loss 0.06016947644470382\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 4, Validation Loss: 0.06444767094217241\n",
      "best loss 0.06016947644470382\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.06006283313035965\n",
      "iteration 1 current loss: 0.05586604028940201\n",
      "iteration 2 current loss: 0.0584726557135582\n",
      "iteration 3 current loss: 0.05994005501270294\n",
      "iteration 4 current loss: 0.05827578902244568\n",
      "iteration 5 current loss: 0.06109300255775452\n",
      "iteration 6 current loss: 0.05790593475103378\n",
      "iteration 7 current loss: 0.061040278524160385\n",
      "iteration 8 current loss: 0.06026928499341011\n",
      "iteration 9 current loss: 0.05693464353680611\n",
      "iteration 10 current loss: 0.058643605560064316\n",
      "iteration 11 current loss: 0.06020647659897804\n",
      "iteration 12 current loss: 0.055922556668519974\n",
      "iteration 13 current loss: 0.05905570462346077\n",
      "iteration 14 current loss: 0.058465808629989624\n",
      "iteration 15 current loss: 0.05831962823867798\n",
      "iteration 16 current loss: 0.05628639832139015\n",
      "iteration 17 current loss: 0.05600319057703018\n",
      "iteration 18 current loss: 0.062071919441223145\n",
      "iteration 19 current loss: 0.06302152574062347\n",
      "iteration 20 current loss: 0.05948324501514435\n",
      "iteration 21 current loss: 0.061670396476984024\n",
      "iteration 22 current loss: 0.060935717076063156\n",
      "iteration 23 current loss: 0.05958601459860802\n",
      "iteration 24 current loss: 0.06255427747964859\n",
      "iteration 25 current loss: 0.06081222742795944\n",
      "iteration 26 current loss: 0.0552896223962307\n",
      "iteration 27 current loss: 0.05960182100534439\n",
      "iteration 28 current loss: 0.060205359011888504\n",
      "iteration 29 current loss: 0.05935671925544739\n",
      "iteration 30 current loss: 0.05916011705994606\n",
      "iteration 31 current loss: 0.06213746592402458\n",
      "iteration 32 current loss: 0.06004432588815689\n",
      "iteration 33 current loss: 0.05935103818774223\n",
      "iteration 34 current loss: 0.05800242722034454\n",
      "iteration 35 current loss: 0.05891473963856697\n",
      "iteration 36 current loss: 0.059070248156785965\n",
      "iteration 37 current loss: 0.0598062165081501\n",
      "iteration 38 current loss: 0.0628531351685524\n",
      "iteration 39 current loss: 0.05722682178020477\n",
      "iteration 40 current loss: 0.056936271488666534\n",
      "iteration 41 current loss: 0.06210619956254959\n",
      "iteration 42 current loss: 0.05688058212399483\n",
      "iteration 43 current loss: 0.05954889953136444\n",
      "iteration 44 current loss: 0.05947263538837433\n",
      "iteration 45 current loss: 0.058552246540784836\n",
      "iteration 46 current loss: 0.05975090339779854\n",
      "iteration 47 current loss: 0.06205660477280617\n",
      "iteration 48 current loss: 0.06095331907272339\n",
      "iteration 49 current loss: 0.059940189123153687\n",
      "iteration 50 current loss: 0.06133994087576866\n",
      "iteration 51 current loss: 0.058314818888902664\n",
      "iteration 52 current loss: 0.0591585636138916\n",
      "iteration 53 current loss: 0.0594622902572155\n",
      "iteration 54 current loss: 0.061379048973321915\n",
      "iteration 55 current loss: 0.06231851130723953\n",
      "iteration 56 current loss: 0.061156101524829865\n",
      "iteration 57 current loss: 0.05866284295916557\n",
      "iteration 58 current loss: 0.060041289776563644\n",
      "iteration 59 current loss: 0.05659715458750725\n",
      "iteration 60 current loss: 0.060549188405275345\n",
      "iteration 61 current loss: 0.06034643575549126\n",
      "iteration 62 current loss: 0.061931245028972626\n",
      "iteration 63 current loss: 0.06025827303528786\n",
      "iteration 64 current loss: 0.06063182279467583\n",
      "iteration 65 current loss: 0.057158831506967545\n",
      "iteration 66 current loss: 0.05937545374035835\n",
      "iteration 67 current loss: 0.05895566940307617\n",
      "iteration 68 current loss: 0.05858201906085014\n",
      "iteration 69 current loss: 0.06380943953990936\n",
      "iteration 70 current loss: 0.05751154571771622\n",
      "iteration 71 current loss: 0.061911292374134064\n",
      "iteration 72 current loss: 0.05950625613331795\n",
      "iteration 73 current loss: 0.06149548292160034\n",
      "iteration 74 current loss: 0.0629616379737854\n",
      "iteration 75 current loss: 0.06297270208597183\n",
      "iteration 76 current loss: 0.05768652260303497\n",
      "iteration 77 current loss: 0.05801732838153839\n",
      "iteration 78 current loss: 0.05810117721557617\n",
      "iteration 79 current loss: 0.06316424906253815\n",
      "iteration 80 current loss: 0.061878450214862823\n",
      "iteration 81 current loss: 0.061058904975652695\n",
      "iteration 82 current loss: 0.060185469686985016\n",
      "iteration 83 current loss: 0.058970775455236435\n",
      "iteration 84 current loss: 0.05907969921827316\n",
      "iteration 85 current loss: 0.06098826602101326\n",
      "iteration 86 current loss: 0.060112420469522476\n",
      "iteration 87 current loss: 0.05995364487171173\n",
      "iteration 88 current loss: 0.060165613889694214\n",
      "iteration 89 current loss: 0.05871136859059334\n",
      "iteration 90 current loss: 0.05828571692109108\n",
      "iteration 91 current loss: 0.060993414372205734\n",
      "iteration 92 current loss: 0.06015881896018982\n",
      "iteration 93 current loss: 0.05858127400279045\n",
      "iteration 94 current loss: 0.05987972766160965\n",
      "iteration 95 current loss: 0.05724077671766281\n",
      "iteration 96 current loss: 0.061114076524972916\n",
      "iteration 97 current loss: 0.05716792866587639\n",
      "iteration 98 current loss: 0.05671526491641998\n",
      "iteration 99 current loss: 0.0567929744720459\n",
      "iteration 100 current loss: 0.059008050709962845\n",
      "iteration 101 current loss: 0.05767679214477539\n",
      "iteration 102 current loss: 0.058849040418863297\n",
      "iteration 103 current loss: 0.05911525711417198\n",
      "iteration 104 current loss: 0.0568046011030674\n",
      "iteration 105 current loss: 0.06311780959367752\n",
      "iteration 106 current loss: 0.0599745437502861\n",
      "iteration 107 current loss: 0.060705166310071945\n",
      "iteration 108 current loss: 0.06016213074326515\n",
      "iteration 109 current loss: 0.058872759342193604\n",
      "iteration 110 current loss: 0.060444947332143784\n",
      "iteration 111 current loss: 0.06122572347521782\n",
      "iteration 112 current loss: 0.06124744936823845\n",
      "iteration 113 current loss: 0.05840253829956055\n",
      "iteration 114 current loss: 0.060754530131816864\n",
      "iteration 115 current loss: 0.05897491052746773\n",
      "iteration 116 current loss: 0.055655207484960556\n",
      "iteration 117 current loss: 0.05590987950563431\n",
      "iteration 118 current loss: 0.0591171458363533\n",
      "iteration 119 current loss: 0.05872340872883797\n",
      "iteration 120 current loss: 0.06047534942626953\n",
      "iteration 121 current loss: 0.05957953631877899\n",
      "iteration 122 current loss: 0.059360768646001816\n",
      "iteration 123 current loss: 0.06027843430638313\n",
      "iteration 124 current loss: 0.05671997368335724\n",
      "iteration 125 current loss: 0.05874299257993698\n",
      "iteration 126 current loss: 0.06055109575390816\n",
      "iteration 127 current loss: 0.05846607685089111\n",
      "iteration 128 current loss: 0.060631196945905685\n",
      "iteration 129 current loss: 0.06007245182991028\n",
      "iteration 130 current loss: 0.06158675253391266\n",
      "iteration 131 current loss: 0.058997780084609985\n",
      "iteration 132 current loss: 0.05795831233263016\n",
      "iteration 133 current loss: 0.061117757111787796\n",
      "iteration 134 current loss: 0.060363151133060455\n",
      "iteration 135 current loss: 0.05742107704281807\n",
      "iteration 136 current loss: 0.05884875729680061\n",
      "iteration 137 current loss: 0.05797248333692551\n",
      "iteration 138 current loss: 0.05841200426220894\n",
      "iteration 139 current loss: 0.060354091227054596\n",
      "iteration 140 current loss: 0.057894397526979446\n",
      "iteration 141 current loss: 0.06685703247785568\n",
      "iteration 142 current loss: 0.05966630578041077\n",
      "iteration 143 current loss: 0.05965716019272804\n",
      "iteration 144 current loss: 0.057829417288303375\n",
      "iteration 145 current loss: 0.058617476373910904\n",
      "iteration 146 current loss: 0.06085744500160217\n",
      "iteration 147 current loss: 0.059992045164108276\n",
      "iteration 148 current loss: 0.06009740009903908\n",
      "iteration 149 current loss: 0.06056711822748184\n",
      "iteration 150 current loss: 0.06037585064768791\n",
      "iteration 151 current loss: 0.06132202595472336\n",
      "iteration 152 current loss: 0.06242523714900017\n",
      "iteration 153 current loss: 0.06110868602991104\n",
      "iteration 154 current loss: 0.06056630238890648\n",
      "iteration 155 current loss: 0.05872969329357147\n",
      "iteration 156 current loss: 0.06565356254577637\n",
      "iteration 157 current loss: 0.055804185569286346\n",
      "iteration 158 current loss: 0.058170028030872345\n",
      "iteration 159 current loss: 0.05803479254245758\n",
      "iteration 160 current loss: 0.058718957006931305\n",
      "iteration 161 current loss: 0.060571834444999695\n",
      "iteration 162 current loss: 0.06070888414978981\n",
      "iteration 163 current loss: 0.05751879885792732\n",
      "iteration 164 current loss: 0.05976531282067299\n",
      "iteration 165 current loss: 0.060676075518131256\n",
      "iteration 166 current loss: 0.05953121930360794\n",
      "iteration 167 current loss: 0.05891833454370499\n",
      "iteration 168 current loss: 0.058066476136446\n",
      "iteration 169 current loss: 0.05739235877990723\n",
      "iteration 170 current loss: 0.05994574353098869\n",
      "iteration 171 current loss: 0.05957085266709328\n",
      "iteration 172 current loss: 0.05530066788196564\n",
      "iteration 173 current loss: 0.0597536563873291\n",
      "iteration 174 current loss: 0.06087877228856087\n",
      "iteration 175 current loss: 0.05590030923485756\n",
      "iteration 176 current loss: 0.058486346155405045\n",
      "iteration 177 current loss: 0.05945878475904465\n",
      "iteration 178 current loss: 0.05477603152394295\n",
      "iteration 179 current loss: 0.058375705033540726\n",
      "iteration 180 current loss: 0.0581313855946064\n",
      "iteration 181 current loss: 0.05936431139707565\n",
      "iteration 182 current loss: 0.060979656875133514\n",
      "iteration 183 current loss: 0.06125050038099289\n",
      "iteration 184 current loss: 0.05984341725707054\n",
      "iteration 185 current loss: 0.060616228729486465\n",
      "iteration 186 current loss: 0.05717773362994194\n",
      "iteration 187 current loss: 0.05916415527462959\n",
      "iteration 188 current loss: 0.06114155426621437\n",
      "iteration 189 current loss: 0.0571352057158947\n",
      "iteration 190 current loss: 0.06440071016550064\n",
      "iteration 191 current loss: 0.06447882950305939\n",
      "iteration 192 current loss: 0.061236727982759476\n",
      "iteration 193 current loss: 0.05869104340672493\n",
      "iteration 194 current loss: 0.06080930680036545\n",
      "iteration 195 current loss: 0.05754325911402702\n",
      "iteration 196 current loss: 0.06097560375928879\n",
      "iteration 197 current loss: 0.06125239282846451\n",
      "iteration 198 current loss: 0.0626760870218277\n",
      "iteration 199 current loss: 0.059377044439315796\n",
      "iteration 200 current loss: 0.057009804993867874\n",
      "iteration 201 current loss: 0.06014678254723549\n",
      "iteration 202 current loss: 0.05825432762503624\n",
      "iteration 203 current loss: 0.05922013893723488\n",
      "iteration 204 current loss: 0.05577266961336136\n",
      "iteration 205 current loss: 0.058463819324970245\n",
      "iteration 206 current loss: 0.0615057609975338\n",
      "iteration 207 current loss: 0.05902649462223053\n",
      "iteration 208 current loss: 0.06067890301346779\n",
      "iteration 209 current loss: 0.06139287352561951\n",
      "iteration 210 current loss: 0.060053110122680664\n",
      "iteration 211 current loss: 0.05854181945323944\n",
      "iteration 212 current loss: 0.05995089188218117\n",
      "iteration 213 current loss: 0.06106279417872429\n",
      "iteration 214 current loss: 0.06230087950825691\n",
      "iteration 215 current loss: 0.05807056650519371\n",
      "iteration 216 current loss: 0.05663633346557617\n",
      "iteration 217 current loss: 0.05745932087302208\n",
      "iteration 218 current loss: 0.05906074121594429\n",
      "iteration 219 current loss: 0.055377546697854996\n",
      "iteration 220 current loss: 0.06231868639588356\n",
      "iteration 221 current loss: 0.05616903305053711\n",
      "iteration 222 current loss: 0.06158839538693428\n",
      "iteration 223 current loss: 0.0606895349919796\n",
      "iteration 224 current loss: 0.06319735199213028\n",
      "iteration 225 current loss: 0.06054457649588585\n",
      "iteration 226 current loss: 0.06074109300971031\n",
      "iteration 227 current loss: 0.06022695451974869\n",
      "iteration 228 current loss: 0.058084167540073395\n",
      "iteration 229 current loss: 0.06079466640949249\n",
      "iteration 230 current loss: 0.060212161391973495\n",
      "iteration 231 current loss: 0.0597916878759861\n",
      "iteration 232 current loss: 0.06267185509204865\n",
      "iteration 233 current loss: 0.06175452843308449\n",
      "iteration 234 current loss: 0.05809221416711807\n",
      "iteration 235 current loss: 0.05895144119858742\n",
      "iteration 236 current loss: 0.05596568435430527\n",
      "iteration 237 current loss: 0.05594918504357338\n",
      "iteration 238 current loss: 0.05647609382867813\n",
      "iteration 239 current loss: 0.06291862577199936\n",
      "iteration 240 current loss: 0.06166995316743851\n",
      "iteration 241 current loss: 0.05959572643041611\n",
      "iteration 242 current loss: 0.06090009585022926\n",
      "iteration 243 current loss: 0.060773059725761414\n",
      "iteration 244 current loss: 0.06250157952308655\n",
      "iteration 245 current loss: 0.060206133872270584\n",
      "iteration 246 current loss: 0.060292378067970276\n",
      "iteration 247 current loss: 0.06013596057891846\n",
      "iteration 248 current loss: 0.058703429996967316\n",
      "iteration 249 current loss: 0.060544077306985855\n",
      "iteration 250 current loss: 0.05895858258008957\n",
      "iteration 251 current loss: 0.05709576606750488\n",
      "iteration 252 current loss: 0.05976692959666252\n",
      "iteration 253 current loss: 0.06181506812572479\n",
      "iteration 254 current loss: 0.06161439046263695\n",
      "iteration 255 current loss: 0.061791542917490005\n",
      "iteration 256 current loss: 0.05680399388074875\n",
      "iteration 257 current loss: 0.06159598007798195\n",
      "iteration 258 current loss: 0.058782126754522324\n",
      "iteration 259 current loss: 0.059693221002817154\n",
      "iteration 260 current loss: 0.05891190841794014\n",
      "iteration 261 current loss: 0.05893551558256149\n",
      "iteration 262 current loss: 0.06077282130718231\n",
      "iteration 263 current loss: 0.058433473110198975\n",
      "iteration 264 current loss: 0.05907345563173294\n",
      "iteration 265 current loss: 0.06108403950929642\n",
      "iteration 266 current loss: 0.06051792949438095\n",
      "iteration 267 current loss: 0.06145184114575386\n",
      "iteration 268 current loss: 0.06025001034140587\n",
      "iteration 269 current loss: 0.0598917193710804\n",
      "iteration 270 current loss: 0.059539273381233215\n",
      "iteration 271 current loss: 0.0592447891831398\n",
      "iteration 272 current loss: 0.06012697517871857\n",
      "iteration 273 current loss: 0.05962410569190979\n",
      "iteration 274 current loss: 0.05879512056708336\n",
      "iteration 275 current loss: 0.05710052698850632\n",
      "iteration 276 current loss: 0.06038854643702507\n",
      "iteration 277 current loss: 0.06042887642979622\n",
      "iteration 278 current loss: 0.058107901364564896\n",
      "iteration 279 current loss: 0.05675525218248367\n",
      "iteration 280 current loss: 0.059385087341070175\n",
      "iteration 281 current loss: 0.056510359048843384\n",
      "iteration 282 current loss: 0.058083392679691315\n",
      "iteration 283 current loss: 0.06094767153263092\n",
      "iteration 284 current loss: 0.057084135711193085\n",
      "iteration 285 current loss: 0.05868196114897728\n",
      "iteration 286 current loss: 0.05977873504161835\n",
      "iteration 287 current loss: 0.06299417465925217\n",
      "iteration 288 current loss: 0.06019189581274986\n",
      "iteration 289 current loss: 0.056134533137083054\n",
      "iteration 290 current loss: 0.058971889317035675\n",
      "iteration 291 current loss: 0.05679662898182869\n",
      "iteration 292 current loss: 0.05898195877671242\n",
      "iteration 293 current loss: 0.057232920080423355\n",
      "iteration 294 current loss: 0.05616753175854683\n",
      "iteration 295 current loss: 0.06398413330316544\n",
      "iteration 296 current loss: 0.06106289103627205\n",
      "iteration 297 current loss: 0.06050540506839752\n",
      "iteration 298 current loss: 0.05814383551478386\n",
      "iteration 299 current loss: 0.061244312673807144\n",
      "iteration 300 current loss: 0.059819068759679794\n",
      "iteration 301 current loss: 0.0593305304646492\n",
      "iteration 302 current loss: 0.05862852558493614\n",
      "iteration 303 current loss: 0.060921430587768555\n",
      "iteration 304 current loss: 0.06034364551305771\n",
      "iteration 305 current loss: 0.05746731534600258\n",
      "iteration 306 current loss: 0.05632380396127701\n",
      "iteration 307 current loss: 0.05706173926591873\n",
      "iteration 308 current loss: 0.06347520649433136\n",
      "iteration 309 current loss: 0.06267117708921432\n",
      "iteration 310 current loss: 0.058835312724113464\n",
      "iteration 311 current loss: 0.058511704206466675\n",
      "iteration 312 current loss: 0.05891607329249382\n",
      "iteration 313 current loss: 0.06060976907610893\n",
      "iteration 314 current loss: 0.05855296179652214\n",
      "iteration 315 current loss: 0.060198552906513214\n",
      "iteration 316 current loss: 0.05697176605463028\n",
      "iteration 317 current loss: 0.055367738008499146\n",
      "iteration 318 current loss: 0.06185378134250641\n",
      "iteration 319 current loss: 0.060573313385248184\n",
      "iteration 320 current loss: 0.06155634671449661\n",
      "iteration 321 current loss: 0.05923459306359291\n",
      "iteration 322 current loss: 0.05850964039564133\n",
      "iteration 323 current loss: 0.05821298807859421\n",
      "iteration 324 current loss: 0.059429433196783066\n",
      "iteration 325 current loss: 0.059308309108018875\n",
      "iteration 326 current loss: 0.058372657746076584\n",
      "iteration 327 current loss: 0.05666450411081314\n",
      "iteration 328 current loss: 0.05886748060584068\n",
      "iteration 329 current loss: 0.05813026428222656\n",
      "iteration 330 current loss: 0.0590100958943367\n",
      "iteration 331 current loss: 0.05836624279618263\n",
      "iteration 332 current loss: 0.056402966380119324\n",
      "iteration 333 current loss: 0.058232344686985016\n",
      "iteration 334 current loss: 0.05599392205476761\n",
      "iteration 335 current loss: 0.0573485866189003\n",
      "iteration 336 current loss: 0.06027483195066452\n",
      "iteration 337 current loss: 0.06008012220263481\n",
      "iteration 338 current loss: 0.06036264821887016\n",
      "iteration 339 current loss: 0.06002715229988098\n",
      "iteration 340 current loss: 0.057970959693193436\n",
      "iteration 341 current loss: 0.06061273813247681\n",
      "iteration 342 current loss: 0.06228193640708923\n",
      "iteration 343 current loss: 0.057689517736434937\n",
      "iteration 344 current loss: 0.0580483078956604\n",
      "iteration 345 current loss: 0.060008902102708817\n",
      "iteration 346 current loss: 0.0571325309574604\n",
      "iteration 347 current loss: 0.06013496592640877\n",
      "iteration 348 current loss: 0.0611720085144043\n",
      "iteration 349 current loss: 0.06151137128472328\n",
      "iteration 350 current loss: 0.058236103504896164\n",
      "iteration 351 current loss: 0.06095902621746063\n",
      "iteration 352 current loss: 0.060530565679073334\n",
      "iteration 353 current loss: 0.05902864411473274\n",
      "iteration 354 current loss: 0.058329418301582336\n",
      "iteration 355 current loss: 0.05929635092616081\n",
      "iteration 356 current loss: 0.058085184544324875\n",
      "iteration 357 current loss: 0.05825959891080856\n",
      "iteration 358 current loss: 0.061037857085466385\n",
      "iteration 359 current loss: 0.057748645544052124\n",
      "iteration 360 current loss: 0.06049535796046257\n",
      "iteration 361 current loss: 0.06019546464085579\n",
      "iteration 362 current loss: 0.05903772637248039\n",
      "iteration 363 current loss: 0.06042230501770973\n",
      "iteration 364 current loss: 0.058441441506147385\n",
      "iteration 365 current loss: 0.06067989394068718\n",
      "iteration 366 current loss: 0.061148788779973984\n",
      "iteration 367 current loss: 0.05859022215008736\n",
      "iteration 368 current loss: 0.06129090487957001\n",
      "iteration 369 current loss: 0.060559049248695374\n",
      "iteration 370 current loss: 0.057632703334093094\n",
      "iteration 371 current loss: 0.05950804054737091\n",
      "iteration 372 current loss: 0.06187083199620247\n",
      "iteration 373 current loss: 0.06068703904747963\n",
      "iteration 374 current loss: 0.060517508536577225\n",
      "iteration 375 current loss: 0.05583542585372925\n",
      "iteration 376 current loss: 0.05911862850189209\n",
      "iteration 377 current loss: 0.05934727564454079\n",
      "iteration 378 current loss: 0.06013784557580948\n",
      "iteration 379 current loss: 0.05958327651023865\n",
      "iteration 380 current loss: 0.05719456821680069\n",
      "iteration 381 current loss: 0.06341825425624847\n",
      "iteration 382 current loss: 0.0614069364964962\n",
      "iteration 383 current loss: 0.057755351066589355\n",
      "iteration 384 current loss: 0.06134262681007385\n",
      "iteration 385 current loss: 0.061725638806819916\n",
      "iteration 386 current loss: 0.06042168661952019\n",
      "iteration 387 current loss: 0.0582251138985157\n",
      "iteration 388 current loss: 0.059491097927093506\n",
      "iteration 389 current loss: 0.06127070635557175\n",
      "iteration 390 current loss: 0.06010649725794792\n",
      "iteration 391 current loss: 0.0595933236181736\n",
      "iteration 392 current loss: 0.05918285250663757\n",
      "iteration 393 current loss: 0.057177990674972534\n",
      "iteration 394 current loss: 0.057368531823158264\n",
      "iteration 395 current loss: 0.05565476417541504\n",
      "iteration 396 current loss: 0.059581559151411057\n",
      "iteration 397 current loss: 0.056907955557107925\n",
      "iteration 398 current loss: 0.06129462644457817\n",
      "iteration 399 current loss: 0.05708510801196098\n",
      "iteration 400 current loss: 0.059523098170757294\n",
      "iteration 401 current loss: 0.05867233872413635\n",
      "iteration 402 current loss: 0.05963994935154915\n",
      "iteration 403 current loss: 0.05786387622356415\n",
      "iteration 404 current loss: 0.059910524636507034\n",
      "iteration 405 current loss: 0.061552226543426514\n",
      "iteration 406 current loss: 0.06410545110702515\n",
      "iteration 407 current loss: 0.057135872542858124\n",
      "iteration 408 current loss: 0.06105542182922363\n",
      "iteration 409 current loss: 0.05797966942191124\n",
      "iteration 410 current loss: 0.059911344200372696\n",
      "\t\tEpoch 5/100 complete. Epoch loss 0.05951137744669786\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 5, Validation Loss: 0.06050979148130864\n",
      "best loss 0.05951137744669786\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.05965467169880867\n",
      "iteration 1 current loss: 0.059921134263277054\n",
      "iteration 2 current loss: 0.05765089392662048\n",
      "iteration 3 current loss: 0.058435674756765366\n",
      "iteration 4 current loss: 0.05808156728744507\n",
      "iteration 5 current loss: 0.05839715525507927\n",
      "iteration 6 current loss: 0.06088785454630852\n",
      "iteration 7 current loss: 0.05749804154038429\n",
      "iteration 8 current loss: 0.062241148203611374\n",
      "iteration 9 current loss: 0.05742814391851425\n",
      "iteration 10 current loss: 0.057206276804208755\n",
      "iteration 11 current loss: 0.05954376608133316\n",
      "iteration 12 current loss: 0.060185521841049194\n",
      "iteration 13 current loss: 0.060163576155900955\n",
      "iteration 14 current loss: 0.05930168926715851\n",
      "iteration 15 current loss: 0.05649246275424957\n",
      "iteration 16 current loss: 0.05847039073705673\n",
      "iteration 17 current loss: 0.05948839709162712\n",
      "iteration 18 current loss: 0.06054851785302162\n",
      "iteration 19 current loss: 0.05765470862388611\n",
      "iteration 20 current loss: 0.05619674175977707\n",
      "iteration 21 current loss: 0.058668605983257294\n",
      "iteration 22 current loss: 0.05982935056090355\n",
      "iteration 23 current loss: 0.0555565282702446\n",
      "iteration 24 current loss: 0.059999097138643265\n",
      "iteration 25 current loss: 0.058586616069078445\n",
      "iteration 26 current loss: 0.060476165264844894\n",
      "iteration 27 current loss: 0.06121932342648506\n",
      "iteration 28 current loss: 0.059269800782203674\n",
      "iteration 29 current loss: 0.06050679460167885\n",
      "iteration 30 current loss: 0.060493018478155136\n",
      "iteration 31 current loss: 0.06125805526971817\n",
      "iteration 32 current loss: 0.06160494312644005\n",
      "iteration 33 current loss: 0.059866633266210556\n",
      "iteration 34 current loss: 0.05686981603503227\n",
      "iteration 35 current loss: 0.05737346410751343\n",
      "iteration 36 current loss: 0.05827748030424118\n",
      "iteration 37 current loss: 0.06054530292749405\n",
      "iteration 38 current loss: 0.05740264803171158\n",
      "iteration 39 current loss: 0.06143617630004883\n",
      "iteration 40 current loss: 0.060431621968746185\n",
      "iteration 41 current loss: 0.05993729084730148\n",
      "iteration 42 current loss: 0.05984586477279663\n",
      "iteration 43 current loss: 0.058859772980213165\n",
      "iteration 44 current loss: 0.06257314234972\n",
      "iteration 45 current loss: 0.061232637614011765\n",
      "iteration 46 current loss: 0.060565657913684845\n",
      "iteration 47 current loss: 0.05954289436340332\n",
      "iteration 48 current loss: 0.05770568549633026\n",
      "iteration 49 current loss: 0.060349129140377045\n",
      "iteration 50 current loss: 0.06273376196622849\n",
      "iteration 51 current loss: 0.06029717996716499\n",
      "iteration 52 current loss: 0.054758284240961075\n",
      "iteration 53 current loss: 0.05869220569729805\n",
      "iteration 54 current loss: 0.061239320784807205\n",
      "iteration 55 current loss: 0.058684855699539185\n",
      "iteration 56 current loss: 0.058677613735198975\n",
      "iteration 57 current loss: 0.06140250712633133\n",
      "iteration 58 current loss: 0.05971210449934006\n",
      "iteration 59 current loss: 0.05832924693822861\n",
      "iteration 60 current loss: 0.06184186413884163\n",
      "iteration 61 current loss: 0.059349872171878815\n",
      "iteration 62 current loss: 0.05611042305827141\n",
      "iteration 63 current loss: 0.060614652931690216\n",
      "iteration 64 current loss: 0.06371180713176727\n",
      "iteration 65 current loss: 0.058896154165267944\n",
      "iteration 66 current loss: 0.05667556822299957\n",
      "iteration 67 current loss: 0.0586138479411602\n",
      "iteration 68 current loss: 0.06034255027770996\n",
      "iteration 69 current loss: 0.05762673169374466\n",
      "iteration 70 current loss: 0.05865001678466797\n",
      "iteration 71 current loss: 0.05931979417800903\n",
      "iteration 72 current loss: 0.05976497754454613\n",
      "iteration 73 current loss: 0.061213843524456024\n",
      "iteration 74 current loss: 0.06253112852573395\n",
      "iteration 75 current loss: 0.06036493554711342\n",
      "iteration 76 current loss: 0.059330813586711884\n",
      "iteration 77 current loss: 0.05747237429022789\n",
      "iteration 78 current loss: 0.05936799198389053\n",
      "iteration 79 current loss: 0.05929772928357124\n",
      "iteration 80 current loss: 0.06299632787704468\n",
      "iteration 81 current loss: 0.059452418237924576\n",
      "iteration 82 current loss: 0.062095850706100464\n",
      "iteration 83 current loss: 0.05952955037355423\n",
      "iteration 84 current loss: 0.05651780590415001\n",
      "iteration 85 current loss: 0.05565427616238594\n",
      "iteration 86 current loss: 0.05915653705596924\n",
      "iteration 87 current loss: 0.05857732519507408\n",
      "iteration 88 current loss: 0.05806862190365791\n",
      "iteration 89 current loss: 0.05649226903915405\n",
      "iteration 90 current loss: 0.058721873909235\n",
      "iteration 91 current loss: 0.06058220565319061\n",
      "iteration 92 current loss: 0.05695182830095291\n",
      "iteration 93 current loss: 0.05902506411075592\n",
      "iteration 94 current loss: 0.05894535779953003\n",
      "iteration 95 current loss: 0.05823283642530441\n",
      "iteration 96 current loss: 0.057158462703228\n",
      "iteration 97 current loss: 0.058981020003557205\n",
      "iteration 98 current loss: 0.05585762485861778\n",
      "iteration 99 current loss: 0.05806760862469673\n",
      "iteration 100 current loss: 0.06398817896842957\n",
      "iteration 101 current loss: 0.060509972274303436\n",
      "iteration 102 current loss: 0.05981385335326195\n",
      "iteration 103 current loss: 0.06166493520140648\n",
      "iteration 104 current loss: 0.060759685933589935\n",
      "iteration 105 current loss: 0.05558868870139122\n",
      "iteration 106 current loss: 0.0603116899728775\n",
      "iteration 107 current loss: 0.06075124442577362\n",
      "iteration 108 current loss: 0.05809786543250084\n",
      "iteration 109 current loss: 0.05805802345275879\n",
      "iteration 110 current loss: 0.05803540349006653\n",
      "iteration 111 current loss: 0.059815071523189545\n",
      "iteration 112 current loss: 0.06057201698422432\n",
      "iteration 113 current loss: 0.057821810245513916\n",
      "iteration 114 current loss: 0.058167096227407455\n",
      "iteration 115 current loss: 0.060315318405628204\n",
      "iteration 116 current loss: 0.05822579935193062\n",
      "iteration 117 current loss: 0.057943347841501236\n",
      "iteration 118 current loss: 0.0571281872689724\n",
      "iteration 119 current loss: 0.05943991243839264\n",
      "iteration 120 current loss: 0.05702199786901474\n",
      "iteration 121 current loss: 0.06194339320063591\n",
      "iteration 122 current loss: 0.05894755199551582\n",
      "iteration 123 current loss: 0.05940431356430054\n",
      "iteration 124 current loss: 0.056983184069395065\n",
      "iteration 125 current loss: 0.05745142698287964\n",
      "iteration 126 current loss: 0.061748769134283066\n",
      "iteration 127 current loss: 0.061600394546985626\n",
      "iteration 128 current loss: 0.06047949567437172\n",
      "iteration 129 current loss: 0.062045618891716\n",
      "iteration 130 current loss: 0.05854075402021408\n",
      "iteration 131 current loss: 0.059522416442632675\n",
      "iteration 132 current loss: 0.05413377657532692\n",
      "iteration 133 current loss: 0.057604897767305374\n",
      "iteration 134 current loss: 0.0575905367732048\n",
      "iteration 135 current loss: 0.05759565159678459\n",
      "iteration 136 current loss: 0.05642447620630264\n",
      "iteration 137 current loss: 0.05830078572034836\n",
      "iteration 138 current loss: 0.062066975980997086\n",
      "iteration 139 current loss: 0.05973929911851883\n",
      "iteration 140 current loss: 0.061981890350580215\n",
      "iteration 141 current loss: 0.05811832472681999\n",
      "iteration 142 current loss: 0.05719621852040291\n",
      "iteration 143 current loss: 0.05912762135267258\n",
      "iteration 144 current loss: 0.05749340355396271\n",
      "iteration 145 current loss: 0.056195542216300964\n",
      "iteration 146 current loss: 0.06063764542341232\n",
      "iteration 147 current loss: 0.06005753576755524\n",
      "iteration 148 current loss: 0.057663291692733765\n",
      "iteration 149 current loss: 0.059605613350868225\n",
      "iteration 150 current loss: 0.0573890283703804\n",
      "iteration 151 current loss: 0.05697179585695267\n",
      "iteration 152 current loss: 0.057780031114816666\n",
      "iteration 153 current loss: 0.05596854165196419\n",
      "iteration 154 current loss: 0.05606817454099655\n",
      "iteration 155 current loss: 0.05615655705332756\n",
      "iteration 156 current loss: 0.059893086552619934\n",
      "iteration 157 current loss: 0.05998893082141876\n",
      "iteration 158 current loss: 0.05855932831764221\n",
      "iteration 159 current loss: 0.058904338628053665\n",
      "iteration 160 current loss: 0.06028721109032631\n",
      "iteration 161 current loss: 0.05936797335743904\n",
      "iteration 162 current loss: 0.059411436319351196\n",
      "iteration 163 current loss: 0.056890539824962616\n",
      "iteration 164 current loss: 0.059429507702589035\n",
      "iteration 165 current loss: 0.059700559824705124\n",
      "iteration 166 current loss: 0.056274332106113434\n",
      "iteration 167 current loss: 0.057817187160253525\n",
      "iteration 168 current loss: 0.060831468552351\n",
      "iteration 169 current loss: 0.06150134280323982\n",
      "iteration 170 current loss: 0.05809895694255829\n",
      "iteration 171 current loss: 0.06097564846277237\n",
      "iteration 172 current loss: 0.05735916271805763\n",
      "iteration 173 current loss: 0.05699073150753975\n",
      "iteration 174 current loss: 0.05824189633131027\n",
      "iteration 175 current loss: 0.057339634746313095\n",
      "iteration 176 current loss: 0.057727791368961334\n",
      "iteration 177 current loss: 0.058217763900756836\n",
      "iteration 178 current loss: 0.05994464457035065\n",
      "iteration 179 current loss: 0.06315886229276657\n",
      "iteration 180 current loss: 0.059578992426395416\n",
      "iteration 181 current loss: 0.0556151419878006\n",
      "iteration 182 current loss: 0.05840315669775009\n",
      "iteration 183 current loss: 0.05852080136537552\n",
      "iteration 184 current loss: 0.057454753667116165\n",
      "iteration 185 current loss: 0.0585685670375824\n",
      "iteration 186 current loss: 0.06022154539823532\n",
      "iteration 187 current loss: 0.05850154533982277\n",
      "iteration 188 current loss: 0.05940741300582886\n",
      "iteration 189 current loss: 0.06118997931480408\n",
      "iteration 190 current loss: 0.06001051142811775\n",
      "iteration 191 current loss: 0.06206274405121803\n",
      "iteration 192 current loss: 0.06125713139772415\n",
      "iteration 193 current loss: 0.0591769739985466\n",
      "iteration 194 current loss: 0.060985010117292404\n",
      "iteration 195 current loss: 0.05920545384287834\n",
      "iteration 196 current loss: 0.05912419781088829\n",
      "iteration 197 current loss: 0.05930900573730469\n",
      "iteration 198 current loss: 0.060182489454746246\n",
      "iteration 199 current loss: 0.05925570800900459\n",
      "iteration 200 current loss: 0.06031601503491402\n",
      "iteration 201 current loss: 0.06410108506679535\n",
      "iteration 202 current loss: 0.06328513473272324\n",
      "iteration 203 current loss: 0.0598582923412323\n",
      "iteration 204 current loss: 0.058780644088983536\n",
      "iteration 205 current loss: 0.0591769739985466\n",
      "iteration 206 current loss: 0.060442280024290085\n",
      "iteration 207 current loss: 0.0618586540222168\n",
      "iteration 208 current loss: 0.060055553913116455\n",
      "iteration 209 current loss: 0.057489123195409775\n",
      "iteration 210 current loss: 0.060401272028684616\n",
      "iteration 211 current loss: 0.05746664106845856\n",
      "iteration 212 current loss: 0.05683904141187668\n",
      "iteration 213 current loss: 0.057654425501823425\n",
      "iteration 214 current loss: 0.06007503345608711\n",
      "iteration 215 current loss: 0.0565432608127594\n",
      "iteration 216 current loss: 0.06318715214729309\n",
      "iteration 217 current loss: 0.06008632853627205\n",
      "iteration 218 current loss: 0.056361179798841476\n",
      "iteration 219 current loss: 0.06027979403734207\n",
      "iteration 220 current loss: 0.061420317739248276\n",
      "iteration 221 current loss: 0.05595859885215759\n",
      "iteration 222 current loss: 0.06186721473932266\n",
      "iteration 223 current loss: 0.05649189651012421\n",
      "iteration 224 current loss: 0.05890848860144615\n",
      "iteration 225 current loss: 0.057389337569475174\n",
      "iteration 226 current loss: 0.0623401440680027\n",
      "iteration 227 current loss: 0.05885619670152664\n",
      "iteration 228 current loss: 0.05562815070152283\n",
      "iteration 229 current loss: 0.05948534235358238\n",
      "iteration 230 current loss: 0.0567576102912426\n",
      "iteration 231 current loss: 0.061517518013715744\n",
      "iteration 232 current loss: 0.05626403167843819\n",
      "iteration 233 current loss: 0.06233055889606476\n",
      "iteration 234 current loss: 0.0618380531668663\n",
      "iteration 235 current loss: 0.05843956395983696\n",
      "iteration 236 current loss: 0.05695034936070442\n",
      "iteration 237 current loss: 0.05868428945541382\n",
      "iteration 238 current loss: 0.05861995741724968\n",
      "iteration 239 current loss: 0.057620953768491745\n",
      "iteration 240 current loss: 0.0580422580242157\n",
      "iteration 241 current loss: 0.05650965869426727\n",
      "iteration 242 current loss: 0.059025052934885025\n",
      "iteration 243 current loss: 0.05984794721007347\n",
      "iteration 244 current loss: 0.05670488253235817\n",
      "iteration 245 current loss: 0.0588388592004776\n",
      "iteration 246 current loss: 0.058481600135564804\n",
      "iteration 247 current loss: 0.05852817744016647\n",
      "iteration 248 current loss: 0.0579397939145565\n",
      "iteration 249 current loss: 0.05983782559633255\n",
      "iteration 250 current loss: 0.059171922504901886\n",
      "iteration 251 current loss: 0.05828670412302017\n",
      "iteration 252 current loss: 0.05447192117571831\n",
      "iteration 253 current loss: 0.06245062127709389\n",
      "iteration 254 current loss: 0.057129502296447754\n",
      "iteration 255 current loss: 0.06154794245958328\n",
      "iteration 256 current loss: 0.058398954570293427\n",
      "iteration 257 current loss: 0.05980691686272621\n",
      "iteration 258 current loss: 0.05889677256345749\n",
      "iteration 259 current loss: 0.0567469485104084\n",
      "iteration 260 current loss: 0.06116752326488495\n",
      "iteration 261 current loss: 0.05874458700418472\n",
      "iteration 262 current loss: 0.0575835183262825\n",
      "iteration 263 current loss: 0.061269715428352356\n",
      "iteration 264 current loss: 0.06126683950424194\n",
      "iteration 265 current loss: 0.05844936519861221\n",
      "iteration 266 current loss: 0.05579884722828865\n",
      "iteration 267 current loss: 0.05840997397899628\n",
      "iteration 268 current loss: 0.05889829620718956\n",
      "iteration 269 current loss: 0.05934137850999832\n",
      "iteration 270 current loss: 0.060901280492544174\n",
      "iteration 271 current loss: 0.06158287450671196\n",
      "iteration 272 current loss: 0.058694638311862946\n",
      "iteration 273 current loss: 0.059113502502441406\n",
      "iteration 274 current loss: 0.05536089092493057\n",
      "iteration 275 current loss: 0.054796114563941956\n",
      "iteration 276 current loss: 0.0587204284965992\n",
      "iteration 277 current loss: 0.0571935698390007\n",
      "iteration 278 current loss: 0.058098144829273224\n",
      "iteration 279 current loss: 0.060562003403902054\n",
      "iteration 280 current loss: 0.06065986305475235\n",
      "iteration 281 current loss: 0.05882320553064346\n",
      "iteration 282 current loss: 0.06036144867539406\n",
      "iteration 283 current loss: 0.06148669496178627\n",
      "iteration 284 current loss: 0.06258710473775864\n",
      "iteration 285 current loss: 0.060292139649391174\n",
      "iteration 286 current loss: 0.05884046107530594\n",
      "iteration 287 current loss: 0.058056898415088654\n",
      "iteration 288 current loss: 0.06086752191185951\n",
      "iteration 289 current loss: 0.05998535454273224\n",
      "iteration 290 current loss: 0.059113938361406326\n",
      "iteration 291 current loss: 0.059334393590688705\n",
      "iteration 292 current loss: 0.05712198466062546\n",
      "iteration 293 current loss: 0.05836569890379906\n",
      "iteration 294 current loss: 0.05783285200595856\n",
      "iteration 295 current loss: 0.059685319662094116\n",
      "iteration 296 current loss: 0.058831945061683655\n",
      "iteration 297 current loss: 0.0576326884329319\n",
      "iteration 298 current loss: 0.05934792011976242\n",
      "iteration 299 current loss: 0.05745263397693634\n",
      "iteration 300 current loss: 0.06231489032506943\n",
      "iteration 301 current loss: 0.05832195281982422\n",
      "iteration 302 current loss: 0.058433011174201965\n",
      "iteration 303 current loss: 0.0620860792696476\n",
      "iteration 304 current loss: 0.05828294903039932\n",
      "iteration 305 current loss: 0.055827755481004715\n",
      "iteration 306 current loss: 0.0601670928299427\n",
      "iteration 307 current loss: 0.05647854506969452\n",
      "iteration 308 current loss: 0.058077048510313034\n",
      "iteration 309 current loss: 0.058335721492767334\n",
      "iteration 310 current loss: 0.06264172494411469\n",
      "iteration 311 current loss: 0.06106388941407204\n",
      "iteration 312 current loss: 0.060312170535326004\n",
      "iteration 313 current loss: 0.05842217430472374\n",
      "iteration 314 current loss: 0.058667704463005066\n",
      "iteration 315 current loss: 0.05820075795054436\n",
      "iteration 316 current loss: 0.058493632823228836\n",
      "iteration 317 current loss: 0.059214796870946884\n",
      "iteration 318 current loss: 0.059944674372673035\n",
      "iteration 319 current loss: 0.057028647512197495\n",
      "iteration 320 current loss: 0.058779969811439514\n",
      "iteration 321 current loss: 0.061426080763339996\n",
      "iteration 322 current loss: 0.057294439524412155\n",
      "iteration 323 current loss: 0.057191044092178345\n",
      "iteration 324 current loss: 0.061700861901044846\n",
      "iteration 325 current loss: 0.056061625480651855\n",
      "iteration 326 current loss: 0.06015462055802345\n",
      "iteration 327 current loss: 0.05832025781273842\n",
      "iteration 328 current loss: 0.058007340878248215\n",
      "iteration 329 current loss: 0.05471010506153107\n",
      "iteration 330 current loss: 0.055788036435842514\n",
      "iteration 331 current loss: 0.0595567487180233\n",
      "iteration 332 current loss: 0.05881645902991295\n",
      "iteration 333 current loss: 0.057026103138923645\n",
      "iteration 334 current loss: 0.05895678326487541\n",
      "iteration 335 current loss: 0.056693244725465775\n",
      "iteration 336 current loss: 0.056777022778987885\n",
      "iteration 337 current loss: 0.05956334248185158\n",
      "iteration 338 current loss: 0.05686149373650551\n",
      "iteration 339 current loss: 0.05656890571117401\n",
      "iteration 340 current loss: 0.05565134435892105\n",
      "iteration 341 current loss: 0.05901500582695007\n",
      "iteration 342 current loss: 0.05816596746444702\n",
      "iteration 343 current loss: 0.0598205104470253\n",
      "iteration 344 current loss: 0.06244984269142151\n",
      "iteration 345 current loss: 0.057453446090221405\n",
      "iteration 346 current loss: 0.058015674352645874\n",
      "iteration 347 current loss: 0.06121056526899338\n",
      "iteration 348 current loss: 0.056440360844135284\n",
      "iteration 349 current loss: 0.05881097540259361\n",
      "iteration 350 current loss: 0.0627826601266861\n",
      "iteration 351 current loss: 0.058113761246204376\n",
      "iteration 352 current loss: 0.059740666300058365\n",
      "iteration 353 current loss: 0.05935732647776604\n",
      "iteration 354 current loss: 0.05950251966714859\n",
      "iteration 355 current loss: 0.05747551470994949\n",
      "iteration 356 current loss: 0.058745939284563065\n",
      "iteration 357 current loss: 0.05937236547470093\n",
      "iteration 358 current loss: 0.06164718419313431\n",
      "iteration 359 current loss: 0.058142900466918945\n",
      "iteration 360 current loss: 0.059049349278211594\n",
      "iteration 361 current loss: 0.0595281645655632\n",
      "iteration 362 current loss: 0.05814013630151749\n",
      "iteration 363 current loss: 0.05899603292346001\n",
      "iteration 364 current loss: 0.06060298904776573\n",
      "iteration 365 current loss: 0.058568324893713\n",
      "iteration 366 current loss: 0.060468364506959915\n",
      "iteration 367 current loss: 0.06299040466547012\n",
      "iteration 368 current loss: 0.05870117247104645\n",
      "iteration 369 current loss: 0.057680487632751465\n",
      "iteration 370 current loss: 0.057467956095933914\n",
      "iteration 371 current loss: 0.05791173130273819\n",
      "iteration 372 current loss: 0.0571616068482399\n",
      "iteration 373 current loss: 0.06090060621500015\n",
      "iteration 374 current loss: 0.059090130031108856\n",
      "iteration 375 current loss: 0.05677177384495735\n",
      "iteration 376 current loss: 0.05546712130308151\n",
      "iteration 377 current loss: 0.05669601634144783\n",
      "iteration 378 current loss: 0.060505617409944534\n",
      "iteration 379 current loss: 0.06017298996448517\n",
      "iteration 380 current loss: 0.05879519134759903\n",
      "iteration 381 current loss: 0.060503121465444565\n",
      "iteration 382 current loss: 0.06214086338877678\n",
      "iteration 383 current loss: 0.05841739475727081\n",
      "iteration 384 current loss: 0.05720515549182892\n",
      "iteration 385 current loss: 0.05823806673288345\n",
      "iteration 386 current loss: 0.05926792696118355\n",
      "iteration 387 current loss: 0.06078404188156128\n",
      "iteration 388 current loss: 0.057103123515844345\n",
      "iteration 389 current loss: 0.05914803966879845\n",
      "iteration 390 current loss: 0.0576825812458992\n",
      "iteration 391 current loss: 0.056562088429927826\n",
      "iteration 392 current loss: 0.05599864199757576\n",
      "iteration 393 current loss: 0.05959586799144745\n",
      "iteration 394 current loss: 0.05478959530591965\n",
      "iteration 395 current loss: 0.06107784062623978\n",
      "iteration 396 current loss: 0.060305334627628326\n",
      "iteration 397 current loss: 0.058352284133434296\n",
      "iteration 398 current loss: 0.056872375309467316\n",
      "iteration 399 current loss: 0.06208415329456329\n",
      "iteration 400 current loss: 0.056515492498874664\n",
      "iteration 401 current loss: 0.0605221688747406\n",
      "iteration 402 current loss: 0.056800443679094315\n",
      "iteration 403 current loss: 0.058374326676130295\n",
      "iteration 404 current loss: 0.059058595448732376\n",
      "iteration 405 current loss: 0.05895394831895828\n",
      "iteration 406 current loss: 0.058666154742240906\n",
      "iteration 407 current loss: 0.05804593488574028\n",
      "iteration 408 current loss: 0.05766398087143898\n",
      "iteration 409 current loss: 0.05985591560602188\n",
      "iteration 410 current loss: 0.06045365333557129\n",
      "\t\tEpoch 6/100 complete. Epoch loss 0.058997671924295794\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 6, Validation Loss: 0.0615820549428463\n",
      "best loss 0.058997671924295794\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.0594484880566597\n",
      "iteration 1 current loss: 0.0589989498257637\n",
      "iteration 2 current loss: 0.056357234716415405\n",
      "iteration 3 current loss: 0.05877518653869629\n",
      "iteration 4 current loss: 0.05809054896235466\n",
      "iteration 5 current loss: 0.058298252522945404\n",
      "iteration 6 current loss: 0.05853285640478134\n",
      "iteration 7 current loss: 0.05817408114671707\n",
      "iteration 8 current loss: 0.05652773752808571\n",
      "iteration 9 current loss: 0.058427661657333374\n",
      "iteration 10 current loss: 0.06366846710443497\n",
      "iteration 11 current loss: 0.05881423503160477\n",
      "iteration 12 current loss: 0.06213662400841713\n",
      "iteration 13 current loss: 0.057584088295698166\n",
      "iteration 14 current loss: 0.05970003083348274\n",
      "iteration 15 current loss: 0.0613802894949913\n",
      "iteration 16 current loss: 0.05802113935351372\n",
      "iteration 17 current loss: 0.05876019224524498\n",
      "iteration 18 current loss: 0.05990252643823624\n",
      "iteration 19 current loss: 0.059420567005872726\n",
      "iteration 20 current loss: 0.05907430127263069\n",
      "iteration 21 current loss: 0.05874466150999069\n",
      "iteration 22 current loss: 0.05710586905479431\n",
      "iteration 23 current loss: 0.05730900168418884\n",
      "iteration 24 current loss: 0.05958694592118263\n",
      "iteration 25 current loss: 0.061581045389175415\n",
      "iteration 26 current loss: 0.05821752920746803\n",
      "iteration 27 current loss: 0.060327641665935516\n",
      "iteration 28 current loss: 0.057144537568092346\n",
      "iteration 29 current loss: 0.058596402406692505\n",
      "iteration 30 current loss: 0.057763416320085526\n",
      "iteration 31 current loss: 0.0633859634399414\n",
      "iteration 32 current loss: 0.06044011190533638\n",
      "iteration 33 current loss: 0.059494201093912125\n",
      "iteration 34 current loss: 0.05853361636400223\n",
      "iteration 35 current loss: 0.060224391520023346\n",
      "iteration 36 current loss: 0.05497831851243973\n",
      "iteration 37 current loss: 0.058254558593034744\n",
      "iteration 38 current loss: 0.059184666723012924\n",
      "iteration 39 current loss: 0.060123100876808167\n",
      "iteration 40 current loss: 0.05903032049536705\n",
      "iteration 41 current loss: 0.05942586064338684\n",
      "iteration 42 current loss: 0.05643019825220108\n",
      "iteration 43 current loss: 0.06061258912086487\n",
      "iteration 44 current loss: 0.05884166434407234\n",
      "iteration 45 current loss: 0.05616477131843567\n",
      "iteration 46 current loss: 0.05777665600180626\n",
      "iteration 47 current loss: 0.05936873331665993\n",
      "iteration 48 current loss: 0.054688021540641785\n",
      "iteration 49 current loss: 0.057442739605903625\n",
      "iteration 50 current loss: 0.05865289643406868\n",
      "iteration 51 current loss: 0.05930674821138382\n",
      "iteration 52 current loss: 0.0597652867436409\n",
      "iteration 53 current loss: 0.05968894436955452\n",
      "iteration 54 current loss: 0.056726761162281036\n",
      "iteration 55 current loss: 0.05785413458943367\n",
      "iteration 56 current loss: 0.059743572026491165\n",
      "iteration 57 current loss: 0.056531429290771484\n",
      "iteration 58 current loss: 0.05934304744005203\n",
      "iteration 59 current loss: 0.060381751507520676\n",
      "iteration 60 current loss: 0.05706847086548805\n",
      "iteration 61 current loss: 0.061139948666095734\n",
      "iteration 62 current loss: 0.05735200643539429\n",
      "iteration 63 current loss: 0.05951731279492378\n",
      "iteration 64 current loss: 0.05891849473118782\n",
      "iteration 65 current loss: 0.057945623993873596\n",
      "iteration 66 current loss: 0.059221260249614716\n",
      "iteration 67 current loss: 0.05911467224359512\n",
      "iteration 68 current loss: 0.059746723622083664\n",
      "iteration 69 current loss: 0.06062467768788338\n",
      "iteration 70 current loss: 0.0573863722383976\n",
      "iteration 71 current loss: 0.0573166124522686\n",
      "iteration 72 current loss: 0.059928279370069504\n",
      "iteration 73 current loss: 0.06014673039317131\n",
      "iteration 74 current loss: 0.05907510966062546\n",
      "iteration 75 current loss: 0.056511763483285904\n",
      "iteration 76 current loss: 0.05442649871110916\n",
      "iteration 77 current loss: 0.058509841561317444\n",
      "iteration 78 current loss: 0.0584990456700325\n",
      "iteration 79 current loss: 0.059849996119737625\n",
      "iteration 80 current loss: 0.05939948931336403\n",
      "iteration 81 current loss: 0.05593380331993103\n",
      "iteration 82 current loss: 0.05844348669052124\n",
      "iteration 83 current loss: 0.061373259872198105\n",
      "iteration 84 current loss: 0.05679335445165634\n",
      "iteration 85 current loss: 0.058069612830877304\n",
      "iteration 86 current loss: 0.05917937681078911\n",
      "iteration 87 current loss: 0.05936551094055176\n",
      "iteration 88 current loss: 0.05987231060862541\n",
      "iteration 89 current loss: 0.05924797058105469\n",
      "iteration 90 current loss: 0.057848162949085236\n",
      "iteration 91 current loss: 0.060416609048843384\n",
      "iteration 92 current loss: 0.05841643735766411\n",
      "iteration 93 current loss: 0.05828854814171791\n",
      "iteration 94 current loss: 0.058551907539367676\n",
      "iteration 95 current loss: 0.0556810088455677\n",
      "iteration 96 current loss: 0.05839958414435387\n",
      "iteration 97 current loss: 0.05928253382444382\n",
      "iteration 98 current loss: 0.0577877014875412\n",
      "iteration 99 current loss: 0.057663995772600174\n",
      "iteration 100 current loss: 0.056281689554452896\n",
      "iteration 101 current loss: 0.05893699452280998\n",
      "iteration 102 current loss: 0.05637959763407707\n",
      "iteration 103 current loss: 0.059628814458847046\n",
      "iteration 104 current loss: 0.060888536274433136\n",
      "iteration 105 current loss: 0.057425618171691895\n",
      "iteration 106 current loss: 0.057899538427591324\n",
      "iteration 107 current loss: 0.05974441021680832\n",
      "iteration 108 current loss: 0.05618557706475258\n",
      "iteration 109 current loss: 0.05561108887195587\n",
      "iteration 110 current loss: 0.057696033269166946\n",
      "iteration 111 current loss: 0.056871891021728516\n",
      "iteration 112 current loss: 0.056352484971284866\n",
      "iteration 113 current loss: 0.05969487875699997\n",
      "iteration 114 current loss: 0.057021595537662506\n",
      "iteration 115 current loss: 0.056747835129499435\n",
      "iteration 116 current loss: 0.05826592817902565\n",
      "iteration 117 current loss: 0.060644328594207764\n",
      "iteration 118 current loss: 0.059050288051366806\n",
      "iteration 119 current loss: 0.059308767318725586\n",
      "iteration 120 current loss: 0.059438854455947876\n",
      "iteration 121 current loss: 0.05946085974574089\n",
      "iteration 122 current loss: 0.05904128775000572\n",
      "iteration 123 current loss: 0.058580152690410614\n",
      "iteration 124 current loss: 0.059887126088142395\n",
      "iteration 125 current loss: 0.06015924736857414\n",
      "iteration 126 current loss: 0.061327483505010605\n",
      "iteration 127 current loss: 0.05571243166923523\n",
      "iteration 128 current loss: 0.0576968714594841\n",
      "iteration 129 current loss: 0.05687917768955231\n",
      "iteration 130 current loss: 0.05460018664598465\n",
      "iteration 131 current loss: 0.05403990298509598\n",
      "iteration 132 current loss: 0.060780543833971024\n",
      "iteration 133 current loss: 0.057741161435842514\n",
      "iteration 134 current loss: 0.060517195612192154\n",
      "iteration 135 current loss: 0.05794227123260498\n",
      "iteration 136 current loss: 0.055708158761262894\n",
      "iteration 137 current loss: 0.05651213601231575\n",
      "iteration 138 current loss: 0.058501459658145905\n",
      "iteration 139 current loss: 0.06111885979771614\n",
      "iteration 140 current loss: 0.06067463383078575\n",
      "iteration 141 current loss: 0.05909179896116257\n",
      "iteration 142 current loss: 0.05931252986192703\n",
      "iteration 143 current loss: 0.057134125381708145\n",
      "iteration 144 current loss: 0.05762414634227753\n",
      "iteration 145 current loss: 0.05764165148139\n",
      "iteration 146 current loss: 0.05914333462715149\n",
      "iteration 147 current loss: 0.06032051518559456\n",
      "iteration 148 current loss: 0.059920601546764374\n",
      "iteration 149 current loss: 0.057924024760723114\n",
      "iteration 150 current loss: 0.05665779858827591\n",
      "iteration 151 current loss: 0.05770792067050934\n",
      "iteration 152 current loss: 0.05887706205248833\n",
      "iteration 153 current loss: 0.05629647523164749\n",
      "iteration 154 current loss: 0.05814727023243904\n",
      "iteration 155 current loss: 0.05591744929552078\n",
      "iteration 156 current loss: 0.06028347089886665\n",
      "iteration 157 current loss: 0.060544203966856\n",
      "iteration 158 current loss: 0.05949298292398453\n",
      "iteration 159 current loss: 0.059396807104349136\n",
      "iteration 160 current loss: 0.057695552706718445\n",
      "iteration 161 current loss: 0.05725640058517456\n",
      "iteration 162 current loss: 0.057711098343133926\n",
      "iteration 163 current loss: 0.062443964183330536\n",
      "iteration 164 current loss: 0.05778105929493904\n",
      "iteration 165 current loss: 0.057644933462142944\n",
      "iteration 166 current loss: 0.05561794713139534\n",
      "iteration 167 current loss: 0.05738838016986847\n",
      "iteration 168 current loss: 0.06092630699276924\n",
      "iteration 169 current loss: 0.05921194329857826\n",
      "iteration 170 current loss: 0.060451436787843704\n",
      "iteration 171 current loss: 0.05629575252532959\n",
      "iteration 172 current loss: 0.054806143045425415\n",
      "iteration 173 current loss: 0.0591282919049263\n",
      "iteration 174 current loss: 0.06074724346399307\n",
      "iteration 175 current loss: 0.05778362974524498\n",
      "iteration 176 current loss: 0.05818858742713928\n",
      "iteration 177 current loss: 0.06156034767627716\n",
      "iteration 178 current loss: 0.05840465798974037\n",
      "iteration 179 current loss: 0.058388326317071915\n",
      "iteration 180 current loss: 0.05982103943824768\n",
      "iteration 181 current loss: 0.05818197503685951\n",
      "iteration 182 current loss: 0.056943897157907486\n",
      "iteration 183 current loss: 0.06094992160797119\n",
      "iteration 184 current loss: 0.061448898166418076\n",
      "iteration 185 current loss: 0.06003189831972122\n",
      "iteration 186 current loss: 0.058761268854141235\n",
      "iteration 187 current loss: 0.05605287477374077\n",
      "iteration 188 current loss: 0.05468902736902237\n",
      "iteration 189 current loss: 0.05653666704893112\n",
      "iteration 190 current loss: 0.0582050122320652\n",
      "iteration 191 current loss: 0.05941545590758324\n",
      "iteration 192 current loss: 0.055059853941202164\n",
      "iteration 193 current loss: 0.05998082458972931\n",
      "iteration 194 current loss: 0.05742181837558746\n",
      "iteration 195 current loss: 0.05766858160495758\n",
      "iteration 196 current loss: 0.058643192052841187\n",
      "iteration 197 current loss: 0.05903875455260277\n",
      "iteration 198 current loss: 0.058963630348443985\n",
      "iteration 199 current loss: 0.05942872166633606\n",
      "iteration 200 current loss: 0.06131898984313011\n",
      "iteration 201 current loss: 0.05590308457612991\n",
      "iteration 202 current loss: 0.05550108104944229\n",
      "iteration 203 current loss: 0.057588983327150345\n",
      "iteration 204 current loss: 0.05440711975097656\n",
      "iteration 205 current loss: 0.05807691812515259\n",
      "iteration 206 current loss: 0.05956656485795975\n",
      "iteration 207 current loss: 0.058341387659311295\n",
      "iteration 208 current loss: 0.05747615173459053\n",
      "iteration 209 current loss: 0.05219703167676926\n",
      "iteration 210 current loss: 0.061444882303476334\n",
      "iteration 211 current loss: 0.05930044874548912\n",
      "iteration 212 current loss: 0.05763079598546028\n",
      "iteration 213 current loss: 0.06034858897328377\n",
      "iteration 214 current loss: 0.05795031040906906\n",
      "iteration 215 current loss: 0.05946316942572594\n",
      "iteration 216 current loss: 0.05730550363659859\n",
      "iteration 217 current loss: 0.05717341601848602\n",
      "iteration 218 current loss: 0.05926678702235222\n",
      "iteration 219 current loss: 0.05920104309916496\n",
      "iteration 220 current loss: 0.05910152196884155\n",
      "iteration 221 current loss: 0.05346905067563057\n",
      "iteration 222 current loss: 0.05956372618675232\n",
      "iteration 223 current loss: 0.05614568665623665\n",
      "iteration 224 current loss: 0.05857762694358826\n",
      "iteration 225 current loss: 0.05850882828235626\n",
      "iteration 226 current loss: 0.05442235991358757\n",
      "iteration 227 current loss: 0.058624859899282455\n",
      "iteration 228 current loss: 0.056864526122808456\n",
      "iteration 229 current loss: 0.05648417770862579\n",
      "iteration 230 current loss: 0.057444214820861816\n",
      "iteration 231 current loss: 0.06094536557793617\n",
      "iteration 232 current loss: 0.06259258091449738\n",
      "iteration 233 current loss: 0.059227943420410156\n",
      "iteration 234 current loss: 0.05984847992658615\n",
      "iteration 235 current loss: 0.05626480281352997\n",
      "iteration 236 current loss: 0.052237749099731445\n",
      "iteration 237 current loss: 0.057755082845687866\n",
      "iteration 238 current loss: 0.06020087003707886\n",
      "iteration 239 current loss: 0.06029964238405228\n",
      "iteration 240 current loss: 0.05753358080983162\n",
      "iteration 241 current loss: 0.06003034487366676\n",
      "iteration 242 current loss: 0.05785943940281868\n",
      "iteration 243 current loss: 0.0627061277627945\n",
      "iteration 244 current loss: 0.05589975416660309\n",
      "iteration 245 current loss: 0.06086602807044983\n",
      "iteration 246 current loss: 0.05980465188622475\n",
      "iteration 247 current loss: 0.060315873473882675\n",
      "iteration 248 current loss: 0.05936911702156067\n",
      "iteration 249 current loss: 0.0576636977493763\n",
      "iteration 250 current loss: 0.05947292596101761\n",
      "iteration 251 current loss: 0.05765879154205322\n",
      "iteration 252 current loss: 0.05887805297970772\n",
      "iteration 253 current loss: 0.05758386105298996\n",
      "iteration 254 current loss: 0.05722761154174805\n",
      "iteration 255 current loss: 0.06157092750072479\n",
      "iteration 256 current loss: 0.05971049517393112\n",
      "iteration 257 current loss: 0.05965377762913704\n",
      "iteration 258 current loss: 0.057439882308244705\n",
      "iteration 259 current loss: 0.057637810707092285\n",
      "iteration 260 current loss: 0.058371402323246\n",
      "iteration 261 current loss: 0.061717528849840164\n",
      "iteration 262 current loss: 0.060731880366802216\n",
      "iteration 263 current loss: 0.06074291840195656\n",
      "iteration 264 current loss: 0.0571996234357357\n",
      "iteration 265 current loss: 0.05836837738752365\n",
      "iteration 266 current loss: 0.05681293085217476\n",
      "iteration 267 current loss: 0.06267745047807693\n",
      "iteration 268 current loss: 0.058345187455415726\n",
      "iteration 269 current loss: 0.054864756762981415\n",
      "iteration 270 current loss: 0.05798184126615524\n",
      "iteration 271 current loss: 0.057056356221437454\n",
      "iteration 272 current loss: 0.06068464741110802\n",
      "iteration 273 current loss: 0.05947534367442131\n",
      "iteration 274 current loss: 0.05698094516992569\n",
      "iteration 275 current loss: 0.057672351598739624\n",
      "iteration 276 current loss: 0.06176561489701271\n",
      "iteration 277 current loss: 0.05669211223721504\n",
      "iteration 278 current loss: 0.05963868647813797\n",
      "iteration 279 current loss: 0.05806449055671692\n",
      "iteration 280 current loss: 0.05787412449717522\n",
      "iteration 281 current loss: 0.052533362060785294\n",
      "iteration 282 current loss: 0.05722080543637276\n",
      "iteration 283 current loss: 0.058562543243169785\n",
      "iteration 284 current loss: 0.05649647116661072\n",
      "iteration 285 current loss: 0.05915892869234085\n",
      "iteration 286 current loss: 0.05611176788806915\n",
      "iteration 287 current loss: 0.05902635306119919\n",
      "iteration 288 current loss: 0.05773301422595978\n",
      "iteration 289 current loss: 0.05823256075382233\n",
      "iteration 290 current loss: 0.058866947889328\n",
      "iteration 291 current loss: 0.059760406613349915\n",
      "iteration 292 current loss: 0.05790862441062927\n",
      "iteration 293 current loss: 0.05808982253074646\n",
      "iteration 294 current loss: 0.06135913357138634\n",
      "iteration 295 current loss: 0.058989062905311584\n",
      "iteration 296 current loss: 0.05748273804783821\n",
      "iteration 297 current loss: 0.06044841185212135\n",
      "iteration 298 current loss: 0.05903381481766701\n",
      "iteration 299 current loss: 0.059260837733745575\n",
      "iteration 300 current loss: 0.061198703944683075\n",
      "iteration 301 current loss: 0.059035301208496094\n",
      "iteration 302 current loss: 0.05838475376367569\n",
      "iteration 303 current loss: 0.055369775742292404\n",
      "iteration 304 current loss: 0.05714058130979538\n",
      "iteration 305 current loss: 0.05913781002163887\n",
      "iteration 306 current loss: 0.05710143595933914\n",
      "iteration 307 current loss: 0.06104857847094536\n",
      "iteration 308 current loss: 0.05985540151596069\n",
      "iteration 309 current loss: 0.060594893991947174\n",
      "iteration 310 current loss: 0.05778469890356064\n",
      "iteration 311 current loss: 0.05545997992157936\n",
      "iteration 312 current loss: 0.05834345519542694\n",
      "iteration 313 current loss: 0.06005914881825447\n",
      "iteration 314 current loss: 0.057145003229379654\n",
      "iteration 315 current loss: 0.057996828109025955\n",
      "iteration 316 current loss: 0.06070078909397125\n",
      "iteration 317 current loss: 0.05946407839655876\n",
      "iteration 318 current loss: 0.05568787083029747\n",
      "iteration 319 current loss: 0.05679294466972351\n",
      "iteration 320 current loss: 0.058670539408922195\n",
      "iteration 321 current loss: 0.05704965442419052\n",
      "iteration 322 current loss: 0.06137474253773689\n",
      "iteration 323 current loss: 0.05807744339108467\n",
      "iteration 324 current loss: 0.05886813625693321\n",
      "iteration 325 current loss: 0.05922801420092583\n",
      "iteration 326 current loss: 0.058365195989608765\n",
      "iteration 327 current loss: 0.057489972561597824\n",
      "iteration 328 current loss: 0.058450013399124146\n",
      "iteration 329 current loss: 0.05772797018289566\n",
      "iteration 330 current loss: 0.0565008670091629\n",
      "iteration 331 current loss: 0.05845021456480026\n",
      "iteration 332 current loss: 0.059208303689956665\n",
      "iteration 333 current loss: 0.05635436996817589\n",
      "iteration 334 current loss: 0.0612570084631443\n",
      "iteration 335 current loss: 0.056788887828588486\n",
      "iteration 336 current loss: 0.05730627104640007\n",
      "iteration 337 current loss: 0.056461140513420105\n",
      "iteration 338 current loss: 0.055535051971673965\n",
      "iteration 339 current loss: 0.05841783434152603\n",
      "iteration 340 current loss: 0.05797645077109337\n",
      "iteration 341 current loss: 0.05599594488739967\n",
      "iteration 342 current loss: 0.057513728737831116\n",
      "iteration 343 current loss: 0.058307163417339325\n",
      "iteration 344 current loss: 0.05971648171544075\n",
      "iteration 345 current loss: 0.05856919288635254\n",
      "iteration 346 current loss: 0.058615364134311676\n",
      "iteration 347 current loss: 0.05639137700200081\n",
      "iteration 348 current loss: 0.057398632168769836\n",
      "iteration 349 current loss: 0.05808020383119583\n",
      "iteration 350 current loss: 0.05886383354663849\n",
      "iteration 351 current loss: 0.05773726850748062\n",
      "iteration 352 current loss: 0.06008383259177208\n",
      "iteration 353 current loss: 0.0556759312748909\n",
      "iteration 354 current loss: 0.056812647730112076\n",
      "iteration 355 current loss: 0.05800941586494446\n",
      "iteration 356 current loss: 0.054896846413612366\n",
      "iteration 357 current loss: 0.058095261454582214\n",
      "iteration 358 current loss: 0.05852743610739708\n",
      "iteration 359 current loss: 0.05956709757447243\n",
      "iteration 360 current loss: 0.05928700417280197\n",
      "iteration 361 current loss: 0.05662400647997856\n",
      "iteration 362 current loss: 0.06075577437877655\n",
      "iteration 363 current loss: 0.05897309258580208\n",
      "iteration 364 current loss: 0.058231957256793976\n",
      "iteration 365 current loss: 0.055428046733140945\n",
      "iteration 366 current loss: 0.06142406910657883\n",
      "iteration 367 current loss: 0.05717933550477028\n",
      "iteration 368 current loss: 0.05794728174805641\n",
      "iteration 369 current loss: 0.06085271015763283\n",
      "iteration 370 current loss: 0.05891348421573639\n",
      "iteration 371 current loss: 0.05685727670788765\n",
      "iteration 372 current loss: 0.05972348526120186\n",
      "iteration 373 current loss: 0.05800022929906845\n",
      "iteration 374 current loss: 0.060829274356365204\n",
      "iteration 375 current loss: 0.06039506569504738\n",
      "iteration 376 current loss: 0.05802646279335022\n",
      "iteration 377 current loss: 0.057089947164058685\n",
      "iteration 378 current loss: 0.058206215500831604\n",
      "iteration 379 current loss: 0.05898893624544144\n",
      "iteration 380 current loss: 0.057240601629018784\n",
      "iteration 381 current loss: 0.05882040783762932\n",
      "iteration 382 current loss: 0.05683567374944687\n",
      "iteration 383 current loss: 0.05994868651032448\n",
      "iteration 384 current loss: 0.05948498472571373\n",
      "iteration 385 current loss: 0.060135602951049805\n",
      "iteration 386 current loss: 0.0570988729596138\n",
      "iteration 387 current loss: 0.05724470689892769\n",
      "iteration 388 current loss: 0.05919609218835831\n",
      "iteration 389 current loss: 0.06219020485877991\n",
      "iteration 390 current loss: 0.06117113307118416\n",
      "iteration 391 current loss: 0.057084519416093826\n",
      "iteration 392 current loss: 0.057796500623226166\n",
      "iteration 393 current loss: 0.060729194432497025\n",
      "iteration 394 current loss: 0.059998027980327606\n",
      "iteration 395 current loss: 0.05401475727558136\n",
      "iteration 396 current loss: 0.06111045181751251\n",
      "iteration 397 current loss: 0.05878971144556999\n",
      "iteration 398 current loss: 0.05948491021990776\n",
      "iteration 399 current loss: 0.05740399286150932\n",
      "iteration 400 current loss: 0.05807122588157654\n",
      "iteration 401 current loss: 0.05867557227611542\n",
      "iteration 402 current loss: 0.06465832889080048\n",
      "iteration 403 current loss: 0.06034720689058304\n",
      "iteration 404 current loss: 0.058539796620607376\n",
      "iteration 405 current loss: 0.05794758349657059\n",
      "iteration 406 current loss: 0.05875074863433838\n",
      "iteration 407 current loss: 0.059738852083683014\n",
      "iteration 408 current loss: 0.05890035629272461\n",
      "iteration 409 current loss: 0.0563923642039299\n",
      "iteration 410 current loss: 0.060945525765419006\n",
      "\t\tEpoch 7/100 complete. Epoch loss 0.05847289869131253\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 7, Validation Loss: 0.0602283701300621\n",
      "best loss 0.05847289869131253\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.0599982887506485\n",
      "iteration 1 current loss: 0.058072272688150406\n",
      "iteration 2 current loss: 0.059978630393743515\n",
      "iteration 3 current loss: 0.059000588953495026\n",
      "iteration 4 current loss: 0.05546410754323006\n",
      "iteration 5 current loss: 0.059456102550029755\n",
      "iteration 6 current loss: 0.05863617733120918\n",
      "iteration 7 current loss: 0.0583086721599102\n",
      "iteration 8 current loss: 0.05790600925683975\n",
      "iteration 9 current loss: 0.058372702449560165\n",
      "iteration 10 current loss: 0.057447902858257294\n",
      "iteration 11 current loss: 0.06088113784790039\n",
      "iteration 12 current loss: 0.05936489254236221\n",
      "iteration 13 current loss: 0.058916497975587845\n",
      "iteration 14 current loss: 0.05864793062210083\n",
      "iteration 15 current loss: 0.05653528869152069\n",
      "iteration 16 current loss: 0.05821115896105766\n",
      "iteration 17 current loss: 0.05875057354569435\n",
      "iteration 18 current loss: 0.05861736088991165\n",
      "iteration 19 current loss: 0.05697242543101311\n",
      "iteration 20 current loss: 0.05729183554649353\n",
      "iteration 21 current loss: 0.05669015645980835\n",
      "iteration 22 current loss: 0.06053261086344719\n",
      "iteration 23 current loss: 0.058618705719709396\n",
      "iteration 24 current loss: 0.05813673883676529\n",
      "iteration 25 current loss: 0.056526876986026764\n",
      "iteration 26 current loss: 0.05637190118432045\n",
      "iteration 27 current loss: 0.05346065014600754\n",
      "iteration 28 current loss: 0.05806255340576172\n",
      "iteration 29 current loss: 0.057190947234630585\n",
      "iteration 30 current loss: 0.057819463312625885\n",
      "iteration 31 current loss: 0.056075289845466614\n",
      "iteration 32 current loss: 0.05901027098298073\n",
      "iteration 33 current loss: 0.06061796471476555\n",
      "iteration 34 current loss: 0.05477208271622658\n",
      "iteration 35 current loss: 0.05839018151164055\n",
      "iteration 36 current loss: 0.057779207825660706\n",
      "iteration 37 current loss: 0.06211867555975914\n",
      "iteration 38 current loss: 0.0594245009124279\n",
      "iteration 39 current loss: 0.05747848004102707\n",
      "iteration 40 current loss: 0.05735905468463898\n",
      "iteration 41 current loss: 0.05855950713157654\n",
      "iteration 42 current loss: 0.05752325803041458\n",
      "iteration 43 current loss: 0.05337187647819519\n",
      "iteration 44 current loss: 0.058393169194459915\n",
      "iteration 45 current loss: 0.06287302076816559\n",
      "iteration 46 current loss: 0.05579683557152748\n",
      "iteration 47 current loss: 0.05598197132349014\n",
      "iteration 48 current loss: 0.05706411972641945\n",
      "iteration 49 current loss: 0.053883910179138184\n",
      "iteration 50 current loss: 0.057213034480810165\n",
      "iteration 51 current loss: 0.056416768580675125\n",
      "iteration 52 current loss: 0.05744965374469757\n",
      "iteration 53 current loss: 0.05831548944115639\n",
      "iteration 54 current loss: 0.05445924401283264\n",
      "iteration 55 current loss: 0.05465446785092354\n",
      "iteration 56 current loss: 0.06110495701432228\n",
      "iteration 57 current loss: 0.05580233782529831\n",
      "iteration 58 current loss: 0.05612753704190254\n",
      "iteration 59 current loss: 0.05516031011939049\n",
      "iteration 60 current loss: 0.05483298748731613\n",
      "iteration 61 current loss: 0.0568753257393837\n",
      "iteration 62 current loss: 0.057537950575351715\n",
      "iteration 63 current loss: 0.05585484206676483\n",
      "iteration 64 current loss: 0.0575377494096756\n",
      "iteration 65 current loss: 0.06004144623875618\n",
      "iteration 66 current loss: 0.05978412181138992\n",
      "iteration 67 current loss: 0.057538844645023346\n",
      "iteration 68 current loss: 0.05786232650279999\n",
      "iteration 69 current loss: 0.059187427163124084\n",
      "iteration 70 current loss: 0.06005425751209259\n",
      "iteration 71 current loss: 0.05735792964696884\n",
      "iteration 72 current loss: 0.057319946587085724\n",
      "iteration 73 current loss: 0.06021224334836006\n",
      "iteration 74 current loss: 0.05530121177434921\n",
      "iteration 75 current loss: 0.05633660778403282\n",
      "iteration 76 current loss: 0.05591440945863724\n",
      "iteration 77 current loss: 0.05820374935865402\n",
      "iteration 78 current loss: 0.05726677179336548\n",
      "iteration 79 current loss: 0.05543065816164017\n",
      "iteration 80 current loss: 0.06241965293884277\n",
      "iteration 81 current loss: 0.05880673602223396\n",
      "iteration 82 current loss: 0.05730922520160675\n",
      "iteration 83 current loss: 0.05608142912387848\n",
      "iteration 84 current loss: 0.05863823741674423\n",
      "iteration 85 current loss: 0.056283753365278244\n",
      "iteration 86 current loss: 0.058682627975940704\n",
      "iteration 87 current loss: 0.06014168635010719\n",
      "iteration 88 current loss: 0.05729443207383156\n",
      "iteration 89 current loss: 0.05831874907016754\n",
      "iteration 90 current loss: 0.05763643980026245\n",
      "iteration 91 current loss: 0.05750812590122223\n",
      "iteration 92 current loss: 0.060440752655267715\n",
      "iteration 93 current loss: 0.06337231397628784\n",
      "iteration 94 current loss: 0.05672432482242584\n",
      "iteration 95 current loss: 0.05835229903459549\n",
      "iteration 96 current loss: 0.058491434901952744\n",
      "iteration 97 current loss: 0.06032603979110718\n",
      "iteration 98 current loss: 0.0590752512216568\n",
      "iteration 99 current loss: 0.05570567026734352\n",
      "iteration 100 current loss: 0.06033387407660484\n",
      "iteration 101 current loss: 0.055452894419431686\n",
      "iteration 102 current loss: 0.058262504637241364\n",
      "iteration 103 current loss: 0.0590609535574913\n",
      "iteration 104 current loss: 0.06016742065548897\n",
      "iteration 105 current loss: 0.05862542986869812\n",
      "iteration 106 current loss: 0.055806055665016174\n",
      "iteration 107 current loss: 0.05585611239075661\n",
      "iteration 108 current loss: 0.05594215542078018\n",
      "iteration 109 current loss: 0.061298009008169174\n",
      "iteration 110 current loss: 0.059142377227544785\n",
      "iteration 111 current loss: 0.0560806579887867\n",
      "iteration 112 current loss: 0.05825944244861603\n",
      "iteration 113 current loss: 0.05810268595814705\n",
      "iteration 114 current loss: 0.057252187281847\n",
      "iteration 115 current loss: 0.05681578814983368\n",
      "iteration 116 current loss: 0.05887262150645256\n",
      "iteration 117 current loss: 0.05712027847766876\n",
      "iteration 118 current loss: 0.060062188655138016\n",
      "iteration 119 current loss: 0.05686243996024132\n",
      "iteration 120 current loss: 0.059567857533693314\n",
      "iteration 121 current loss: 0.0571412518620491\n",
      "iteration 122 current loss: 0.05509691685438156\n",
      "iteration 123 current loss: 0.0553378164768219\n",
      "iteration 124 current loss: 0.055821649730205536\n",
      "iteration 125 current loss: 0.059450045228004456\n",
      "iteration 126 current loss: 0.05687573179602623\n",
      "iteration 127 current loss: 0.05748774856328964\n",
      "iteration 128 current loss: 0.05615486204624176\n",
      "iteration 129 current loss: 0.058915749192237854\n",
      "iteration 130 current loss: 0.05471634864807129\n",
      "iteration 131 current loss: 0.05761387199163437\n",
      "iteration 132 current loss: 0.0600278414785862\n",
      "iteration 133 current loss: 0.05123899132013321\n",
      "iteration 134 current loss: 0.05619091913104057\n",
      "iteration 135 current loss: 0.05776114761829376\n",
      "iteration 136 current loss: 0.056047745048999786\n",
      "iteration 137 current loss: 0.05791345238685608\n",
      "iteration 138 current loss: 0.05575377121567726\n",
      "iteration 139 current loss: 0.056820180267095566\n",
      "iteration 140 current loss: 0.05939814820885658\n",
      "iteration 141 current loss: 0.057042211294174194\n",
      "iteration 142 current loss: 0.06057325005531311\n",
      "iteration 143 current loss: 0.055900026112794876\n",
      "iteration 144 current loss: 0.058694545179605484\n",
      "iteration 145 current loss: 0.05824880301952362\n",
      "iteration 146 current loss: 0.05967939272522926\n",
      "iteration 147 current loss: 0.061764542013406754\n",
      "iteration 148 current loss: 0.05909499153494835\n",
      "iteration 149 current loss: 0.0564429797232151\n",
      "iteration 150 current loss: 0.05980594456195831\n",
      "iteration 151 current loss: 0.05724472552537918\n",
      "iteration 152 current loss: 0.058357831090688705\n",
      "iteration 153 current loss: 0.054845500737428665\n",
      "iteration 154 current loss: 0.0576942153275013\n",
      "iteration 155 current loss: 0.05931231752038002\n",
      "iteration 156 current loss: 0.056349657475948334\n",
      "iteration 157 current loss: 0.05588383227586746\n",
      "iteration 158 current loss: 0.05660133808851242\n",
      "iteration 159 current loss: 0.05738040432333946\n",
      "iteration 160 current loss: 0.05785495787858963\n",
      "iteration 161 current loss: 0.055654674768447876\n",
      "iteration 162 current loss: 0.058237817138433456\n",
      "iteration 163 current loss: 0.05933602154254913\n",
      "iteration 164 current loss: 0.058606475591659546\n",
      "iteration 165 current loss: 0.05928022786974907\n",
      "iteration 166 current loss: 0.055115144699811935\n",
      "iteration 167 current loss: 0.05824630334973335\n",
      "iteration 168 current loss: 0.05353258177638054\n",
      "iteration 169 current loss: 0.05903447046875954\n",
      "iteration 170 current loss: 0.060013316571712494\n",
      "iteration 171 current loss: 0.0541861429810524\n",
      "iteration 172 current loss: 0.05628303438425064\n",
      "iteration 173 current loss: 0.06209586188197136\n",
      "iteration 174 current loss: 0.0565432533621788\n",
      "iteration 175 current loss: 0.05481686815619469\n",
      "iteration 176 current loss: 0.057494014501571655\n",
      "iteration 177 current loss: 0.05616254732012749\n",
      "iteration 178 current loss: 0.05797869339585304\n",
      "iteration 179 current loss: 0.05572911724448204\n",
      "iteration 180 current loss: 0.05729425698518753\n",
      "iteration 181 current loss: 0.05762574449181557\n",
      "iteration 182 current loss: 0.0607428140938282\n",
      "iteration 183 current loss: 0.058563727885484695\n",
      "iteration 184 current loss: 0.05816951394081116\n",
      "iteration 185 current loss: 0.05872917175292969\n",
      "iteration 186 current loss: 0.05618078634142876\n",
      "iteration 187 current loss: 0.05637676268815994\n",
      "iteration 188 current loss: 0.059361089020967484\n",
      "iteration 189 current loss: 0.05663617327809334\n",
      "iteration 190 current loss: 0.06255174428224564\n",
      "iteration 191 current loss: 0.059544067829847336\n",
      "iteration 192 current loss: 0.05538239702582359\n",
      "iteration 193 current loss: 0.05830200016498566\n",
      "iteration 194 current loss: 0.05845717340707779\n",
      "iteration 195 current loss: 0.061267975717782974\n",
      "iteration 196 current loss: 0.059874434024095535\n",
      "iteration 197 current loss: 0.055622030049562454\n",
      "iteration 198 current loss: 0.05748429894447327\n",
      "iteration 199 current loss: 0.0580061711370945\n",
      "iteration 200 current loss: 0.05772407725453377\n",
      "iteration 201 current loss: 0.057986050844192505\n",
      "iteration 202 current loss: 0.058123357594013214\n",
      "iteration 203 current loss: 0.05779433250427246\n",
      "iteration 204 current loss: 0.05858786776661873\n",
      "iteration 205 current loss: 0.055668264627456665\n",
      "iteration 206 current loss: 0.06130435690283775\n",
      "iteration 207 current loss: 0.06005677953362465\n",
      "iteration 208 current loss: 0.05693209916353226\n",
      "iteration 209 current loss: 0.05863921344280243\n",
      "iteration 210 current loss: 0.059085097163915634\n",
      "iteration 211 current loss: 0.06116610765457153\n",
      "iteration 212 current loss: 0.0590360164642334\n",
      "iteration 213 current loss: 0.05620083585381508\n",
      "iteration 214 current loss: 0.05685701221227646\n",
      "iteration 215 current loss: 0.05601047724485397\n",
      "iteration 216 current loss: 0.0611395537853241\n",
      "iteration 217 current loss: 0.056520119309425354\n",
      "iteration 218 current loss: 0.05737023055553436\n",
      "iteration 219 current loss: 0.05847890302538872\n",
      "iteration 220 current loss: 0.06085474416613579\n",
      "iteration 221 current loss: 0.05910606309771538\n",
      "iteration 222 current loss: 0.059072405099868774\n",
      "iteration 223 current loss: 0.05917661637067795\n",
      "iteration 224 current loss: 0.05917804315686226\n",
      "iteration 225 current loss: 0.05897689610719681\n",
      "iteration 226 current loss: 0.05904807895421982\n",
      "iteration 227 current loss: 0.055912796407938004\n",
      "iteration 228 current loss: 0.058350853621959686\n",
      "iteration 229 current loss: 0.058356523513793945\n",
      "iteration 230 current loss: 0.05492353439331055\n",
      "iteration 231 current loss: 0.06074029579758644\n",
      "iteration 232 current loss: 0.05923967435956001\n",
      "iteration 233 current loss: 0.05991159752011299\n",
      "iteration 234 current loss: 0.05593731999397278\n",
      "iteration 235 current loss: 0.06377783417701721\n",
      "iteration 236 current loss: 0.058391183614730835\n",
      "iteration 237 current loss: 0.05983822047710419\n",
      "iteration 238 current loss: 0.05594741925597191\n",
      "iteration 239 current loss: 0.05862138792872429\n",
      "iteration 240 current loss: 0.06023888289928436\n",
      "iteration 241 current loss: 0.0567820705473423\n",
      "iteration 242 current loss: 0.05940673500299454\n",
      "iteration 243 current loss: 0.0584777295589447\n",
      "iteration 244 current loss: 0.0573577880859375\n",
      "iteration 245 current loss: 0.05946618691086769\n",
      "iteration 246 current loss: 0.05920836701989174\n",
      "iteration 247 current loss: 0.05905584990978241\n",
      "iteration 248 current loss: 0.058632172644138336\n",
      "iteration 249 current loss: 0.06064634770154953\n",
      "iteration 250 current loss: 0.058482587337493896\n",
      "iteration 251 current loss: 0.05907919257879257\n",
      "iteration 252 current loss: 0.060406673699617386\n",
      "iteration 253 current loss: 0.055017855018377304\n",
      "iteration 254 current loss: 0.060061074793338776\n",
      "iteration 255 current loss: 0.05705731362104416\n",
      "iteration 256 current loss: 0.05794339254498482\n",
      "iteration 257 current loss: 0.05918903648853302\n",
      "iteration 258 current loss: 0.056784868240356445\n",
      "iteration 259 current loss: 0.05966367572546005\n",
      "iteration 260 current loss: 0.057676393538713455\n",
      "iteration 261 current loss: 0.05458628758788109\n",
      "iteration 262 current loss: 0.056578148156404495\n",
      "iteration 263 current loss: 0.05791785195469856\n",
      "iteration 264 current loss: 0.060043815523386\n",
      "iteration 265 current loss: 0.06097175553441048\n",
      "iteration 266 current loss: 0.0578138642013073\n",
      "iteration 267 current loss: 0.05750067159533501\n",
      "iteration 268 current loss: 0.05852597951889038\n",
      "iteration 269 current loss: 0.05735942721366882\n",
      "iteration 270 current loss: 0.0594518668949604\n",
      "iteration 271 current loss: 0.059890586882829666\n",
      "iteration 272 current loss: 0.059190113097429276\n",
      "iteration 273 current loss: 0.05563467741012573\n",
      "iteration 274 current loss: 0.05921897664666176\n",
      "iteration 275 current loss: 0.05739518627524376\n",
      "iteration 276 current loss: 0.057544268667697906\n",
      "iteration 277 current loss: 0.058615975081920624\n",
      "iteration 278 current loss: 0.05924488976597786\n",
      "iteration 279 current loss: 0.056118883192539215\n",
      "iteration 280 current loss: 0.0557011179625988\n",
      "iteration 281 current loss: 0.0557590052485466\n",
      "iteration 282 current loss: 0.05533003434538841\n",
      "iteration 283 current loss: 0.05903332680463791\n",
      "iteration 284 current loss: 0.05862700566649437\n",
      "iteration 285 current loss: 0.06219589337706566\n",
      "iteration 286 current loss: 0.05802294239401817\n",
      "iteration 287 current loss: 0.05845014750957489\n",
      "iteration 288 current loss: 0.059387385845184326\n",
      "iteration 289 current loss: 0.056830037385225296\n",
      "iteration 290 current loss: 0.054445136338472366\n",
      "iteration 291 current loss: 0.05591418221592903\n",
      "iteration 292 current loss: 0.05815131589770317\n",
      "iteration 293 current loss: 0.058372754603624344\n",
      "iteration 294 current loss: 0.057477544993162155\n",
      "iteration 295 current loss: 0.057119157165288925\n",
      "iteration 296 current loss: 0.05475606769323349\n",
      "iteration 297 current loss: 0.05953439697623253\n",
      "iteration 298 current loss: 0.05644042789936066\n",
      "iteration 299 current loss: 0.05904398113489151\n",
      "iteration 300 current loss: 0.05747325345873833\n",
      "iteration 301 current loss: 0.05599603429436684\n",
      "iteration 302 current loss: 0.06083047389984131\n",
      "iteration 303 current loss: 0.057630158960819244\n",
      "iteration 304 current loss: 0.05935804918408394\n",
      "iteration 305 current loss: 0.05971783399581909\n",
      "iteration 306 current loss: 0.056749943643808365\n",
      "iteration 307 current loss: 0.056459903717041016\n",
      "iteration 308 current loss: 0.05798427015542984\n",
      "iteration 309 current loss: 0.057104844599962234\n",
      "iteration 310 current loss: 0.06122127175331116\n",
      "iteration 311 current loss: 0.058274950832128525\n",
      "iteration 312 current loss: 0.054150450974702835\n",
      "iteration 313 current loss: 0.05771449953317642\n",
      "iteration 314 current loss: 0.058474719524383545\n",
      "iteration 315 current loss: 0.058207858353853226\n",
      "iteration 316 current loss: 0.058068130165338516\n",
      "iteration 317 current loss: 0.057389035820961\n",
      "iteration 318 current loss: 0.053601089864969254\n",
      "iteration 319 current loss: 0.058278098702430725\n",
      "iteration 320 current loss: 0.05886401608586311\n",
      "iteration 321 current loss: 0.06016192212700844\n",
      "iteration 322 current loss: 0.05731792002916336\n",
      "iteration 323 current loss: 0.056872930377721786\n",
      "iteration 324 current loss: 0.05790333449840546\n",
      "iteration 325 current loss: 0.05837380513548851\n",
      "iteration 326 current loss: 0.059651508927345276\n",
      "iteration 327 current loss: 0.05909692868590355\n",
      "iteration 328 current loss: 0.058406997472047806\n",
      "iteration 329 current loss: 0.058338042348623276\n",
      "iteration 330 current loss: 0.054992035031318665\n",
      "iteration 331 current loss: 0.060555584728717804\n",
      "iteration 332 current loss: 0.05852873995900154\n",
      "iteration 333 current loss: 0.058722615242004395\n",
      "iteration 334 current loss: 0.058339718729257584\n",
      "iteration 335 current loss: 0.05599326640367508\n",
      "iteration 336 current loss: 0.061973512172698975\n",
      "iteration 337 current loss: 0.057063739746809006\n",
      "iteration 338 current loss: 0.05912898853421211\n",
      "iteration 339 current loss: 0.05959923192858696\n",
      "iteration 340 current loss: 0.05584368109703064\n",
      "iteration 341 current loss: 0.05784546211361885\n",
      "iteration 342 current loss: 0.060423657298088074\n",
      "iteration 343 current loss: 0.05982949957251549\n",
      "iteration 344 current loss: 0.05619656294584274\n",
      "iteration 345 current loss: 0.0540950670838356\n",
      "iteration 346 current loss: 0.05524547025561333\n",
      "iteration 347 current loss: 0.05899215489625931\n",
      "iteration 348 current loss: 0.06165608763694763\n",
      "iteration 349 current loss: 0.057191699743270874\n",
      "iteration 350 current loss: 0.054700128734111786\n",
      "iteration 351 current loss: 0.0566103532910347\n",
      "iteration 352 current loss: 0.059989456087350845\n",
      "iteration 353 current loss: 0.0554458349943161\n",
      "iteration 354 current loss: 0.06004840508103371\n",
      "iteration 355 current loss: 0.05768156051635742\n",
      "iteration 356 current loss: 0.057403210550546646\n",
      "iteration 357 current loss: 0.058552052825689316\n",
      "iteration 358 current loss: 0.0551886111497879\n",
      "iteration 359 current loss: 0.05803019180893898\n",
      "iteration 360 current loss: 0.05675315111875534\n",
      "iteration 361 current loss: 0.05295705422759056\n",
      "iteration 362 current loss: 0.058472394943237305\n",
      "iteration 363 current loss: 0.06176195293664932\n",
      "iteration 364 current loss: 0.058539558202028275\n",
      "iteration 365 current loss: 0.059138014912605286\n",
      "iteration 366 current loss: 0.0577201284468174\n",
      "iteration 367 current loss: 0.057561203837394714\n",
      "iteration 368 current loss: 0.05775127559900284\n",
      "iteration 369 current loss: 0.056571561843156815\n",
      "iteration 370 current loss: 0.05586773529648781\n",
      "iteration 371 current loss: 0.059563037008047104\n",
      "iteration 372 current loss: 0.059836629778146744\n",
      "iteration 373 current loss: 0.05800213664770126\n",
      "iteration 374 current loss: 0.05927117168903351\n",
      "iteration 375 current loss: 0.06090077757835388\n",
      "iteration 376 current loss: 0.059671834111213684\n",
      "iteration 377 current loss: 0.05742766335606575\n",
      "iteration 378 current loss: 0.05901780724525452\n",
      "iteration 379 current loss: 0.058124177157878876\n",
      "iteration 380 current loss: 0.056832440197467804\n",
      "iteration 381 current loss: 0.05807539448142052\n",
      "iteration 382 current loss: 0.05705002695322037\n",
      "iteration 383 current loss: 0.05835539475083351\n",
      "iteration 384 current loss: 0.054812829941511154\n",
      "iteration 385 current loss: 0.059423208236694336\n",
      "iteration 386 current loss: 0.061888765543699265\n",
      "iteration 387 current loss: 0.05723056197166443\n",
      "iteration 388 current loss: 0.058349668979644775\n",
      "iteration 389 current loss: 0.05916271358728409\n",
      "iteration 390 current loss: 0.057475555688142776\n",
      "iteration 391 current loss: 0.0567585825920105\n",
      "iteration 392 current loss: 0.06141986697912216\n",
      "iteration 393 current loss: 0.05885341763496399\n",
      "iteration 394 current loss: 0.05894332006573677\n",
      "iteration 395 current loss: 0.057304497808218\n",
      "iteration 396 current loss: 0.058823347091674805\n",
      "iteration 397 current loss: 0.05484249070286751\n",
      "iteration 398 current loss: 0.05951521173119545\n",
      "iteration 399 current loss: 0.0600493922829628\n",
      "iteration 400 current loss: 0.05559474602341652\n",
      "iteration 401 current loss: 0.06288979202508926\n",
      "iteration 402 current loss: 0.059347204864025116\n",
      "iteration 403 current loss: 0.058906715363264084\n",
      "iteration 404 current loss: 0.05673033744096756\n",
      "iteration 405 current loss: 0.05666037276387215\n",
      "iteration 406 current loss: 0.05704788118600845\n",
      "iteration 407 current loss: 0.058626677840948105\n",
      "iteration 408 current loss: 0.05786252021789551\n",
      "iteration 409 current loss: 0.05800371244549751\n",
      "iteration 410 current loss: 0.06636227667331696\n",
      "\t\tEpoch 8/100 complete. Epoch loss 0.057985390235110214\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 8, Validation Loss: 0.05941306205932051\n",
      "best loss 0.057985390235110214\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.05735249072313309\n",
      "iteration 1 current loss: 0.05810769274830818\n",
      "iteration 2 current loss: 0.060858167707920074\n",
      "iteration 3 current loss: 0.05853057652711868\n",
      "iteration 4 current loss: 0.05612954869866371\n",
      "iteration 5 current loss: 0.0581941157579422\n",
      "iteration 6 current loss: 0.06061841920018196\n",
      "iteration 7 current loss: 0.057782553136348724\n",
      "iteration 8 current loss: 0.060269586741924286\n",
      "iteration 9 current loss: 0.05655992776155472\n",
      "iteration 10 current loss: 0.05783027410507202\n",
      "iteration 11 current loss: 0.05626210197806358\n",
      "iteration 12 current loss: 0.058172620832920074\n",
      "iteration 13 current loss: 0.0599282942712307\n",
      "iteration 14 current loss: 0.05955246835947037\n",
      "iteration 15 current loss: 0.05899940803647041\n",
      "iteration 16 current loss: 0.05494895577430725\n",
      "iteration 17 current loss: 0.05690618231892586\n",
      "iteration 18 current loss: 0.05704259127378464\n",
      "iteration 19 current loss: 0.05403515323996544\n",
      "iteration 20 current loss: 0.05951056256890297\n",
      "iteration 21 current loss: 0.057996608316898346\n",
      "iteration 22 current loss: 0.05775255709886551\n",
      "iteration 23 current loss: 0.06204545497894287\n",
      "iteration 24 current loss: 0.059408921748399734\n",
      "iteration 25 current loss: 0.057629216462373734\n",
      "iteration 26 current loss: 0.058390796184539795\n",
      "iteration 27 current loss: 0.05501547455787659\n",
      "iteration 28 current loss: 0.055138103663921356\n",
      "iteration 29 current loss: 0.05675102025270462\n",
      "iteration 30 current loss: 0.05623112991452217\n",
      "iteration 31 current loss: 0.0596376433968544\n",
      "iteration 32 current loss: 0.05644475668668747\n",
      "iteration 33 current loss: 0.06317625194787979\n",
      "iteration 34 current loss: 0.06011098995804787\n",
      "iteration 35 current loss: 0.058591678738594055\n",
      "iteration 36 current loss: 0.059859663248062134\n",
      "iteration 37 current loss: 0.0593566969037056\n",
      "iteration 38 current loss: 0.058144111186265945\n",
      "iteration 39 current loss: 0.058568015694618225\n",
      "iteration 40 current loss: 0.055321190506219864\n",
      "iteration 41 current loss: 0.060481760650873184\n",
      "iteration 42 current loss: 0.0545179583132267\n",
      "iteration 43 current loss: 0.05657560005784035\n",
      "iteration 44 current loss: 0.05795685201883316\n",
      "iteration 45 current loss: 0.05778195336461067\n",
      "iteration 46 current loss: 0.0550399087369442\n",
      "iteration 47 current loss: 0.058400169014930725\n",
      "iteration 48 current loss: 0.0593922920525074\n",
      "iteration 49 current loss: 0.05548551306128502\n",
      "iteration 50 current loss: 0.055093374103307724\n",
      "iteration 51 current loss: 0.05668174475431442\n",
      "iteration 52 current loss: 0.05931198224425316\n",
      "iteration 53 current loss: 0.05819162726402283\n",
      "iteration 54 current loss: 0.058313608169555664\n",
      "iteration 55 current loss: 0.057480890303850174\n",
      "iteration 56 current loss: 0.05802663788199425\n",
      "iteration 57 current loss: 0.05953099951148033\n",
      "iteration 58 current loss: 0.05892590433359146\n",
      "iteration 59 current loss: 0.05900471657514572\n",
      "iteration 60 current loss: 0.06219310313463211\n",
      "iteration 61 current loss: 0.05803988128900528\n",
      "iteration 62 current loss: 0.056749630719423294\n",
      "iteration 63 current loss: 0.05516732484102249\n",
      "iteration 64 current loss: 0.05796279013156891\n",
      "iteration 65 current loss: 0.056579478085041046\n",
      "iteration 66 current loss: 0.056054115295410156\n",
      "iteration 67 current loss: 0.05808829516172409\n",
      "iteration 68 current loss: 0.05837278068065643\n",
      "iteration 69 current loss: 0.05521351099014282\n",
      "iteration 70 current loss: 0.05812660977244377\n",
      "iteration 71 current loss: 0.05253386124968529\n",
      "iteration 72 current loss: 0.05824435129761696\n",
      "iteration 73 current loss: 0.05535247549414635\n",
      "iteration 74 current loss: 0.057375576347112656\n",
      "iteration 75 current loss: 0.05719415098428726\n",
      "iteration 76 current loss: 0.05636505410075188\n",
      "iteration 77 current loss: 0.056416694074869156\n",
      "iteration 78 current loss: 0.058069340884685516\n",
      "iteration 79 current loss: 0.05395860970020294\n",
      "iteration 80 current loss: 0.05587167292833328\n",
      "iteration 81 current loss: 0.0575244165956974\n",
      "iteration 82 current loss: 0.06036028265953064\n",
      "iteration 83 current loss: 0.056368254125118256\n",
      "iteration 84 current loss: 0.05715533718466759\n",
      "iteration 85 current loss: 0.05900442972779274\n",
      "iteration 86 current loss: 0.05748380720615387\n",
      "iteration 87 current loss: 0.056641560047864914\n",
      "iteration 88 current loss: 0.05713848024606705\n",
      "iteration 89 current loss: 0.05425836145877838\n",
      "iteration 90 current loss: 0.05629957094788551\n",
      "iteration 91 current loss: 0.05649612098932266\n",
      "iteration 92 current loss: 0.056626103818416595\n",
      "iteration 93 current loss: 0.05779619514942169\n",
      "iteration 94 current loss: 0.05926338955760002\n",
      "iteration 95 current loss: 0.05654119327664375\n",
      "iteration 96 current loss: 0.05716314539313316\n",
      "iteration 97 current loss: 0.05970295891165733\n",
      "iteration 98 current loss: 0.05605405569076538\n",
      "iteration 99 current loss: 0.06030314415693283\n",
      "iteration 100 current loss: 0.05841383337974548\n",
      "iteration 101 current loss: 0.058599136769771576\n",
      "iteration 102 current loss: 0.058372948318719864\n",
      "iteration 103 current loss: 0.0573234036564827\n",
      "iteration 104 current loss: 0.053057167679071426\n",
      "iteration 105 current loss: 0.05885665863752365\n",
      "iteration 106 current loss: 0.05886628478765488\n",
      "iteration 107 current loss: 0.05815892666578293\n",
      "iteration 108 current loss: 0.05835263058543205\n",
      "iteration 109 current loss: 0.05541764572262764\n",
      "iteration 110 current loss: 0.053501278162002563\n",
      "iteration 111 current loss: 0.054673098027706146\n",
      "iteration 112 current loss: 0.05923027545213699\n",
      "iteration 113 current loss: 0.05833638086915016\n",
      "iteration 114 current loss: 0.059498902410268784\n",
      "iteration 115 current loss: 0.05676380917429924\n",
      "iteration 116 current loss: 0.05638451874256134\n",
      "iteration 117 current loss: 0.057734813541173935\n",
      "iteration 118 current loss: 0.05787781625986099\n",
      "iteration 119 current loss: 0.057384755462408066\n",
      "iteration 120 current loss: 0.055908434092998505\n",
      "iteration 121 current loss: 0.05860636755824089\n",
      "iteration 122 current loss: 0.05978831276297569\n",
      "iteration 123 current loss: 0.059370256960392\n",
      "iteration 124 current loss: 0.05679792910814285\n",
      "iteration 125 current loss: 0.060347821563482285\n",
      "iteration 126 current loss: 0.05499342828989029\n",
      "iteration 127 current loss: 0.054898668080568314\n",
      "iteration 128 current loss: 0.057262588292360306\n",
      "iteration 129 current loss: 0.05764959380030632\n",
      "iteration 130 current loss: 0.0537104532122612\n",
      "iteration 131 current loss: 0.05829071253538132\n",
      "iteration 132 current loss: 0.06016483157873154\n",
      "iteration 133 current loss: 0.05599287897348404\n",
      "iteration 134 current loss: 0.05788413807749748\n",
      "iteration 135 current loss: 0.0573059543967247\n",
      "iteration 136 current loss: 0.05668963864445686\n",
      "iteration 137 current loss: 0.05685213953256607\n",
      "iteration 138 current loss: 0.05490264669060707\n",
      "iteration 139 current loss: 0.05804646387696266\n",
      "iteration 140 current loss: 0.05782617628574371\n",
      "iteration 141 current loss: 0.056927695870399475\n",
      "iteration 142 current loss: 0.06096822768449783\n",
      "iteration 143 current loss: 0.05880042538046837\n",
      "iteration 144 current loss: 0.05655490979552269\n",
      "iteration 145 current loss: 0.0557742565870285\n",
      "iteration 146 current loss: 0.05726964771747589\n",
      "iteration 147 current loss: 0.05673341825604439\n",
      "iteration 148 current loss: 0.05992636829614639\n",
      "iteration 149 current loss: 0.05577928200364113\n",
      "iteration 150 current loss: 0.06061813235282898\n",
      "iteration 151 current loss: 0.057062000036239624\n",
      "iteration 152 current loss: 0.05924730375409126\n",
      "iteration 153 current loss: 0.05597133934497833\n",
      "iteration 154 current loss: 0.05940845608711243\n",
      "iteration 155 current loss: 0.05985114350914955\n",
      "iteration 156 current loss: 0.05529608204960823\n",
      "iteration 157 current loss: 0.058028366416692734\n",
      "iteration 158 current loss: 0.058840371668338776\n",
      "iteration 159 current loss: 0.05942681059241295\n",
      "iteration 160 current loss: 0.056961361318826675\n",
      "iteration 161 current loss: 0.06200261041522026\n",
      "iteration 162 current loss: 0.061043448746204376\n",
      "iteration 163 current loss: 0.05879871919751167\n",
      "iteration 164 current loss: 0.060500599443912506\n",
      "iteration 165 current loss: 0.06022945046424866\n",
      "iteration 166 current loss: 0.058715708553791046\n",
      "iteration 167 current loss: 0.05812198668718338\n",
      "iteration 168 current loss: 0.06042284518480301\n",
      "iteration 169 current loss: 0.05720598250627518\n",
      "iteration 170 current loss: 0.057251330465078354\n",
      "iteration 171 current loss: 0.05640542134642601\n",
      "iteration 172 current loss: 0.056642841547727585\n",
      "iteration 173 current loss: 0.057962313294410706\n",
      "iteration 174 current loss: 0.055752065032720566\n",
      "iteration 175 current loss: 0.05831528082489967\n",
      "iteration 176 current loss: 0.054705433547496796\n",
      "iteration 177 current loss: 0.05920588597655296\n",
      "iteration 178 current loss: 0.05982653424143791\n",
      "iteration 179 current loss: 0.05676887556910515\n",
      "iteration 180 current loss: 0.05495758727192879\n",
      "iteration 181 current loss: 0.058700647205114365\n",
      "iteration 182 current loss: 0.0593407116830349\n",
      "iteration 183 current loss: 0.060280900448560715\n",
      "iteration 184 current loss: 0.05662192776799202\n",
      "iteration 185 current loss: 0.055864982306957245\n",
      "iteration 186 current loss: 0.05495543032884598\n",
      "iteration 187 current loss: 0.0598301887512207\n",
      "iteration 188 current loss: 0.0604449100792408\n",
      "iteration 189 current loss: 0.05775124952197075\n",
      "iteration 190 current loss: 0.05964355915784836\n",
      "iteration 191 current loss: 0.057003047317266464\n",
      "iteration 192 current loss: 0.05800657719373703\n",
      "iteration 193 current loss: 0.05736885219812393\n",
      "iteration 194 current loss: 0.054796602576971054\n",
      "iteration 195 current loss: 0.0535147450864315\n",
      "iteration 196 current loss: 0.059800852090120316\n",
      "iteration 197 current loss: 0.05750569701194763\n",
      "iteration 198 current loss: 0.056887008249759674\n",
      "iteration 199 current loss: 0.05720365419983864\n",
      "iteration 200 current loss: 0.05797974020242691\n",
      "iteration 201 current loss: 0.057529378682374954\n",
      "iteration 202 current loss: 0.05758064240217209\n",
      "iteration 203 current loss: 0.058339428156614304\n",
      "iteration 204 current loss: 0.056676991283893585\n",
      "iteration 205 current loss: 0.06032762676477432\n",
      "iteration 206 current loss: 0.05847885459661484\n",
      "iteration 207 current loss: 0.05791877955198288\n",
      "iteration 208 current loss: 0.05672825872898102\n",
      "iteration 209 current loss: 0.058334339410066605\n",
      "iteration 210 current loss: 0.05801209807395935\n",
      "iteration 211 current loss: 0.05929727852344513\n",
      "iteration 212 current loss: 0.05766542628407478\n",
      "iteration 213 current loss: 0.056417066603899\n",
      "iteration 214 current loss: 0.05566798523068428\n",
      "iteration 215 current loss: 0.056275624781847\n",
      "iteration 216 current loss: 0.05480637773871422\n",
      "iteration 217 current loss: 0.05676238238811493\n",
      "iteration 218 current loss: 0.05943240225315094\n",
      "iteration 219 current loss: 0.058077484369277954\n",
      "iteration 220 current loss: 0.056076209992170334\n",
      "iteration 221 current loss: 0.05914778262376785\n",
      "iteration 222 current loss: 0.05748513340950012\n",
      "iteration 223 current loss: 0.058902837336063385\n",
      "iteration 224 current loss: 0.05968878045678139\n",
      "iteration 225 current loss: 0.05356205254793167\n",
      "iteration 226 current loss: 0.059289053082466125\n",
      "iteration 227 current loss: 0.05960847809910774\n",
      "iteration 228 current loss: 0.05991619825363159\n",
      "iteration 229 current loss: 0.05740533024072647\n",
      "iteration 230 current loss: 0.05813281238079071\n",
      "iteration 231 current loss: 0.05635062977671623\n",
      "iteration 232 current loss: 0.05982697010040283\n",
      "iteration 233 current loss: 0.06006350368261337\n",
      "iteration 234 current loss: 0.05829592049121857\n",
      "iteration 235 current loss: 0.05731067806482315\n",
      "iteration 236 current loss: 0.05707068368792534\n",
      "iteration 237 current loss: 0.05572509765625\n",
      "iteration 238 current loss: 0.05537858232855797\n",
      "iteration 239 current loss: 0.056728195399045944\n",
      "iteration 240 current loss: 0.05852220579981804\n",
      "iteration 241 current loss: 0.05565915256738663\n",
      "iteration 242 current loss: 0.05758942663669586\n",
      "iteration 243 current loss: 0.058317676186561584\n",
      "iteration 244 current loss: 0.05999632552266121\n",
      "iteration 245 current loss: 0.060169342905282974\n",
      "iteration 246 current loss: 0.05743267759680748\n",
      "iteration 247 current loss: 0.05769076943397522\n",
      "iteration 248 current loss: 0.057629700750112534\n",
      "iteration 249 current loss: 0.059336546808481216\n",
      "iteration 250 current loss: 0.05977804586291313\n",
      "iteration 251 current loss: 0.05826422572135925\n",
      "iteration 252 current loss: 0.05850233882665634\n",
      "iteration 253 current loss: 0.05751292034983635\n",
      "iteration 254 current loss: 0.057081807404756546\n",
      "iteration 255 current loss: 0.05771614611148834\n",
      "iteration 256 current loss: 0.059112850576639175\n",
      "iteration 257 current loss: 0.060294199734926224\n",
      "iteration 258 current loss: 0.05562217906117439\n",
      "iteration 259 current loss: 0.055316485464572906\n",
      "iteration 260 current loss: 0.05699574574828148\n",
      "iteration 261 current loss: 0.05726547911763191\n",
      "iteration 262 current loss: 0.06032678857445717\n",
      "iteration 263 current loss: 0.06138329952955246\n",
      "iteration 264 current loss: 0.057792164385318756\n",
      "iteration 265 current loss: 0.058365464210510254\n",
      "iteration 266 current loss: 0.060019008815288544\n",
      "iteration 267 current loss: 0.058248091489076614\n",
      "iteration 268 current loss: 0.061955373734235764\n",
      "iteration 269 current loss: 0.05885838717222214\n",
      "iteration 270 current loss: 0.058385446667671204\n",
      "iteration 271 current loss: 0.055533964186906815\n",
      "iteration 272 current loss: 0.05885641276836395\n",
      "iteration 273 current loss: 0.05564787611365318\n",
      "iteration 274 current loss: 0.058209460228681564\n",
      "iteration 275 current loss: 0.06263504177331924\n",
      "iteration 276 current loss: 0.06001770868897438\n",
      "iteration 277 current loss: 0.0614883229136467\n",
      "iteration 278 current loss: 0.05862853676080704\n",
      "iteration 279 current loss: 0.057725682854652405\n",
      "iteration 280 current loss: 0.05772125720977783\n",
      "iteration 281 current loss: 0.05898236110806465\n",
      "iteration 282 current loss: 0.05906814709305763\n",
      "iteration 283 current loss: 0.059692591428756714\n",
      "iteration 284 current loss: 0.058194611221551895\n",
      "iteration 285 current loss: 0.05702941492199898\n",
      "iteration 286 current loss: 0.05679808929562569\n",
      "iteration 287 current loss: 0.05949311703443527\n",
      "iteration 288 current loss: 0.05847020447254181\n",
      "iteration 289 current loss: 0.055828023701906204\n",
      "iteration 290 current loss: 0.05868200957775116\n",
      "iteration 291 current loss: 0.05574555695056915\n",
      "iteration 292 current loss: 0.058781933039426804\n",
      "iteration 293 current loss: 0.05684304237365723\n",
      "iteration 294 current loss: 0.06009208783507347\n",
      "iteration 295 current loss: 0.0565623939037323\n",
      "iteration 296 current loss: 0.05835403501987457\n",
      "iteration 297 current loss: 0.0581108033657074\n",
      "iteration 298 current loss: 0.06174921244382858\n",
      "iteration 299 current loss: 0.0597967766225338\n",
      "iteration 300 current loss: 0.0615813285112381\n",
      "iteration 301 current loss: 0.06067879870533943\n",
      "iteration 302 current loss: 0.05578744038939476\n",
      "iteration 303 current loss: 0.05572085082530975\n",
      "iteration 304 current loss: 0.0603201799094677\n",
      "iteration 305 current loss: 0.05598558858036995\n",
      "iteration 306 current loss: 0.05778956040740013\n",
      "iteration 307 current loss: 0.058392927050590515\n",
      "iteration 308 current loss: 0.05152255296707153\n",
      "iteration 309 current loss: 0.05506126955151558\n",
      "iteration 310 current loss: 0.05520857125520706\n",
      "iteration 311 current loss: 0.05687537044286728\n",
      "iteration 312 current loss: 0.0570269413292408\n",
      "iteration 313 current loss: 0.05546588450670242\n",
      "iteration 314 current loss: 0.05517294630408287\n",
      "iteration 315 current loss: 0.05860114097595215\n",
      "iteration 316 current loss: 0.06475353986024857\n",
      "iteration 317 current loss: 0.05556656792759895\n",
      "iteration 318 current loss: 0.05626359581947327\n",
      "iteration 319 current loss: 0.05791695788502693\n",
      "iteration 320 current loss: 0.05161473527550697\n",
      "iteration 321 current loss: 0.05679165944457054\n",
      "iteration 322 current loss: 0.057199377566576004\n",
      "iteration 323 current loss: 0.05807483568787575\n",
      "iteration 324 current loss: 0.05732183903455734\n",
      "iteration 325 current loss: 0.06036362051963806\n",
      "iteration 326 current loss: 0.06180846318602562\n",
      "iteration 327 current loss: 0.05505123734474182\n",
      "iteration 328 current loss: 0.05753043293952942\n",
      "iteration 329 current loss: 0.05682310089468956\n",
      "iteration 330 current loss: 0.060740843415260315\n",
      "iteration 331 current loss: 0.059542085975408554\n",
      "iteration 332 current loss: 0.058677028864622116\n",
      "iteration 333 current loss: 0.0570371076464653\n",
      "iteration 334 current loss: 0.059519581496715546\n",
      "iteration 335 current loss: 0.05436484143137932\n",
      "iteration 336 current loss: 0.05808321759104729\n",
      "iteration 337 current loss: 0.059224970638751984\n",
      "iteration 338 current loss: 0.05746106058359146\n",
      "iteration 339 current loss: 0.05724126473069191\n",
      "iteration 340 current loss: 0.05983399972319603\n",
      "iteration 341 current loss: 0.05974218621850014\n",
      "iteration 342 current loss: 0.057743337005376816\n",
      "iteration 343 current loss: 0.0588986873626709\n",
      "iteration 344 current loss: 0.05488014966249466\n",
      "iteration 345 current loss: 0.05892393738031387\n",
      "iteration 346 current loss: 0.0597638301551342\n",
      "iteration 347 current loss: 0.0572696179151535\n",
      "iteration 348 current loss: 0.059000518172979355\n",
      "iteration 349 current loss: 0.054791320115327835\n",
      "iteration 350 current loss: 0.05691876262426376\n",
      "iteration 351 current loss: 0.05595837160944939\n",
      "iteration 352 current loss: 0.059323929250240326\n",
      "iteration 353 current loss: 0.05700210481882095\n",
      "iteration 354 current loss: 0.05687461420893669\n",
      "iteration 355 current loss: 0.05612051859498024\n",
      "iteration 356 current loss: 0.05895970016717911\n",
      "iteration 357 current loss: 0.05596950277686119\n",
      "iteration 358 current loss: 0.05987095832824707\n",
      "iteration 359 current loss: 0.05634856969118118\n",
      "iteration 360 current loss: 0.059622228145599365\n",
      "iteration 361 current loss: 0.055957045406103134\n",
      "iteration 362 current loss: 0.059168003499507904\n",
      "iteration 363 current loss: 0.05422615259885788\n",
      "iteration 364 current loss: 0.05706959217786789\n",
      "iteration 365 current loss: 0.058484096080064774\n",
      "iteration 366 current loss: 0.05726610869169235\n",
      "iteration 367 current loss: 0.058943651616573334\n",
      "iteration 368 current loss: 0.05796871334314346\n",
      "iteration 369 current loss: 0.05651222541928291\n",
      "iteration 370 current loss: 0.054166387766599655\n",
      "iteration 371 current loss: 0.05571947246789932\n",
      "iteration 372 current loss: 0.05973860248923302\n",
      "iteration 373 current loss: 0.06051453948020935\n",
      "iteration 374 current loss: 0.05601177364587784\n",
      "iteration 375 current loss: 0.05722355842590332\n",
      "iteration 376 current loss: 0.06017383933067322\n",
      "iteration 377 current loss: 0.05681956186890602\n",
      "iteration 378 current loss: 0.05631350725889206\n",
      "iteration 379 current loss: 0.0606643445789814\n",
      "iteration 380 current loss: 0.055573929101228714\n",
      "iteration 381 current loss: 0.05887060612440109\n",
      "iteration 382 current loss: 0.05686761811375618\n",
      "iteration 383 current loss: 0.05818452686071396\n",
      "iteration 384 current loss: 0.05854635685682297\n",
      "iteration 385 current loss: 0.05543004348874092\n",
      "iteration 386 current loss: 0.05855122208595276\n",
      "iteration 387 current loss: 0.05710803344845772\n",
      "iteration 388 current loss: 0.058573197573423386\n",
      "iteration 389 current loss: 0.060818374156951904\n",
      "iteration 390 current loss: 0.05826876312494278\n",
      "iteration 391 current loss: 0.05751720070838928\n",
      "iteration 392 current loss: 0.05701727047562599\n",
      "iteration 393 current loss: 0.055788829922676086\n",
      "iteration 394 current loss: 0.056125909090042114\n",
      "iteration 395 current loss: 0.058310266584157944\n",
      "iteration 396 current loss: 0.055815599858760834\n",
      "iteration 397 current loss: 0.05712004005908966\n",
      "iteration 398 current loss: 0.05971208214759827\n",
      "iteration 399 current loss: 0.05914321169257164\n",
      "iteration 400 current loss: 0.057848814874887466\n",
      "iteration 401 current loss: 0.05978861078619957\n",
      "iteration 402 current loss: 0.058665744960308075\n",
      "iteration 403 current loss: 0.05740733817219734\n",
      "iteration 404 current loss: 0.057303279638290405\n",
      "iteration 405 current loss: 0.058768078684806824\n",
      "iteration 406 current loss: 0.05614383891224861\n",
      "iteration 407 current loss: 0.05791997164487839\n",
      "iteration 408 current loss: 0.05589287728071213\n",
      "iteration 409 current loss: 0.05957156792283058\n",
      "iteration 410 current loss: 0.06957205384969711\n",
      "\t\tEpoch 9/100 complete. Epoch loss 0.05780695863702581\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 9, Validation Loss: 0.060982404625974596\n",
      "best loss 0.05780695863702581\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.0586000457406044\n",
      "iteration 1 current loss: 0.05415555462241173\n",
      "iteration 2 current loss: 0.05342971161007881\n",
      "iteration 3 current loss: 0.05838973820209503\n",
      "iteration 4 current loss: 0.058250028640031815\n",
      "iteration 5 current loss: 0.05492651090025902\n",
      "iteration 6 current loss: 0.05526081845164299\n",
      "iteration 7 current loss: 0.06322965770959854\n",
      "iteration 8 current loss: 0.0603141188621521\n",
      "iteration 9 current loss: 0.05583655834197998\n",
      "iteration 10 current loss: 0.058410193771123886\n",
      "iteration 11 current loss: 0.05873643979430199\n",
      "iteration 12 current loss: 0.05811059847474098\n",
      "iteration 13 current loss: 0.056138768792152405\n",
      "iteration 14 current loss: 0.05886738747358322\n",
      "iteration 15 current loss: 0.05483610928058624\n",
      "iteration 16 current loss: 0.055738985538482666\n",
      "iteration 17 current loss: 0.05782542750239372\n",
      "iteration 18 current loss: 0.05852415785193443\n",
      "iteration 19 current loss: 0.05725047364830971\n",
      "iteration 20 current loss: 0.056079618632793427\n",
      "iteration 21 current loss: 0.053715724498033524\n",
      "iteration 22 current loss: 0.059278883039951324\n",
      "iteration 23 current loss: 0.06085168942809105\n",
      "iteration 24 current loss: 0.05784282460808754\n",
      "iteration 25 current loss: 0.05970204621553421\n",
      "iteration 26 current loss: 0.05525173246860504\n",
      "iteration 27 current loss: 0.05720383673906326\n",
      "iteration 28 current loss: 0.06052754074335098\n",
      "iteration 29 current loss: 0.05796439200639725\n",
      "iteration 30 current loss: 0.059298593550920486\n",
      "iteration 31 current loss: 0.05546246096491814\n",
      "iteration 32 current loss: 0.05940389633178711\n",
      "iteration 33 current loss: 0.05861796438694\n",
      "iteration 34 current loss: 0.05754261463880539\n",
      "iteration 35 current loss: 0.058808401226997375\n",
      "iteration 36 current loss: 0.056551311165094376\n",
      "iteration 37 current loss: 0.058832187205553055\n",
      "iteration 38 current loss: 0.05550145357847214\n",
      "iteration 39 current loss: 0.0555076003074646\n",
      "iteration 40 current loss: 0.05622582137584686\n",
      "iteration 41 current loss: 0.05596042424440384\n",
      "iteration 42 current loss: 0.055446501821279526\n",
      "iteration 43 current loss: 0.05761701241135597\n",
      "iteration 44 current loss: 0.05880182981491089\n",
      "iteration 45 current loss: 0.05236155167222023\n",
      "iteration 46 current loss: 0.05926468223333359\n",
      "iteration 47 current loss: 0.057691290974617004\n",
      "iteration 48 current loss: 0.0561295710504055\n",
      "iteration 49 current loss: 0.05854283645749092\n",
      "iteration 50 current loss: 0.059795577079057693\n",
      "iteration 51 current loss: 0.05521639063954353\n",
      "iteration 52 current loss: 0.05586646869778633\n",
      "iteration 53 current loss: 0.05786820873618126\n",
      "iteration 54 current loss: 0.05903247743844986\n",
      "iteration 55 current loss: 0.05521909520030022\n",
      "iteration 56 current loss: 0.05932158976793289\n",
      "iteration 57 current loss: 0.05435214191675186\n",
      "iteration 58 current loss: 0.05870373919606209\n",
      "iteration 59 current loss: 0.05881105735898018\n",
      "iteration 60 current loss: 0.05681168660521507\n",
      "iteration 61 current loss: 0.05470866709947586\n",
      "iteration 62 current loss: 0.059860050678253174\n",
      "iteration 63 current loss: 0.05617490038275719\n",
      "iteration 64 current loss: 0.05573372542858124\n",
      "iteration 65 current loss: 0.0555582232773304\n",
      "iteration 66 current loss: 0.05523154139518738\n",
      "iteration 67 current loss: 0.059651993215084076\n",
      "iteration 68 current loss: 0.05682871490716934\n",
      "iteration 69 current loss: 0.06105601415038109\n",
      "iteration 70 current loss: 0.057620398700237274\n",
      "iteration 71 current loss: 0.05725856125354767\n",
      "iteration 72 current loss: 0.05545901507139206\n",
      "iteration 73 current loss: 0.06016026809811592\n",
      "iteration 74 current loss: 0.05689814314246178\n",
      "iteration 75 current loss: 0.05606374889612198\n",
      "iteration 76 current loss: 0.0572076216340065\n",
      "iteration 77 current loss: 0.056633565574884415\n",
      "iteration 78 current loss: 0.056354720145463943\n",
      "iteration 79 current loss: 0.05340884253382683\n",
      "iteration 80 current loss: 0.057994164526462555\n",
      "iteration 81 current loss: 0.05671517923474312\n",
      "iteration 82 current loss: 0.05931274592876434\n",
      "iteration 83 current loss: 0.05847478657960892\n",
      "iteration 84 current loss: 0.05513806268572807\n",
      "iteration 85 current loss: 0.0581657811999321\n",
      "iteration 86 current loss: 0.05850619822740555\n",
      "iteration 87 current loss: 0.05875854939222336\n",
      "iteration 88 current loss: 0.05649223178625107\n",
      "iteration 89 current loss: 0.05561279505491257\n",
      "iteration 90 current loss: 0.05855889618396759\n",
      "iteration 91 current loss: 0.061425916850566864\n",
      "iteration 92 current loss: 0.05830693989992142\n",
      "iteration 93 current loss: 0.05369623377919197\n",
      "iteration 94 current loss: 0.05750209838151932\n",
      "iteration 95 current loss: 0.06022169068455696\n",
      "iteration 96 current loss: 0.05812569335103035\n",
      "iteration 97 current loss: 0.05949665978550911\n",
      "iteration 98 current loss: 0.057688381522893906\n",
      "iteration 99 current loss: 0.05646054446697235\n",
      "iteration 100 current loss: 0.05854157358407974\n",
      "iteration 101 current loss: 0.058813370764255524\n",
      "iteration 102 current loss: 0.05562461167573929\n",
      "iteration 103 current loss: 0.05875281244516373\n",
      "iteration 104 current loss: 0.05949067324399948\n",
      "iteration 105 current loss: 0.05718038231134415\n",
      "iteration 106 current loss: 0.05729241669178009\n",
      "iteration 107 current loss: 0.057584427297115326\n",
      "iteration 108 current loss: 0.055846311151981354\n",
      "iteration 109 current loss: 0.05826577544212341\n",
      "iteration 110 current loss: 0.0563247911632061\n",
      "iteration 111 current loss: 0.056639865040779114\n",
      "iteration 112 current loss: 0.0592203363776207\n",
      "iteration 113 current loss: 0.05868981406092644\n",
      "iteration 114 current loss: 0.055825185030698776\n",
      "iteration 115 current loss: 0.056649163365364075\n",
      "iteration 116 current loss: 0.05921716243028641\n",
      "iteration 117 current loss: 0.05468365177512169\n",
      "iteration 118 current loss: 0.057695284485816956\n",
      "iteration 119 current loss: 0.05514409765601158\n",
      "iteration 120 current loss: 0.05546033754944801\n",
      "iteration 121 current loss: 0.05714542046189308\n",
      "iteration 122 current loss: 0.059295654296875\n",
      "iteration 123 current loss: 0.058489106595516205\n",
      "iteration 124 current loss: 0.060012105852365494\n",
      "iteration 125 current loss: 0.05852725729346275\n",
      "iteration 126 current loss: 0.06030972674489021\n",
      "iteration 127 current loss: 0.055502839386463165\n",
      "iteration 128 current loss: 0.059124018996953964\n",
      "iteration 129 current loss: 0.05807601660490036\n",
      "iteration 130 current loss: 0.05834004282951355\n",
      "iteration 131 current loss: 0.05601133406162262\n",
      "iteration 132 current loss: 0.05889490991830826\n",
      "iteration 133 current loss: 0.05727119371294975\n",
      "iteration 134 current loss: 0.059878088533878326\n",
      "iteration 135 current loss: 0.05550524592399597\n",
      "iteration 136 current loss: 0.0605475939810276\n",
      "iteration 137 current loss: 0.05730409175157547\n",
      "iteration 138 current loss: 0.05539531260728836\n",
      "iteration 139 current loss: 0.057922571897506714\n",
      "iteration 140 current loss: 0.05843810737133026\n",
      "iteration 141 current loss: 0.05523550137877464\n",
      "iteration 142 current loss: 0.06056106463074684\n",
      "iteration 143 current loss: 0.05273449048399925\n",
      "iteration 144 current loss: 0.05612730234861374\n",
      "iteration 145 current loss: 0.0565398633480072\n",
      "iteration 146 current loss: 0.060218341648578644\n",
      "iteration 147 current loss: 0.05586094409227371\n",
      "iteration 148 current loss: 0.0575503408908844\n",
      "iteration 149 current loss: 0.05930441617965698\n",
      "iteration 150 current loss: 0.05842944607138634\n",
      "iteration 151 current loss: 0.059809986501932144\n",
      "iteration 152 current loss: 0.058896079659461975\n",
      "iteration 153 current loss: 0.056295398622751236\n",
      "iteration 154 current loss: 0.05907340720295906\n",
      "iteration 155 current loss: 0.05823478102684021\n",
      "iteration 156 current loss: 0.05926471948623657\n",
      "iteration 157 current loss: 0.057554617524147034\n",
      "iteration 158 current loss: 0.06013435125350952\n",
      "iteration 159 current loss: 0.05484922602772713\n",
      "iteration 160 current loss: 0.05793001875281334\n",
      "iteration 161 current loss: 0.0565243661403656\n",
      "iteration 162 current loss: 0.05601868778467178\n",
      "iteration 163 current loss: 0.053916651755571365\n",
      "iteration 164 current loss: 0.059204623103141785\n",
      "iteration 165 current loss: 0.05628681927919388\n",
      "iteration 166 current loss: 0.05993456766009331\n",
      "iteration 167 current loss: 0.05768272653222084\n",
      "iteration 168 current loss: 0.058995261788368225\n",
      "iteration 169 current loss: 0.0567755363881588\n",
      "iteration 170 current loss: 0.05633153021335602\n",
      "iteration 171 current loss: 0.06032390892505646\n",
      "iteration 172 current loss: 0.05732269957661629\n",
      "iteration 173 current loss: 0.055753324180841446\n",
      "iteration 174 current loss: 0.05536212772130966\n",
      "iteration 175 current loss: 0.058559294790029526\n",
      "iteration 176 current loss: 0.058023303747177124\n",
      "iteration 177 current loss: 0.056564632803201675\n",
      "iteration 178 current loss: 0.05669400095939636\n",
      "iteration 179 current loss: 0.06004747748374939\n",
      "iteration 180 current loss: 0.058115068823099136\n",
      "iteration 181 current loss: 0.0568661242723465\n",
      "iteration 182 current loss: 0.06033002585172653\n",
      "iteration 183 current loss: 0.05839485675096512\n",
      "iteration 184 current loss: 0.05408221855759621\n",
      "iteration 185 current loss: 0.061091650277376175\n",
      "iteration 186 current loss: 0.05493972823023796\n",
      "iteration 187 current loss: 0.05715153366327286\n",
      "iteration 188 current loss: 0.05638386309146881\n",
      "iteration 189 current loss: 0.05803317576646805\n",
      "iteration 190 current loss: 0.05468158423900604\n",
      "iteration 191 current loss: 0.057196907699108124\n",
      "iteration 192 current loss: 0.0591571107506752\n",
      "iteration 193 current loss: 0.05791423097252846\n",
      "iteration 194 current loss: 0.05805937573313713\n",
      "iteration 195 current loss: 0.053006093949079514\n",
      "iteration 196 current loss: 0.060283906757831573\n",
      "iteration 197 current loss: 0.05706179514527321\n",
      "iteration 198 current loss: 0.05842093378305435\n",
      "iteration 199 current loss: 0.0558231845498085\n",
      "iteration 200 current loss: 0.05774340033531189\n",
      "iteration 201 current loss: 0.05842248350381851\n",
      "iteration 202 current loss: 0.05692102387547493\n",
      "iteration 203 current loss: 0.055998027324676514\n",
      "iteration 204 current loss: 0.05591880902647972\n",
      "iteration 205 current loss: 0.05790533125400543\n",
      "iteration 206 current loss: 0.06059564650058746\n",
      "iteration 207 current loss: 0.055704113095998764\n",
      "iteration 208 current loss: 0.05799229070544243\n",
      "iteration 209 current loss: 0.059081535786390305\n",
      "iteration 210 current loss: 0.055829815566539764\n",
      "iteration 211 current loss: 0.05538462847471237\n",
      "iteration 212 current loss: 0.05410967022180557\n",
      "iteration 213 current loss: 0.05324733257293701\n",
      "iteration 214 current loss: 0.05811459198594093\n",
      "iteration 215 current loss: 0.054478906095027924\n",
      "iteration 216 current loss: 0.05362258478999138\n",
      "iteration 217 current loss: 0.056834764778614044\n",
      "iteration 218 current loss: 0.05801679566502571\n",
      "iteration 219 current loss: 0.054592281579971313\n",
      "iteration 220 current loss: 0.05869868025183678\n",
      "iteration 221 current loss: 0.05601254850625992\n",
      "iteration 222 current loss: 0.05807938054203987\n",
      "iteration 223 current loss: 0.05739199370145798\n",
      "iteration 224 current loss: 0.05589134246110916\n",
      "iteration 225 current loss: 0.05729098990559578\n",
      "iteration 226 current loss: 0.06256751716136932\n",
      "iteration 227 current loss: 0.05701226368546486\n",
      "iteration 228 current loss: 0.0581572949886322\n",
      "iteration 229 current loss: 0.054661884903907776\n",
      "iteration 230 current loss: 0.06033550575375557\n",
      "iteration 231 current loss: 0.05975089222192764\n",
      "iteration 232 current loss: 0.055705077946186066\n",
      "iteration 233 current loss: 0.05760239437222481\n",
      "iteration 234 current loss: 0.05697140097618103\n",
      "iteration 235 current loss: 0.05677793547511101\n",
      "iteration 236 current loss: 0.05571362003684044\n",
      "iteration 237 current loss: 0.05613120272755623\n",
      "iteration 238 current loss: 0.05795656517148018\n",
      "iteration 239 current loss: 0.056030601263046265\n",
      "iteration 240 current loss: 0.057956263422966\n",
      "iteration 241 current loss: 0.05709552392363548\n",
      "iteration 242 current loss: 0.05832980200648308\n",
      "iteration 243 current loss: 0.059645336121320724\n",
      "iteration 244 current loss: 0.056110888719558716\n",
      "iteration 245 current loss: 0.0567970871925354\n",
      "iteration 246 current loss: 0.05880855768918991\n",
      "iteration 247 current loss: 0.05625704675912857\n",
      "iteration 248 current loss: 0.059085287153720856\n",
      "iteration 249 current loss: 0.057578179985284805\n",
      "iteration 250 current loss: 0.05636199191212654\n",
      "iteration 251 current loss: 0.05813632160425186\n",
      "iteration 252 current loss: 0.05447535216808319\n",
      "iteration 253 current loss: 0.05860711634159088\n",
      "iteration 254 current loss: 0.05780081823468208\n",
      "iteration 255 current loss: 0.05692613497376442\n",
      "iteration 256 current loss: 0.05654754117131233\n",
      "iteration 257 current loss: 0.05566243827342987\n",
      "iteration 258 current loss: 0.055952250957489014\n",
      "iteration 259 current loss: 0.055299095809459686\n",
      "iteration 260 current loss: 0.05601125955581665\n",
      "iteration 261 current loss: 0.058236174285411835\n",
      "iteration 262 current loss: 0.05813039094209671\n",
      "iteration 263 current loss: 0.05756404995918274\n",
      "iteration 264 current loss: 0.05617716163396835\n",
      "iteration 265 current loss: 0.056435804814100266\n",
      "iteration 266 current loss: 0.05540032684803009\n",
      "iteration 267 current loss: 0.05630507320165634\n",
      "iteration 268 current loss: 0.05739787966012955\n",
      "iteration 269 current loss: 0.05643641576170921\n",
      "iteration 270 current loss: 0.05403846502304077\n",
      "iteration 271 current loss: 0.0567016527056694\n",
      "iteration 272 current loss: 0.0585758239030838\n",
      "iteration 273 current loss: 0.056080348789691925\n",
      "iteration 274 current loss: 0.057031817734241486\n",
      "iteration 275 current loss: 0.05708055943250656\n",
      "iteration 276 current loss: 0.05557466298341751\n",
      "iteration 277 current loss: 0.055478863418102264\n",
      "iteration 278 current loss: 0.06017685681581497\n",
      "iteration 279 current loss: 0.057052142918109894\n",
      "iteration 280 current loss: 0.05795906111598015\n",
      "iteration 281 current loss: 0.057138703763484955\n",
      "iteration 282 current loss: 0.0579947866499424\n",
      "iteration 283 current loss: 0.05490891635417938\n",
      "iteration 284 current loss: 0.06085960194468498\n",
      "iteration 285 current loss: 0.05849907547235489\n",
      "iteration 286 current loss: 0.05881640315055847\n",
      "iteration 287 current loss: 0.058371711522340775\n",
      "iteration 288 current loss: 0.05801445618271828\n",
      "iteration 289 current loss: 0.056743208318948746\n",
      "iteration 290 current loss: 0.05686144530773163\n",
      "iteration 291 current loss: 0.05668651685118675\n",
      "iteration 292 current loss: 0.056211717426776886\n",
      "iteration 293 current loss: 0.054201193153858185\n",
      "iteration 294 current loss: 0.05851859971880913\n",
      "iteration 295 current loss: 0.05531955510377884\n",
      "iteration 296 current loss: 0.05697300657629967\n",
      "iteration 297 current loss: 0.05541636422276497\n",
      "iteration 298 current loss: 0.05501959100365639\n",
      "iteration 299 current loss: 0.05632396414875984\n",
      "iteration 300 current loss: 0.05842754244804382\n",
      "iteration 301 current loss: 0.05552586913108826\n",
      "iteration 302 current loss: 0.055932845920324326\n",
      "iteration 303 current loss: 0.058986593037843704\n",
      "iteration 304 current loss: 0.057596057653427124\n",
      "iteration 305 current loss: 0.05794071778655052\n",
      "iteration 306 current loss: 0.057049460709095\n",
      "iteration 307 current loss: 0.05417444184422493\n",
      "iteration 308 current loss: 0.05685713142156601\n",
      "iteration 309 current loss: 0.0578867569565773\n",
      "iteration 310 current loss: 0.05228770151734352\n",
      "iteration 311 current loss: 0.057918284088373184\n",
      "iteration 312 current loss: 0.055840685963630676\n",
      "iteration 313 current loss: 0.05781296640634537\n",
      "iteration 314 current loss: 0.05784435197710991\n",
      "iteration 315 current loss: 0.05861309915781021\n",
      "iteration 316 current loss: 0.0560477040708065\n",
      "iteration 317 current loss: 0.057686541229486465\n",
      "iteration 318 current loss: 0.05981667712330818\n",
      "iteration 319 current loss: 0.05819310247898102\n",
      "iteration 320 current loss: 0.054509226232767105\n",
      "iteration 321 current loss: 0.05749107897281647\n",
      "iteration 322 current loss: 0.055821217596530914\n",
      "iteration 323 current loss: 0.05732905492186546\n",
      "iteration 324 current loss: 0.05561773478984833\n",
      "iteration 325 current loss: 0.05513467639684677\n",
      "iteration 326 current loss: 0.05683427304029465\n",
      "iteration 327 current loss: 0.05601798743009567\n",
      "iteration 328 current loss: 0.0585460402071476\n",
      "iteration 329 current loss: 0.057069532573223114\n",
      "iteration 330 current loss: 0.05521124601364136\n",
      "iteration 331 current loss: 0.05740077421069145\n",
      "iteration 332 current loss: 0.05842696875333786\n",
      "iteration 333 current loss: 0.05599217861890793\n",
      "iteration 334 current loss: 0.06036749482154846\n",
      "iteration 335 current loss: 0.058372143656015396\n",
      "iteration 336 current loss: 0.05580686405301094\n",
      "iteration 337 current loss: 0.057895585894584656\n",
      "iteration 338 current loss: 0.05828423053026199\n",
      "iteration 339 current loss: 0.05836959555745125\n",
      "iteration 340 current loss: 0.05971144884824753\n",
      "iteration 341 current loss: 0.05932152271270752\n",
      "iteration 342 current loss: 0.05568782612681389\n",
      "iteration 343 current loss: 0.058194875717163086\n",
      "iteration 344 current loss: 0.05593748390674591\n",
      "iteration 345 current loss: 0.05672363191843033\n",
      "iteration 346 current loss: 0.05447429418563843\n",
      "iteration 347 current loss: 0.053424496203660965\n",
      "iteration 348 current loss: 0.06029731407761574\n",
      "iteration 349 current loss: 0.05828293785452843\n",
      "iteration 350 current loss: 0.056131474673748016\n",
      "iteration 351 current loss: 0.05421040579676628\n",
      "iteration 352 current loss: 0.05959782749414444\n",
      "iteration 353 current loss: 0.058325864374637604\n",
      "iteration 354 current loss: 0.059426046907901764\n",
      "iteration 355 current loss: 0.058082111179828644\n",
      "iteration 356 current loss: 0.05737927183508873\n",
      "iteration 357 current loss: 0.056929200887680054\n",
      "iteration 358 current loss: 0.06053450331091881\n",
      "iteration 359 current loss: 0.05891214683651924\n",
      "iteration 360 current loss: 0.060951560735702515\n",
      "iteration 361 current loss: 0.06092507764697075\n",
      "iteration 362 current loss: 0.053500380367040634\n",
      "iteration 363 current loss: 0.05453529208898544\n",
      "iteration 364 current loss: 0.06066020950675011\n",
      "iteration 365 current loss: 0.05768822878599167\n",
      "iteration 366 current loss: 0.05924733355641365\n",
      "iteration 367 current loss: 0.05942115932703018\n",
      "iteration 368 current loss: 0.05785596743226051\n",
      "iteration 369 current loss: 0.055575575679540634\n",
      "iteration 370 current loss: 0.05442779138684273\n",
      "iteration 371 current loss: 0.05751100182533264\n",
      "iteration 372 current loss: 0.05762702599167824\n",
      "iteration 373 current loss: 0.057260662317276\n",
      "iteration 374 current loss: 0.05748381465673447\n",
      "iteration 375 current loss: 0.05741412937641144\n",
      "iteration 376 current loss: 0.05258096754550934\n",
      "iteration 377 current loss: 0.05617581680417061\n",
      "iteration 378 current loss: 0.055012334138154984\n",
      "iteration 379 current loss: 0.05884445086121559\n",
      "iteration 380 current loss: 0.056551218032836914\n",
      "iteration 381 current loss: 0.05998469889163971\n",
      "iteration 382 current loss: 0.05762363225221634\n",
      "iteration 383 current loss: 0.05730142444372177\n",
      "iteration 384 current loss: 0.05454186722636223\n",
      "iteration 385 current loss: 0.05914805456995964\n",
      "iteration 386 current loss: 0.05707130581140518\n",
      "iteration 387 current loss: 0.06014147028326988\n",
      "iteration 388 current loss: 0.05380913242697716\n",
      "iteration 389 current loss: 0.05839020758867264\n",
      "iteration 390 current loss: 0.059827256947755814\n",
      "iteration 391 current loss: 0.056639984250068665\n",
      "iteration 392 current loss: 0.060876041650772095\n",
      "iteration 393 current loss: 0.05489707738161087\n",
      "iteration 394 current loss: 0.056157100945711136\n",
      "iteration 395 current loss: 0.05821043998003006\n",
      "iteration 396 current loss: 0.05269971117377281\n",
      "iteration 397 current loss: 0.054943766444921494\n",
      "iteration 398 current loss: 0.05905971676111221\n",
      "iteration 399 current loss: 0.05621635541319847\n",
      "iteration 400 current loss: 0.05874267965555191\n",
      "iteration 401 current loss: 0.05936632305383682\n",
      "iteration 402 current loss: 0.06058475747704506\n",
      "iteration 403 current loss: 0.06116644665598869\n",
      "iteration 404 current loss: 0.055626288056373596\n",
      "iteration 405 current loss: 0.05780233442783356\n",
      "iteration 406 current loss: 0.056022759526968\n",
      "iteration 407 current loss: 0.05866463854908943\n",
      "iteration 408 current loss: 0.05644267052412033\n",
      "iteration 409 current loss: 0.059654299169778824\n",
      "iteration 410 current loss: 0.06426487863063812\n",
      "\t\tEpoch 10/100 complete. Epoch loss 0.057338803825297206\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 10, Validation Loss: 0.058327609789557755\n",
      "best loss 0.057338803825297206\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.05662763491272926\n",
      "iteration 1 current loss: 0.054854799062013626\n",
      "iteration 2 current loss: 0.05575885623693466\n",
      "iteration 3 current loss: 0.06232032552361488\n",
      "iteration 4 current loss: 0.05715131014585495\n",
      "iteration 5 current loss: 0.0542713887989521\n",
      "iteration 6 current loss: 0.05610135197639465\n",
      "iteration 7 current loss: 0.05773532763123512\n",
      "iteration 8 current loss: 0.05962321534752846\n",
      "iteration 9 current loss: 0.05394139885902405\n",
      "iteration 10 current loss: 0.05342984199523926\n",
      "iteration 11 current loss: 0.0564877912402153\n",
      "iteration 12 current loss: 0.06096946820616722\n",
      "iteration 13 current loss: 0.058227527886629105\n",
      "iteration 14 current loss: 0.059097032994031906\n",
      "iteration 15 current loss: 0.05187201499938965\n",
      "iteration 16 current loss: 0.05706357955932617\n",
      "iteration 17 current loss: 0.057015061378479004\n",
      "iteration 18 current loss: 0.05715036019682884\n",
      "iteration 19 current loss: 0.0546715222299099\n",
      "iteration 20 current loss: 0.055929992347955704\n",
      "iteration 21 current loss: 0.056233927607536316\n",
      "iteration 22 current loss: 0.05201546475291252\n",
      "iteration 23 current loss: 0.056980330497026443\n",
      "iteration 24 current loss: 0.057297397404909134\n",
      "iteration 25 current loss: 0.053972095251083374\n",
      "iteration 26 current loss: 0.05239829793572426\n",
      "iteration 27 current loss: 0.05597100406885147\n",
      "iteration 28 current loss: 0.05593312159180641\n",
      "iteration 29 current loss: 0.052969690412282944\n",
      "iteration 30 current loss: 0.05220503360033035\n",
      "iteration 31 current loss: 0.05838771164417267\n",
      "iteration 32 current loss: 0.05597974732518196\n",
      "iteration 33 current loss: 0.05464036390185356\n",
      "iteration 34 current loss: 0.057407256215810776\n",
      "iteration 35 current loss: 0.056197576224803925\n",
      "iteration 36 current loss: 0.05249736085534096\n",
      "iteration 37 current loss: 0.052426911890506744\n",
      "iteration 38 current loss: 0.05942909047007561\n",
      "iteration 39 current loss: 0.05805240944027901\n",
      "iteration 40 current loss: 0.05900932103395462\n",
      "iteration 41 current loss: 0.060578081756830215\n",
      "iteration 42 current loss: 0.0563000924885273\n",
      "iteration 43 current loss: 0.055952075868844986\n",
      "iteration 44 current loss: 0.05841817706823349\n",
      "iteration 45 current loss: 0.05976326763629913\n",
      "iteration 46 current loss: 0.059011466801166534\n",
      "iteration 47 current loss: 0.05532271787524223\n",
      "iteration 48 current loss: 0.05775136500597\n",
      "iteration 49 current loss: 0.05987583473324776\n",
      "iteration 50 current loss: 0.0591486394405365\n",
      "iteration 51 current loss: 0.05711660161614418\n",
      "iteration 52 current loss: 0.0560116209089756\n",
      "iteration 53 current loss: 0.05802294984459877\n",
      "iteration 54 current loss: 0.05589757114648819\n",
      "iteration 55 current loss: 0.05572216957807541\n",
      "iteration 56 current loss: 0.05664456635713577\n",
      "iteration 57 current loss: 0.054825421422719955\n",
      "iteration 58 current loss: 0.05643467605113983\n",
      "iteration 59 current loss: 0.0573933981359005\n",
      "iteration 60 current loss: 0.05660025030374527\n",
      "iteration 61 current loss: 0.05391783267259598\n",
      "iteration 62 current loss: 0.057247914373874664\n",
      "iteration 63 current loss: 0.057719189673662186\n",
      "iteration 64 current loss: 0.055010195821523666\n",
      "iteration 65 current loss: 0.05674482882022858\n",
      "iteration 66 current loss: 0.057585958391427994\n",
      "iteration 67 current loss: 0.05854930728673935\n",
      "iteration 68 current loss: 0.05302613973617554\n",
      "iteration 69 current loss: 0.055447403341531754\n",
      "iteration 70 current loss: 0.05750967562198639\n",
      "iteration 71 current loss: 0.06037849187850952\n",
      "iteration 72 current loss: 0.05680382624268532\n",
      "iteration 73 current loss: 0.057627078145742416\n",
      "iteration 74 current loss: 0.05398985370993614\n",
      "iteration 75 current loss: 0.05727284029126167\n",
      "iteration 76 current loss: 0.05602145567536354\n",
      "iteration 77 current loss: 0.057706911116838455\n",
      "iteration 78 current loss: 0.05634787306189537\n",
      "iteration 79 current loss: 0.06112401559948921\n",
      "iteration 80 current loss: 0.059119049459695816\n",
      "iteration 81 current loss: 0.05829872190952301\n",
      "iteration 82 current loss: 0.06023790314793587\n",
      "iteration 83 current loss: 0.057881224900484085\n",
      "iteration 84 current loss: 0.056751154363155365\n",
      "iteration 85 current loss: 0.05516491457819939\n",
      "iteration 86 current loss: 0.05665852501988411\n",
      "iteration 87 current loss: 0.058298420161008835\n",
      "iteration 88 current loss: 0.05812810733914375\n",
      "iteration 89 current loss: 0.058114249259233475\n",
      "iteration 90 current loss: 0.06014654412865639\n",
      "iteration 91 current loss: 0.05626353621482849\n",
      "iteration 92 current loss: 0.05960996821522713\n",
      "iteration 93 current loss: 0.058864083141088486\n",
      "iteration 94 current loss: 0.0570584237575531\n",
      "iteration 95 current loss: 0.056314967572689056\n",
      "iteration 96 current loss: 0.05807921290397644\n",
      "iteration 97 current loss: 0.05681447684764862\n",
      "iteration 98 current loss: 0.05987387150526047\n",
      "iteration 99 current loss: 0.05891859903931618\n",
      "iteration 100 current loss: 0.05494067445397377\n",
      "iteration 101 current loss: 0.0550491102039814\n",
      "iteration 102 current loss: 0.05293063074350357\n",
      "iteration 103 current loss: 0.0585651658475399\n",
      "iteration 104 current loss: 0.058059923350811005\n",
      "iteration 105 current loss: 0.060486942529678345\n",
      "iteration 106 current loss: 0.05833493545651436\n",
      "iteration 107 current loss: 0.056219667196273804\n",
      "iteration 108 current loss: 0.05859140306711197\n",
      "iteration 109 current loss: 0.05439102277159691\n",
      "iteration 110 current loss: 0.05736791715025902\n",
      "iteration 111 current loss: 0.053515221923589706\n",
      "iteration 112 current loss: 0.05778012052178383\n",
      "iteration 113 current loss: 0.05774113908410072\n",
      "iteration 114 current loss: 0.055424753576517105\n",
      "iteration 115 current loss: 0.05752649903297424\n",
      "iteration 116 current loss: 0.058931443840265274\n",
      "iteration 117 current loss: 0.05388741195201874\n",
      "iteration 118 current loss: 0.060025013983249664\n",
      "iteration 119 current loss: 0.05493190139532089\n",
      "iteration 120 current loss: 0.05444099009037018\n",
      "iteration 121 current loss: 0.05815822258591652\n",
      "iteration 122 current loss: 0.056119155138731\n",
      "iteration 123 current loss: 0.05462445318698883\n",
      "iteration 124 current loss: 0.055355314165353775\n",
      "iteration 125 current loss: 0.05647065117955208\n",
      "iteration 126 current loss: 0.055845923721790314\n",
      "iteration 127 current loss: 0.057391468435525894\n",
      "iteration 128 current loss: 0.057009369134902954\n",
      "iteration 129 current loss: 0.05758735537528992\n",
      "iteration 130 current loss: 0.055096834897994995\n",
      "iteration 131 current loss: 0.05410384386777878\n",
      "iteration 132 current loss: 0.057508114725351334\n",
      "iteration 133 current loss: 0.05507059395313263\n",
      "iteration 134 current loss: 0.05568781495094299\n",
      "iteration 135 current loss: 0.056312862783670425\n",
      "iteration 136 current loss: 0.055563654750585556\n",
      "iteration 137 current loss: 0.05640542879700661\n",
      "iteration 138 current loss: 0.05704236030578613\n",
      "iteration 139 current loss: 0.05510378256440163\n",
      "iteration 140 current loss: 0.056710630655288696\n",
      "iteration 141 current loss: 0.056738536804914474\n",
      "iteration 142 current loss: 0.05641086772084236\n",
      "iteration 143 current loss: 0.05432894080877304\n",
      "iteration 144 current loss: 0.05620913207530975\n",
      "iteration 145 current loss: 0.05565030500292778\n",
      "iteration 146 current loss: 0.055779051035642624\n",
      "iteration 147 current loss: 0.060456838458776474\n",
      "iteration 148 current loss: 0.05739206075668335\n",
      "iteration 149 current loss: 0.0570739321410656\n",
      "iteration 150 current loss: 0.056403856724500656\n",
      "iteration 151 current loss: 0.05765019729733467\n",
      "iteration 152 current loss: 0.05791519209742546\n",
      "iteration 153 current loss: 0.05591883137822151\n",
      "iteration 154 current loss: 0.05679548531770706\n",
      "iteration 155 current loss: 0.05743284150958061\n",
      "iteration 156 current loss: 0.05712810158729553\n",
      "iteration 157 current loss: 0.058609962463378906\n",
      "iteration 158 current loss: 0.057857759296894073\n",
      "iteration 159 current loss: 0.05550553649663925\n",
      "iteration 160 current loss: 0.059872474521398544\n",
      "iteration 161 current loss: 0.05668487399816513\n",
      "iteration 162 current loss: 0.05744818598031998\n",
      "iteration 163 current loss: 0.0578790158033371\n",
      "iteration 164 current loss: 0.05703761801123619\n",
      "iteration 165 current loss: 0.059226538985967636\n",
      "iteration 166 current loss: 0.05781063809990883\n",
      "iteration 167 current loss: 0.059226542711257935\n",
      "iteration 168 current loss: 0.05530720204114914\n",
      "iteration 169 current loss: 0.05670662224292755\n",
      "iteration 170 current loss: 0.05467511713504791\n",
      "iteration 171 current loss: 0.0562036968767643\n",
      "iteration 172 current loss: 0.05505262687802315\n",
      "iteration 173 current loss: 0.05631108209490776\n",
      "iteration 174 current loss: 0.0532490499317646\n",
      "iteration 175 current loss: 0.05381011217832565\n",
      "iteration 176 current loss: 0.059669554233551025\n",
      "iteration 177 current loss: 0.05483289062976837\n",
      "iteration 178 current loss: 0.05607251822948456\n",
      "iteration 179 current loss: 0.05693301185965538\n",
      "iteration 180 current loss: 0.05560477450489998\n",
      "iteration 181 current loss: 0.05615507438778877\n",
      "iteration 182 current loss: 0.057922542095184326\n",
      "iteration 183 current loss: 0.05775607004761696\n",
      "iteration 184 current loss: 0.05537816882133484\n",
      "iteration 185 current loss: 0.05702883377671242\n",
      "iteration 186 current loss: 0.05459534749388695\n",
      "iteration 187 current loss: 0.05230015516281128\n",
      "iteration 188 current loss: 0.054687269032001495\n",
      "iteration 189 current loss: 0.05713866278529167\n",
      "iteration 190 current loss: 0.05760827288031578\n",
      "iteration 191 current loss: 0.05682508274912834\n",
      "iteration 192 current loss: 0.05705489218235016\n",
      "iteration 193 current loss: 0.056531935930252075\n",
      "iteration 194 current loss: 0.056631650775671005\n",
      "iteration 195 current loss: 0.060541264712810516\n",
      "iteration 196 current loss: 0.054089128971099854\n",
      "iteration 197 current loss: 0.05944199860095978\n",
      "iteration 198 current loss: 0.05541231855750084\n",
      "iteration 199 current loss: 0.0579378604888916\n",
      "iteration 200 current loss: 0.05737614631652832\n",
      "iteration 201 current loss: 0.055306173861026764\n",
      "iteration 202 current loss: 0.05750645697116852\n",
      "iteration 203 current loss: 0.05885648727416992\n",
      "iteration 204 current loss: 0.05719205364584923\n",
      "iteration 205 current loss: 0.057355236262083054\n",
      "iteration 206 current loss: 0.05537883937358856\n",
      "iteration 207 current loss: 0.059889912605285645\n",
      "iteration 208 current loss: 0.058259524405002594\n",
      "iteration 209 current loss: 0.05634898692369461\n",
      "iteration 210 current loss: 0.052023980766534805\n",
      "iteration 211 current loss: 0.058442335575819016\n",
      "iteration 212 current loss: 0.05785137414932251\n",
      "iteration 213 current loss: 0.05788299813866615\n",
      "iteration 214 current loss: 0.05661741644144058\n",
      "iteration 215 current loss: 0.0580933541059494\n",
      "iteration 216 current loss: 0.057589299976825714\n",
      "iteration 217 current loss: 0.0557933934032917\n",
      "iteration 218 current loss: 0.05743871256709099\n",
      "iteration 219 current loss: 0.052570562809705734\n",
      "iteration 220 current loss: 0.05570108816027641\n",
      "iteration 221 current loss: 0.05660582333803177\n",
      "iteration 222 current loss: 0.05622631683945656\n",
      "iteration 223 current loss: 0.05562083423137665\n",
      "iteration 224 current loss: 0.057077500969171524\n",
      "iteration 225 current loss: 0.05563470348715782\n",
      "iteration 226 current loss: 0.05534190684556961\n",
      "iteration 227 current loss: 0.0561230406165123\n",
      "iteration 228 current loss: 0.05615049973130226\n",
      "iteration 229 current loss: 0.05686485022306442\n",
      "iteration 230 current loss: 0.05816655233502388\n",
      "iteration 231 current loss: 0.05919156223535538\n",
      "iteration 232 current loss: 0.05587649345397949\n",
      "iteration 233 current loss: 0.05941750481724739\n",
      "iteration 234 current loss: 0.05663406476378441\n",
      "iteration 235 current loss: 0.05797889456152916\n",
      "iteration 236 current loss: 0.056656792759895325\n",
      "iteration 237 current loss: 0.05553726106882095\n",
      "iteration 238 current loss: 0.056412629783153534\n",
      "iteration 239 current loss: 0.05592494457960129\n",
      "iteration 240 current loss: 0.058522630482912064\n",
      "iteration 241 current loss: 0.05640973150730133\n",
      "iteration 242 current loss: 0.05923189967870712\n",
      "iteration 243 current loss: 0.056984998285770416\n",
      "iteration 244 current loss: 0.0566217377781868\n",
      "iteration 245 current loss: 0.059936076402664185\n",
      "iteration 246 current loss: 0.05480494722723961\n",
      "iteration 247 current loss: 0.05977080389857292\n",
      "iteration 248 current loss: 0.057091545313596725\n",
      "iteration 249 current loss: 0.05861382931470871\n",
      "iteration 250 current loss: 0.05837847664952278\n",
      "iteration 251 current loss: 0.05727939307689667\n",
      "iteration 252 current loss: 0.058352090418338776\n",
      "iteration 253 current loss: 0.05848860740661621\n",
      "iteration 254 current loss: 0.059777725487947464\n",
      "iteration 255 current loss: 0.057881493121385574\n",
      "iteration 256 current loss: 0.055537834763526917\n",
      "iteration 257 current loss: 0.058410413563251495\n",
      "iteration 258 current loss: 0.05709654465317726\n",
      "iteration 259 current loss: 0.05737907811999321\n",
      "iteration 260 current loss: 0.056676097214221954\n",
      "iteration 261 current loss: 0.05659976601600647\n",
      "iteration 262 current loss: 0.060411542654037476\n",
      "iteration 263 current loss: 0.05850299820303917\n",
      "iteration 264 current loss: 0.05818183347582817\n",
      "iteration 265 current loss: 0.058413971215486526\n",
      "iteration 266 current loss: 0.054465435445308685\n",
      "iteration 267 current loss: 0.05556432530283928\n",
      "iteration 268 current loss: 0.055850762873888016\n",
      "iteration 269 current loss: 0.05637158453464508\n",
      "iteration 270 current loss: 0.0561135895550251\n",
      "iteration 271 current loss: 0.056783437728881836\n",
      "iteration 272 current loss: 0.05481143668293953\n",
      "iteration 273 current loss: 0.05468260869383812\n",
      "iteration 274 current loss: 0.054818976670503616\n",
      "iteration 275 current loss: 0.05762394145131111\n",
      "iteration 276 current loss: 0.05372392013669014\n",
      "iteration 277 current loss: 0.05912312865257263\n",
      "iteration 278 current loss: 0.053085971623659134\n",
      "iteration 279 current loss: 0.055924732238054276\n",
      "iteration 280 current loss: 0.05543246492743492\n",
      "iteration 281 current loss: 0.05857736989855766\n",
      "iteration 282 current loss: 0.05925615504384041\n",
      "iteration 283 current loss: 0.05576049163937569\n",
      "iteration 284 current loss: 0.05600675940513611\n",
      "iteration 285 current loss: 0.05467698350548744\n",
      "iteration 286 current loss: 0.05682210624217987\n",
      "iteration 287 current loss: 0.059454113245010376\n",
      "iteration 288 current loss: 0.05368110164999962\n",
      "iteration 289 current loss: 0.05805299058556557\n",
      "iteration 290 current loss: 0.05821054056286812\n",
      "iteration 291 current loss: 0.054704755544662476\n",
      "iteration 292 current loss: 0.055972855538129807\n",
      "iteration 293 current loss: 0.0553329698741436\n",
      "iteration 294 current loss: 0.056610848754644394\n",
      "iteration 295 current loss: 0.05803820863366127\n",
      "iteration 296 current loss: 0.053955066949129105\n",
      "iteration 297 current loss: 0.06012020260095596\n",
      "iteration 298 current loss: 0.05415545776486397\n",
      "iteration 299 current loss: 0.057136207818984985\n",
      "iteration 300 current loss: 0.05711266025900841\n",
      "iteration 301 current loss: 0.056354690343141556\n",
      "iteration 302 current loss: 0.05780138820409775\n",
      "iteration 303 current loss: 0.05749625712633133\n",
      "iteration 304 current loss: 0.05563395097851753\n",
      "iteration 305 current loss: 0.05945267528295517\n",
      "iteration 306 current loss: 0.05464151129126549\n",
      "iteration 307 current loss: 0.05833808332681656\n",
      "iteration 308 current loss: 0.05680961161851883\n",
      "iteration 309 current loss: 0.062276359647512436\n",
      "iteration 310 current loss: 0.06149396300315857\n",
      "iteration 311 current loss: 0.0570586621761322\n",
      "iteration 312 current loss: 0.05499636381864548\n",
      "iteration 313 current loss: 0.05386808142066002\n",
      "iteration 314 current loss: 0.05605520308017731\n",
      "iteration 315 current loss: 0.05661553516983986\n",
      "iteration 316 current loss: 0.0559704564511776\n",
      "iteration 317 current loss: 0.05378492549061775\n",
      "iteration 318 current loss: 0.059031251817941666\n",
      "iteration 319 current loss: 0.05597544461488724\n",
      "iteration 320 current loss: 0.05660705268383026\n",
      "iteration 321 current loss: 0.05765189230442047\n",
      "iteration 322 current loss: 0.05358121916651726\n",
      "iteration 323 current loss: 0.05591839924454689\n",
      "iteration 324 current loss: 0.05771644413471222\n",
      "iteration 325 current loss: 0.059183698147535324\n",
      "iteration 326 current loss: 0.05415792763233185\n",
      "iteration 327 current loss: 0.05804008990526199\n",
      "iteration 328 current loss: 0.05649080127477646\n",
      "iteration 329 current loss: 0.054490335285663605\n",
      "iteration 330 current loss: 0.05974443629384041\n",
      "iteration 331 current loss: 0.05714000016450882\n",
      "iteration 332 current loss: 0.060298383235931396\n",
      "iteration 333 current loss: 0.05890398472547531\n",
      "iteration 334 current loss: 0.05492575094103813\n",
      "iteration 335 current loss: 0.055903732776641846\n",
      "iteration 336 current loss: 0.05599948763847351\n",
      "iteration 337 current loss: 0.05780547857284546\n",
      "iteration 338 current loss: 0.054435208439826965\n",
      "iteration 339 current loss: 0.057705819606781006\n",
      "iteration 340 current loss: 0.059399452060461044\n",
      "iteration 341 current loss: 0.05690907686948776\n",
      "iteration 342 current loss: 0.056121062487363815\n",
      "iteration 343 current loss: 0.05626968666911125\n",
      "iteration 344 current loss: 0.055235378444194794\n",
      "iteration 345 current loss: 0.05901341140270233\n",
      "iteration 346 current loss: 0.056523218750953674\n",
      "iteration 347 current loss: 0.05834095552563667\n",
      "iteration 348 current loss: 0.05575227737426758\n",
      "iteration 349 current loss: 0.05814956873655319\n",
      "iteration 350 current loss: 0.05637771636247635\n",
      "iteration 351 current loss: 0.060102228075265884\n",
      "iteration 352 current loss: 0.057810671627521515\n",
      "iteration 353 current loss: 0.05655503645539284\n",
      "iteration 354 current loss: 0.05407292768359184\n",
      "iteration 355 current loss: 0.05698774382472038\n",
      "iteration 356 current loss: 0.059827618300914764\n",
      "iteration 357 current loss: 0.05467831715941429\n",
      "iteration 358 current loss: 0.05634422227740288\n",
      "iteration 359 current loss: 0.05742263048887253\n",
      "iteration 360 current loss: 0.058352869004011154\n",
      "iteration 361 current loss: 0.0565115287899971\n",
      "iteration 362 current loss: 0.05806643143296242\n",
      "iteration 363 current loss: 0.05631667748093605\n",
      "iteration 364 current loss: 0.055792465806007385\n",
      "iteration 365 current loss: 0.06197917461395264\n",
      "iteration 366 current loss: 0.05321578308939934\n",
      "iteration 367 current loss: 0.05720677599310875\n",
      "iteration 368 current loss: 0.05528392642736435\n",
      "iteration 369 current loss: 0.05786110460758209\n",
      "iteration 370 current loss: 0.062059227377176285\n",
      "iteration 371 current loss: 0.05409099534153938\n",
      "iteration 372 current loss: 0.056111760437488556\n",
      "iteration 373 current loss: 0.055276863276958466\n",
      "iteration 374 current loss: 0.05980175733566284\n",
      "iteration 375 current loss: 0.05560286343097687\n",
      "iteration 376 current loss: 0.058807648718357086\n",
      "iteration 377 current loss: 0.05600501969456673\n",
      "iteration 378 current loss: 0.05784614384174347\n",
      "iteration 379 current loss: 0.05681002512574196\n",
      "iteration 380 current loss: 0.05788879096508026\n",
      "iteration 381 current loss: 0.054033465683460236\n",
      "iteration 382 current loss: 0.05445468798279762\n",
      "iteration 383 current loss: 0.05760343745350838\n",
      "iteration 384 current loss: 0.056659962981939316\n",
      "iteration 385 current loss: 0.05728652700781822\n",
      "iteration 386 current loss: 0.055850256234407425\n",
      "iteration 387 current loss: 0.05804690718650818\n",
      "iteration 388 current loss: 0.05639404058456421\n",
      "iteration 389 current loss: 0.05642744526267052\n",
      "iteration 390 current loss: 0.05974239856004715\n",
      "iteration 391 current loss: 0.055925510823726654\n",
      "iteration 392 current loss: 0.053597915917634964\n",
      "iteration 393 current loss: 0.05901554971933365\n",
      "iteration 394 current loss: 0.057313960045576096\n",
      "iteration 395 current loss: 0.05704483389854431\n",
      "iteration 396 current loss: 0.05574483424425125\n",
      "iteration 397 current loss: 0.05668625980615616\n",
      "iteration 398 current loss: 0.055077601224184036\n",
      "iteration 399 current loss: 0.05547627806663513\n",
      "iteration 400 current loss: 0.05478798598051071\n",
      "iteration 401 current loss: 0.05621127784252167\n",
      "iteration 402 current loss: 0.05944141373038292\n",
      "iteration 403 current loss: 0.05809192731976509\n",
      "iteration 404 current loss: 0.06056741625070572\n",
      "iteration 405 current loss: 0.05913364887237549\n",
      "iteration 406 current loss: 0.06008361652493477\n",
      "iteration 407 current loss: 0.057900164276361465\n",
      "iteration 408 current loss: 0.058878783136606216\n",
      "iteration 409 current loss: 0.05655457451939583\n",
      "iteration 410 current loss: 0.06011909246444702\n",
      "\t\tEpoch 11/100 complete. Epoch loss 0.05685526407221808\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 11, Validation Loss: 0.058749521733261645\n",
      "best loss 0.05685526407221808\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.055498018860816956\n",
      "iteration 1 current loss: 0.054713860154151917\n",
      "iteration 2 current loss: 0.05748758465051651\n",
      "iteration 3 current loss: 0.05755186080932617\n",
      "iteration 4 current loss: 0.05817680060863495\n",
      "iteration 5 current loss: 0.05810471996665001\n",
      "iteration 6 current loss: 0.055116213858127594\n",
      "iteration 7 current loss: 0.053859319537878036\n",
      "iteration 8 current loss: 0.054796524345874786\n",
      "iteration 9 current loss: 0.05568540841341019\n",
      "iteration 10 current loss: 0.05907224863767624\n",
      "iteration 11 current loss: 0.055929165333509445\n",
      "iteration 12 current loss: 0.056564465165138245\n",
      "iteration 13 current loss: 0.05779644474387169\n",
      "iteration 14 current loss: 0.05620672181248665\n",
      "iteration 15 current loss: 0.05681082233786583\n",
      "iteration 16 current loss: 0.05843013525009155\n",
      "iteration 17 current loss: 0.05665730684995651\n",
      "iteration 18 current loss: 0.057143062353134155\n",
      "iteration 19 current loss: 0.053790442645549774\n",
      "iteration 20 current loss: 0.054751794785261154\n",
      "iteration 21 current loss: 0.05536190792918205\n",
      "iteration 22 current loss: 0.05557772517204285\n",
      "iteration 23 current loss: 0.05658385530114174\n",
      "iteration 24 current loss: 0.057032398879528046\n",
      "iteration 25 current loss: 0.05396411567926407\n",
      "iteration 26 current loss: 0.05609171837568283\n",
      "iteration 27 current loss: 0.05454669147729874\n",
      "iteration 28 current loss: 0.05970609560608864\n",
      "iteration 29 current loss: 0.060164935886859894\n",
      "iteration 30 current loss: 0.05650470778346062\n",
      "iteration 31 current loss: 0.05749203637242317\n",
      "iteration 32 current loss: 0.05708454176783562\n",
      "iteration 33 current loss: 0.057439710944890976\n",
      "iteration 34 current loss: 0.0596267394721508\n",
      "iteration 35 current loss: 0.05703270435333252\n",
      "iteration 36 current loss: 0.05516238883137703\n",
      "iteration 37 current loss: 0.055514831095933914\n",
      "iteration 38 current loss: 0.056615713983774185\n",
      "iteration 39 current loss: 0.05866410583257675\n",
      "iteration 40 current loss: 0.05700542405247688\n",
      "iteration 41 current loss: 0.05790843442082405\n",
      "iteration 42 current loss: 0.05686917155981064\n",
      "iteration 43 current loss: 0.05820167437195778\n",
      "iteration 44 current loss: 0.054757192730903625\n",
      "iteration 45 current loss: 0.05410127341747284\n",
      "iteration 46 current loss: 0.05431210622191429\n",
      "iteration 47 current loss: 0.05751943588256836\n",
      "iteration 48 current loss: 0.0583450011909008\n",
      "iteration 49 current loss: 0.056176889687776566\n",
      "iteration 50 current loss: 0.05844351276755333\n",
      "iteration 51 current loss: 0.05636925250291824\n",
      "iteration 52 current loss: 0.056040119379758835\n",
      "iteration 53 current loss: 0.05433058366179466\n",
      "iteration 54 current loss: 0.05939292907714844\n",
      "iteration 55 current loss: 0.059015098959207535\n",
      "iteration 56 current loss: 0.05843485891819\n",
      "iteration 57 current loss: 0.060735154896974564\n",
      "iteration 58 current loss: 0.05675571411848068\n",
      "iteration 59 current loss: 0.05635010823607445\n",
      "iteration 60 current loss: 0.055054038763046265\n",
      "iteration 61 current loss: 0.056869227439165115\n",
      "iteration 62 current loss: 0.058112435042858124\n",
      "iteration 63 current loss: 0.05244894325733185\n",
      "iteration 64 current loss: 0.05932334065437317\n",
      "iteration 65 current loss: 0.054952189326286316\n",
      "iteration 66 current loss: 0.053150396794080734\n",
      "iteration 67 current loss: 0.059200532734394073\n",
      "iteration 68 current loss: 0.05514674633741379\n",
      "iteration 69 current loss: 0.05714988708496094\n",
      "iteration 70 current loss: 0.05902259424328804\n",
      "iteration 71 current loss: 0.05619826912879944\n",
      "iteration 72 current loss: 0.05749356746673584\n",
      "iteration 73 current loss: 0.05775975435972214\n",
      "iteration 74 current loss: 0.05768256261944771\n",
      "iteration 75 current loss: 0.05839001014828682\n",
      "iteration 76 current loss: 0.0571732223033905\n",
      "iteration 77 current loss: 0.05682017281651497\n",
      "iteration 78 current loss: 0.0519549585878849\n",
      "iteration 79 current loss: 0.059035591781139374\n",
      "iteration 80 current loss: 0.05553897097706795\n",
      "iteration 81 current loss: 0.055475085973739624\n",
      "iteration 82 current loss: 0.059396274387836456\n",
      "iteration 83 current loss: 0.057637762278318405\n",
      "iteration 84 current loss: 0.05298218876123428\n",
      "iteration 85 current loss: 0.05981770157814026\n",
      "iteration 86 current loss: 0.05501669645309448\n",
      "iteration 87 current loss: 0.05556623265147209\n",
      "iteration 88 current loss: 0.05662335827946663\n",
      "iteration 89 current loss: 0.0533415861427784\n",
      "iteration 90 current loss: 0.05727643519639969\n",
      "iteration 91 current loss: 0.05726746469736099\n",
      "iteration 92 current loss: 0.05873407796025276\n",
      "iteration 93 current loss: 0.05882512778043747\n",
      "iteration 94 current loss: 0.058522120118141174\n",
      "iteration 95 current loss: 0.058556344360113144\n",
      "iteration 96 current loss: 0.056182924658060074\n",
      "iteration 97 current loss: 0.05299441143870354\n",
      "iteration 98 current loss: 0.055814143270254135\n",
      "iteration 99 current loss: 0.05371559038758278\n",
      "iteration 100 current loss: 0.05597682297229767\n",
      "iteration 101 current loss: 0.05632210522890091\n",
      "iteration 102 current loss: 0.051167480647563934\n",
      "iteration 103 current loss: 0.06073887273669243\n",
      "iteration 104 current loss: 0.059077244251966476\n",
      "iteration 105 current loss: 0.057616233825683594\n",
      "iteration 106 current loss: 0.0547577366232872\n",
      "iteration 107 current loss: 0.05698896199464798\n",
      "iteration 108 current loss: 0.05627789348363876\n",
      "iteration 109 current loss: 0.05399825796484947\n",
      "iteration 110 current loss: 0.05775076523423195\n",
      "iteration 111 current loss: 0.05779491737484932\n",
      "iteration 112 current loss: 0.05586837977170944\n",
      "iteration 113 current loss: 0.05648408085107803\n",
      "iteration 114 current loss: 0.05779247730970383\n",
      "iteration 115 current loss: 0.0563330203294754\n",
      "iteration 116 current loss: 0.056699320673942566\n",
      "iteration 117 current loss: 0.05607916787266731\n",
      "iteration 118 current loss: 0.05495956167578697\n",
      "iteration 119 current loss: 0.05851222574710846\n",
      "iteration 120 current loss: 0.053337398916482925\n",
      "iteration 121 current loss: 0.05490856245160103\n",
      "iteration 122 current loss: 0.05354435741901398\n",
      "iteration 123 current loss: 0.056602027267217636\n",
      "iteration 124 current loss: 0.05587753653526306\n",
      "iteration 125 current loss: 0.05806073173880577\n",
      "iteration 126 current loss: 0.056446269154548645\n",
      "iteration 127 current loss: 0.05606959015130997\n",
      "iteration 128 current loss: 0.05710350349545479\n",
      "iteration 129 current loss: 0.05702954903244972\n",
      "iteration 130 current loss: 0.05695514753460884\n",
      "iteration 131 current loss: 0.05339173972606659\n",
      "iteration 132 current loss: 0.058838989585638046\n",
      "iteration 133 current loss: 0.05591495707631111\n",
      "iteration 134 current loss: 0.05475854128599167\n",
      "iteration 135 current loss: 0.058382678776979446\n",
      "iteration 136 current loss: 0.0575137659907341\n",
      "iteration 137 current loss: 0.05393423140048981\n",
      "iteration 138 current loss: 0.05507101118564606\n",
      "iteration 139 current loss: 0.057290591299533844\n",
      "iteration 140 current loss: 0.05638739839196205\n",
      "iteration 141 current loss: 0.05393664538860321\n",
      "iteration 142 current loss: 0.05504398047924042\n",
      "iteration 143 current loss: 0.05331270024180412\n",
      "iteration 144 current loss: 0.06030271202325821\n",
      "iteration 145 current loss: 0.05886247754096985\n",
      "iteration 146 current loss: 0.054967932403087616\n",
      "iteration 147 current loss: 0.056373097002506256\n",
      "iteration 148 current loss: 0.054741863161325455\n",
      "iteration 149 current loss: 0.05607520416378975\n",
      "iteration 150 current loss: 0.05569612607359886\n",
      "iteration 151 current loss: 0.05420377478003502\n",
      "iteration 152 current loss: 0.056510746479034424\n",
      "iteration 153 current loss: 0.057485420256853104\n",
      "iteration 154 current loss: 0.05606919899582863\n",
      "iteration 155 current loss: 0.05645304173231125\n",
      "iteration 156 current loss: 0.05694153159856796\n",
      "iteration 157 current loss: 0.05487668886780739\n",
      "iteration 158 current loss: 0.057435326278209686\n",
      "iteration 159 current loss: 0.05691453814506531\n",
      "iteration 160 current loss: 0.05594605952501297\n",
      "iteration 161 current loss: 0.05900023505091667\n",
      "iteration 162 current loss: 0.055176399648189545\n",
      "iteration 163 current loss: 0.058078207075595856\n",
      "iteration 164 current loss: 0.056389499455690384\n",
      "iteration 165 current loss: 0.055456824600696564\n",
      "iteration 166 current loss: 0.05543725565075874\n",
      "iteration 167 current loss: 0.05619625747203827\n",
      "iteration 168 current loss: 0.05587642267346382\n",
      "iteration 169 current loss: 0.05625647306442261\n",
      "iteration 170 current loss: 0.05727378651499748\n",
      "iteration 171 current loss: 0.05620491877198219\n",
      "iteration 172 current loss: 0.054580964148044586\n",
      "iteration 173 current loss: 0.05633421242237091\n",
      "iteration 174 current loss: 0.05679243803024292\n",
      "iteration 175 current loss: 0.0562998503446579\n",
      "iteration 176 current loss: 0.05581676587462425\n",
      "iteration 177 current loss: 0.057894423604011536\n",
      "iteration 178 current loss: 0.05776974931359291\n",
      "iteration 179 current loss: 0.05682817101478577\n",
      "iteration 180 current loss: 0.05651240795850754\n",
      "iteration 181 current loss: 0.055168267339468\n",
      "iteration 182 current loss: 0.05847986415028572\n",
      "iteration 183 current loss: 0.05315537005662918\n",
      "iteration 184 current loss: 0.054421309381723404\n",
      "iteration 185 current loss: 0.05488324537873268\n",
      "iteration 186 current loss: 0.056643933057785034\n",
      "iteration 187 current loss: 0.056716009974479675\n",
      "iteration 188 current loss: 0.05602896958589554\n",
      "iteration 189 current loss: 0.05922963097691536\n",
      "iteration 190 current loss: 0.05465900897979736\n",
      "iteration 191 current loss: 0.055376503616571426\n",
      "iteration 192 current loss: 0.057630546391010284\n",
      "iteration 193 current loss: 0.05719023197889328\n",
      "iteration 194 current loss: 0.05352116376161575\n",
      "iteration 195 current loss: 0.05702861770987511\n",
      "iteration 196 current loss: 0.05883358046412468\n",
      "iteration 197 current loss: 0.05678613856434822\n",
      "iteration 198 current loss: 0.05721729248762131\n",
      "iteration 199 current loss: 0.05722291022539139\n",
      "iteration 200 current loss: 0.05335206165909767\n",
      "iteration 201 current loss: 0.05616683512926102\n",
      "iteration 202 current loss: 0.055795419961214066\n",
      "iteration 203 current loss: 0.05718417838215828\n",
      "iteration 204 current loss: 0.05619295313954353\n",
      "iteration 205 current loss: 0.0548570342361927\n",
      "iteration 206 current loss: 0.05836787074804306\n",
      "iteration 207 current loss: 0.05755597725510597\n",
      "iteration 208 current loss: 0.058446165174245834\n",
      "iteration 209 current loss: 0.05628328025341034\n",
      "iteration 210 current loss: 0.05809955298900604\n",
      "iteration 211 current loss: 0.05586974695324898\n",
      "iteration 212 current loss: 0.05193774029612541\n",
      "iteration 213 current loss: 0.054747432470321655\n",
      "iteration 214 current loss: 0.057139553129673004\n",
      "iteration 215 current loss: 0.0569763109087944\n",
      "iteration 216 current loss: 0.05789388716220856\n",
      "iteration 217 current loss: 0.05582370609045029\n",
      "iteration 218 current loss: 0.05499011650681496\n",
      "iteration 219 current loss: 0.05519908666610718\n",
      "iteration 220 current loss: 0.05678547918796539\n",
      "iteration 221 current loss: 0.059381600469350815\n",
      "iteration 222 current loss: 0.05845986679196358\n",
      "iteration 223 current loss: 0.05704887583851814\n",
      "iteration 224 current loss: 0.05913248285651207\n",
      "iteration 225 current loss: 0.05526505038142204\n",
      "iteration 226 current loss: 0.05598655343055725\n",
      "iteration 227 current loss: 0.05774668604135513\n",
      "iteration 228 current loss: 0.06150966137647629\n",
      "iteration 229 current loss: 0.05797576159238815\n",
      "iteration 230 current loss: 0.05573315918445587\n",
      "iteration 231 current loss: 0.05765395611524582\n",
      "iteration 232 current loss: 0.05709010735154152\n",
      "iteration 233 current loss: 0.058142874389886856\n",
      "iteration 234 current loss: 0.056030094623565674\n",
      "iteration 235 current loss: 0.05456119775772095\n",
      "iteration 236 current loss: 0.05388962849974632\n",
      "iteration 237 current loss: 0.05728733912110329\n",
      "iteration 238 current loss: 0.05729370564222336\n",
      "iteration 239 current loss: 0.05732935667037964\n",
      "iteration 240 current loss: 0.05767093226313591\n",
      "iteration 241 current loss: 0.05756045877933502\n",
      "iteration 242 current loss: 0.0566592700779438\n",
      "iteration 243 current loss: 0.05525106191635132\n",
      "iteration 244 current loss: 0.05663749575614929\n",
      "iteration 245 current loss: 0.056500427424907684\n",
      "iteration 246 current loss: 0.05497782304883003\n",
      "iteration 247 current loss: 0.054995257407426834\n",
      "iteration 248 current loss: 0.05722290650010109\n",
      "iteration 249 current loss: 0.05678601562976837\n",
      "iteration 250 current loss: 0.059392232447862625\n",
      "iteration 251 current loss: 0.05540767312049866\n",
      "iteration 252 current loss: 0.05831184610724449\n",
      "iteration 253 current loss: 0.05780356377363205\n",
      "iteration 254 current loss: 0.056572429835796356\n",
      "iteration 255 current loss: 0.054848574101924896\n",
      "iteration 256 current loss: 0.055713869631290436\n",
      "iteration 257 current loss: 0.05803585797548294\n",
      "iteration 258 current loss: 0.05710897967219353\n",
      "iteration 259 current loss: 0.05496847629547119\n",
      "iteration 260 current loss: 0.05501251667737961\n",
      "iteration 261 current loss: 0.057908374816179276\n",
      "iteration 262 current loss: 0.053807757794857025\n",
      "iteration 263 current loss: 0.0581136979162693\n",
      "iteration 264 current loss: 0.05793201923370361\n",
      "iteration 265 current loss: 0.05379094183444977\n",
      "iteration 266 current loss: 0.05981549620628357\n",
      "iteration 267 current loss: 0.05691004544496536\n",
      "iteration 268 current loss: 0.056707337498664856\n",
      "iteration 269 current loss: 0.05577118322253227\n",
      "iteration 270 current loss: 0.05363894999027252\n",
      "iteration 271 current loss: 0.053572338074445724\n",
      "iteration 272 current loss: 0.0558944009244442\n",
      "iteration 273 current loss: 0.05684645473957062\n",
      "iteration 274 current loss: 0.058014992624521255\n",
      "iteration 275 current loss: 0.0541335754096508\n",
      "iteration 276 current loss: 0.057267095893621445\n",
      "iteration 277 current loss: 0.055608272552490234\n",
      "iteration 278 current loss: 0.05895667523145676\n",
      "iteration 279 current loss: 0.052724797278642654\n",
      "iteration 280 current loss: 0.0572427436709404\n",
      "iteration 281 current loss: 0.05524144694209099\n",
      "iteration 282 current loss: 0.055916715413331985\n",
      "iteration 283 current loss: 0.055691853165626526\n",
      "iteration 284 current loss: 0.057091448456048965\n",
      "iteration 285 current loss: 0.06074979156255722\n",
      "iteration 286 current loss: 0.05613546818494797\n",
      "iteration 287 current loss: 0.05594692379236221\n",
      "iteration 288 current loss: 0.057248204946517944\n",
      "iteration 289 current loss: 0.054126545786857605\n",
      "iteration 290 current loss: 0.057142503559589386\n",
      "iteration 291 current loss: 0.05604775622487068\n",
      "iteration 292 current loss: 0.05705227330327034\n",
      "iteration 293 current loss: 0.055328406393527985\n",
      "iteration 294 current loss: 0.05470527708530426\n",
      "iteration 295 current loss: 0.05593735724687576\n",
      "iteration 296 current loss: 0.05425543338060379\n",
      "iteration 297 current loss: 0.058511748909950256\n",
      "iteration 298 current loss: 0.05515838414430618\n",
      "iteration 299 current loss: 0.05364682152867317\n",
      "iteration 300 current loss: 0.05523338168859482\n",
      "iteration 301 current loss: 0.05749738588929176\n",
      "iteration 302 current loss: 0.05687488988041878\n",
      "iteration 303 current loss: 0.058325957506895065\n",
      "iteration 304 current loss: 0.05710161104798317\n",
      "iteration 305 current loss: 0.05543170124292374\n",
      "iteration 306 current loss: 0.054518647491931915\n",
      "iteration 307 current loss: 0.05623443052172661\n",
      "iteration 308 current loss: 0.0567457489669323\n",
      "iteration 309 current loss: 0.06046677380800247\n",
      "iteration 310 current loss: 0.05997798964381218\n",
      "iteration 311 current loss: 0.056408047676086426\n",
      "iteration 312 current loss: 0.05611815303564072\n",
      "iteration 313 current loss: 0.056427136063575745\n",
      "iteration 314 current loss: 0.054843969643116\n",
      "iteration 315 current loss: 0.05361216142773628\n",
      "iteration 316 current loss: 0.05812660977244377\n",
      "iteration 317 current loss: 0.05905585363507271\n",
      "iteration 318 current loss: 0.05350562930107117\n",
      "iteration 319 current loss: 0.059068549424409866\n",
      "iteration 320 current loss: 0.05715560168027878\n",
      "iteration 321 current loss: 0.05782827362418175\n",
      "iteration 322 current loss: 0.05682474747300148\n",
      "iteration 323 current loss: 0.05625004321336746\n",
      "iteration 324 current loss: 0.05882934480905533\n",
      "iteration 325 current loss: 0.05856920778751373\n",
      "iteration 326 current loss: 0.05674042925238609\n",
      "iteration 327 current loss: 0.05539718270301819\n",
      "iteration 328 current loss: 0.05569405108690262\n",
      "iteration 329 current loss: 0.05678880959749222\n",
      "iteration 330 current loss: 0.058475710451602936\n",
      "iteration 331 current loss: 0.05565455183386803\n",
      "iteration 332 current loss: 0.05548251047730446\n",
      "iteration 333 current loss: 0.059974927455186844\n",
      "iteration 334 current loss: 0.05557851493358612\n",
      "iteration 335 current loss: 0.0580761581659317\n",
      "iteration 336 current loss: 0.05600981041789055\n",
      "iteration 337 current loss: 0.05699508264660835\n",
      "iteration 338 current loss: 0.05667763948440552\n",
      "iteration 339 current loss: 0.05897054821252823\n",
      "iteration 340 current loss: 0.053868617862463\n",
      "iteration 341 current loss: 0.057493556290864944\n",
      "iteration 342 current loss: 0.05535556748509407\n",
      "iteration 343 current loss: 0.05921843275427818\n",
      "iteration 344 current loss: 0.05489733815193176\n",
      "iteration 345 current loss: 0.06066019460558891\n",
      "iteration 346 current loss: 0.05481478199362755\n",
      "iteration 347 current loss: 0.05340481549501419\n",
      "iteration 348 current loss: 0.05692300200462341\n",
      "iteration 349 current loss: 0.05444465950131416\n",
      "iteration 350 current loss: 0.05969686061143875\n",
      "iteration 351 current loss: 0.05624573305249214\n",
      "iteration 352 current loss: 0.05718206614255905\n",
      "iteration 353 current loss: 0.05571947246789932\n",
      "iteration 354 current loss: 0.058825936168432236\n",
      "iteration 355 current loss: 0.05398688465356827\n",
      "iteration 356 current loss: 0.05425723269581795\n",
      "iteration 357 current loss: 0.05519716441631317\n",
      "iteration 358 current loss: 0.056105755269527435\n",
      "iteration 359 current loss: 0.05973958224058151\n",
      "iteration 360 current loss: 0.057872410863637924\n",
      "iteration 361 current loss: 0.05754828453063965\n",
      "iteration 362 current loss: 0.056551702320575714\n",
      "iteration 363 current loss: 0.059094298630952835\n",
      "iteration 364 current loss: 0.05607924982905388\n",
      "iteration 365 current loss: 0.05751843750476837\n",
      "iteration 366 current loss: 0.05801510810852051\n",
      "iteration 367 current loss: 0.05545741692185402\n",
      "iteration 368 current loss: 0.06049492582678795\n",
      "iteration 369 current loss: 0.05452127754688263\n",
      "iteration 370 current loss: 0.059068385511636734\n",
      "iteration 371 current loss: 0.05519893392920494\n",
      "iteration 372 current loss: 0.05680649355053902\n",
      "iteration 373 current loss: 0.05496038869023323\n",
      "iteration 374 current loss: 0.05955712869763374\n",
      "iteration 375 current loss: 0.062497347593307495\n",
      "iteration 376 current loss: 0.05767100304365158\n",
      "iteration 377 current loss: 0.05763581022620201\n",
      "iteration 378 current loss: 0.05638920143246651\n",
      "iteration 379 current loss: 0.05669521167874336\n",
      "iteration 380 current loss: 0.05746883898973465\n",
      "iteration 381 current loss: 0.056889597326517105\n",
      "iteration 382 current loss: 0.05727633833885193\n",
      "iteration 383 current loss: 0.054925814270973206\n",
      "iteration 384 current loss: 0.05392356216907501\n",
      "iteration 385 current loss: 0.05372408777475357\n",
      "iteration 386 current loss: 0.05759638547897339\n",
      "iteration 387 current loss: 0.05796375870704651\n",
      "iteration 388 current loss: 0.05184093862771988\n",
      "iteration 389 current loss: 0.057569388300180435\n",
      "iteration 390 current loss: 0.05452699214220047\n",
      "iteration 391 current loss: 0.05995647981762886\n",
      "iteration 392 current loss: 0.05780215561389923\n",
      "iteration 393 current loss: 0.05859706178307533\n",
      "iteration 394 current loss: 0.056022759526968\n",
      "iteration 395 current loss: 0.05269356444478035\n",
      "iteration 396 current loss: 0.059413302689790726\n",
      "iteration 397 current loss: 0.06001610308885574\n",
      "iteration 398 current loss: 0.05485649034380913\n",
      "iteration 399 current loss: 0.05530207231640816\n",
      "iteration 400 current loss: 0.05417921021580696\n",
      "iteration 401 current loss: 0.05693066492676735\n",
      "iteration 402 current loss: 0.05674537643790245\n",
      "iteration 403 current loss: 0.057064302265644073\n",
      "iteration 404 current loss: 0.054438404738903046\n",
      "iteration 405 current loss: 0.06100735813379288\n",
      "iteration 406 current loss: 0.055853333324193954\n",
      "iteration 407 current loss: 0.058644603937864304\n",
      "iteration 408 current loss: 0.057715464383363724\n",
      "iteration 409 current loss: 0.0569591261446476\n",
      "iteration 410 current loss: 0.06553877145051956\n",
      "\t\tEpoch 12/100 complete. Epoch loss 0.05660000005668967\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 12, Validation Loss: 0.05854433053173125\n",
      "best loss 0.05660000005668967\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.05675041675567627\n",
      "iteration 1 current loss: 0.05402811989188194\n",
      "iteration 2 current loss: 0.05669337883591652\n",
      "iteration 3 current loss: 0.05662911385297775\n",
      "iteration 4 current loss: 0.05961441993713379\n",
      "iteration 5 current loss: 0.05556850880384445\n",
      "iteration 6 current loss: 0.05703072249889374\n",
      "iteration 7 current loss: 0.05668509379029274\n",
      "iteration 8 current loss: 0.05719873681664467\n",
      "iteration 9 current loss: 0.053593188524246216\n",
      "iteration 10 current loss: 0.05556948482990265\n",
      "iteration 11 current loss: 0.05738722160458565\n",
      "iteration 12 current loss: 0.06022335961461067\n",
      "iteration 13 current loss: 0.054190099239349365\n",
      "iteration 14 current loss: 0.055324532091617584\n",
      "iteration 15 current loss: 0.053884733468294144\n",
      "iteration 16 current loss: 0.05597579479217529\n",
      "iteration 17 current loss: 0.05512407049536705\n",
      "iteration 18 current loss: 0.056316617876291275\n",
      "iteration 19 current loss: 0.05457581579685211\n",
      "iteration 20 current loss: 0.05650651082396507\n",
      "iteration 21 current loss: 0.05846274271607399\n",
      "iteration 22 current loss: 0.055670708417892456\n",
      "iteration 23 current loss: 0.058082837611436844\n",
      "iteration 24 current loss: 0.055893395096063614\n",
      "iteration 25 current loss: 0.052896130830049515\n",
      "iteration 26 current loss: 0.05755571275949478\n",
      "iteration 27 current loss: 0.05891495198011398\n",
      "iteration 28 current loss: 0.05013784393668175\n",
      "iteration 29 current loss: 0.05670272931456566\n",
      "iteration 30 current loss: 0.056391194462776184\n",
      "iteration 31 current loss: 0.05406689643859863\n",
      "iteration 32 current loss: 0.05690164864063263\n",
      "iteration 33 current loss: 0.05892298370599747\n",
      "iteration 34 current loss: 0.05759715661406517\n",
      "iteration 35 current loss: 0.056876905262470245\n",
      "iteration 36 current loss: 0.056107018142938614\n",
      "iteration 37 current loss: 0.056641023606061935\n",
      "iteration 38 current loss: 0.0568450391292572\n",
      "iteration 39 current loss: 0.05467035248875618\n",
      "iteration 40 current loss: 0.05329527705907822\n",
      "iteration 41 current loss: 0.05844821780920029\n",
      "iteration 42 current loss: 0.0569203644990921\n",
      "iteration 43 current loss: 0.05523664504289627\n",
      "iteration 44 current loss: 0.05875704437494278\n",
      "iteration 45 current loss: 0.058523088693618774\n",
      "iteration 46 current loss: 0.05351977050304413\n",
      "iteration 47 current loss: 0.05709055811166763\n",
      "iteration 48 current loss: 0.057167742401361465\n",
      "iteration 49 current loss: 0.05481307581067085\n",
      "iteration 50 current loss: 0.05833260715007782\n",
      "iteration 51 current loss: 0.05759419873356819\n",
      "iteration 52 current loss: 0.058758676052093506\n",
      "iteration 53 current loss: 0.05357685685157776\n",
      "iteration 54 current loss: 0.055579230189323425\n",
      "iteration 55 current loss: 0.054400600492954254\n",
      "iteration 56 current loss: 0.05568131431937218\n",
      "iteration 57 current loss: 0.052861250936985016\n",
      "iteration 58 current loss: 0.058007922023534775\n",
      "iteration 59 current loss: 0.05701141804456711\n",
      "iteration 60 current loss: 0.056623466312885284\n",
      "iteration 61 current loss: 0.054672278463840485\n",
      "iteration 62 current loss: 0.05684346705675125\n",
      "iteration 63 current loss: 0.056310176849365234\n",
      "iteration 64 current loss: 0.052021175622940063\n",
      "iteration 65 current loss: 0.054311301559209824\n",
      "iteration 66 current loss: 0.05312681943178177\n",
      "iteration 67 current loss: 0.05649526044726372\n",
      "iteration 68 current loss: 0.056531839072704315\n",
      "iteration 69 current loss: 0.05808797478675842\n",
      "iteration 70 current loss: 0.05639290437102318\n",
      "iteration 71 current loss: 0.052813831716775894\n",
      "iteration 72 current loss: 0.056107163429260254\n",
      "iteration 73 current loss: 0.05483283847570419\n",
      "iteration 74 current loss: 0.05671778321266174\n",
      "iteration 75 current loss: 0.05452990531921387\n",
      "iteration 76 current loss: 0.05455050989985466\n",
      "iteration 77 current loss: 0.05721454322338104\n",
      "iteration 78 current loss: 0.05298204347491264\n",
      "iteration 79 current loss: 0.05440372973680496\n",
      "iteration 80 current loss: 0.05807352066040039\n",
      "iteration 81 current loss: 0.0541338212788105\n",
      "iteration 82 current loss: 0.05588072910904884\n",
      "iteration 83 current loss: 0.05452227592468262\n",
      "iteration 84 current loss: 0.052695732563734055\n",
      "iteration 85 current loss: 0.05727117508649826\n",
      "iteration 86 current loss: 0.05690143257379532\n",
      "iteration 87 current loss: 0.05593709647655487\n",
      "iteration 88 current loss: 0.056328702718019485\n",
      "iteration 89 current loss: 0.055286526679992676\n",
      "iteration 90 current loss: 0.05842416360974312\n",
      "iteration 91 current loss: 0.05586526542901993\n",
      "iteration 92 current loss: 0.05268454551696777\n",
      "iteration 93 current loss: 0.052608516067266464\n",
      "iteration 94 current loss: 0.05444943159818649\n",
      "iteration 95 current loss: 0.05675990507006645\n",
      "iteration 96 current loss: 0.05347347632050514\n",
      "iteration 97 current loss: 0.055460330098867416\n",
      "iteration 98 current loss: 0.05182625353336334\n",
      "iteration 99 current loss: 0.05709903687238693\n",
      "iteration 100 current loss: 0.05248114839196205\n",
      "iteration 101 current loss: 0.057403989136219025\n",
      "iteration 102 current loss: 0.05791439861059189\n",
      "iteration 103 current loss: 0.055982377380132675\n",
      "iteration 104 current loss: 0.0565653033554554\n",
      "iteration 105 current loss: 0.056031934916973114\n",
      "iteration 106 current loss: 0.05645278841257095\n",
      "iteration 107 current loss: 0.055106621235609055\n",
      "iteration 108 current loss: 0.059221051633358\n",
      "iteration 109 current loss: 0.05748342350125313\n",
      "iteration 110 current loss: 0.05721215903759003\n",
      "iteration 111 current loss: 0.05730118975043297\n",
      "iteration 112 current loss: 0.056004587560892105\n",
      "iteration 113 current loss: 0.05899433046579361\n",
      "iteration 114 current loss: 0.05445072427392006\n",
      "iteration 115 current loss: 0.05291149765253067\n",
      "iteration 116 current loss: 0.05610645189881325\n",
      "iteration 117 current loss: 0.05131493881344795\n",
      "iteration 118 current loss: 0.05451269820332527\n",
      "iteration 119 current loss: 0.056237149983644485\n",
      "iteration 120 current loss: 0.0571700744330883\n",
      "iteration 121 current loss: 0.056690990924835205\n",
      "iteration 122 current loss: 0.05331353098154068\n",
      "iteration 123 current loss: 0.053903091698884964\n",
      "iteration 124 current loss: 0.054537978023290634\n",
      "iteration 125 current loss: 0.05331936106085777\n",
      "iteration 126 current loss: 0.05107862874865532\n",
      "iteration 127 current loss: 0.055734772235155106\n",
      "iteration 128 current loss: 0.05856115743517876\n",
      "iteration 129 current loss: 0.05652868375182152\n",
      "iteration 130 current loss: 0.05436272919178009\n",
      "iteration 131 current loss: 0.05787119269371033\n",
      "iteration 132 current loss: 0.05637217313051224\n",
      "iteration 133 current loss: 0.05838454142212868\n",
      "iteration 134 current loss: 0.05426689609885216\n",
      "iteration 135 current loss: 0.05617501959204674\n",
      "iteration 136 current loss: 0.05595296993851662\n",
      "iteration 137 current loss: 0.0585327073931694\n",
      "iteration 138 current loss: 0.05597357079386711\n",
      "iteration 139 current loss: 0.058005254715681076\n",
      "iteration 140 current loss: 0.05495357885956764\n",
      "iteration 141 current loss: 0.053520504385232925\n",
      "iteration 142 current loss: 0.05830875039100647\n",
      "iteration 143 current loss: 0.05481233447790146\n",
      "iteration 144 current loss: 0.05789913982152939\n",
      "iteration 145 current loss: 0.055047813802957535\n",
      "iteration 146 current loss: 0.05372529476881027\n",
      "iteration 147 current loss: 0.058368343859910965\n",
      "iteration 148 current loss: 0.05493689328432083\n",
      "iteration 149 current loss: 0.057719893753528595\n",
      "iteration 150 current loss: 0.05789223313331604\n",
      "iteration 151 current loss: 0.052347827702760696\n",
      "iteration 152 current loss: 0.05467088520526886\n",
      "iteration 153 current loss: 0.053183663636446\n",
      "iteration 154 current loss: 0.05741538852453232\n",
      "iteration 155 current loss: 0.05689629912376404\n",
      "iteration 156 current loss: 0.05556342750787735\n",
      "iteration 157 current loss: 0.05716982111334801\n",
      "iteration 158 current loss: 0.05688421055674553\n",
      "iteration 159 current loss: 0.05779417231678963\n",
      "iteration 160 current loss: 0.05338253080844879\n",
      "iteration 161 current loss: 0.05659795552492142\n",
      "iteration 162 current loss: 0.05267388001084328\n",
      "iteration 163 current loss: 0.056766610592603683\n",
      "iteration 164 current loss: 0.05415291711688042\n",
      "iteration 165 current loss: 0.05476148799061775\n",
      "iteration 166 current loss: 0.05616094544529915\n",
      "iteration 167 current loss: 0.057494089007377625\n",
      "iteration 168 current loss: 0.05475133657455444\n",
      "iteration 169 current loss: 0.05637329816818237\n",
      "iteration 170 current loss: 0.05785197764635086\n",
      "iteration 171 current loss: 0.05993332713842392\n",
      "iteration 172 current loss: 0.05615901201963425\n",
      "iteration 173 current loss: 0.056338049471378326\n",
      "iteration 174 current loss: 0.05601942539215088\n",
      "iteration 175 current loss: 0.05546402931213379\n",
      "iteration 176 current loss: 0.05545993149280548\n",
      "iteration 177 current loss: 0.05545814707875252\n",
      "iteration 178 current loss: 0.054698292165994644\n",
      "iteration 179 current loss: 0.055493105202913284\n",
      "iteration 180 current loss: 0.05494546517729759\n",
      "iteration 181 current loss: 0.05604305490851402\n",
      "iteration 182 current loss: 0.057644978165626526\n",
      "iteration 183 current loss: 0.05792391300201416\n",
      "iteration 184 current loss: 0.057848382741212845\n",
      "iteration 185 current loss: 0.056159451603889465\n",
      "iteration 186 current loss: 0.05935606360435486\n",
      "iteration 187 current loss: 0.05519166216254234\n",
      "iteration 188 current loss: 0.055893100798130035\n",
      "iteration 189 current loss: 0.05649973824620247\n",
      "iteration 190 current loss: 0.05553937703371048\n",
      "iteration 191 current loss: 0.051763396710157394\n",
      "iteration 192 current loss: 0.05460762232542038\n",
      "iteration 193 current loss: 0.053287260234355927\n",
      "iteration 194 current loss: 0.05722760781645775\n",
      "iteration 195 current loss: 0.05719614773988724\n",
      "iteration 196 current loss: 0.05445142090320587\n",
      "iteration 197 current loss: 0.05779344215989113\n",
      "iteration 198 current loss: 0.05643226578831673\n",
      "iteration 199 current loss: 0.05797408148646355\n",
      "iteration 200 current loss: 0.055881112813949585\n",
      "iteration 201 current loss: 0.055007219314575195\n",
      "iteration 202 current loss: 0.05782020092010498\n",
      "iteration 203 current loss: 0.05560000613331795\n",
      "iteration 204 current loss: 0.053366802632808685\n",
      "iteration 205 current loss: 0.05777977034449577\n",
      "iteration 206 current loss: 0.056194182485342026\n",
      "iteration 207 current loss: 0.05658763647079468\n",
      "iteration 208 current loss: 0.05534505099058151\n",
      "iteration 209 current loss: 0.057545606046915054\n",
      "iteration 210 current loss: 0.05587926506996155\n",
      "iteration 211 current loss: 0.05552969127893448\n",
      "iteration 212 current loss: 0.057431936264038086\n",
      "iteration 213 current loss: 0.055862314999103546\n",
      "iteration 214 current loss: 0.05690937489271164\n",
      "iteration 215 current loss: 0.056904908269643784\n",
      "iteration 216 current loss: 0.05794999375939369\n",
      "iteration 217 current loss: 0.055087924003601074\n",
      "iteration 218 current loss: 0.05571741610765457\n",
      "iteration 219 current loss: 0.05634456127882004\n",
      "iteration 220 current loss: 0.05562478303909302\n",
      "iteration 221 current loss: 0.0536053441464901\n",
      "iteration 222 current loss: 0.05912492051720619\n",
      "iteration 223 current loss: 0.055073339492082596\n",
      "iteration 224 current loss: 0.056567639112472534\n",
      "iteration 225 current loss: 0.05448548495769501\n",
      "iteration 226 current loss: 0.056540004909038544\n",
      "iteration 227 current loss: 0.055614057928323746\n",
      "iteration 228 current loss: 0.05811721459031105\n",
      "iteration 229 current loss: 0.055500272661447525\n",
      "iteration 230 current loss: 0.053840700536966324\n",
      "iteration 231 current loss: 0.0542931966483593\n",
      "iteration 232 current loss: 0.059029169380664825\n",
      "iteration 233 current loss: 0.05776382237672806\n",
      "iteration 234 current loss: 0.05636785179376602\n",
      "iteration 235 current loss: 0.05773771181702614\n",
      "iteration 236 current loss: 0.05805739760398865\n",
      "iteration 237 current loss: 0.057961732149124146\n",
      "iteration 238 current loss: 0.05634614825248718\n",
      "iteration 239 current loss: 0.05606634542346001\n",
      "iteration 240 current loss: 0.05576147884130478\n",
      "iteration 241 current loss: 0.05860569328069687\n",
      "iteration 242 current loss: 0.05660473182797432\n",
      "iteration 243 current loss: 0.05758904293179512\n",
      "iteration 244 current loss: 0.05844602733850479\n",
      "iteration 245 current loss: 0.058128226548433304\n",
      "iteration 246 current loss: 0.06150642782449722\n",
      "iteration 247 current loss: 0.05846858024597168\n",
      "iteration 248 current loss: 0.05727303400635719\n",
      "iteration 249 current loss: 0.056079424917697906\n",
      "iteration 250 current loss: 0.057017307728528976\n",
      "iteration 251 current loss: 0.05344824492931366\n",
      "iteration 252 current loss: 0.05529789626598358\n",
      "iteration 253 current loss: 0.05516846850514412\n",
      "iteration 254 current loss: 0.05486084893345833\n",
      "iteration 255 current loss: 0.058917902410030365\n",
      "iteration 256 current loss: 0.05396890267729759\n",
      "iteration 257 current loss: 0.060834456235170364\n",
      "iteration 258 current loss: 0.055737514048814774\n",
      "iteration 259 current loss: 0.05806801840662956\n",
      "iteration 260 current loss: 0.05543661117553711\n",
      "iteration 261 current loss: 0.058054253458976746\n",
      "iteration 262 current loss: 0.055298857390880585\n",
      "iteration 263 current loss: 0.05325250327587128\n",
      "iteration 264 current loss: 0.056198883801698685\n",
      "iteration 265 current loss: 0.058550864458084106\n",
      "iteration 266 current loss: 0.054063890129327774\n",
      "iteration 267 current loss: 0.05528391897678375\n",
      "iteration 268 current loss: 0.05544281750917435\n",
      "iteration 269 current loss: 0.055279042571783066\n",
      "iteration 270 current loss: 0.054524485021829605\n",
      "iteration 271 current loss: 0.0564853809773922\n",
      "iteration 272 current loss: 0.05911184102296829\n",
      "iteration 273 current loss: 0.05544855073094368\n",
      "iteration 274 current loss: 0.05377629026770592\n",
      "iteration 275 current loss: 0.05517195537686348\n",
      "iteration 276 current loss: 0.05786437541246414\n",
      "iteration 277 current loss: 0.05746908485889435\n",
      "iteration 278 current loss: 0.05542408302426338\n",
      "iteration 279 current loss: 0.05641290545463562\n",
      "iteration 280 current loss: 0.05595269799232483\n",
      "iteration 281 current loss: 0.056707922369241714\n",
      "iteration 282 current loss: 0.0550239272415638\n",
      "iteration 283 current loss: 0.05893007665872574\n",
      "iteration 284 current loss: 0.059565287083387375\n",
      "iteration 285 current loss: 0.05746827647089958\n",
      "iteration 286 current loss: 0.055391304194927216\n",
      "iteration 287 current loss: 0.05453510582447052\n",
      "iteration 288 current loss: 0.056190021336078644\n",
      "iteration 289 current loss: 0.05644049495458603\n",
      "iteration 290 current loss: 0.060521241277456284\n",
      "iteration 291 current loss: 0.0588463693857193\n",
      "iteration 292 current loss: 0.056382112205028534\n",
      "iteration 293 current loss: 0.0557745136320591\n",
      "iteration 294 current loss: 0.05655885115265846\n",
      "iteration 295 current loss: 0.05602400377392769\n",
      "iteration 296 current loss: 0.05774533003568649\n",
      "iteration 297 current loss: 0.05675504729151726\n",
      "iteration 298 current loss: 0.053871039301157\n",
      "iteration 299 current loss: 0.05824577808380127\n",
      "iteration 300 current loss: 0.05371212959289551\n",
      "iteration 301 current loss: 0.052817776799201965\n",
      "iteration 302 current loss: 0.055183328688144684\n",
      "iteration 303 current loss: 0.0561523400247097\n",
      "iteration 304 current loss: 0.05709713697433472\n",
      "iteration 305 current loss: 0.05916403606534004\n",
      "iteration 306 current loss: 0.05386071652173996\n",
      "iteration 307 current loss: 0.057324714958667755\n",
      "iteration 308 current loss: 0.05598660930991173\n",
      "iteration 309 current loss: 0.05734896659851074\n",
      "iteration 310 current loss: 0.05616119131445885\n",
      "iteration 311 current loss: 0.06054269149899483\n",
      "iteration 312 current loss: 0.05667879059910774\n",
      "iteration 313 current loss: 0.05601643770933151\n",
      "iteration 314 current loss: 0.05693342909216881\n",
      "iteration 315 current loss: 0.05571356043219566\n",
      "iteration 316 current loss: 0.060417335480451584\n",
      "iteration 317 current loss: 0.055293578654527664\n",
      "iteration 318 current loss: 0.05660904198884964\n",
      "iteration 319 current loss: 0.055651284754276276\n",
      "iteration 320 current loss: 0.05396566912531853\n",
      "iteration 321 current loss: 0.05452951788902283\n",
      "iteration 322 current loss: 0.057873573154211044\n",
      "iteration 323 current loss: 0.05573853850364685\n",
      "iteration 324 current loss: 0.05799843370914459\n",
      "iteration 325 current loss: 0.058453675359487534\n",
      "iteration 326 current loss: 0.056492675095796585\n",
      "iteration 327 current loss: 0.05642509460449219\n",
      "iteration 328 current loss: 0.05501652508974075\n",
      "iteration 329 current loss: 0.05595977231860161\n",
      "iteration 330 current loss: 0.05657589063048363\n",
      "iteration 331 current loss: 0.057543251663446426\n",
      "iteration 332 current loss: 0.055786825716495514\n",
      "iteration 333 current loss: 0.058333009481430054\n",
      "iteration 334 current loss: 0.056917715817689896\n",
      "iteration 335 current loss: 0.05801302567124367\n",
      "iteration 336 current loss: 0.05773475766181946\n",
      "iteration 337 current loss: 0.058807019144296646\n",
      "iteration 338 current loss: 0.05987924337387085\n",
      "iteration 339 current loss: 0.055669981986284256\n",
      "iteration 340 current loss: 0.05480428412556648\n",
      "iteration 341 current loss: 0.056218791753053665\n",
      "iteration 342 current loss: 0.055454738438129425\n",
      "iteration 343 current loss: 0.05476932227611542\n",
      "iteration 344 current loss: 0.057949379086494446\n",
      "iteration 345 current loss: 0.05500292778015137\n",
      "iteration 346 current loss: 0.058710794895887375\n",
      "iteration 347 current loss: 0.05720899626612663\n",
      "iteration 348 current loss: 0.05681919679045677\n",
      "iteration 349 current loss: 0.053273219615221024\n",
      "iteration 350 current loss: 0.057214971631765366\n",
      "iteration 351 current loss: 0.05555896461009979\n",
      "iteration 352 current loss: 0.05725104734301567\n",
      "iteration 353 current loss: 0.05794544517993927\n",
      "iteration 354 current loss: 0.05437805876135826\n",
      "iteration 355 current loss: 0.053382508456707\n",
      "iteration 356 current loss: 0.05587137117981911\n",
      "iteration 357 current loss: 0.054340362548828125\n",
      "iteration 358 current loss: 0.05797364190220833\n",
      "iteration 359 current loss: 0.05654595047235489\n",
      "iteration 360 current loss: 0.05786577984690666\n",
      "iteration 361 current loss: 0.0542987585067749\n",
      "iteration 362 current loss: 0.055013496428728104\n",
      "iteration 363 current loss: 0.055966563522815704\n",
      "iteration 364 current loss: 0.056784383952617645\n",
      "iteration 365 current loss: 0.05578675866127014\n",
      "iteration 366 current loss: 0.053024858236312866\n",
      "iteration 367 current loss: 0.056940335780382156\n",
      "iteration 368 current loss: 0.05587533861398697\n",
      "iteration 369 current loss: 0.05528424307703972\n",
      "iteration 370 current loss: 0.056288473308086395\n",
      "iteration 371 current loss: 0.057405177503824234\n",
      "iteration 372 current loss: 0.05626992881298065\n",
      "iteration 373 current loss: 0.05584416538476944\n",
      "iteration 374 current loss: 0.05733197182416916\n",
      "iteration 375 current loss: 0.05470406636595726\n",
      "iteration 376 current loss: 0.05639092996716499\n",
      "iteration 377 current loss: 0.05427901819348335\n",
      "iteration 378 current loss: 0.060486845672130585\n",
      "iteration 379 current loss: 0.05685889720916748\n",
      "iteration 380 current loss: 0.058303333818912506\n",
      "iteration 381 current loss: 0.056737277656793594\n",
      "iteration 382 current loss: 0.05468592792749405\n",
      "iteration 383 current loss: 0.056063998490571976\n",
      "iteration 384 current loss: 0.05510034039616585\n",
      "iteration 385 current loss: 0.058892205357551575\n",
      "iteration 386 current loss: 0.05417322367429733\n",
      "iteration 387 current loss: 0.05382302403450012\n",
      "iteration 388 current loss: 0.05644478276371956\n",
      "iteration 389 current loss: 0.05249127745628357\n",
      "iteration 390 current loss: 0.05387522652745247\n",
      "iteration 391 current loss: 0.05437416583299637\n",
      "iteration 392 current loss: 0.06017389893531799\n",
      "iteration 393 current loss: 0.0566389374434948\n",
      "iteration 394 current loss: 0.05804471671581268\n",
      "iteration 395 current loss: 0.0543217808008194\n",
      "iteration 396 current loss: 0.056377239525318146\n",
      "iteration 397 current loss: 0.05738133564591408\n",
      "iteration 398 current loss: 0.057641275227069855\n",
      "iteration 399 current loss: 0.054547496140003204\n",
      "iteration 400 current loss: 0.05669248476624489\n",
      "iteration 401 current loss: 0.05793994665145874\n",
      "iteration 402 current loss: 0.057272374629974365\n",
      "iteration 403 current loss: 0.054575104266405106\n",
      "iteration 404 current loss: 0.05670755356550217\n",
      "iteration 405 current loss: 0.057180069386959076\n",
      "iteration 406 current loss: 0.05807948112487793\n",
      "iteration 407 current loss: 0.056049004197120667\n",
      "iteration 408 current loss: 0.056856200098991394\n",
      "iteration 409 current loss: 0.05668267235159874\n",
      "iteration 410 current loss: 0.056049998849630356\n",
      "\t\tEpoch 13/100 complete. Epoch loss 0.0561654289817288\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 13, Validation Loss: 0.05849833448883146\n",
      "best loss 0.0561654289817288\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.05446270853281021\n",
      "iteration 1 current loss: 0.055464982986450195\n",
      "iteration 2 current loss: 0.05658147111535072\n",
      "iteration 3 current loss: 0.05192440748214722\n",
      "iteration 4 current loss: 0.05572943389415741\n",
      "iteration 5 current loss: 0.05880952626466751\n",
      "iteration 6 current loss: 0.0540488176047802\n",
      "iteration 7 current loss: 0.055204011499881744\n",
      "iteration 8 current loss: 0.05487287417054176\n",
      "iteration 9 current loss: 0.0550834946334362\n",
      "iteration 10 current loss: 0.05704454705119133\n",
      "iteration 11 current loss: 0.0568929947912693\n",
      "iteration 12 current loss: 0.05328628048300743\n",
      "iteration 13 current loss: 0.05346326902508736\n",
      "iteration 14 current loss: 0.05684054270386696\n",
      "iteration 15 current loss: 0.05912129208445549\n",
      "iteration 16 current loss: 0.05859825760126114\n",
      "iteration 17 current loss: 0.05715759098529816\n",
      "iteration 18 current loss: 0.057292573153972626\n",
      "iteration 19 current loss: 0.05655495449900627\n",
      "iteration 20 current loss: 0.058168500661849976\n",
      "iteration 21 current loss: 0.0542803518474102\n",
      "iteration 22 current loss: 0.057619668543338776\n",
      "iteration 23 current loss: 0.055514223873615265\n",
      "iteration 24 current loss: 0.05744319409132004\n",
      "iteration 25 current loss: 0.05772151052951813\n",
      "iteration 26 current loss: 0.05292799696326256\n",
      "iteration 27 current loss: 0.05771491676568985\n",
      "iteration 28 current loss: 0.05638652294874191\n",
      "iteration 29 current loss: 0.05467364192008972\n",
      "iteration 30 current loss: 0.05188056826591492\n",
      "iteration 31 current loss: 0.05606052652001381\n",
      "iteration 32 current loss: 0.05665350332856178\n",
      "iteration 33 current loss: 0.05679744482040405\n",
      "iteration 34 current loss: 0.05602545663714409\n",
      "iteration 35 current loss: 0.056145306676626205\n",
      "iteration 36 current loss: 0.054791416972875595\n",
      "iteration 37 current loss: 0.056131936609745026\n",
      "iteration 38 current loss: 0.05572371184825897\n",
      "iteration 39 current loss: 0.05549575760960579\n",
      "iteration 40 current loss: 0.056105922907590866\n",
      "iteration 41 current loss: 0.05888253450393677\n",
      "iteration 42 current loss: 0.05764498561620712\n",
      "iteration 43 current loss: 0.05661427229642868\n",
      "iteration 44 current loss: 0.05673873797059059\n",
      "iteration 45 current loss: 0.05902460962533951\n",
      "iteration 46 current loss: 0.058279749006032944\n",
      "iteration 47 current loss: 0.055763959884643555\n",
      "iteration 48 current loss: 0.054168857634067535\n",
      "iteration 49 current loss: 0.05904850736260414\n",
      "iteration 50 current loss: 0.05755815654993057\n",
      "iteration 51 current loss: 0.05543004348874092\n",
      "iteration 52 current loss: 0.058593105524778366\n",
      "iteration 53 current loss: 0.056302547454833984\n",
      "iteration 54 current loss: 0.055157631635665894\n",
      "iteration 55 current loss: 0.05472941696643829\n",
      "iteration 56 current loss: 0.05458753556013107\n",
      "iteration 57 current loss: 0.05687486752867699\n",
      "iteration 58 current loss: 0.057801440358161926\n",
      "iteration 59 current loss: 0.05626126006245613\n",
      "iteration 60 current loss: 0.058255672454833984\n",
      "iteration 61 current loss: 0.054996345192193985\n",
      "iteration 62 current loss: 0.057702258229255676\n",
      "iteration 63 current loss: 0.05621081963181496\n",
      "iteration 64 current loss: 0.05799413472414017\n",
      "iteration 65 current loss: 0.055409397929906845\n",
      "iteration 66 current loss: 0.055638957768678665\n",
      "iteration 67 current loss: 0.05583561211824417\n",
      "iteration 68 current loss: 0.05357901379466057\n",
      "iteration 69 current loss: 0.056795138865709305\n",
      "iteration 70 current loss: 0.0559827946126461\n",
      "iteration 71 current loss: 0.054030004888772964\n",
      "iteration 72 current loss: 0.05845681577920914\n",
      "iteration 73 current loss: 0.05359072983264923\n",
      "iteration 74 current loss: 0.05718104913830757\n",
      "iteration 75 current loss: 0.05343058332800865\n",
      "iteration 76 current loss: 0.056742288172245026\n",
      "iteration 77 current loss: 0.054562732577323914\n",
      "iteration 78 current loss: 0.054334353655576706\n",
      "iteration 79 current loss: 0.054192427545785904\n",
      "iteration 80 current loss: 0.05733964219689369\n",
      "iteration 81 current loss: 0.054226353764534\n",
      "iteration 82 current loss: 0.05392133817076683\n",
      "iteration 83 current loss: 0.057145219296216965\n",
      "iteration 84 current loss: 0.05525834858417511\n",
      "iteration 85 current loss: 0.05580867826938629\n",
      "iteration 86 current loss: 0.05634738877415657\n",
      "iteration 87 current loss: 0.055046796798706055\n",
      "iteration 88 current loss: 0.055841896682977676\n",
      "iteration 89 current loss: 0.05824808031320572\n",
      "iteration 90 current loss: 0.05649616941809654\n",
      "iteration 91 current loss: 0.05751277878880501\n",
      "iteration 92 current loss: 0.05742255970835686\n",
      "iteration 93 current loss: 0.05496831610798836\n",
      "iteration 94 current loss: 0.05524837225675583\n",
      "iteration 95 current loss: 0.056235022842884064\n",
      "iteration 96 current loss: 0.06197075545787811\n",
      "iteration 97 current loss: 0.05582509562373161\n",
      "iteration 98 current loss: 0.05314908176660538\n",
      "iteration 99 current loss: 0.0545027032494545\n",
      "iteration 100 current loss: 0.058745887130498886\n",
      "iteration 101 current loss: 0.05840499699115753\n",
      "iteration 102 current loss: 0.05635569617152214\n",
      "iteration 103 current loss: 0.05986931547522545\n",
      "iteration 104 current loss: 0.05665741115808487\n",
      "iteration 105 current loss: 0.054042305797338486\n",
      "iteration 106 current loss: 0.05411046743392944\n",
      "iteration 107 current loss: 0.05855222046375275\n",
      "iteration 108 current loss: 0.055123329162597656\n",
      "iteration 109 current loss: 0.0554354153573513\n",
      "iteration 110 current loss: 0.05928941071033478\n",
      "iteration 111 current loss: 0.05573081970214844\n",
      "iteration 112 current loss: 0.054662417620420456\n",
      "iteration 113 current loss: 0.053781405091285706\n",
      "iteration 114 current loss: 0.054244473576545715\n",
      "iteration 115 current loss: 0.055568501353263855\n",
      "iteration 116 current loss: 0.05750696733593941\n",
      "iteration 117 current loss: 0.05493611842393875\n",
      "iteration 118 current loss: 0.05390821769833565\n",
      "iteration 119 current loss: 0.0543631948530674\n",
      "iteration 120 current loss: 0.05686045438051224\n",
      "iteration 121 current loss: 0.05796309933066368\n",
      "iteration 122 current loss: 0.05232659727334976\n",
      "iteration 123 current loss: 0.05518079176545143\n",
      "iteration 124 current loss: 0.057259898632764816\n",
      "iteration 125 current loss: 0.05706198513507843\n",
      "iteration 126 current loss: 0.053979866206645966\n",
      "iteration 127 current loss: 0.05658694729208946\n",
      "iteration 128 current loss: 0.05389458313584328\n",
      "iteration 129 current loss: 0.05124833434820175\n",
      "iteration 130 current loss: 0.051717039197683334\n",
      "iteration 131 current loss: 0.05614614114165306\n",
      "iteration 132 current loss: 0.057191796600818634\n",
      "iteration 133 current loss: 0.055710431188344955\n",
      "iteration 134 current loss: 0.05653767287731171\n",
      "iteration 135 current loss: 0.05611162632703781\n",
      "iteration 136 current loss: 0.055803801864385605\n",
      "iteration 137 current loss: 0.05482988804578781\n",
      "iteration 138 current loss: 0.0572129487991333\n",
      "iteration 139 current loss: 0.05652884766459465\n",
      "iteration 140 current loss: 0.055020350962877274\n",
      "iteration 141 current loss: 0.059479862451553345\n",
      "iteration 142 current loss: 0.05694643780589104\n",
      "iteration 143 current loss: 0.056191399693489075\n",
      "iteration 144 current loss: 0.056432366371154785\n",
      "iteration 145 current loss: 0.05503932759165764\n",
      "iteration 146 current loss: 0.054081447422504425\n",
      "iteration 147 current loss: 0.054895687848329544\n",
      "iteration 148 current loss: 0.0556863509118557\n",
      "iteration 149 current loss: 0.056095488369464874\n",
      "iteration 150 current loss: 0.057832974940538406\n",
      "iteration 151 current loss: 0.05585435777902603\n",
      "iteration 152 current loss: 0.05363674461841583\n",
      "iteration 153 current loss: 0.05631408095359802\n",
      "iteration 154 current loss: 0.05069493129849434\n",
      "iteration 155 current loss: 0.0563487708568573\n",
      "iteration 156 current loss: 0.05698350816965103\n",
      "iteration 157 current loss: 0.05353016033768654\n",
      "iteration 158 current loss: 0.0557103306055069\n",
      "iteration 159 current loss: 0.0548098161816597\n",
      "iteration 160 current loss: 0.061351168900728226\n",
      "iteration 161 current loss: 0.05830874294042587\n",
      "iteration 162 current loss: 0.058479193598032\n",
      "iteration 163 current loss: 0.05918732285499573\n",
      "iteration 164 current loss: 0.05743212252855301\n",
      "iteration 165 current loss: 0.06022946164011955\n",
      "iteration 166 current loss: 0.05466597527265549\n",
      "iteration 167 current loss: 0.05767028406262398\n",
      "iteration 168 current loss: 0.05824500322341919\n",
      "iteration 169 current loss: 0.054291486740112305\n",
      "iteration 170 current loss: 0.056241098791360855\n",
      "iteration 171 current loss: 0.05477048456668854\n",
      "iteration 172 current loss: 0.0559806153178215\n",
      "iteration 173 current loss: 0.05870302394032478\n",
      "iteration 174 current loss: 0.0585506409406662\n",
      "iteration 175 current loss: 0.05426657572388649\n",
      "iteration 176 current loss: 0.053649868816137314\n",
      "iteration 177 current loss: 0.05642184615135193\n",
      "iteration 178 current loss: 0.05596446245908737\n",
      "iteration 179 current loss: 0.05537518858909607\n",
      "iteration 180 current loss: 0.05385007709264755\n",
      "iteration 181 current loss: 0.05374091863632202\n",
      "iteration 182 current loss: 0.0543363019824028\n",
      "iteration 183 current loss: 0.05423123762011528\n",
      "iteration 184 current loss: 0.05655143782496452\n",
      "iteration 185 current loss: 0.05620821192860603\n",
      "iteration 186 current loss: 0.05744864046573639\n",
      "iteration 187 current loss: 0.053604841232299805\n",
      "iteration 188 current loss: 0.05766947939991951\n",
      "iteration 189 current loss: 0.056806404143571854\n",
      "iteration 190 current loss: 0.05916183069348335\n",
      "iteration 191 current loss: 0.05580393597483635\n",
      "iteration 192 current loss: 0.05638307332992554\n",
      "iteration 193 current loss: 0.05322927609086037\n",
      "iteration 194 current loss: 0.05415063351392746\n",
      "iteration 195 current loss: 0.05571167543530464\n",
      "iteration 196 current loss: 0.05609382688999176\n",
      "iteration 197 current loss: 0.055392470210790634\n",
      "iteration 198 current loss: 0.05518652871251106\n",
      "iteration 199 current loss: 0.055017367005348206\n",
      "iteration 200 current loss: 0.056503959000110626\n",
      "iteration 201 current loss: 0.05521532893180847\n",
      "iteration 202 current loss: 0.05740418657660484\n",
      "iteration 203 current loss: 0.05660698935389519\n",
      "iteration 204 current loss: 0.05686819925904274\n",
      "iteration 205 current loss: 0.05570389702916145\n",
      "iteration 206 current loss: 0.05447869002819061\n",
      "iteration 207 current loss: 0.056902289390563965\n",
      "iteration 208 current loss: 0.054799824953079224\n",
      "iteration 209 current loss: 0.05433544144034386\n",
      "iteration 210 current loss: 0.0571170300245285\n",
      "iteration 211 current loss: 0.056832075119018555\n",
      "iteration 212 current loss: 0.05838443711400032\n",
      "iteration 213 current loss: 0.05586887151002884\n",
      "iteration 214 current loss: 0.057255394756793976\n",
      "iteration 215 current loss: 0.05795486643910408\n",
      "iteration 216 current loss: 0.05480801686644554\n",
      "iteration 217 current loss: 0.055370308458805084\n",
      "iteration 218 current loss: 0.056222811341285706\n",
      "iteration 219 current loss: 0.057796962559223175\n",
      "iteration 220 current loss: 0.05565740913152695\n",
      "iteration 221 current loss: 0.05573825538158417\n",
      "iteration 222 current loss: 0.05681101605296135\n",
      "iteration 223 current loss: 0.05595744028687477\n",
      "iteration 224 current loss: 0.05520686134696007\n",
      "iteration 225 current loss: 0.0585455559194088\n",
      "iteration 226 current loss: 0.05856497958302498\n",
      "iteration 227 current loss: 0.05803293734788895\n",
      "iteration 228 current loss: 0.0591297410428524\n",
      "iteration 229 current loss: 0.0532083697617054\n",
      "iteration 230 current loss: 0.051551610231399536\n",
      "iteration 231 current loss: 0.05579198896884918\n",
      "iteration 232 current loss: 0.060028523206710815\n",
      "iteration 233 current loss: 0.05659417062997818\n",
      "iteration 234 current loss: 0.05640120804309845\n",
      "iteration 235 current loss: 0.05854930356144905\n",
      "iteration 236 current loss: 0.05387740582227707\n",
      "iteration 237 current loss: 0.05540359765291214\n",
      "iteration 238 current loss: 0.059156596660614014\n",
      "iteration 239 current loss: 0.052596576511859894\n",
      "iteration 240 current loss: 0.05364427715539932\n",
      "iteration 241 current loss: 0.05861179903149605\n",
      "iteration 242 current loss: 0.057318247854709625\n",
      "iteration 243 current loss: 0.05716094747185707\n",
      "iteration 244 current loss: 0.056966498494148254\n",
      "iteration 245 current loss: 0.05724463611841202\n",
      "iteration 246 current loss: 0.055887870490550995\n",
      "iteration 247 current loss: 0.054824285209178925\n",
      "iteration 248 current loss: 0.05668541416525841\n",
      "iteration 249 current loss: 0.05368863418698311\n",
      "iteration 250 current loss: 0.056704383343458176\n",
      "iteration 251 current loss: 0.05752255395054817\n",
      "iteration 252 current loss: 0.055155493319034576\n",
      "iteration 253 current loss: 0.05356257036328316\n",
      "iteration 254 current loss: 0.05518060177564621\n",
      "iteration 255 current loss: 0.05559211224317551\n",
      "iteration 256 current loss: 0.05553055554628372\n",
      "iteration 257 current loss: 0.05144074559211731\n",
      "iteration 258 current loss: 0.05707687512040138\n",
      "iteration 259 current loss: 0.056596871465444565\n",
      "iteration 260 current loss: 0.05721055343747139\n",
      "iteration 261 current loss: 0.054757069796323776\n",
      "iteration 262 current loss: 0.05445067211985588\n",
      "iteration 263 current loss: 0.05788462609052658\n",
      "iteration 264 current loss: 0.054935384541749954\n",
      "iteration 265 current loss: 0.05440688878297806\n",
      "iteration 266 current loss: 0.05540325120091438\n",
      "iteration 267 current loss: 0.053403712809085846\n",
      "iteration 268 current loss: 0.05367989093065262\n",
      "iteration 269 current loss: 0.05730782821774483\n",
      "iteration 270 current loss: 0.05558224394917488\n",
      "iteration 271 current loss: 0.05658469349145889\n",
      "iteration 272 current loss: 0.05647788196802139\n",
      "iteration 273 current loss: 0.05332589149475098\n",
      "iteration 274 current loss: 0.05437743663787842\n",
      "iteration 275 current loss: 0.05768519639968872\n",
      "iteration 276 current loss: 0.055463675409555435\n",
      "iteration 277 current loss: 0.054204072803258896\n",
      "iteration 278 current loss: 0.053540948778390884\n",
      "iteration 279 current loss: 0.0590997152030468\n",
      "iteration 280 current loss: 0.0520864762365818\n",
      "iteration 281 current loss: 0.05331975594162941\n",
      "iteration 282 current loss: 0.05502941831946373\n",
      "iteration 283 current loss: 0.053164172917604446\n",
      "iteration 284 current loss: 0.05933339148759842\n",
      "iteration 285 current loss: 0.0551181323826313\n",
      "iteration 286 current loss: 0.05473608151078224\n",
      "iteration 287 current loss: 0.057252682745456696\n",
      "iteration 288 current loss: 0.057109612971544266\n",
      "iteration 289 current loss: 0.056939151138067245\n",
      "iteration 290 current loss: 0.0529305525124073\n",
      "iteration 291 current loss: 0.051736824214458466\n",
      "iteration 292 current loss: 0.05458192899823189\n",
      "iteration 293 current loss: 0.05384926497936249\n",
      "iteration 294 current loss: 0.054900724440813065\n",
      "iteration 295 current loss: 0.057047173380851746\n",
      "iteration 296 current loss: 0.05832313746213913\n",
      "iteration 297 current loss: 0.057960882782936096\n",
      "iteration 298 current loss: 0.05744994059205055\n",
      "iteration 299 current loss: 0.05604763701558113\n",
      "iteration 300 current loss: 0.05650799721479416\n",
      "iteration 301 current loss: 0.05540185421705246\n",
      "iteration 302 current loss: 0.05557795986533165\n",
      "iteration 303 current loss: 0.058857351541519165\n",
      "iteration 304 current loss: 0.0570477657020092\n",
      "iteration 305 current loss: 0.05519826337695122\n",
      "iteration 306 current loss: 0.05290648713707924\n",
      "iteration 307 current loss: 0.05596914142370224\n",
      "iteration 308 current loss: 0.05121327191591263\n",
      "iteration 309 current loss: 0.05510379374027252\n",
      "iteration 310 current loss: 0.055538300424814224\n",
      "iteration 311 current loss: 0.05470132455229759\n",
      "iteration 312 current loss: 0.054864123463630676\n",
      "iteration 313 current loss: 0.056016091257333755\n",
      "iteration 314 current loss: 0.05602123960852623\n",
      "iteration 315 current loss: 0.05614183470606804\n",
      "iteration 316 current loss: 0.05806900933384895\n",
      "iteration 317 current loss: 0.056463200598955154\n",
      "iteration 318 current loss: 0.058046963065862656\n",
      "iteration 319 current loss: 0.05472317337989807\n",
      "iteration 320 current loss: 0.05591559410095215\n",
      "iteration 321 current loss: 0.05702931806445122\n",
      "iteration 322 current loss: 0.05604434013366699\n",
      "iteration 323 current loss: 0.05539366975426674\n",
      "iteration 324 current loss: 0.057486966252326965\n",
      "iteration 325 current loss: 0.05808934569358826\n",
      "iteration 326 current loss: 0.055901605635881424\n",
      "iteration 327 current loss: 0.05608515441417694\n",
      "iteration 328 current loss: 0.0561889111995697\n",
      "iteration 329 current loss: 0.05872268229722977\n",
      "iteration 330 current loss: 0.05313611403107643\n",
      "iteration 331 current loss: 0.05661473795771599\n",
      "iteration 332 current loss: 0.059435296803712845\n",
      "iteration 333 current loss: 0.055521365255117416\n",
      "iteration 334 current loss: 0.0607144758105278\n",
      "iteration 335 current loss: 0.054936788976192474\n",
      "iteration 336 current loss: 0.05562338978052139\n",
      "iteration 337 current loss: 0.057434771209955215\n",
      "iteration 338 current loss: 0.05464816838502884\n",
      "iteration 339 current loss: 0.05922605097293854\n",
      "iteration 340 current loss: 0.05215108022093773\n",
      "iteration 341 current loss: 0.05708504468202591\n",
      "iteration 342 current loss: 0.056097667664289474\n",
      "iteration 343 current loss: 0.05708464980125427\n",
      "iteration 344 current loss: 0.05253944173455238\n",
      "iteration 345 current loss: 0.056628767400979996\n",
      "iteration 346 current loss: 0.05745484307408333\n",
      "iteration 347 current loss: 0.05498342588543892\n",
      "iteration 348 current loss: 0.059090111404657364\n",
      "iteration 349 current loss: 0.05741491541266441\n",
      "iteration 350 current loss: 0.05478839576244354\n",
      "iteration 351 current loss: 0.05780718848109245\n",
      "iteration 352 current loss: 0.05532339960336685\n",
      "iteration 353 current loss: 0.05849413946270943\n",
      "iteration 354 current loss: 0.056227561086416245\n",
      "iteration 355 current loss: 0.05342448502779007\n",
      "iteration 356 current loss: 0.05621853098273277\n",
      "iteration 357 current loss: 0.055570293217897415\n",
      "iteration 358 current loss: 0.05642128363251686\n",
      "iteration 359 current loss: 0.05802528187632561\n",
      "iteration 360 current loss: 0.0531163290143013\n",
      "iteration 361 current loss: 0.059170596301555634\n",
      "iteration 362 current loss: 0.054490964859724045\n",
      "iteration 363 current loss: 0.058667853474617004\n",
      "iteration 364 current loss: 0.05642586201429367\n",
      "iteration 365 current loss: 0.05880890041589737\n",
      "iteration 366 current loss: 0.054015714675188065\n",
      "iteration 367 current loss: 0.053504034876823425\n",
      "iteration 368 current loss: 0.05768319591879845\n",
      "iteration 369 current loss: 0.054270144551992416\n",
      "iteration 370 current loss: 0.05327322334051132\n",
      "iteration 371 current loss: 0.05726853013038635\n",
      "iteration 372 current loss: 0.058508291840553284\n",
      "iteration 373 current loss: 0.05321304500102997\n",
      "iteration 374 current loss: 0.05800050497055054\n",
      "iteration 375 current loss: 0.055095210671424866\n",
      "iteration 376 current loss: 0.05576171725988388\n",
      "iteration 377 current loss: 0.054879844188690186\n",
      "iteration 378 current loss: 0.05511746555566788\n",
      "iteration 379 current loss: 0.056472018361091614\n",
      "iteration 380 current loss: 0.05604152753949165\n",
      "iteration 381 current loss: 0.056170880794525146\n",
      "iteration 382 current loss: 0.05566120892763138\n",
      "iteration 383 current loss: 0.053182270377874374\n",
      "iteration 384 current loss: 0.05434966832399368\n",
      "iteration 385 current loss: 0.05837002396583557\n",
      "iteration 386 current loss: 0.05553127080202103\n",
      "iteration 387 current loss: 0.05536818504333496\n",
      "iteration 388 current loss: 0.05289581045508385\n",
      "iteration 389 current loss: 0.05662247911095619\n",
      "iteration 390 current loss: 0.056464917957782745\n",
      "iteration 391 current loss: 0.05555379390716553\n",
      "iteration 392 current loss: 0.05529956519603729\n",
      "iteration 393 current loss: 0.057693544775247574\n",
      "iteration 394 current loss: 0.056636642664670944\n",
      "iteration 395 current loss: 0.05779731646180153\n",
      "iteration 396 current loss: 0.05786411091685295\n",
      "iteration 397 current loss: 0.05343468859791756\n",
      "iteration 398 current loss: 0.05684186890721321\n",
      "iteration 399 current loss: 0.05336485058069229\n",
      "iteration 400 current loss: 0.05231795459985733\n",
      "iteration 401 current loss: 0.05541470646858215\n",
      "iteration 402 current loss: 0.05490546301007271\n",
      "iteration 403 current loss: 0.05668316036462784\n",
      "iteration 404 current loss: 0.05744754895567894\n",
      "iteration 405 current loss: 0.05593135580420494\n",
      "iteration 406 current loss: 0.052730992436409\n",
      "iteration 407 current loss: 0.052002567797899246\n",
      "iteration 408 current loss: 0.056388162076473236\n",
      "iteration 409 current loss: 0.057075001299381256\n",
      "iteration 410 current loss: 0.05503775179386139\n",
      "\t\tEpoch 14/100 complete. Epoch loss 0.05596467329601592\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 14, Validation Loss: 0.05657935584895313\n",
      "best loss 0.05596467329601592\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.058370258659124374\n",
      "iteration 1 current loss: 0.05648460611701012\n",
      "iteration 2 current loss: 0.05801384150981903\n",
      "iteration 3 current loss: 0.05420668423175812\n",
      "iteration 4 current loss: 0.05382368713617325\n",
      "iteration 5 current loss: 0.05446447432041168\n",
      "iteration 6 current loss: 0.05535947158932686\n",
      "iteration 7 current loss: 0.054101407527923584\n",
      "iteration 8 current loss: 0.05236463248729706\n",
      "iteration 9 current loss: 0.056642062962055206\n",
      "iteration 10 current loss: 0.054749149829149246\n",
      "iteration 11 current loss: 0.057530686259269714\n",
      "iteration 12 current loss: 0.05796898901462555\n",
      "iteration 13 current loss: 0.055940307676792145\n",
      "iteration 14 current loss: 0.053636759519577026\n",
      "iteration 15 current loss: 0.05229492485523224\n",
      "iteration 16 current loss: 0.055391911417245865\n",
      "iteration 17 current loss: 0.05248551815748215\n",
      "iteration 18 current loss: 0.05413806810975075\n",
      "iteration 19 current loss: 0.05426356568932533\n",
      "iteration 20 current loss: 0.05255710706114769\n",
      "iteration 21 current loss: 0.05545106530189514\n",
      "iteration 22 current loss: 0.05413138493895531\n",
      "iteration 23 current loss: 0.054260484874248505\n",
      "iteration 24 current loss: 0.05345746874809265\n",
      "iteration 25 current loss: 0.05175197497010231\n",
      "iteration 26 current loss: 0.054650940001010895\n",
      "iteration 27 current loss: 0.0535142682492733\n",
      "iteration 28 current loss: 0.05565881356596947\n",
      "iteration 29 current loss: 0.05030573159456253\n",
      "iteration 30 current loss: 0.05586865544319153\n",
      "iteration 31 current loss: 0.05369202047586441\n",
      "iteration 32 current loss: 0.05521494895219803\n",
      "iteration 33 current loss: 0.05535052344202995\n",
      "iteration 34 current loss: 0.0559898316860199\n",
      "iteration 35 current loss: 0.05401546135544777\n",
      "iteration 36 current loss: 0.05430085211992264\n",
      "iteration 37 current loss: 0.05427155643701553\n",
      "iteration 38 current loss: 0.050975143909454346\n",
      "iteration 39 current loss: 0.05667991563677788\n",
      "iteration 40 current loss: 0.05461259186267853\n",
      "iteration 41 current loss: 0.05678173899650574\n",
      "iteration 42 current loss: 0.05690939724445343\n",
      "iteration 43 current loss: 0.05445041507482529\n",
      "iteration 44 current loss: 0.059344980865716934\n",
      "iteration 45 current loss: 0.05610814318060875\n",
      "iteration 46 current loss: 0.05454698204994202\n",
      "iteration 47 current loss: 0.05399664118885994\n",
      "iteration 48 current loss: 0.0574270635843277\n",
      "iteration 49 current loss: 0.05651379004120827\n",
      "iteration 50 current loss: 0.056481439620256424\n",
      "iteration 51 current loss: 0.057146068662405014\n",
      "iteration 52 current loss: 0.05322474241256714\n",
      "iteration 53 current loss: 0.05688901245594025\n",
      "iteration 54 current loss: 0.052851587533950806\n",
      "iteration 55 current loss: 0.057808488607406616\n",
      "iteration 56 current loss: 0.0572180412709713\n",
      "iteration 57 current loss: 0.05601463466882706\n",
      "iteration 58 current loss: 0.0562247708439827\n",
      "iteration 59 current loss: 0.05331391468644142\n",
      "iteration 60 current loss: 0.05594387277960777\n",
      "iteration 61 current loss: 0.05562613531947136\n",
      "iteration 62 current loss: 0.05345524847507477\n",
      "iteration 63 current loss: 0.05462777987122536\n",
      "iteration 64 current loss: 0.05615338683128357\n",
      "iteration 65 current loss: 0.05318092554807663\n",
      "iteration 66 current loss: 0.055929191410541534\n",
      "iteration 67 current loss: 0.05533342435956001\n",
      "iteration 68 current loss: 0.0545438751578331\n",
      "iteration 69 current loss: 0.056202810257673264\n",
      "iteration 70 current loss: 0.05761835724115372\n",
      "iteration 71 current loss: 0.05475311726331711\n",
      "iteration 72 current loss: 0.05556902289390564\n",
      "iteration 73 current loss: 0.05546342581510544\n",
      "iteration 74 current loss: 0.05777814984321594\n",
      "iteration 75 current loss: 0.054548684507608414\n",
      "iteration 76 current loss: 0.055326346307992935\n",
      "iteration 77 current loss: 0.0533561110496521\n",
      "iteration 78 current loss: 0.056999702006578445\n",
      "iteration 79 current loss: 0.05409771576523781\n",
      "iteration 80 current loss: 0.05515240505337715\n",
      "iteration 81 current loss: 0.055521100759506226\n",
      "iteration 82 current loss: 0.054550930857658386\n",
      "iteration 83 current loss: 0.054768193513154984\n",
      "iteration 84 current loss: 0.056250639259815216\n",
      "iteration 85 current loss: 0.05369099974632263\n",
      "iteration 86 current loss: 0.05793092027306557\n",
      "iteration 87 current loss: 0.055123407393693924\n",
      "iteration 88 current loss: 0.05561801418662071\n",
      "iteration 89 current loss: 0.05656605213880539\n",
      "iteration 90 current loss: 0.05582360178232193\n",
      "iteration 91 current loss: 0.05588291212916374\n",
      "iteration 92 current loss: 0.056516844779253006\n",
      "iteration 93 current loss: 0.05622730404138565\n",
      "iteration 94 current loss: 0.055576059967279434\n",
      "iteration 95 current loss: 0.056163810193538666\n",
      "iteration 96 current loss: 0.0540490597486496\n",
      "iteration 97 current loss: 0.05383724346756935\n",
      "iteration 98 current loss: 0.055574867874383926\n",
      "iteration 99 current loss: 0.05656391754746437\n",
      "iteration 100 current loss: 0.05385374277830124\n",
      "iteration 101 current loss: 0.053373001515865326\n",
      "iteration 102 current loss: 0.05710890144109726\n",
      "iteration 103 current loss: 0.05263034626841545\n",
      "iteration 104 current loss: 0.05562850832939148\n",
      "iteration 105 current loss: 0.05803912132978439\n",
      "iteration 106 current loss: 0.058577269315719604\n",
      "iteration 107 current loss: 0.054779402911663055\n",
      "iteration 108 current loss: 0.055707208812236786\n",
      "iteration 109 current loss: 0.05512385815382004\n",
      "iteration 110 current loss: 0.053112782537937164\n",
      "iteration 111 current loss: 0.05431612953543663\n",
      "iteration 112 current loss: 0.055560652166604996\n",
      "iteration 113 current loss: 0.05833053216338158\n",
      "iteration 114 current loss: 0.054524682462215424\n",
      "iteration 115 current loss: 0.05497297644615173\n",
      "iteration 116 current loss: 0.056118860840797424\n",
      "iteration 117 current loss: 0.0539437010884285\n",
      "iteration 118 current loss: 0.052612438797950745\n",
      "iteration 119 current loss: 0.053357698023319244\n",
      "iteration 120 current loss: 0.05054974555969238\n",
      "iteration 121 current loss: 0.053512394428253174\n",
      "iteration 122 current loss: 0.05624046549201012\n",
      "iteration 123 current loss: 0.05231785029172897\n",
      "iteration 124 current loss: 0.0564165823161602\n",
      "iteration 125 current loss: 0.051661085337400436\n",
      "iteration 126 current loss: 0.054113008081912994\n",
      "iteration 127 current loss: 0.055044546723365784\n",
      "iteration 128 current loss: 0.05604982376098633\n",
      "iteration 129 current loss: 0.05412794649600983\n",
      "iteration 130 current loss: 0.05322868004441261\n",
      "iteration 131 current loss: 0.05418434739112854\n",
      "iteration 132 current loss: 0.05421261489391327\n",
      "iteration 133 current loss: 0.05361289530992508\n",
      "iteration 134 current loss: 0.05681469663977623\n",
      "iteration 135 current loss: 0.05620657652616501\n",
      "iteration 136 current loss: 0.05663757771253586\n",
      "iteration 137 current loss: 0.05280232056975365\n",
      "iteration 138 current loss: 0.057946134358644485\n",
      "iteration 139 current loss: 0.057973455637693405\n",
      "iteration 140 current loss: 0.055697329342365265\n",
      "iteration 141 current loss: 0.053361523896455765\n",
      "iteration 142 current loss: 0.05730427801609039\n",
      "iteration 143 current loss: 0.052836839109659195\n",
      "iteration 144 current loss: 0.053349874913692474\n",
      "iteration 145 current loss: 0.055624283850193024\n",
      "iteration 146 current loss: 0.05546244978904724\n",
      "iteration 147 current loss: 0.05532616376876831\n",
      "iteration 148 current loss: 0.05340924859046936\n",
      "iteration 149 current loss: 0.058921344578266144\n",
      "iteration 150 current loss: 0.05458660423755646\n",
      "iteration 151 current loss: 0.05520377680659294\n",
      "iteration 152 current loss: 0.05271526798605919\n",
      "iteration 153 current loss: 0.056459516286849976\n",
      "iteration 154 current loss: 0.05545663833618164\n",
      "iteration 155 current loss: 0.0580902136862278\n",
      "iteration 156 current loss: 0.056986287236213684\n",
      "iteration 157 current loss: 0.05508603900671005\n",
      "iteration 158 current loss: 0.058394212275743484\n",
      "iteration 159 current loss: 0.05446520075201988\n",
      "iteration 160 current loss: 0.05312205106019974\n",
      "iteration 161 current loss: 0.055718887597322464\n",
      "iteration 162 current loss: 0.056111257523298264\n",
      "iteration 163 current loss: 0.0569487139582634\n",
      "iteration 164 current loss: 0.0557064563035965\n",
      "iteration 165 current loss: 0.05710197985172272\n",
      "iteration 166 current loss: 0.05358889326453209\n",
      "iteration 167 current loss: 0.05436524376273155\n",
      "iteration 168 current loss: 0.051913633942604065\n",
      "iteration 169 current loss: 0.055893849581480026\n",
      "iteration 170 current loss: 0.05487499386072159\n",
      "iteration 171 current loss: 0.05889368802309036\n",
      "iteration 172 current loss: 0.05547774210572243\n",
      "iteration 173 current loss: 0.05520668253302574\n",
      "iteration 174 current loss: 0.05503728613257408\n",
      "iteration 175 current loss: 0.05524386465549469\n",
      "iteration 176 current loss: 0.05153627693653107\n",
      "iteration 177 current loss: 0.054175496101379395\n",
      "iteration 178 current loss: 0.053433798253536224\n",
      "iteration 179 current loss: 0.05610961467027664\n",
      "iteration 180 current loss: 0.05351518839597702\n",
      "iteration 181 current loss: 0.05642173811793327\n",
      "iteration 182 current loss: 0.056139249354600906\n",
      "iteration 183 current loss: 0.0552593395113945\n",
      "iteration 184 current loss: 0.05565967410802841\n",
      "iteration 185 current loss: 0.05492110177874565\n",
      "iteration 186 current loss: 0.053957726806402206\n",
      "iteration 187 current loss: 0.05371386930346489\n",
      "iteration 188 current loss: 0.05859765037894249\n",
      "iteration 189 current loss: 0.05625441297888756\n",
      "iteration 190 current loss: 0.05680747702717781\n",
      "iteration 191 current loss: 0.05570460855960846\n",
      "iteration 192 current loss: 0.05501117557287216\n",
      "iteration 193 current loss: 0.053897492587566376\n",
      "iteration 194 current loss: 0.05895300209522247\n",
      "iteration 195 current loss: 0.05524878948926926\n",
      "iteration 196 current loss: 0.05671221390366554\n",
      "iteration 197 current loss: 0.055275578051805496\n",
      "iteration 198 current loss: 0.05429653823375702\n",
      "iteration 199 current loss: 0.05466194823384285\n",
      "iteration 200 current loss: 0.057892199605703354\n",
      "iteration 201 current loss: 0.05348065122961998\n",
      "iteration 202 current loss: 0.056755226105451584\n",
      "iteration 203 current loss: 0.052597831934690475\n",
      "iteration 204 current loss: 0.05359181389212608\n",
      "iteration 205 current loss: 0.05578362196683884\n",
      "iteration 206 current loss: 0.055952854454517365\n",
      "iteration 207 current loss: 0.054793812334537506\n",
      "iteration 208 current loss: 0.05519353970885277\n",
      "iteration 209 current loss: 0.052658725529909134\n",
      "iteration 210 current loss: 0.05590420216321945\n",
      "iteration 211 current loss: 0.054442740976810455\n",
      "iteration 212 current loss: 0.056476082652807236\n",
      "iteration 213 current loss: 0.057433441281318665\n",
      "iteration 214 current loss: 0.06051880493760109\n",
      "iteration 215 current loss: 0.05248125270009041\n",
      "iteration 216 current loss: 0.057120587676763535\n",
      "iteration 217 current loss: 0.05562437325716019\n",
      "iteration 218 current loss: 0.05658687651157379\n",
      "iteration 219 current loss: 0.05294032022356987\n",
      "iteration 220 current loss: 0.055828750133514404\n",
      "iteration 221 current loss: 0.05607197433710098\n",
      "iteration 222 current loss: 0.05570515990257263\n",
      "iteration 223 current loss: 0.05780136585235596\n",
      "iteration 224 current loss: 0.056516267359256744\n",
      "iteration 225 current loss: 0.054880473762750626\n",
      "iteration 226 current loss: 0.05405150726437569\n",
      "iteration 227 current loss: 0.0536290667951107\n",
      "iteration 228 current loss: 0.05819113180041313\n",
      "iteration 229 current loss: 0.056939903646707535\n",
      "iteration 230 current loss: 0.056939929723739624\n",
      "iteration 231 current loss: 0.056616418063640594\n",
      "iteration 232 current loss: 0.05536280944943428\n",
      "iteration 233 current loss: 0.05575589835643768\n",
      "iteration 234 current loss: 0.05751483142375946\n",
      "iteration 235 current loss: 0.05575229972600937\n",
      "iteration 236 current loss: 0.050708893686532974\n",
      "iteration 237 current loss: 0.05762395262718201\n",
      "iteration 238 current loss: 0.05705463886260986\n",
      "iteration 239 current loss: 0.057834070175886154\n",
      "iteration 240 current loss: 0.055455952882766724\n",
      "iteration 241 current loss: 0.055529940873384476\n",
      "iteration 242 current loss: 0.05470402166247368\n",
      "iteration 243 current loss: 0.05341554060578346\n",
      "iteration 244 current loss: 0.052456021308898926\n",
      "iteration 245 current loss: 0.056436583399772644\n",
      "iteration 246 current loss: 0.056328967213630676\n",
      "iteration 247 current loss: 0.05734667927026749\n",
      "iteration 248 current loss: 0.05450773239135742\n",
      "iteration 249 current loss: 0.058560989797115326\n",
      "iteration 250 current loss: 0.053105320781469345\n",
      "iteration 251 current loss: 0.05447585880756378\n",
      "iteration 252 current loss: 0.05689995363354683\n",
      "iteration 253 current loss: 0.052029795944690704\n",
      "iteration 254 current loss: 0.05585655942559242\n",
      "iteration 255 current loss: 0.05570768564939499\n",
      "iteration 256 current loss: 0.058101292699575424\n",
      "iteration 257 current loss: 0.05290839448571205\n",
      "iteration 258 current loss: 0.0511026568710804\n",
      "iteration 259 current loss: 0.056834738701581955\n",
      "iteration 260 current loss: 0.05592165142297745\n",
      "iteration 261 current loss: 0.056707363575696945\n",
      "iteration 262 current loss: 0.05448237806558609\n",
      "iteration 263 current loss: 0.05734667927026749\n",
      "iteration 264 current loss: 0.05541133135557175\n",
      "iteration 265 current loss: 0.0560012087225914\n",
      "iteration 266 current loss: 0.05537746474146843\n",
      "iteration 267 current loss: 0.05387067049741745\n",
      "iteration 268 current loss: 0.052335403859615326\n",
      "iteration 269 current loss: 0.05689810961484909\n",
      "iteration 270 current loss: 0.055195920169353485\n",
      "iteration 271 current loss: 0.05626356229186058\n",
      "iteration 272 current loss: 0.05684860423207283\n",
      "iteration 273 current loss: 0.05725572258234024\n",
      "iteration 274 current loss: 0.05355849862098694\n",
      "iteration 275 current loss: 0.05610041320323944\n",
      "iteration 276 current loss: 0.05859715864062309\n",
      "iteration 277 current loss: 0.05998699739575386\n",
      "iteration 278 current loss: 0.0546998456120491\n",
      "iteration 279 current loss: 0.051988665014505386\n",
      "iteration 280 current loss: 0.05373634397983551\n",
      "iteration 281 current loss: 0.05393484979867935\n",
      "iteration 282 current loss: 0.053792111575603485\n",
      "iteration 283 current loss: 0.052728526294231415\n",
      "iteration 284 current loss: 0.05736354738473892\n",
      "iteration 285 current loss: 0.052591633051633835\n",
      "iteration 286 current loss: 0.056163277477025986\n",
      "iteration 287 current loss: 0.05615996569395065\n",
      "iteration 288 current loss: 0.0557403564453125\n",
      "iteration 289 current loss: 0.05772053077816963\n",
      "iteration 290 current loss: 0.054773684591054916\n",
      "iteration 291 current loss: 0.05505150556564331\n",
      "iteration 292 current loss: 0.05775252729654312\n",
      "iteration 293 current loss: 0.055344026535749435\n",
      "iteration 294 current loss: 0.0573691725730896\n",
      "iteration 295 current loss: 0.05699963867664337\n",
      "iteration 296 current loss: 0.05825577303767204\n",
      "iteration 297 current loss: 0.05309344828128815\n",
      "iteration 298 current loss: 0.056709449738264084\n",
      "iteration 299 current loss: 0.05728623643517494\n",
      "iteration 300 current loss: 0.053741928189992905\n",
      "iteration 301 current loss: 0.05564522743225098\n",
      "iteration 302 current loss: 0.055737610906362534\n",
      "iteration 303 current loss: 0.05769650638103485\n",
      "iteration 304 current loss: 0.05346722900867462\n",
      "iteration 305 current loss: 0.05494213104248047\n",
      "iteration 306 current loss: 0.055733852088451385\n",
      "iteration 307 current loss: 0.05710933730006218\n",
      "iteration 308 current loss: 0.053758684545755386\n",
      "iteration 309 current loss: 0.055806491523981094\n",
      "iteration 310 current loss: 0.0540146566927433\n",
      "iteration 311 current loss: 0.05629115551710129\n",
      "iteration 312 current loss: 0.053819674998521805\n",
      "iteration 313 current loss: 0.05784448981285095\n",
      "iteration 314 current loss: 0.05520077049732208\n",
      "iteration 315 current loss: 0.05609208345413208\n",
      "iteration 316 current loss: 0.057342566549777985\n",
      "iteration 317 current loss: 0.05540958791971207\n",
      "iteration 318 current loss: 0.05789274349808693\n",
      "iteration 319 current loss: 0.05673527345061302\n",
      "iteration 320 current loss: 0.05414130166172981\n",
      "iteration 321 current loss: 0.05522724613547325\n",
      "iteration 322 current loss: 0.0539296455681324\n",
      "iteration 323 current loss: 0.056183528155088425\n",
      "iteration 324 current loss: 0.05529768764972687\n",
      "iteration 325 current loss: 0.06009988486766815\n",
      "iteration 326 current loss: 0.0564718171954155\n",
      "iteration 327 current loss: 0.05691756308078766\n",
      "iteration 328 current loss: 0.053636446595191956\n",
      "iteration 329 current loss: 0.05287264660000801\n",
      "iteration 330 current loss: 0.05576513335108757\n",
      "iteration 331 current loss: 0.055726829916238785\n",
      "iteration 332 current loss: 0.05616059899330139\n",
      "iteration 333 current loss: 0.056747011840343475\n",
      "iteration 334 current loss: 0.05873961001634598\n",
      "iteration 335 current loss: 0.0586540624499321\n",
      "iteration 336 current loss: 0.056908756494522095\n",
      "iteration 337 current loss: 0.05809929221868515\n",
      "iteration 338 current loss: 0.05438718944787979\n",
      "iteration 339 current loss: 0.056815773248672485\n",
      "iteration 340 current loss: 0.05540046840906143\n",
      "iteration 341 current loss: 0.05794278904795647\n",
      "iteration 342 current loss: 0.05756741017103195\n",
      "iteration 343 current loss: 0.0581965446472168\n",
      "iteration 344 current loss: 0.05577695742249489\n",
      "iteration 345 current loss: 0.05576840043067932\n",
      "iteration 346 current loss: 0.058711495250463486\n",
      "iteration 347 current loss: 0.055534008890390396\n",
      "iteration 348 current loss: 0.05685071274638176\n",
      "iteration 349 current loss: 0.05841977149248123\n",
      "iteration 350 current loss: 0.05319586023688316\n",
      "iteration 351 current loss: 0.05509299784898758\n",
      "iteration 352 current loss: 0.05613543465733528\n",
      "iteration 353 current loss: 0.05541618540883064\n",
      "iteration 354 current loss: 0.05598447844386101\n",
      "iteration 355 current loss: 0.055817265063524246\n",
      "iteration 356 current loss: 0.053517043590545654\n",
      "iteration 357 current loss: 0.05641917139291763\n",
      "iteration 358 current loss: 0.053965188562870026\n",
      "iteration 359 current loss: 0.05877812206745148\n",
      "iteration 360 current loss: 0.05745146796107292\n",
      "iteration 361 current loss: 0.05864062160253525\n",
      "iteration 362 current loss: 0.05936726927757263\n",
      "iteration 363 current loss: 0.057140614837408066\n",
      "iteration 364 current loss: 0.05495879426598549\n",
      "iteration 365 current loss: 0.055006612092256546\n",
      "iteration 366 current loss: 0.05321444571018219\n",
      "iteration 367 current loss: 0.05744711682200432\n",
      "iteration 368 current loss: 0.05569540336728096\n",
      "iteration 369 current loss: 0.057941894978284836\n",
      "iteration 370 current loss: 0.055980101227760315\n",
      "iteration 371 current loss: 0.05529012531042099\n",
      "iteration 372 current loss: 0.05383659154176712\n",
      "iteration 373 current loss: 0.056312330067157745\n",
      "iteration 374 current loss: 0.05352415889501572\n",
      "iteration 375 current loss: 0.055778443813323975\n",
      "iteration 376 current loss: 0.05696769058704376\n",
      "iteration 377 current loss: 0.0552491657435894\n",
      "iteration 378 current loss: 0.05609356239438057\n",
      "iteration 379 current loss: 0.05521142855286598\n",
      "iteration 380 current loss: 0.057155292481184006\n",
      "iteration 381 current loss: 0.05819655954837799\n",
      "iteration 382 current loss: 0.0588669553399086\n",
      "iteration 383 current loss: 0.05478455498814583\n",
      "iteration 384 current loss: 0.0537550151348114\n",
      "iteration 385 current loss: 0.05481312796473503\n",
      "iteration 386 current loss: 0.05488918349146843\n",
      "iteration 387 current loss: 0.0552082434296608\n",
      "iteration 388 current loss: 0.05330556631088257\n",
      "iteration 389 current loss: 0.056008558720350266\n",
      "iteration 390 current loss: 0.0568959005177021\n",
      "iteration 391 current loss: 0.05623842030763626\n",
      "iteration 392 current loss: 0.057470597326755524\n",
      "iteration 393 current loss: 0.0581679604947567\n",
      "iteration 394 current loss: 0.05588079243898392\n",
      "iteration 395 current loss: 0.05593596771359444\n",
      "iteration 396 current loss: 0.05514334514737129\n",
      "iteration 397 current loss: 0.05755336582660675\n",
      "iteration 398 current loss: 0.05432654172182083\n",
      "iteration 399 current loss: 0.05659116804599762\n",
      "iteration 400 current loss: 0.05864739418029785\n",
      "iteration 401 current loss: 0.057706840336322784\n",
      "iteration 402 current loss: 0.053639307618141174\n",
      "iteration 403 current loss: 0.056252725422382355\n",
      "iteration 404 current loss: 0.057090114802122116\n",
      "iteration 405 current loss: 0.055941298604011536\n",
      "iteration 406 current loss: 0.0562657006084919\n",
      "iteration 407 current loss: 0.054096248000860214\n",
      "iteration 408 current loss: 0.055880527943372726\n",
      "iteration 409 current loss: 0.05347795784473419\n",
      "iteration 410 current loss: 0.06002763658761978\n",
      "\t\tEpoch 15/100 complete. Epoch loss 0.05551504206882196\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 15, Validation Loss: 0.05761072598397732\n",
      "best loss 0.05551504206882196\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.05740014836192131\n",
      "iteration 1 current loss: 0.05266672372817993\n",
      "iteration 2 current loss: 0.055486053228378296\n",
      "iteration 3 current loss: 0.05537356808781624\n",
      "iteration 4 current loss: 0.05391455441713333\n",
      "iteration 5 current loss: 0.058750350028276443\n",
      "iteration 6 current loss: 0.05434582009911537\n",
      "iteration 7 current loss: 0.055092230439186096\n",
      "iteration 8 current loss: 0.055413030087947845\n",
      "iteration 9 current loss: 0.05590257793664932\n",
      "iteration 10 current loss: 0.05565967783331871\n",
      "iteration 11 current loss: 0.05620763450860977\n",
      "iteration 12 current loss: 0.05429072678089142\n",
      "iteration 13 current loss: 0.05778947100043297\n",
      "iteration 14 current loss: 0.05709033086895943\n",
      "iteration 15 current loss: 0.055393971502780914\n",
      "iteration 16 current loss: 0.05575661361217499\n",
      "iteration 17 current loss: 0.051936257630586624\n",
      "iteration 18 current loss: 0.05778957158327103\n",
      "iteration 19 current loss: 0.05704918131232262\n",
      "iteration 20 current loss: 0.05657196044921875\n",
      "iteration 21 current loss: 0.053557138890028\n",
      "iteration 22 current loss: 0.054787423461675644\n",
      "iteration 23 current loss: 0.05419573932886124\n",
      "iteration 24 current loss: 0.0542629212141037\n",
      "iteration 25 current loss: 0.055239588022232056\n",
      "iteration 26 current loss: 0.05552974343299866\n",
      "iteration 27 current loss: 0.05709666758775711\n",
      "iteration 28 current loss: 0.05336158722639084\n",
      "iteration 29 current loss: 0.05757840350270271\n",
      "iteration 30 current loss: 0.05511132627725601\n",
      "iteration 31 current loss: 0.054952073842287064\n",
      "iteration 32 current loss: 0.05399973317980766\n",
      "iteration 33 current loss: 0.05300077423453331\n",
      "iteration 34 current loss: 0.05630899965763092\n",
      "iteration 35 current loss: 0.05368872359395027\n",
      "iteration 36 current loss: 0.05889862775802612\n",
      "iteration 37 current loss: 0.05398907512426376\n",
      "iteration 38 current loss: 0.055673182010650635\n",
      "iteration 39 current loss: 0.054755307734012604\n",
      "iteration 40 current loss: 0.05605265125632286\n",
      "iteration 41 current loss: 0.05432230606675148\n",
      "iteration 42 current loss: 0.052664536982774734\n",
      "iteration 43 current loss: 0.05234630033373833\n",
      "iteration 44 current loss: 0.055446069687604904\n",
      "iteration 45 current loss: 0.05598178133368492\n",
      "iteration 46 current loss: 0.05548635125160217\n",
      "iteration 47 current loss: 0.054501887410879135\n",
      "iteration 48 current loss: 0.05415133386850357\n",
      "iteration 49 current loss: 0.05481785535812378\n",
      "iteration 50 current loss: 0.05535072088241577\n",
      "iteration 51 current loss: 0.054543014615774155\n",
      "iteration 52 current loss: 0.05634591728448868\n",
      "iteration 53 current loss: 0.05473725125193596\n",
      "iteration 54 current loss: 0.056967947632074356\n",
      "iteration 55 current loss: 0.05470766872167587\n",
      "iteration 56 current loss: 0.054527394473552704\n",
      "iteration 57 current loss: 0.05705726519227028\n",
      "iteration 58 current loss: 0.05861850082874298\n",
      "iteration 59 current loss: 0.05857017636299133\n",
      "iteration 60 current loss: 0.05329720303416252\n",
      "iteration 61 current loss: 0.0559537298977375\n",
      "iteration 62 current loss: 0.057561058551073074\n",
      "iteration 63 current loss: 0.05337289720773697\n",
      "iteration 64 current loss: 0.052555639296770096\n",
      "iteration 65 current loss: 0.05319822579622269\n",
      "iteration 66 current loss: 0.0543576180934906\n",
      "iteration 67 current loss: 0.05054083466529846\n",
      "iteration 68 current loss: 0.05285317078232765\n",
      "iteration 69 current loss: 0.05563339218497276\n",
      "iteration 70 current loss: 0.05515625327825546\n",
      "iteration 71 current loss: 0.05413849279284477\n",
      "iteration 72 current loss: 0.05887395143508911\n",
      "iteration 73 current loss: 0.05466095730662346\n",
      "iteration 74 current loss: 0.053456950932741165\n",
      "iteration 75 current loss: 0.052936915308237076\n",
      "iteration 76 current loss: 0.055749911814928055\n",
      "iteration 77 current loss: 0.05789016932249069\n",
      "iteration 78 current loss: 0.05597732588648796\n",
      "iteration 79 current loss: 0.0551360547542572\n",
      "iteration 80 current loss: 0.052951835095882416\n",
      "iteration 81 current loss: 0.05485517904162407\n",
      "iteration 82 current loss: 0.055051229894161224\n",
      "iteration 83 current loss: 0.05292983725667\n",
      "iteration 84 current loss: 0.05324834585189819\n",
      "iteration 85 current loss: 0.055784791707992554\n",
      "iteration 86 current loss: 0.055319949984550476\n",
      "iteration 87 current loss: 0.05410480499267578\n",
      "iteration 88 current loss: 0.055237043648958206\n",
      "iteration 89 current loss: 0.055959030985832214\n",
      "iteration 90 current loss: 0.0557929165661335\n",
      "iteration 91 current loss: 0.05206757038831711\n",
      "iteration 92 current loss: 0.055336251854896545\n",
      "iteration 93 current loss: 0.05308670550584793\n",
      "iteration 94 current loss: 0.05764179304242134\n",
      "iteration 95 current loss: 0.05516291409730911\n",
      "iteration 96 current loss: 0.0565347820520401\n",
      "iteration 97 current loss: 0.05382577329874039\n",
      "iteration 98 current loss: 0.052626073360443115\n",
      "iteration 99 current loss: 0.05533813685178757\n",
      "iteration 100 current loss: 0.05596749857068062\n",
      "iteration 101 current loss: 0.054348174482584\n",
      "iteration 102 current loss: 0.05316094309091568\n",
      "iteration 103 current loss: 0.055171504616737366\n",
      "iteration 104 current loss: 0.05512840673327446\n",
      "iteration 105 current loss: 0.05349559709429741\n",
      "iteration 106 current loss: 0.05454045534133911\n",
      "iteration 107 current loss: 0.05417067930102348\n",
      "iteration 108 current loss: 0.05319433659315109\n",
      "iteration 109 current loss: 0.05396801605820656\n",
      "iteration 110 current loss: 0.05778415873646736\n",
      "iteration 111 current loss: 0.054822422564029694\n",
      "iteration 112 current loss: 0.054033707827329636\n",
      "iteration 113 current loss: 0.05534152314066887\n",
      "iteration 114 current loss: 0.0551346018910408\n",
      "iteration 115 current loss: 0.05847232788801193\n",
      "iteration 116 current loss: 0.054034776985645294\n",
      "iteration 117 current loss: 0.055220481008291245\n",
      "iteration 118 current loss: 0.05651029571890831\n",
      "iteration 119 current loss: 0.05271036550402641\n",
      "iteration 120 current loss: 0.05625659599900246\n",
      "iteration 121 current loss: 0.05379757285118103\n",
      "iteration 122 current loss: 0.05392970144748688\n",
      "iteration 123 current loss: 0.05428208410739899\n",
      "iteration 124 current loss: 0.0547620914876461\n",
      "iteration 125 current loss: 0.05431409180164337\n",
      "iteration 126 current loss: 0.056595344096422195\n",
      "iteration 127 current loss: 0.055668238550424576\n",
      "iteration 128 current loss: 0.05705700069665909\n",
      "iteration 129 current loss: 0.05227122828364372\n",
      "iteration 130 current loss: 0.05428805574774742\n",
      "iteration 131 current loss: 0.05767752602696419\n",
      "iteration 132 current loss: 0.05220980942249298\n",
      "iteration 133 current loss: 0.052924685180187225\n",
      "iteration 134 current loss: 0.0584002323448658\n",
      "iteration 135 current loss: 0.0542178750038147\n",
      "iteration 136 current loss: 0.05393905192613602\n",
      "iteration 137 current loss: 0.054586928337812424\n",
      "iteration 138 current loss: 0.05816580727696419\n",
      "iteration 139 current loss: 0.05695990473031998\n",
      "iteration 140 current loss: 0.05388553813099861\n",
      "iteration 141 current loss: 0.055518727749586105\n",
      "iteration 142 current loss: 0.056486669927835464\n",
      "iteration 143 current loss: 0.055758047848939896\n",
      "iteration 144 current loss: 0.0556783601641655\n",
      "iteration 145 current loss: 0.054567303508520126\n",
      "iteration 146 current loss: 0.05605671554803848\n",
      "iteration 147 current loss: 0.055429890751838684\n",
      "iteration 148 current loss: 0.05473623052239418\n",
      "iteration 149 current loss: 0.05765700340270996\n",
      "iteration 150 current loss: 0.05428702384233475\n",
      "iteration 151 current loss: 0.055853668600320816\n",
      "iteration 152 current loss: 0.05577932670712471\n",
      "iteration 153 current loss: 0.053504578769207\n",
      "iteration 154 current loss: 0.05447900667786598\n",
      "iteration 155 current loss: 0.05731290951371193\n",
      "iteration 156 current loss: 0.051746219396591187\n",
      "iteration 157 current loss: 0.053458768874406815\n",
      "iteration 158 current loss: 0.05578336864709854\n",
      "iteration 159 current loss: 0.05344018340110779\n",
      "iteration 160 current loss: 0.055377297103405\n",
      "iteration 161 current loss: 0.05545789375901222\n",
      "iteration 162 current loss: 0.05459355190396309\n",
      "iteration 163 current loss: 0.05520937591791153\n",
      "iteration 164 current loss: 0.05198483169078827\n",
      "iteration 165 current loss: 0.05447344481945038\n",
      "iteration 166 current loss: 0.055414583534002304\n",
      "iteration 167 current loss: 0.05587911978363991\n",
      "iteration 168 current loss: 0.05473865941166878\n",
      "iteration 169 current loss: 0.055107276886701584\n",
      "iteration 170 current loss: 0.05206775665283203\n",
      "iteration 171 current loss: 0.05439896136522293\n",
      "iteration 172 current loss: 0.056739192456007004\n",
      "iteration 173 current loss: 0.05345902591943741\n",
      "iteration 174 current loss: 0.05696726590394974\n",
      "iteration 175 current loss: 0.05908837169408798\n",
      "iteration 176 current loss: 0.057169340550899506\n",
      "iteration 177 current loss: 0.055262405425310135\n",
      "iteration 178 current loss: 0.05534178018569946\n",
      "iteration 179 current loss: 0.05852023884654045\n",
      "iteration 180 current loss: 0.054569780826568604\n",
      "iteration 181 current loss: 0.054052259773015976\n",
      "iteration 182 current loss: 0.05428338795900345\n",
      "iteration 183 current loss: 0.05565614625811577\n",
      "iteration 184 current loss: 0.05574442818760872\n",
      "iteration 185 current loss: 0.059833087027072906\n",
      "iteration 186 current loss: 0.05692616105079651\n",
      "iteration 187 current loss: 0.056550003588199615\n",
      "iteration 188 current loss: 0.055091243237257004\n",
      "iteration 189 current loss: 0.0541362427175045\n",
      "iteration 190 current loss: 0.0549091137945652\n",
      "iteration 191 current loss: 0.05408324673771858\n",
      "iteration 192 current loss: 0.05774376541376114\n",
      "iteration 193 current loss: 0.05389540269970894\n",
      "iteration 194 current loss: 0.054344456642866135\n",
      "iteration 195 current loss: 0.05567068234086037\n",
      "iteration 196 current loss: 0.0579993911087513\n",
      "iteration 197 current loss: 0.05237923935055733\n",
      "iteration 198 current loss: 0.05583681911230087\n",
      "iteration 199 current loss: 0.05189770460128784\n",
      "iteration 200 current loss: 0.05174906924366951\n",
      "iteration 201 current loss: 0.05651228502392769\n",
      "iteration 202 current loss: 0.053892508149147034\n",
      "iteration 203 current loss: 0.053728967905044556\n",
      "iteration 204 current loss: 0.0564969927072525\n",
      "iteration 205 current loss: 0.05402473360300064\n",
      "iteration 206 current loss: 0.058694273233413696\n",
      "iteration 207 current loss: 0.05607211962342262\n",
      "iteration 208 current loss: 0.05634850263595581\n",
      "iteration 209 current loss: 0.056849174201488495\n",
      "iteration 210 current loss: 0.056134242564439774\n",
      "iteration 211 current loss: 0.05407462269067764\n",
      "iteration 212 current loss: 0.0522468164563179\n",
      "iteration 213 current loss: 0.05602255463600159\n",
      "iteration 214 current loss: 0.05444939061999321\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 66\u001b[0m\n\u001b[1;32m     61\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Update All Data\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m all_loss\u001b[38;5;241m.\u001b[39mappend(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m current loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Log All metrics to wandb\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m#wandb.log({\"All Loss\": loss.item()})\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \n\u001b[1;32m     74\u001b[0m \n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Update Epoch Data\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "\n",
    "best_loss = float('inf')\n",
    "best_model_state = None\n",
    "num_train = len(train_dataset)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Set up total loss/acc trackers\n",
    "all_loss = []\n",
    "all_acc = []\n",
    "all_correct = 0\n",
    "train_running_total = 0\n",
    "\n",
    "\n",
    "\n",
    "# Set up epochal loss/acc trackers\n",
    "epoch_loss = []\n",
    "epoch_acc = []\n",
    "\n",
    "\n",
    "# Set up validation loss/acc trackers\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "val_running_total = 0\n",
    "\n",
    "\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "\n",
    "# For each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    # Refresh Epoch Statistics\n",
    "    print('reset epoch statistics')\n",
    "    epoch_correct = 0\n",
    "    epoch_loss_val = 0\n",
    "\n",
    "    \n",
    "    # Set Network to Train Mode\n",
    "    netD.train()\n",
    "\n",
    "    \n",
    "    # For each batch in the dataloader\n",
    "    for i, (data, _) in enumerate(train_loader, 0):\n",
    "\n",
    "        # Put train data to device (CPU, GPU, or TPU)\n",
    "        x = data.to(device)\n",
    "\n",
    "        #  what does this do? why is this needed here?\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass batch through D\n",
    "        z = netD(x)\n",
    "        \n",
    "        x_bar = netG(z)\n",
    "        \n",
    "        # Calculate loss on batch\n",
    "        loss = criterion(x_bar, x)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "\n",
    "        # Update All Data\n",
    "        all_loss.append(loss.item())\n",
    "        \n",
    "\n",
    "        print(f'iteration {i} current loss: {loss.item()}')\n",
    "        \n",
    "        # Log All metrics to wandb\n",
    "        #wandb.log({\"All Loss\": loss.item()})\n",
    "\n",
    "\n",
    "        # Update Epoch Data\n",
    "        epoch_loss_val += loss.item()\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "    # Compute Epoch Loss at end of Epoch\n",
    "\n",
    "    avg_epoch_loss = epoch_loss_val / len(train_loader)\n",
    "    epoch_loss.append(avg_epoch_loss)\n",
    "\n",
    "    print(f'\\t\\tEpoch {epoch}/{num_epochs} complete. Epoch loss {avg_epoch_loss}')\n",
    "    \n",
    "    # Log Epoch metrics to wandb\n",
    "    #wandb.log({\"Epoch Loss\": avg_epoch_loss})\n",
    "\n",
    "\n",
    "\n",
    "    # Validation Step\n",
    "    print('Starting Validation Loop...')\n",
    "\n",
    "\n",
    "    \n",
    "    # Refresh Validation Statistics\n",
    "    print('reset Validation statistics')\n",
    "    val_correct = 0\n",
    "    val_loss_value = 0\n",
    "\n",
    "    \n",
    "    # Set the model to valuation mode\n",
    "    netD.eval()  \n",
    "\n",
    "    \n",
    "    # Iterate over the validation dataset in batches\n",
    "    with torch.no_grad():\n",
    "        for data, _ in test_loader:\n",
    "            # Put val data to device (CPU, GPU, or TPU)\n",
    "            x = data.to(device)\n",
    "\n",
    "            \n",
    "            # Forward pass batch through D\n",
    "            z = netD(x)\n",
    "\n",
    "            # Forward pass z through G\n",
    "            x_hat = netG(z)\n",
    "\n",
    "            # Calculate loss on validation batch\n",
    "            v_loss = criterion(x_hat, x)\n",
    "            #wandb.log({\"Epoch val_loss\": v_loss.item()}) \n",
    "        \n",
    "            \n",
    "            # Update Val Data\n",
    "            val_loss_value += v_loss.item()\n",
    "\n",
    "\n",
    "    val_loss_value /= len(test_loader)\n",
    "    \n",
    "    val_loss.append(val_loss_value)\n",
    "    \n",
    "    print(f\"\\t\\tValidation Epoch {epoch}, Validation Loss: {val_loss_value}\")\n",
    "\n",
    "\n",
    "    # Log metrics to wandb\n",
    "    #wandb.log({\"Validation Loss\": val_loss_value})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if avg_epoch_loss < best_loss:\n",
    "        best_loss = avg_epoch_loss\n",
    "        print(f'best loss {best_loss}')\n",
    "        best_model_state = netD.state_dict()\n",
    "    \n",
    "    \n",
    "# Load the best model\n",
    "if best_model_state is not None:\n",
    "    PATH = './models/ae_pretraining.pth'\n",
    "    torch.save(best_model_state, PATH)\n",
    "    print(\"Loaded the model with the lowest loss.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8ec6fe1-3841-4d94-b614-d9cd015e9d2f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1184571265.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[19], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    model.load_state_dict(checkpotint_for_netD) -> not all keys in the checkpoint, because you have fc\u001b[0m\n\u001b[0m                                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "model.main.load_state_dict(checkpoint_for_netD)\n",
    "model.load_state_dict(checkpotint_for_netD) -> not all keys in the checkpoint, because you have fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6cda0e91-e9f1-4177-a3b8-fede18e23867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['main.0.weight', 'main.2.weight', 'main.3.weight', 'main.3.bias', 'main.3.running_mean', 'main.3.running_var', 'main.3.num_batches_tracked', 'main.5.weight', 'main.6.weight', 'main.6.bias', 'main.6.running_mean', 'main.6.running_var', 'main.6.num_batches_tracked', 'main.8.weight', 'main.9.weight', 'main.9.bias', 'main.9.running_mean', 'main.9.running_var', 'main.9.num_batches_tracked', 'main.11.weight'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netD.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7d94f2b9-243c-4734-9ea7-c03cedaa0ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Supervised(nn.Module):\n",
    "    def __init__(self, ngpu, dim_z, num_classes):\n",
    "        super(Supervised, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        nc = 3  # Number of input channels for the 96x96x3 image\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 96 x 96\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size: (ndf) x 48 x 48\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size: (ndf*2) x 24 x 24\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size: (ndf*4) x 12 x 12\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size: (ndf*8) x 6 x 6\n",
    "            nn.Conv2d(ndf * 8, dim_z, 6, 1, 0, bias=False)\n",
    "        )\n",
    "        self.fc = nn.Linear(dim_z, num_classes)\n",
    "\n",
    "    def forward(self, input):\n",
    "        z = self.main(input)\n",
    "        z = z.view(input.size(0), -1)  # Flatten z to (batch_size, dim_z)\n",
    "        c = self.fc(z)\n",
    "        return c\n",
    "\n",
    "# Instantiate the model\n",
    "model = Supervised(ngpu=0, dim_z=64, num_classes=10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6072d438-12a8-4115-9f32-182461947892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['main.0.weight', 'main.2.weight', 'main.3.weight', 'main.3.bias', 'main.3.running_mean', 'main.3.running_var', 'main.3.num_batches_tracked', 'main.5.weight', 'main.6.weight', 'main.6.bias', 'main.6.running_mean', 'main.6.running_var', 'main.6.num_batches_tracked', 'main.8.weight', 'main.9.weight', 'main.9.bias', 'main.9.running_mean', 'main.9.running_var', 'main.9.num_batches_tracked', 'main.11.weight', 'fc.weight', 'fc.bias'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "af83f46b-6133-4baa-8e05-a2071d269d3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.main.load_state_dict(netD.main.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9b10386d-61c9-4e9e-80ad-b32009f357ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['fc.weight', 'fc.bias'], unexpected_keys=[])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(netD.state_dict(), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "66ee94e3-e7ab-4f97-ba3f-8bb9798c159a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0, device='cuda:0')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(model.main[0].weight != netD.main[0].weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "561453fc-33a0-4c1d-bef4-1d9923f3a245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['main.0.weight', 'main.2.weight', 'main.3.weight', 'main.3.bias', 'main.3.running_mean', 'main.3.running_var', 'main.3.num_batches_tracked', 'main.5.weight', 'main.6.weight', 'main.6.bias', 'main.6.running_mean', 'main.6.running_var', 'main.6.num_batches_tracked', 'main.8.weight', 'main.9.weight', 'main.9.bias', 'main.9.running_mean', 'main.9.running_var', 'main.9.num_batches_tracked', 'main.11.weight', 'fc.weight', 'fc.bias'])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "86668ddd-7bc5-4e4a-8e48-d41eef7dd731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d4826e-c072-443e-80b1-7e18e19a6493",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
