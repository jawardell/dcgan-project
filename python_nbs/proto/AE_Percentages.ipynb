{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e9b7cf-2a71-4468-b8e3-032f6cda3477",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import STL10\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "\n",
    "\n",
    "learning_rate = float(sys.argv[1])\n",
    "batch_size = int(sys.argv[2])\n",
    "weight_decay = float(sys.argv[3])\n",
    "\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "manualSeed = 999\n",
    "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "torch.use_deterministic_algorithms(True) # Needed for reproducible results\n",
    "\n",
    "\n",
    "\n",
    "# Number of channels in the training images. For color images this is 3\n",
    "nc = 3\n",
    "\n",
    "# Size of z latent vector (i.e. size of generator input)\n",
    "nz = 64\n",
    "\n",
    "# Size of feature maps in discriminator\n",
    "ndf = 96\n",
    "\n",
    "# Size of feature maps in generator\n",
    "ngf = 96\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 100\n",
    "\n",
    "# Learning rate for optimizers\n",
    "lr=learning_rate\n",
    "\n",
    "# Beta1 hyperparameter for Adam optimizers\n",
    "beta1 = 0.5\n",
    "\n",
    "# Number of GPUs available. Use 0 for CPU mode.\n",
    "ngpu = 1\n",
    "\n",
    "\n",
    "# Decide which device we want to run on\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "\n",
    "weight_decay = weight_decay\n",
    "\n",
    "batch_size = batch_size\n",
    "image_size = 96\n",
    "\n",
    "# Create a new transformation that resizes the images\n",
    "transform=transforms.Compose([\n",
    "                               transforms.Resize(image_size),\n",
    "                               transforms.CenterCrop(image_size),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "\n",
    "# Load STL-10 dataset\n",
    "train_dataset = STL10(root='./data', split='train+unlabeled', transform=transform, download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "print(len(train_dataset))\n",
    "print(len(train_loader))\n",
    "\n",
    "test_dataset = STL10(root='./data', split='test', transform=transform, download=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "print(len(test_dataset))\n",
    "print(len(test_loader))\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, ngpu, dim_z):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        nc = 3  # Number of input channels for the 96x96x3 image\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 96 x 96\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size: (ndf) x 48 x 48\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size: (ndf*2) x 24 x 24\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size: (ndf*4) x 12 x 12\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size: (ndf*8) x 6 x 6\n",
    "            nn.Conv2d(ndf * 8, dim_z, 6, 1, 0, bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        z = self.main(input)\n",
    "        return z\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate the encoder\n",
    "encoder = Encoder(ngpu=0, dim_z=64).to(device)\n",
    "\n",
    "# Handle multi-GPU\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    encoder = nn.DataParallel(encoder, list(range(ngpu)))\n",
    "\n",
    "# Randomly initialize all weights\n",
    "encoder.apply(weights_init)\n",
    "\n",
    "# Print the model\n",
    "print(encoder)\n",
    "\n",
    "\n",
    "# Generator Code\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, ngpu, dim_z):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z 64x1x1, going into a convolution\n",
    "            nn.ConvTranspose2d( dim_z, ngf * 8, 6, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size: (ndf*8) x 6 x 6\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size: (ndf*4) x 12 x 12\n",
    "            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size: (ndf*2) x 24 x 24\n",
    "            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size: (ndf) x 48 x 48\n",
    "            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # input is (nc) x 96 x 96\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Lists to store data for plotting\n",
    "percentages = []\n",
    "avg_val_accs = []\n",
    "\n",
    "# Training loop for different percentages of labeled data\n",
    "for percent in range(10, 101, 10):  # Train on 10%, 20%, ..., 100% of the labeled data\n",
    "    # Calculate the number of samples to use\n",
    "    num_samples = int(num_train * percent / 100)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Instantiate the decoder\n",
    "    decoder = Decoder(ngpu=0, dim_z=64).to(device)\n",
    "    \n",
    "    # Handle multi-GPU\n",
    "    if (device.type == 'cuda') and (ngpu > 1):\n",
    "        decoder = nn.DataParallel(decoder, list(range(ngpu)))\n",
    "    \n",
    "    # Randomly initialize all weights\n",
    "    decoder.apply(weights_init)\n",
    "    \n",
    "    # Print the model\n",
    "    print(decoder)\n",
    "\n",
    "    \n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    \n",
    "    params = list(encoder.parameters()) + list(decoder.parameters())\n",
    "    \n",
    "    optimizer = torch.optim.Adam(params, lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "\n",
    "# set up wandb\n",
    "wandb.login()\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"dcgan-project\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": lr,\n",
    "    \"weight_decay\": weight_decay,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"architecture\": \"AE-Percentages\",\n",
    "    \"dataset\": \"STL-10\",\n",
    "    \"epochs\": num_epochs,\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "# Training loop\n",
    "\n",
    "best_loss = float('inf')\n",
    "best_model_state = None\n",
    "num_train = len(train_dataset)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Set up total loss/acc trackers\n",
    "all_loss = []\n",
    "all_acc = []\n",
    "all_correct = 0\n",
    "train_running_total = 0\n",
    "\n",
    "\n",
    "\n",
    "# Set up epochal loss/acc trackers\n",
    "epoch_loss = []\n",
    "epoch_acc = []\n",
    "\n",
    "\n",
    "# Set up validation loss/acc trackers\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "val_running_total = 0\n",
    "\n",
    "\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "\n",
    "# For each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    # Refresh Epoch Statistics\n",
    "    print('reset epoch statistics')\n",
    "    epoch_correct = 0\n",
    "    epoch_loss_val = 0\n",
    "\n",
    "    \n",
    "    # Set Network to Train Mode\n",
    "    encoder.train()\n",
    "\n",
    "    \n",
    "    # For each batch in the dataloader\n",
    "    for i, (data, _) in enumerate(train_loader, 0):\n",
    "\n",
    "        # Put train data to device (CPU, GPU, or TPU)\n",
    "        x = data.to(device)\n",
    "\n",
    "        #  what does this do? why is this needed here?\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass batch through D\n",
    "        z = encoder(x)\n",
    "        \n",
    "        x_bar = decoder(z)\n",
    "        \n",
    "        # Calculate loss on batch\n",
    "        loss = criterion(x_bar, x)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "\n",
    "        # Update All Data\n",
    "        all_loss.append(loss.item())\n",
    "        \n",
    "\n",
    "        print(f'iteration {i} current loss: {loss.item()}')\n",
    "        \n",
    "        # Log All metrics to wandb\n",
    "        wandb.log({\"All Loss\": loss.item()})\n",
    "\n",
    "\n",
    "        # Update Epoch Data\n",
    "        epoch_loss_val += loss.item()\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "    # Compute Epoch Loss at end of Epoch\n",
    "\n",
    "    avg_epoch_loss = epoch_loss_val / len(train_loader)\n",
    "    epoch_loss.append(avg_epoch_loss)\n",
    "\n",
    "    print(f'\\t\\tEpoch {epoch}/{num_epochs} complete. Epoch loss {avg_epoch_loss}')\n",
    "    \n",
    "    # Log Epoch metrics to wandb\n",
    "    wandb.log({\"Epoch Loss\": avg_epoch_loss})\n",
    "\n",
    "\n",
    "\n",
    "    # Validation Step\n",
    "    print('Starting Validation Loop...')\n",
    "\n",
    "\n",
    "    \n",
    "    # Refresh Validation Statistics\n",
    "    print('reset Validation statistics')\n",
    "    val_correct = 0\n",
    "    val_loss_value = 0\n",
    "\n",
    "    \n",
    "    # Set the model to valuation mode\n",
    "    encoder.eval()  \n",
    "\n",
    "    \n",
    "    # Iterate over the validation dataset in batches\n",
    "    with torch.no_grad():\n",
    "        for data, _ in test_loader:\n",
    "            # Put val data to device (CPU, GPU, or TPU)\n",
    "            x = data.to(device)\n",
    "\n",
    "            \n",
    "            # Forward pass batch through D\n",
    "            z = encoder(x)\n",
    "\n",
    "            # Forward pass z through G\n",
    "            x_hat = decoder(z)\n",
    "\n",
    "            # Calculate loss on validation batch\n",
    "            v_loss = criterion(x_hat, x)\n",
    "            wandb.log({\"Epoch val_loss\": v_loss.item()}) \n",
    "        \n",
    "            \n",
    "            # Update Val Data\n",
    "            val_loss_value += v_loss.item()\n",
    "\n",
    "\n",
    "    val_loss_value /= len(test_loader)\n",
    "    \n",
    "    val_loss.append(val_loss_value)\n",
    "    \n",
    "    print(f\"\\t\\tValidation Epoch {epoch}, Validation Loss: {val_loss_value}\")\n",
    "\n",
    "\n",
    "    # Log metrics to wandb\n",
    "    wandb.log({\"Validation Loss\": val_loss_value})\n",
    "\n",
    "\n",
    "\n",
    "    if avg_epoch_loss < best_loss:\n",
    "        best_loss = avg_epoch_loss\n",
    "        print(f'best loss {best_loss}')\n",
    "        best_model_state = encoder.main.state_dict()\n",
    "    \n",
    "    \n",
    "# Save the best model\n",
    "if best_model_state is not None:\n",
    "    PATH = '../models/ae_pretraining_{}_{}_{}.pth'.format(learning_rate, batch_size, weight_decay)\n",
    "    torch.save(best_model_state, PATH)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
