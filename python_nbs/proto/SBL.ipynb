{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c70bad2d-a4d8-43ac-b53b-f54350b46107",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "5000\n",
      "79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "8000\n",
      "125\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:62l9tn53) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e89f765cc5ad42fb8f4ebf38b685a3ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.013 MB of 0.013 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>TrainAccuracy</td><td>▁▂▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇█████</td></tr><tr><td>TrainLoss</td><td>▇██▄▆▄▆▆▆▃▄▄▆▄▃▆▂▄▃▃▄▄▂▁▇▄▄▂▂▃▃▂▂▅</td></tr><tr><td>ValidationAccuracy</td><td>▁▂▂▃▃▄▅▅▅▅▆▅▆▅▆▇▇▇▇▆▇▇▇▇▇▇█▇█████</td></tr><tr><td>ValidationLoss</td><td>█▅▄▃▄▃▂▂▂▂▂▃▂▂▂▁▁▁▁▂▂▁▁▁▂▂▁▁▁▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>TrainAccuracy</td><td>0.7362</td></tr><tr><td>TrainLoss</td><td>1.51896</td></tr><tr><td>ValidationAccuracy</td><td>0.49838</td></tr><tr><td>ValidationLoss</td><td>1.62803</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fearless-lake-178</strong> at: <a href='https://wandb.ai/jawardell/dcgan-project/runs/62l9tn53' target=\"_blank\">https://wandb.ai/jawardell/dcgan-project/runs/62l9tn53</a><br/> View job at <a href='https://wandb.ai/jawardell/dcgan-project/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjExOTIzMzE4OQ==/version_details/v6' target=\"_blank\">https://wandb.ai/jawardell/dcgan-project/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjExOTIzMzE4OQ==/version_details/v6</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231128_022145-62l9tn53/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:62l9tn53). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e0da6c9c8964e5faab0db5dabefe58e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112011133486198, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/users2/jwardell1/dcgan-project/python_nbs/wandb/run-20231128_023301-eglcshzp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jawardell/dcgan-project/runs/eglcshzp' target=\"_blank\">apricot-deluge-179</a></strong> to <a href='https://wandb.ai/jawardell/dcgan-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jawardell/dcgan-project' target=\"_blank\">https://wandb.ai/jawardell/dcgan-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jawardell/dcgan-project/runs/eglcshzp' target=\"_blank\">https://wandb.ai/jawardell/dcgan-project/runs/eglcshzp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Loop...\n",
      "iteration 0 current loss: 3.497817277908325 current acc: 0.0008\n",
      "iteration 1 current loss: 3.45418381690979 current acc: 0.0022\n",
      "iteration 2 current loss: 3.767062187194824 current acc: 0.0032\n",
      "iteration 3 current loss: 3.4746174812316895 current acc: 0.0044\n",
      "iteration 4 current loss: 3.52549409866333 current acc: 0.0058\n",
      "iteration 5 current loss: 3.3579235076904297 current acc: 0.0082\n",
      "iteration 6 current loss: 3.4131417274475098 current acc: 0.0106\n",
      "iteration 7 current loss: 3.0337750911712646 current acc: 0.0136\n",
      "iteration 8 current loss: 3.0320680141448975 current acc: 0.0158\n",
      "iteration 9 current loss: 3.455893039703369 current acc: 0.0176\n",
      "iteration 10 current loss: 3.1720356941223145 current acc: 0.02\n",
      "iteration 11 current loss: 3.3657970428466797 current acc: 0.0214\n",
      "iteration 12 current loss: 3.3791987895965576 current acc: 0.0238\n",
      "iteration 13 current loss: 2.883157968521118 current acc: 0.0268\n",
      "iteration 14 current loss: 3.0440192222595215 current acc: 0.0298\n",
      "iteration 15 current loss: 3.147401809692383 current acc: 0.0318\n",
      "iteration 16 current loss: 2.9988279342651367 current acc: 0.035\n",
      "iteration 17 current loss: 3.119230270385742 current acc: 0.0368\n",
      "iteration 18 current loss: 3.181837797164917 current acc: 0.0396\n",
      "iteration 19 current loss: 2.9769442081451416 current acc: 0.0432\n",
      "iteration 20 current loss: 2.8968029022216797 current acc: 0.0462\n",
      "iteration 21 current loss: 2.728748083114624 current acc: 0.049\n",
      "iteration 22 current loss: 3.2185144424438477 current acc: 0.0512\n",
      "iteration 23 current loss: 3.551482677459717 current acc: 0.0534\n",
      "iteration 24 current loss: 3.3777854442596436 current acc: 0.0556\n",
      "iteration 25 current loss: 2.900916814804077 current acc: 0.0588\n",
      "iteration 26 current loss: 2.972705125808716 current acc: 0.0618\n",
      "iteration 27 current loss: 3.1137681007385254 current acc: 0.0644\n",
      "iteration 28 current loss: 3.0840816497802734 current acc: 0.0666\n",
      "iteration 29 current loss: 2.915076971054077 current acc: 0.0702\n",
      "iteration 30 current loss: 3.1015567779541016 current acc: 0.0726\n",
      "iteration 31 current loss: 2.855980634689331 current acc: 0.0756\n",
      "iteration 32 current loss: 2.9544787406921387 current acc: 0.078\n",
      "iteration 33 current loss: 2.946897029876709 current acc: 0.0808\n",
      "iteration 34 current loss: 2.9153647422790527 current acc: 0.0844\n",
      "iteration 35 current loss: 2.9131622314453125 current acc: 0.0878\n",
      "iteration 36 current loss: 2.195465326309204 current acc: 0.0916\n",
      "iteration 37 current loss: 3.0502145290374756 current acc: 0.0942\n",
      "iteration 38 current loss: 2.352252721786499 current acc: 0.098\n",
      "iteration 39 current loss: 2.9684643745422363 current acc: 0.1014\n",
      "iteration 40 current loss: 2.5097615718841553 current acc: 0.1052\n",
      "iteration 41 current loss: 3.1871330738067627 current acc: 0.1082\n",
      "iteration 42 current loss: 3.1977343559265137 current acc: 0.1106\n",
      "iteration 43 current loss: 2.2354836463928223 current acc: 0.1146\n",
      "iteration 44 current loss: 2.474592685699463 current acc: 0.1186\n",
      "iteration 45 current loss: 2.947786569595337 current acc: 0.1206\n",
      "iteration 46 current loss: 2.832486152648926 current acc: 0.1246\n",
      "iteration 47 current loss: 2.507472038269043 current acc: 0.1278\n",
      "iteration 48 current loss: 3.2862632274627686 current acc: 0.13\n",
      "iteration 49 current loss: 2.7769265174865723 current acc: 0.1328\n",
      "iteration 50 current loss: 2.8714046478271484 current acc: 0.1356\n",
      "iteration 51 current loss: 2.4541218280792236 current acc: 0.1394\n",
      "iteration 52 current loss: 2.5297963619232178 current acc: 0.1428\n",
      "iteration 53 current loss: 3.0939035415649414 current acc: 0.1448\n",
      "iteration 54 current loss: 2.4633824825286865 current acc: 0.1488\n",
      "iteration 55 current loss: 2.583763599395752 current acc: 0.151\n",
      "iteration 56 current loss: 2.1175570487976074 current acc: 0.1554\n",
      "iteration 57 current loss: 3.07724928855896 current acc: 0.1584\n",
      "iteration 58 current loss: 2.9112117290496826 current acc: 0.1624\n",
      "iteration 59 current loss: 2.982126474380493 current acc: 0.1652\n",
      "iteration 60 current loss: 2.5405752658843994 current acc: 0.1688\n",
      "iteration 61 current loss: 2.7245724201202393 current acc: 0.1708\n",
      "iteration 62 current loss: 2.5718350410461426 current acc: 0.174\n",
      "iteration 63 current loss: 2.277273654937744 current acc: 0.1778\n",
      "iteration 64 current loss: 2.662254571914673 current acc: 0.1816\n",
      "iteration 65 current loss: 2.607318878173828 current acc: 0.1858\n",
      "iteration 66 current loss: 2.3017172813415527 current acc: 0.19\n",
      "iteration 67 current loss: 2.3480265140533447 current acc: 0.1946\n",
      "iteration 68 current loss: 2.594465732574463 current acc: 0.1978\n",
      "iteration 69 current loss: 2.635544538497925 current acc: 0.2004\n",
      "iteration 70 current loss: 2.766162872314453 current acc: 0.2032\n",
      "iteration 71 current loss: 2.7802886962890625 current acc: 0.206\n",
      "iteration 72 current loss: 2.0798661708831787 current acc: 0.2092\n",
      "iteration 73 current loss: 2.742449998855591 current acc: 0.2132\n",
      "iteration 74 current loss: 2.414771795272827 current acc: 0.2164\n",
      "iteration 75 current loss: 2.547290086746216 current acc: 0.2194\n",
      "iteration 76 current loss: 2.7941055297851562 current acc: 0.2226\n",
      "iteration 77 current loss: 2.5994699001312256 current acc: 0.226\n",
      "iteration 78 current loss: 2.4888410568237305 current acc: 0.2262\n",
      "\t\tTrain Epoch 0/100,Train Accuracy: 0.2262, Train Loss: 2.883685229699823.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 0/100, Validation Accuracy: 0.284125, Validation Loss: 2.3997790927886964\n",
      "best loss 2.883685229699823\n",
      "iteration 0 current loss: 2.83353590965271 current acc: 0.0024\n",
      "iteration 1 current loss: 1.8926832675933838 current acc: 0.0074\n",
      "iteration 2 current loss: 2.486440658569336 current acc: 0.01\n",
      "iteration 3 current loss: 2.110034227371216 current acc: 0.0144\n",
      "iteration 4 current loss: 2.545854091644287 current acc: 0.0182\n",
      "iteration 5 current loss: 2.373542547225952 current acc: 0.0228\n",
      "iteration 6 current loss: 2.6243209838867188 current acc: 0.0262\n",
      "iteration 7 current loss: 2.351255178451538 current acc: 0.029\n",
      "iteration 8 current loss: 2.459819793701172 current acc: 0.0314\n",
      "iteration 9 current loss: 2.058581829071045 current acc: 0.0364\n",
      "iteration 10 current loss: 2.457470655441284 current acc: 0.0396\n",
      "iteration 11 current loss: 2.6586546897888184 current acc: 0.0426\n",
      "iteration 12 current loss: 2.378384590148926 current acc: 0.0468\n",
      "iteration 13 current loss: 2.39176607131958 current acc: 0.0506\n",
      "iteration 14 current loss: 1.8261574506759644 current acc: 0.0558\n",
      "iteration 15 current loss: 2.073133707046509 current acc: 0.0598\n",
      "iteration 16 current loss: 2.221665382385254 current acc: 0.063\n",
      "iteration 17 current loss: 1.9658474922180176 current acc: 0.0676\n",
      "iteration 18 current loss: 2.2353339195251465 current acc: 0.0718\n",
      "iteration 19 current loss: 2.863755226135254 current acc: 0.075\n",
      "iteration 20 current loss: 2.307338237762451 current acc: 0.0788\n",
      "iteration 21 current loss: 2.184957981109619 current acc: 0.082\n",
      "iteration 22 current loss: 2.3819143772125244 current acc: 0.0854\n",
      "iteration 23 current loss: 1.9463046789169312 current acc: 0.0896\n",
      "iteration 24 current loss: 1.9153767824172974 current acc: 0.0944\n",
      "iteration 25 current loss: 2.1705427169799805 current acc: 0.0986\n",
      "iteration 26 current loss: 2.3182196617126465 current acc: 0.1018\n",
      "iteration 27 current loss: 2.3676207065582275 current acc: 0.1058\n",
      "iteration 28 current loss: 2.205613851547241 current acc: 0.1086\n",
      "iteration 29 current loss: 2.1677656173706055 current acc: 0.1124\n",
      "iteration 30 current loss: 2.328566074371338 current acc: 0.116\n",
      "iteration 31 current loss: 2.3179101943969727 current acc: 0.1196\n",
      "iteration 32 current loss: 2.0435709953308105 current acc: 0.1244\n",
      "iteration 33 current loss: 2.8951416015625 current acc: 0.126\n",
      "iteration 34 current loss: 2.2813339233398438 current acc: 0.13\n",
      "iteration 35 current loss: 2.127755641937256 current acc: 0.1324\n",
      "iteration 36 current loss: 2.3553857803344727 current acc: 0.1362\n",
      "iteration 37 current loss: 1.8632644414901733 current acc: 0.1408\n",
      "iteration 38 current loss: 2.3628768920898438 current acc: 0.1452\n",
      "iteration 39 current loss: 1.908055067062378 current acc: 0.149\n",
      "iteration 40 current loss: 2.2676146030426025 current acc: 0.1534\n",
      "iteration 41 current loss: 2.1335127353668213 current acc: 0.1578\n",
      "iteration 42 current loss: 2.1029269695281982 current acc: 0.1624\n",
      "iteration 43 current loss: 2.2006523609161377 current acc: 0.1664\n",
      "iteration 44 current loss: 2.0484819412231445 current acc: 0.1706\n",
      "iteration 45 current loss: 2.5479483604431152 current acc: 0.1738\n",
      "iteration 46 current loss: 2.110081672668457 current acc: 0.179\n",
      "iteration 47 current loss: 2.207749843597412 current acc: 0.183\n",
      "iteration 48 current loss: 2.430819034576416 current acc: 0.1858\n",
      "iteration 49 current loss: 2.2117950916290283 current acc: 0.1898\n",
      "iteration 50 current loss: 2.167160987854004 current acc: 0.1936\n",
      "iteration 51 current loss: 2.1501245498657227 current acc: 0.1974\n",
      "iteration 52 current loss: 2.2460741996765137 current acc: 0.2016\n",
      "iteration 53 current loss: 1.9384636878967285 current acc: 0.206\n",
      "iteration 54 current loss: 2.178135395050049 current acc: 0.21\n",
      "iteration 55 current loss: 2.247774362564087 current acc: 0.2142\n",
      "iteration 56 current loss: 2.3135581016540527 current acc: 0.2172\n",
      "iteration 57 current loss: 2.1724374294281006 current acc: 0.2208\n",
      "iteration 58 current loss: 1.9667224884033203 current acc: 0.2252\n",
      "iteration 59 current loss: 2.411543130874634 current acc: 0.2284\n",
      "iteration 60 current loss: 2.1361958980560303 current acc: 0.2318\n",
      "iteration 61 current loss: 2.2329063415527344 current acc: 0.2362\n",
      "iteration 62 current loss: 1.7436895370483398 current acc: 0.2406\n",
      "iteration 63 current loss: 2.0471913814544678 current acc: 0.2442\n",
      "iteration 64 current loss: 2.0206549167633057 current acc: 0.2484\n",
      "iteration 65 current loss: 2.4278523921966553 current acc: 0.2512\n",
      "iteration 66 current loss: 2.081916570663452 current acc: 0.2552\n",
      "iteration 67 current loss: 2.2741012573242188 current acc: 0.2596\n",
      "iteration 68 current loss: 2.2575459480285645 current acc: 0.2628\n",
      "iteration 69 current loss: 2.1942999362945557 current acc: 0.2656\n",
      "iteration 70 current loss: 2.6756651401519775 current acc: 0.2682\n",
      "iteration 71 current loss: 2.12650728225708 current acc: 0.2722\n",
      "iteration 72 current loss: 2.0397777557373047 current acc: 0.2762\n",
      "iteration 73 current loss: 2.1883652210235596 current acc: 0.2794\n",
      "iteration 74 current loss: 2.366842746734619 current acc: 0.2834\n",
      "iteration 75 current loss: 2.266310214996338 current acc: 0.2872\n",
      "iteration 76 current loss: 1.9249868392944336 current acc: 0.2912\n",
      "iteration 77 current loss: 2.038301706314087 current acc: 0.2942\n",
      "iteration 78 current loss: 1.8039876222610474 current acc: 0.2946\n",
      "\t\tTrain Epoch 1/100,Train Accuracy: 0.2946, Train Loss: 2.229853525946412.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 1/100, Validation Accuracy: 0.29775, Validation Loss: 2.234764423370361\n",
      "best loss 2.229853525946412\n",
      "iteration 0 current loss: 2.0046653747558594 current acc: 0.0048\n",
      "iteration 1 current loss: 2.16253399848938 current acc: 0.0082\n",
      "iteration 2 current loss: 2.0971455574035645 current acc: 0.0122\n",
      "iteration 3 current loss: 2.2838995456695557 current acc: 0.0162\n",
      "iteration 4 current loss: 2.1915667057037354 current acc: 0.0204\n",
      "iteration 5 current loss: 1.9073855876922607 current acc: 0.0258\n",
      "iteration 6 current loss: 1.9235577583312988 current acc: 0.0306\n",
      "iteration 7 current loss: 2.1597609519958496 current acc: 0.0352\n",
      "iteration 8 current loss: 1.9650071859359741 current acc: 0.0398\n",
      "iteration 9 current loss: 2.0854854583740234 current acc: 0.0438\n",
      "iteration 10 current loss: 2.0761146545410156 current acc: 0.0482\n",
      "iteration 11 current loss: 1.8283003568649292 current acc: 0.0528\n",
      "iteration 12 current loss: 1.8532806634902954 current acc: 0.0572\n",
      "iteration 13 current loss: 1.6149619817733765 current acc: 0.0624\n",
      "iteration 14 current loss: 1.7927393913269043 current acc: 0.0676\n",
      "iteration 15 current loss: 2.1257729530334473 current acc: 0.0712\n",
      "iteration 16 current loss: 1.6454277038574219 current acc: 0.0758\n",
      "iteration 17 current loss: 1.9481197595596313 current acc: 0.0808\n",
      "iteration 18 current loss: 1.983787178993225 current acc: 0.085\n",
      "iteration 19 current loss: 2.2732791900634766 current acc: 0.0878\n",
      "iteration 20 current loss: 1.918560266494751 current acc: 0.0918\n",
      "iteration 21 current loss: 1.9762382507324219 current acc: 0.0968\n",
      "iteration 22 current loss: 1.7265063524246216 current acc: 0.1026\n",
      "iteration 23 current loss: 2.311582088470459 current acc: 0.1064\n",
      "iteration 24 current loss: 1.8169560432434082 current acc: 0.1112\n",
      "iteration 25 current loss: 2.007993221282959 current acc: 0.116\n",
      "iteration 26 current loss: 1.9863255023956299 current acc: 0.1208\n",
      "iteration 27 current loss: 1.898589849472046 current acc: 0.125\n",
      "iteration 28 current loss: 2.1861798763275146 current acc: 0.1284\n",
      "iteration 29 current loss: 1.849777102470398 current acc: 0.1332\n",
      "iteration 30 current loss: 2.224029302597046 current acc: 0.1366\n",
      "iteration 31 current loss: 1.8803752660751343 current acc: 0.1408\n",
      "iteration 32 current loss: 2.0526018142700195 current acc: 0.1452\n",
      "iteration 33 current loss: 2.124070167541504 current acc: 0.1486\n",
      "iteration 34 current loss: 1.8400099277496338 current acc: 0.1524\n",
      "iteration 35 current loss: 1.9604747295379639 current acc: 0.1564\n",
      "iteration 36 current loss: 2.1695661544799805 current acc: 0.1604\n",
      "iteration 37 current loss: 1.9129494428634644 current acc: 0.165\n",
      "iteration 38 current loss: 2.137561082839966 current acc: 0.1686\n",
      "iteration 39 current loss: 2.303706169128418 current acc: 0.1714\n",
      "iteration 40 current loss: 1.8427103757858276 current acc: 0.1762\n",
      "iteration 41 current loss: 1.8854130506515503 current acc: 0.1814\n",
      "iteration 42 current loss: 1.8678178787231445 current acc: 0.1854\n",
      "iteration 43 current loss: 1.9992554187774658 current acc: 0.1904\n",
      "iteration 44 current loss: 2.3872992992401123 current acc: 0.1936\n",
      "iteration 45 current loss: 1.7615536451339722 current acc: 0.198\n",
      "iteration 46 current loss: 2.28196382522583 current acc: 0.2016\n",
      "iteration 47 current loss: 1.9684666395187378 current acc: 0.2056\n",
      "iteration 48 current loss: 1.812268853187561 current acc: 0.21\n",
      "iteration 49 current loss: 1.74439537525177 current acc: 0.2148\n",
      "iteration 50 current loss: 2.011644124984741 current acc: 0.2194\n",
      "iteration 51 current loss: 2.2809298038482666 current acc: 0.2222\n",
      "iteration 52 current loss: 1.8990530967712402 current acc: 0.226\n",
      "iteration 53 current loss: 1.6846635341644287 current acc: 0.231\n",
      "iteration 54 current loss: 2.090888261795044 current acc: 0.235\n",
      "iteration 55 current loss: 1.7595335245132446 current acc: 0.2402\n",
      "iteration 56 current loss: 1.8828943967819214 current acc: 0.2444\n",
      "iteration 57 current loss: 1.7252099514007568 current acc: 0.2498\n",
      "iteration 58 current loss: 1.794217824935913 current acc: 0.2544\n",
      "iteration 59 current loss: 1.9711508750915527 current acc: 0.2586\n",
      "iteration 60 current loss: 2.136657238006592 current acc: 0.262\n",
      "iteration 61 current loss: 2.2099976539611816 current acc: 0.2658\n",
      "iteration 62 current loss: 1.6478029489517212 current acc: 0.2714\n",
      "iteration 63 current loss: 2.1125731468200684 current acc: 0.275\n",
      "iteration 64 current loss: 1.8196724653244019 current acc: 0.2796\n",
      "iteration 65 current loss: 2.1097605228424072 current acc: 0.2836\n",
      "iteration 66 current loss: 1.9828133583068848 current acc: 0.2878\n",
      "iteration 67 current loss: 2.077725887298584 current acc: 0.2926\n",
      "iteration 68 current loss: 1.7716827392578125 current acc: 0.2972\n",
      "iteration 69 current loss: 1.996971607208252 current acc: 0.3012\n",
      "iteration 70 current loss: 2.2273848056793213 current acc: 0.304\n",
      "iteration 71 current loss: 2.0877223014831543 current acc: 0.3076\n",
      "iteration 72 current loss: 1.8225593566894531 current acc: 0.312\n",
      "iteration 73 current loss: 1.901194453239441 current acc: 0.3162\n",
      "iteration 74 current loss: 1.915482521057129 current acc: 0.3204\n",
      "iteration 75 current loss: 1.4373290538787842 current acc: 0.3274\n",
      "iteration 76 current loss: 1.8646254539489746 current acc: 0.332\n",
      "iteration 77 current loss: 1.7900387048721313 current acc: 0.3368\n",
      "iteration 78 current loss: 1.389326572418213 current acc: 0.3374\n",
      "\t\tTrain Epoch 2/100,Train Accuracy: 0.3374, Train Loss: 1.9640442419655715.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 2/100, Validation Accuracy: 0.319375, Validation Loss: 2.0051303005218504\n",
      "best loss 1.9640442419655715\n",
      "iteration 0 current loss: 1.6768467426300049 current acc: 0.0058\n",
      "iteration 1 current loss: 1.7668451070785522 current acc: 0.0102\n",
      "iteration 2 current loss: 2.187809467315674 current acc: 0.0132\n",
      "iteration 3 current loss: 1.9358019828796387 current acc: 0.019\n",
      "iteration 4 current loss: 2.2167575359344482 current acc: 0.0228\n",
      "iteration 5 current loss: 1.8787986040115356 current acc: 0.0274\n",
      "iteration 6 current loss: 2.0343542098999023 current acc: 0.032\n",
      "iteration 7 current loss: 2.0967602729797363 current acc: 0.0368\n",
      "iteration 8 current loss: 2.3364031314849854 current acc: 0.0406\n",
      "iteration 9 current loss: 1.8693780899047852 current acc: 0.0444\n",
      "iteration 10 current loss: 1.7164340019226074 current acc: 0.0492\n",
      "iteration 11 current loss: 1.7701903581619263 current acc: 0.0534\n",
      "iteration 12 current loss: 1.813518762588501 current acc: 0.0582\n",
      "iteration 13 current loss: 1.741959571838379 current acc: 0.0644\n",
      "iteration 14 current loss: 2.009763479232788 current acc: 0.0692\n",
      "iteration 15 current loss: 1.9997406005859375 current acc: 0.0726\n",
      "iteration 16 current loss: 1.8951575756072998 current acc: 0.0766\n",
      "iteration 17 current loss: 1.763209581375122 current acc: 0.0808\n",
      "iteration 18 current loss: 1.8532688617706299 current acc: 0.0848\n",
      "iteration 19 current loss: 1.7199037075042725 current acc: 0.0908\n",
      "iteration 20 current loss: 1.905594825744629 current acc: 0.0946\n",
      "iteration 21 current loss: 1.8275494575500488 current acc: 0.1002\n",
      "iteration 22 current loss: 1.8475873470306396 current acc: 0.1056\n",
      "iteration 23 current loss: 2.106492042541504 current acc: 0.1096\n",
      "iteration 24 current loss: 1.928717851638794 current acc: 0.1142\n",
      "iteration 25 current loss: 1.8267345428466797 current acc: 0.1186\n",
      "iteration 26 current loss: 1.93199622631073 current acc: 0.123\n",
      "iteration 27 current loss: 1.6182762384414673 current acc: 0.1284\n",
      "iteration 28 current loss: 1.418692708015442 current acc: 0.1346\n",
      "iteration 29 current loss: 2.095128059387207 current acc: 0.1388\n",
      "iteration 30 current loss: 1.7864434719085693 current acc: 0.144\n",
      "iteration 31 current loss: 1.7398825883865356 current acc: 0.1492\n",
      "iteration 32 current loss: 1.420590877532959 current acc: 0.155\n",
      "iteration 33 current loss: 1.6646090745925903 current acc: 0.1598\n",
      "iteration 34 current loss: 2.1970810890197754 current acc: 0.1632\n",
      "iteration 35 current loss: 1.785111427307129 current acc: 0.1676\n",
      "iteration 36 current loss: 1.9883936643600464 current acc: 0.1716\n",
      "iteration 37 current loss: 1.9588218927383423 current acc: 0.1748\n",
      "iteration 38 current loss: 1.9264200925827026 current acc: 0.1792\n",
      "iteration 39 current loss: 1.7702525854110718 current acc: 0.1846\n",
      "iteration 40 current loss: 1.6884987354278564 current acc: 0.1886\n",
      "iteration 41 current loss: 2.0091991424560547 current acc: 0.1922\n",
      "iteration 42 current loss: 1.4991501569747925 current acc: 0.1982\n",
      "iteration 43 current loss: 1.8197003602981567 current acc: 0.2014\n",
      "iteration 44 current loss: 1.8073924779891968 current acc: 0.2064\n",
      "iteration 45 current loss: 2.054678201675415 current acc: 0.21\n",
      "iteration 46 current loss: 1.7445422410964966 current acc: 0.2154\n",
      "iteration 47 current loss: 1.7612402439117432 current acc: 0.2216\n",
      "iteration 48 current loss: 1.7721529006958008 current acc: 0.2274\n",
      "iteration 49 current loss: 1.9136637449264526 current acc: 0.231\n",
      "iteration 50 current loss: 1.7954800128936768 current acc: 0.236\n",
      "iteration 51 current loss: 1.870593547821045 current acc: 0.2404\n",
      "iteration 52 current loss: 1.7369358539581299 current acc: 0.2458\n",
      "iteration 53 current loss: 1.76213538646698 current acc: 0.2502\n",
      "iteration 54 current loss: 1.7336184978485107 current acc: 0.2552\n",
      "iteration 55 current loss: 1.6524673700332642 current acc: 0.2608\n",
      "iteration 56 current loss: 2.0256197452545166 current acc: 0.2644\n",
      "iteration 57 current loss: 1.9736623764038086 current acc: 0.269\n",
      "iteration 58 current loss: 1.8432514667510986 current acc: 0.274\n",
      "iteration 59 current loss: 1.6539071798324585 current acc: 0.2798\n",
      "iteration 60 current loss: 1.8231974840164185 current acc: 0.2848\n",
      "iteration 61 current loss: 1.862009048461914 current acc: 0.2884\n",
      "iteration 62 current loss: 1.8378781080245972 current acc: 0.292\n",
      "iteration 63 current loss: 1.950042963027954 current acc: 0.2958\n",
      "iteration 64 current loss: 1.9126360416412354 current acc: 0.3004\n",
      "iteration 65 current loss: 1.5038666725158691 current acc: 0.3062\n",
      "iteration 66 current loss: 1.9193572998046875 current acc: 0.3102\n",
      "iteration 67 current loss: 1.7220489978790283 current acc: 0.3146\n",
      "iteration 68 current loss: 1.4378416538238525 current acc: 0.3204\n",
      "iteration 69 current loss: 1.8217931985855103 current acc: 0.325\n",
      "iteration 70 current loss: 1.8669471740722656 current acc: 0.3296\n",
      "iteration 71 current loss: 1.8087780475616455 current acc: 0.3346\n",
      "iteration 72 current loss: 1.6800116300582886 current acc: 0.3398\n",
      "iteration 73 current loss: 1.5019097328186035 current acc: 0.347\n",
      "iteration 74 current loss: 1.7453984022140503 current acc: 0.352\n",
      "iteration 75 current loss: 1.7148470878601074 current acc: 0.3566\n",
      "iteration 76 current loss: 1.854612946510315 current acc: 0.3612\n",
      "iteration 77 current loss: 1.7394160032272339 current acc: 0.3654\n",
      "iteration 78 current loss: 2.200939655303955 current acc: 0.3658\n",
      "\t\tTrain Epoch 3/100,Train Accuracy: 0.3658, Train Loss: 1.8360594873186908.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 3/100, Validation Accuracy: 0.341625, Validation Loss: 1.8889951038360595\n",
      "best loss 1.8360594873186908\n",
      "iteration 0 current loss: 1.6833884716033936 current acc: 0.0046\n",
      "iteration 1 current loss: 1.8146560192108154 current acc: 0.0094\n",
      "iteration 2 current loss: 1.7141979932785034 current acc: 0.0138\n",
      "iteration 3 current loss: 1.8036671876907349 current acc: 0.0186\n",
      "iteration 4 current loss: 1.9975749254226685 current acc: 0.0232\n",
      "iteration 5 current loss: 1.739816665649414 current acc: 0.0288\n",
      "iteration 6 current loss: 2.0440027713775635 current acc: 0.0334\n",
      "iteration 7 current loss: 1.5666850805282593 current acc: 0.0404\n",
      "iteration 8 current loss: 1.8214024305343628 current acc: 0.0458\n",
      "iteration 9 current loss: 2.0880517959594727 current acc: 0.0496\n",
      "iteration 10 current loss: 2.058802843093872 current acc: 0.0542\n",
      "iteration 11 current loss: 1.734805703163147 current acc: 0.0594\n",
      "iteration 12 current loss: 1.931735873222351 current acc: 0.0638\n",
      "iteration 13 current loss: 1.8153424263000488 current acc: 0.0676\n",
      "iteration 14 current loss: 1.943190336227417 current acc: 0.0716\n",
      "iteration 15 current loss: 1.8387264013290405 current acc: 0.0764\n",
      "iteration 16 current loss: 1.773941993713379 current acc: 0.0812\n",
      "iteration 17 current loss: 2.1566569805145264 current acc: 0.085\n",
      "iteration 18 current loss: 1.8642864227294922 current acc: 0.0888\n",
      "iteration 19 current loss: 1.6125057935714722 current acc: 0.0948\n",
      "iteration 20 current loss: 1.7273211479187012 current acc: 0.0996\n",
      "iteration 21 current loss: 1.9498323202133179 current acc: 0.1034\n",
      "iteration 22 current loss: 1.6383634805679321 current acc: 0.1094\n",
      "iteration 23 current loss: 1.6569536924362183 current acc: 0.1144\n",
      "iteration 24 current loss: 1.9931975603103638 current acc: 0.1194\n",
      "iteration 25 current loss: 2.035736322402954 current acc: 0.123\n",
      "iteration 26 current loss: 1.9084982872009277 current acc: 0.1274\n",
      "iteration 27 current loss: 1.764228343963623 current acc: 0.1318\n",
      "iteration 28 current loss: 1.8842216730117798 current acc: 0.1366\n",
      "iteration 29 current loss: 1.4671392440795898 current acc: 0.1428\n",
      "iteration 30 current loss: 1.7399146556854248 current acc: 0.1474\n",
      "iteration 31 current loss: 1.789296269416809 current acc: 0.1526\n",
      "iteration 32 current loss: 2.0710082054138184 current acc: 0.1568\n",
      "iteration 33 current loss: 1.6956267356872559 current acc: 0.1612\n",
      "iteration 34 current loss: 1.8498276472091675 current acc: 0.1662\n",
      "iteration 35 current loss: 2.0007145404815674 current acc: 0.1704\n",
      "iteration 36 current loss: 1.7487902641296387 current acc: 0.1752\n",
      "iteration 37 current loss: 1.6977577209472656 current acc: 0.1818\n",
      "iteration 38 current loss: 1.568601369857788 current acc: 0.1882\n",
      "iteration 39 current loss: 1.7672330141067505 current acc: 0.1926\n",
      "iteration 40 current loss: 1.6552700996398926 current acc: 0.1982\n",
      "iteration 41 current loss: 1.8900448083877563 current acc: 0.203\n",
      "iteration 42 current loss: 1.761656403541565 current acc: 0.2076\n",
      "iteration 43 current loss: 1.700137972831726 current acc: 0.2124\n",
      "iteration 44 current loss: 1.7230721712112427 current acc: 0.2168\n",
      "iteration 45 current loss: 1.6894439458847046 current acc: 0.2216\n",
      "iteration 46 current loss: 1.5823863744735718 current acc: 0.2268\n",
      "iteration 47 current loss: 1.683483362197876 current acc: 0.2318\n",
      "iteration 48 current loss: 1.7850944995880127 current acc: 0.2366\n",
      "iteration 49 current loss: 1.7726638317108154 current acc: 0.2422\n",
      "iteration 50 current loss: 1.737471342086792 current acc: 0.247\n",
      "iteration 51 current loss: 1.5233570337295532 current acc: 0.2526\n",
      "iteration 52 current loss: 1.8986618518829346 current acc: 0.2572\n",
      "iteration 53 current loss: 1.7936614751815796 current acc: 0.2626\n",
      "iteration 54 current loss: 2.0434253215789795 current acc: 0.266\n",
      "iteration 55 current loss: 1.5043606758117676 current acc: 0.2718\n",
      "iteration 56 current loss: 1.65279221534729 current acc: 0.2776\n",
      "iteration 57 current loss: 1.7628107070922852 current acc: 0.282\n",
      "iteration 58 current loss: 1.833390712738037 current acc: 0.2862\n",
      "iteration 59 current loss: 1.7835264205932617 current acc: 0.291\n",
      "iteration 60 current loss: 1.7314035892486572 current acc: 0.2966\n",
      "iteration 61 current loss: 1.6387790441513062 current acc: 0.3014\n",
      "iteration 62 current loss: 1.6671005487442017 current acc: 0.3068\n",
      "iteration 63 current loss: 2.0362210273742676 current acc: 0.3108\n",
      "iteration 64 current loss: 1.5685073137283325 current acc: 0.3164\n",
      "iteration 65 current loss: 1.6398577690124512 current acc: 0.3212\n",
      "iteration 66 current loss: 1.4663636684417725 current acc: 0.3266\n",
      "iteration 67 current loss: 1.568743109703064 current acc: 0.3318\n",
      "iteration 68 current loss: 1.8741482496261597 current acc: 0.3356\n",
      "iteration 69 current loss: 1.584774136543274 current acc: 0.3404\n",
      "iteration 70 current loss: 1.5458375215530396 current acc: 0.3462\n",
      "iteration 71 current loss: 1.6393567323684692 current acc: 0.352\n",
      "iteration 72 current loss: 1.6921988725662231 current acc: 0.3566\n",
      "iteration 73 current loss: 1.9665228128433228 current acc: 0.3604\n",
      "iteration 74 current loss: 1.3294874429702759 current acc: 0.3662\n",
      "iteration 75 current loss: 1.849931240081787 current acc: 0.3706\n",
      "iteration 76 current loss: 1.743692398071289 current acc: 0.3756\n",
      "iteration 77 current loss: 1.6745046377182007 current acc: 0.3816\n",
      "iteration 78 current loss: 1.8390259742736816 current acc: 0.382\n",
      "\t\tTrain Epoch 4/100,Train Accuracy: 0.382, Train Loss: 1.76950465727456.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 4/100, Validation Accuracy: 0.370125, Validation Loss: 1.8191746168136598\n",
      "best loss 1.76950465727456\n",
      "iteration 0 current loss: 1.7089449167251587 current acc: 0.0052\n",
      "iteration 1 current loss: 1.7680498361587524 current acc: 0.011\n",
      "iteration 2 current loss: 1.5291309356689453 current acc: 0.0158\n",
      "iteration 3 current loss: 1.8933683633804321 current acc: 0.0198\n",
      "iteration 4 current loss: 1.6709356307983398 current acc: 0.0252\n",
      "iteration 5 current loss: 1.5402839183807373 current acc: 0.0316\n",
      "iteration 6 current loss: 1.5828176736831665 current acc: 0.037\n",
      "iteration 7 current loss: 1.7815172672271729 current acc: 0.0418\n",
      "iteration 8 current loss: 1.8078382015228271 current acc: 0.0468\n",
      "iteration 9 current loss: 1.7546759843826294 current acc: 0.0518\n",
      "iteration 10 current loss: 1.730061411857605 current acc: 0.0574\n",
      "iteration 11 current loss: 1.6865860223770142 current acc: 0.0618\n",
      "iteration 12 current loss: 2.058317184448242 current acc: 0.066\n",
      "iteration 13 current loss: 1.4506933689117432 current acc: 0.0724\n",
      "iteration 14 current loss: 1.4903514385223389 current acc: 0.0788\n",
      "iteration 15 current loss: 1.8813689947128296 current acc: 0.0836\n",
      "iteration 16 current loss: 1.6845892667770386 current acc: 0.0894\n",
      "iteration 17 current loss: 1.9158703088760376 current acc: 0.0936\n",
      "iteration 18 current loss: 1.6590291261672974 current acc: 0.0984\n",
      "iteration 19 current loss: 1.6572345495224 current acc: 0.1036\n",
      "iteration 20 current loss: 1.2252326011657715 current acc: 0.1108\n",
      "iteration 21 current loss: 1.648167371749878 current acc: 0.117\n",
      "iteration 22 current loss: 1.5651578903198242 current acc: 0.1228\n",
      "iteration 23 current loss: 1.454444408416748 current acc: 0.1288\n",
      "iteration 24 current loss: 1.7624305486679077 current acc: 0.1338\n",
      "iteration 25 current loss: 1.4066814184188843 current acc: 0.1404\n",
      "iteration 26 current loss: 1.5951024293899536 current acc: 0.1464\n",
      "iteration 27 current loss: 1.8323285579681396 current acc: 0.1506\n",
      "iteration 28 current loss: 1.6900745630264282 current acc: 0.1554\n",
      "iteration 29 current loss: 1.4982463121414185 current acc: 0.1614\n",
      "iteration 30 current loss: 1.4908452033996582 current acc: 0.1668\n",
      "iteration 31 current loss: 1.5681095123291016 current acc: 0.172\n",
      "iteration 32 current loss: 1.8511250019073486 current acc: 0.1772\n",
      "iteration 33 current loss: 1.8110103607177734 current acc: 0.1814\n",
      "iteration 34 current loss: 1.7870901823043823 current acc: 0.186\n",
      "iteration 35 current loss: 1.560066819190979 current acc: 0.1918\n",
      "iteration 36 current loss: 1.7056219577789307 current acc: 0.1972\n",
      "iteration 37 current loss: 1.822245717048645 current acc: 0.2024\n",
      "iteration 38 current loss: 1.8056215047836304 current acc: 0.2064\n",
      "iteration 39 current loss: 1.9001909494400024 current acc: 0.211\n",
      "iteration 40 current loss: 1.6471620798110962 current acc: 0.216\n",
      "iteration 41 current loss: 1.5986473560333252 current acc: 0.2214\n",
      "iteration 42 current loss: 1.5384758710861206 current acc: 0.2276\n",
      "iteration 43 current loss: 1.4231419563293457 current acc: 0.2334\n",
      "iteration 44 current loss: 1.2500739097595215 current acc: 0.2404\n",
      "iteration 45 current loss: 1.5421397686004639 current acc: 0.2452\n",
      "iteration 46 current loss: 2.0068650245666504 current acc: 0.2486\n",
      "iteration 47 current loss: 1.620834231376648 current acc: 0.2538\n",
      "iteration 48 current loss: 1.9295287132263184 current acc: 0.2576\n",
      "iteration 49 current loss: 1.6307405233383179 current acc: 0.2624\n",
      "iteration 50 current loss: 1.8271783590316772 current acc: 0.2674\n",
      "iteration 51 current loss: 1.696435809135437 current acc: 0.272\n",
      "iteration 52 current loss: 1.5796645879745483 current acc: 0.2778\n",
      "iteration 53 current loss: 1.5382578372955322 current acc: 0.2836\n",
      "iteration 54 current loss: 1.865854024887085 current acc: 0.2882\n",
      "iteration 55 current loss: 1.6236371994018555 current acc: 0.2934\n",
      "iteration 56 current loss: 1.8180774450302124 current acc: 0.2978\n",
      "iteration 57 current loss: 1.7249282598495483 current acc: 0.3028\n",
      "iteration 58 current loss: 1.6810023784637451 current acc: 0.3074\n",
      "iteration 59 current loss: 1.3890634775161743 current acc: 0.314\n",
      "iteration 60 current loss: 1.7267117500305176 current acc: 0.3186\n",
      "iteration 61 current loss: 1.7877485752105713 current acc: 0.3236\n",
      "iteration 62 current loss: 1.6852540969848633 current acc: 0.3286\n",
      "iteration 63 current loss: 1.5717037916183472 current acc: 0.335\n",
      "iteration 64 current loss: 2.101813793182373 current acc: 0.3384\n",
      "iteration 65 current loss: 1.6921908855438232 current acc: 0.344\n",
      "iteration 66 current loss: 1.6265623569488525 current acc: 0.3484\n",
      "iteration 67 current loss: 1.8908250331878662 current acc: 0.3532\n",
      "iteration 68 current loss: 1.6936877965927124 current acc: 0.3576\n",
      "iteration 69 current loss: 1.6590542793273926 current acc: 0.3622\n",
      "iteration 70 current loss: 1.9887521266937256 current acc: 0.3662\n",
      "iteration 71 current loss: 1.5988463163375854 current acc: 0.3718\n",
      "iteration 72 current loss: 1.584437608718872 current acc: 0.3772\n",
      "iteration 73 current loss: 1.3529808521270752 current acc: 0.385\n",
      "iteration 74 current loss: 1.639103889465332 current acc: 0.3906\n",
      "iteration 75 current loss: 1.8459769487380981 current acc: 0.3956\n",
      "iteration 76 current loss: 1.7645666599273682 current acc: 0.3996\n",
      "iteration 77 current loss: 1.6999666690826416 current acc: 0.4052\n",
      "iteration 78 current loss: 1.7518112659454346 current acc: 0.4058\n",
      "\t\tTrain Epoch 5/100,Train Accuracy: 0.4058, Train Loss: 1.6813311546663694.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 5/100, Validation Accuracy: 0.380375, Validation Loss: 1.812189968109131\n",
      "best loss 1.6813311546663694\n",
      "iteration 0 current loss: 1.6316272020339966 current acc: 0.0056\n",
      "iteration 1 current loss: 1.6174006462097168 current acc: 0.0106\n",
      "iteration 2 current loss: 1.3727939128875732 current acc: 0.0166\n",
      "iteration 3 current loss: 1.3333784341812134 current acc: 0.0232\n",
      "iteration 4 current loss: 1.6402479410171509 current acc: 0.028\n",
      "iteration 5 current loss: 1.4038788080215454 current acc: 0.0348\n",
      "iteration 6 current loss: 1.9549742937088013 current acc: 0.039\n",
      "iteration 7 current loss: 1.6636767387390137 current acc: 0.0442\n",
      "iteration 8 current loss: 1.6114338636398315 current acc: 0.049\n",
      "iteration 9 current loss: 1.5764445066452026 current acc: 0.0534\n",
      "iteration 10 current loss: 1.3974570035934448 current acc: 0.0598\n",
      "iteration 11 current loss: 1.4643090963363647 current acc: 0.0662\n",
      "iteration 12 current loss: 1.7088056802749634 current acc: 0.071\n",
      "iteration 13 current loss: 1.4158371686935425 current acc: 0.0778\n",
      "iteration 14 current loss: 1.6886520385742188 current acc: 0.0828\n",
      "iteration 15 current loss: 1.4859809875488281 current acc: 0.0884\n",
      "iteration 16 current loss: 1.7804728746414185 current acc: 0.0926\n",
      "iteration 17 current loss: 1.7069051265716553 current acc: 0.0976\n",
      "iteration 18 current loss: 1.7932276725769043 current acc: 0.1022\n",
      "iteration 19 current loss: 1.7378733158111572 current acc: 0.1074\n",
      "iteration 20 current loss: 1.4729918241500854 current acc: 0.1134\n",
      "iteration 21 current loss: 1.4324945211410522 current acc: 0.1202\n",
      "iteration 22 current loss: 1.4860252141952515 current acc: 0.1266\n",
      "iteration 23 current loss: 1.467533826828003 current acc: 0.1328\n",
      "iteration 24 current loss: 1.819986343383789 current acc: 0.1364\n",
      "iteration 25 current loss: 1.3400222063064575 current acc: 0.1422\n",
      "iteration 26 current loss: 1.6333671808242798 current acc: 0.1476\n",
      "iteration 27 current loss: 1.7708640098571777 current acc: 0.1518\n",
      "iteration 28 current loss: 1.4444361925125122 current acc: 0.158\n",
      "iteration 29 current loss: 1.649455189704895 current acc: 0.1634\n",
      "iteration 30 current loss: 1.7574042081832886 current acc: 0.1692\n",
      "iteration 31 current loss: 1.5010743141174316 current acc: 0.175\n",
      "iteration 32 current loss: 1.5407569408416748 current acc: 0.181\n",
      "iteration 33 current loss: 1.8526709079742432 current acc: 0.1858\n",
      "iteration 34 current loss: 1.6828422546386719 current acc: 0.1912\n",
      "iteration 35 current loss: 1.3802038431167603 current acc: 0.1978\n",
      "iteration 36 current loss: 1.8840243816375732 current acc: 0.203\n",
      "iteration 37 current loss: 1.5363467931747437 current acc: 0.2092\n",
      "iteration 38 current loss: 1.5459967851638794 current acc: 0.2154\n",
      "iteration 39 current loss: 1.490360140800476 current acc: 0.2216\n",
      "iteration 40 current loss: 1.6616772413253784 current acc: 0.227\n",
      "iteration 41 current loss: 2.01281476020813 current acc: 0.2316\n",
      "iteration 42 current loss: 1.5434768199920654 current acc: 0.2378\n",
      "iteration 43 current loss: 1.6510295867919922 current acc: 0.2428\n",
      "iteration 44 current loss: 1.6056017875671387 current acc: 0.2486\n",
      "iteration 45 current loss: 1.3451839685440063 current acc: 0.255\n",
      "iteration 46 current loss: 1.567671775817871 current acc: 0.261\n",
      "iteration 47 current loss: 1.8191145658493042 current acc: 0.2646\n",
      "iteration 48 current loss: 1.7576631307601929 current acc: 0.2694\n",
      "iteration 49 current loss: 1.5833309888839722 current acc: 0.275\n",
      "iteration 50 current loss: 1.6358388662338257 current acc: 0.2816\n",
      "iteration 51 current loss: 1.6137498617172241 current acc: 0.288\n",
      "iteration 52 current loss: 1.9740347862243652 current acc: 0.2918\n",
      "iteration 53 current loss: 1.5721721649169922 current acc: 0.2978\n",
      "iteration 54 current loss: 1.5855436325073242 current acc: 0.303\n",
      "iteration 55 current loss: 1.6066625118255615 current acc: 0.3086\n",
      "iteration 56 current loss: 1.6306160688400269 current acc: 0.3144\n",
      "iteration 57 current loss: 1.4336320161819458 current acc: 0.3206\n",
      "iteration 58 current loss: 1.524990200996399 current acc: 0.3274\n",
      "iteration 59 current loss: 1.5722784996032715 current acc: 0.3324\n",
      "iteration 60 current loss: 1.4151757955551147 current acc: 0.3376\n",
      "iteration 61 current loss: 1.4132360219955444 current acc: 0.344\n",
      "iteration 62 current loss: 1.7613773345947266 current acc: 0.3486\n",
      "iteration 63 current loss: 1.5943102836608887 current acc: 0.3536\n",
      "iteration 64 current loss: 1.5696048736572266 current acc: 0.3586\n",
      "iteration 65 current loss: 1.411162257194519 current acc: 0.365\n",
      "iteration 66 current loss: 1.9925309419631958 current acc: 0.3692\n",
      "iteration 67 current loss: 1.6673994064331055 current acc: 0.3732\n",
      "iteration 68 current loss: 1.7501468658447266 current acc: 0.3786\n",
      "iteration 69 current loss: 1.5540629625320435 current acc: 0.3836\n",
      "iteration 70 current loss: 1.6998916864395142 current acc: 0.3886\n",
      "iteration 71 current loss: 1.585857629776001 current acc: 0.3946\n",
      "iteration 72 current loss: 1.571076512336731 current acc: 0.3998\n",
      "iteration 73 current loss: 1.6274410486221313 current acc: 0.4056\n",
      "iteration 74 current loss: 1.640560507774353 current acc: 0.411\n",
      "iteration 75 current loss: 1.665446400642395 current acc: 0.416\n",
      "iteration 76 current loss: 1.469744324684143 current acc: 0.4218\n",
      "iteration 77 current loss: 1.253820776939392 current acc: 0.429\n",
      "iteration 78 current loss: 1.9356122016906738 current acc: 0.4296\n",
      "\t\tTrain Epoch 6/100,Train Accuracy: 0.4296, Train Loss: 1.606959183004838.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 6/100, Validation Accuracy: 0.391375, Validation Loss: 1.7316120338439942\n",
      "best loss 1.606959183004838\n",
      "iteration 0 current loss: 1.6195927858352661 current acc: 0.006\n",
      "iteration 1 current loss: 1.7430895566940308 current acc: 0.0112\n",
      "iteration 2 current loss: 1.7464745044708252 current acc: 0.0168\n",
      "iteration 3 current loss: 1.3698605298995972 current acc: 0.0232\n",
      "iteration 4 current loss: 1.4917690753936768 current acc: 0.0292\n",
      "iteration 5 current loss: 1.6075363159179688 current acc: 0.0338\n",
      "iteration 6 current loss: 1.4529061317443848 current acc: 0.0414\n",
      "iteration 7 current loss: 1.428510308265686 current acc: 0.047\n",
      "iteration 8 current loss: 1.6171693801879883 current acc: 0.0526\n",
      "iteration 9 current loss: 1.8489667177200317 current acc: 0.0576\n",
      "iteration 10 current loss: 1.7723547220230103 current acc: 0.0628\n",
      "iteration 11 current loss: 1.8807255029678345 current acc: 0.067\n",
      "iteration 12 current loss: 1.4021121263504028 current acc: 0.073\n",
      "iteration 13 current loss: 1.5021272897720337 current acc: 0.0794\n",
      "iteration 14 current loss: 1.5116174221038818 current acc: 0.0858\n",
      "iteration 15 current loss: 1.6078543663024902 current acc: 0.0912\n",
      "iteration 16 current loss: 1.2655243873596191 current acc: 0.0984\n",
      "iteration 17 current loss: 1.2871766090393066 current acc: 0.1056\n",
      "iteration 18 current loss: 1.434283971786499 current acc: 0.1126\n",
      "iteration 19 current loss: 1.5346484184265137 current acc: 0.1182\n",
      "iteration 20 current loss: 1.5152324438095093 current acc: 0.124\n",
      "iteration 21 current loss: 1.6775013208389282 current acc: 0.1296\n",
      "iteration 22 current loss: 1.3771346807479858 current acc: 0.1356\n",
      "iteration 23 current loss: 1.6078017950057983 current acc: 0.1406\n",
      "iteration 24 current loss: 1.3956438302993774 current acc: 0.1472\n",
      "iteration 25 current loss: 1.6852130889892578 current acc: 0.1528\n",
      "iteration 26 current loss: 1.5251853466033936 current acc: 0.1584\n",
      "iteration 27 current loss: 1.4046459197998047 current acc: 0.1652\n",
      "iteration 28 current loss: 1.5050803422927856 current acc: 0.1708\n",
      "iteration 29 current loss: 1.4252893924713135 current acc: 0.1776\n",
      "iteration 30 current loss: 1.322374701499939 current acc: 0.1842\n",
      "iteration 31 current loss: 1.3391677141189575 current acc: 0.19\n",
      "iteration 32 current loss: 1.6395012140274048 current acc: 0.1948\n",
      "iteration 33 current loss: 1.5775657892227173 current acc: 0.2002\n",
      "iteration 34 current loss: 1.4795305728912354 current acc: 0.206\n",
      "iteration 35 current loss: 1.4685686826705933 current acc: 0.2108\n",
      "iteration 36 current loss: 1.4107202291488647 current acc: 0.2174\n",
      "iteration 37 current loss: 1.5750300884246826 current acc: 0.2228\n",
      "iteration 38 current loss: 1.60293710231781 current acc: 0.2288\n",
      "iteration 39 current loss: 1.4457087516784668 current acc: 0.2344\n",
      "iteration 40 current loss: 1.4225022792816162 current acc: 0.2408\n",
      "iteration 41 current loss: 1.5945301055908203 current acc: 0.2458\n",
      "iteration 42 current loss: 1.5470268726348877 current acc: 0.2518\n",
      "iteration 43 current loss: 1.410828948020935 current acc: 0.258\n",
      "iteration 44 current loss: 1.383986234664917 current acc: 0.264\n",
      "iteration 45 current loss: 1.7508788108825684 current acc: 0.2682\n",
      "iteration 46 current loss: 1.3980751037597656 current acc: 0.2744\n",
      "iteration 47 current loss: 1.4484949111938477 current acc: 0.2802\n",
      "iteration 48 current loss: 1.4737049341201782 current acc: 0.2864\n",
      "iteration 49 current loss: 1.2505203485488892 current acc: 0.2938\n",
      "iteration 50 current loss: 1.6120840311050415 current acc: 0.2986\n",
      "iteration 51 current loss: 1.4855035543441772 current acc: 0.304\n",
      "iteration 52 current loss: 1.358251690864563 current acc: 0.3102\n",
      "iteration 53 current loss: 1.8534555435180664 current acc: 0.3148\n",
      "iteration 54 current loss: 1.9280816316604614 current acc: 0.3196\n",
      "iteration 55 current loss: 1.715796947479248 current acc: 0.3248\n",
      "iteration 56 current loss: 1.2859573364257812 current acc: 0.3314\n",
      "iteration 57 current loss: 1.5502095222473145 current acc: 0.3368\n",
      "iteration 58 current loss: 1.5334720611572266 current acc: 0.3426\n",
      "iteration 59 current loss: 1.6020407676696777 current acc: 0.3484\n",
      "iteration 60 current loss: 1.546473741531372 current acc: 0.3528\n",
      "iteration 61 current loss: 1.716400146484375 current acc: 0.3578\n",
      "iteration 62 current loss: 1.545198917388916 current acc: 0.3624\n",
      "iteration 63 current loss: 1.5345748662948608 current acc: 0.3676\n",
      "iteration 64 current loss: 1.621799349784851 current acc: 0.373\n",
      "iteration 65 current loss: 1.7263983488082886 current acc: 0.3778\n",
      "iteration 66 current loss: 1.4151577949523926 current acc: 0.3838\n",
      "iteration 67 current loss: 1.6305205821990967 current acc: 0.3888\n",
      "iteration 68 current loss: 1.4720234870910645 current acc: 0.3946\n",
      "iteration 69 current loss: 1.6150751113891602 current acc: 0.4002\n",
      "iteration 70 current loss: 1.5531781911849976 current acc: 0.406\n",
      "iteration 71 current loss: 1.3630110025405884 current acc: 0.413\n",
      "iteration 72 current loss: 1.5083491802215576 current acc: 0.419\n",
      "iteration 73 current loss: 1.3219350576400757 current acc: 0.4268\n",
      "iteration 74 current loss: 1.4888043403625488 current acc: 0.4336\n",
      "iteration 75 current loss: 1.6387054920196533 current acc: 0.4388\n",
      "iteration 76 current loss: 1.4298255443572998 current acc: 0.4448\n",
      "iteration 77 current loss: 1.7230305671691895 current acc: 0.4506\n",
      "iteration 78 current loss: 1.828372836112976 current acc: 0.451\n",
      "\t\tTrain Epoch 7/100,Train Accuracy: 0.451, Train Loss: 1.5365616369851027.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 7/100, Validation Accuracy: 0.395125, Validation Loss: 1.73460817527771\n",
      "best loss 1.5365616369851027\n",
      "iteration 0 current loss: 1.4086412191390991 current acc: 0.006\n",
      "iteration 1 current loss: 1.4477285146713257 current acc: 0.0124\n",
      "iteration 2 current loss: 1.4624685049057007 current acc: 0.0182\n",
      "iteration 3 current loss: 1.7827481031417847 current acc: 0.024\n",
      "iteration 4 current loss: 1.507861614227295 current acc: 0.031\n",
      "iteration 5 current loss: 1.6718214750289917 current acc: 0.0366\n",
      "iteration 6 current loss: 1.5029990673065186 current acc: 0.0422\n",
      "iteration 7 current loss: 1.6262423992156982 current acc: 0.047\n",
      "iteration 8 current loss: 1.4531570672988892 current acc: 0.0532\n",
      "iteration 9 current loss: 1.7268779277801514 current acc: 0.0582\n",
      "iteration 10 current loss: 1.3762706518173218 current acc: 0.0648\n",
      "iteration 11 current loss: 1.4891703128814697 current acc: 0.0716\n",
      "iteration 12 current loss: 1.5273315906524658 current acc: 0.078\n",
      "iteration 13 current loss: 1.3469233512878418 current acc: 0.0836\n",
      "iteration 14 current loss: 1.630989670753479 current acc: 0.0896\n",
      "iteration 15 current loss: 1.5276516675949097 current acc: 0.0948\n",
      "iteration 16 current loss: 1.5431195497512817 current acc: 0.1006\n",
      "iteration 17 current loss: 1.3257505893707275 current acc: 0.1064\n",
      "iteration 18 current loss: 1.491555094718933 current acc: 0.1122\n",
      "iteration 19 current loss: 1.3600115776062012 current acc: 0.1186\n",
      "iteration 20 current loss: 1.3371208906173706 current acc: 0.126\n",
      "iteration 21 current loss: 1.5657461881637573 current acc: 0.1316\n",
      "iteration 22 current loss: 1.5164813995361328 current acc: 0.1372\n",
      "iteration 23 current loss: 1.4562115669250488 current acc: 0.143\n",
      "iteration 24 current loss: 1.7408709526062012 current acc: 0.148\n",
      "iteration 25 current loss: 1.3684293031692505 current acc: 0.1542\n",
      "iteration 26 current loss: 1.4465818405151367 current acc: 0.1598\n",
      "iteration 27 current loss: 1.4789060354232788 current acc: 0.166\n",
      "iteration 28 current loss: 1.1695365905761719 current acc: 0.1736\n",
      "iteration 29 current loss: 1.4939478635787964 current acc: 0.18\n",
      "iteration 30 current loss: 1.6505322456359863 current acc: 0.1856\n",
      "iteration 31 current loss: 1.496287226676941 current acc: 0.1918\n",
      "iteration 32 current loss: 1.5619442462921143 current acc: 0.197\n",
      "iteration 33 current loss: 1.6070709228515625 current acc: 0.2024\n",
      "iteration 34 current loss: 1.4928103685379028 current acc: 0.2082\n",
      "iteration 35 current loss: 1.7256128787994385 current acc: 0.2136\n",
      "iteration 36 current loss: 1.528136134147644 current acc: 0.2178\n",
      "iteration 37 current loss: 1.4942268133163452 current acc: 0.2238\n",
      "iteration 38 current loss: 1.6475601196289062 current acc: 0.2278\n",
      "iteration 39 current loss: 1.5808677673339844 current acc: 0.235\n",
      "iteration 40 current loss: 1.330600380897522 current acc: 0.2422\n",
      "iteration 41 current loss: 1.4419612884521484 current acc: 0.2488\n",
      "iteration 42 current loss: 1.4950014352798462 current acc: 0.2548\n",
      "iteration 43 current loss: 1.514274001121521 current acc: 0.26\n",
      "iteration 44 current loss: 1.4910551309585571 current acc: 0.265\n",
      "iteration 45 current loss: 1.3242043256759644 current acc: 0.2716\n",
      "iteration 46 current loss: 1.606562852859497 current acc: 0.2776\n",
      "iteration 47 current loss: 1.4033527374267578 current acc: 0.2844\n",
      "iteration 48 current loss: 1.6875221729278564 current acc: 0.2884\n",
      "iteration 49 current loss: 1.6492995023727417 current acc: 0.2948\n",
      "iteration 50 current loss: 1.506263256072998 current acc: 0.3008\n",
      "iteration 51 current loss: 1.587803602218628 current acc: 0.3066\n",
      "iteration 52 current loss: 1.2571309804916382 current acc: 0.3142\n",
      "iteration 53 current loss: 1.664063811302185 current acc: 0.3192\n",
      "iteration 54 current loss: 1.4629466533660889 current acc: 0.3254\n",
      "iteration 55 current loss: 1.5622756481170654 current acc: 0.3306\n",
      "iteration 56 current loss: 1.553751826286316 current acc: 0.3366\n",
      "iteration 57 current loss: 1.4953908920288086 current acc: 0.3424\n",
      "iteration 58 current loss: 1.6349132061004639 current acc: 0.3474\n",
      "iteration 59 current loss: 1.4621952772140503 current acc: 0.3538\n",
      "iteration 60 current loss: 1.5676624774932861 current acc: 0.3598\n",
      "iteration 61 current loss: 1.5273040533065796 current acc: 0.365\n",
      "iteration 62 current loss: 1.7291688919067383 current acc: 0.3696\n",
      "iteration 63 current loss: 1.4498645067214966 current acc: 0.3752\n",
      "iteration 64 current loss: 1.3670406341552734 current acc: 0.3824\n",
      "iteration 65 current loss: 1.6953496932983398 current acc: 0.3876\n",
      "iteration 66 current loss: 1.659308671951294 current acc: 0.3932\n",
      "iteration 67 current loss: 1.3490041494369507 current acc: 0.3994\n",
      "iteration 68 current loss: 1.4735349416732788 current acc: 0.4042\n",
      "iteration 69 current loss: 1.5429601669311523 current acc: 0.411\n",
      "iteration 70 current loss: 1.398115634918213 current acc: 0.4176\n",
      "iteration 71 current loss: 1.5730479955673218 current acc: 0.4232\n",
      "iteration 72 current loss: 1.3488024473190308 current acc: 0.43\n",
      "iteration 73 current loss: 1.6395647525787354 current acc: 0.435\n",
      "iteration 74 current loss: 1.3268674612045288 current acc: 0.4418\n",
      "iteration 75 current loss: 1.4894684553146362 current acc: 0.4468\n",
      "iteration 76 current loss: 1.5235381126403809 current acc: 0.4524\n",
      "iteration 77 current loss: 1.4602470397949219 current acc: 0.4592\n",
      "iteration 78 current loss: 1.7111366987228394 current acc: 0.4596\n",
      "\t\tTrain Epoch 8/100,Train Accuracy: 0.4596, Train Loss: 1.5130475578428824.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 8/100, Validation Accuracy: 0.394625, Validation Loss: 1.719023196220398\n",
      "best loss 1.5130475578428824\n",
      "iteration 0 current loss: 1.4083653688430786 current acc: 0.005\n",
      "iteration 1 current loss: 1.5355578660964966 current acc: 0.0108\n",
      "iteration 2 current loss: 1.648680567741394 current acc: 0.0156\n",
      "iteration 3 current loss: 1.5649230480194092 current acc: 0.0216\n",
      "iteration 4 current loss: 1.6841903924942017 current acc: 0.0268\n",
      "iteration 5 current loss: 1.4410001039505005 current acc: 0.0334\n",
      "iteration 6 current loss: 1.39009428024292 current acc: 0.0392\n",
      "iteration 7 current loss: 1.363259196281433 current acc: 0.0458\n",
      "iteration 8 current loss: 1.7037246227264404 current acc: 0.051\n",
      "iteration 9 current loss: 1.1616060733795166 current acc: 0.0582\n",
      "iteration 10 current loss: 1.5128982067108154 current acc: 0.064\n",
      "iteration 11 current loss: 1.2190178632736206 current acc: 0.071\n",
      "iteration 12 current loss: 1.4950906038284302 current acc: 0.0762\n",
      "iteration 13 current loss: 1.7607123851776123 current acc: 0.0812\n",
      "iteration 14 current loss: 1.6241165399551392 current acc: 0.0864\n",
      "iteration 15 current loss: 1.614547610282898 current acc: 0.0914\n",
      "iteration 16 current loss: 1.7819567918777466 current acc: 0.0966\n",
      "iteration 17 current loss: 1.5594980716705322 current acc: 0.103\n",
      "iteration 18 current loss: 1.7801518440246582 current acc: 0.1082\n",
      "iteration 19 current loss: 1.8272504806518555 current acc: 0.1128\n",
      "iteration 20 current loss: 1.6734248399734497 current acc: 0.1176\n",
      "iteration 21 current loss: 1.489292025566101 current acc: 0.1234\n",
      "iteration 22 current loss: 1.2240352630615234 current acc: 0.1308\n",
      "iteration 23 current loss: 1.3807086944580078 current acc: 0.138\n",
      "iteration 24 current loss: 1.709083080291748 current acc: 0.1432\n",
      "iteration 25 current loss: 1.3530502319335938 current acc: 0.15\n",
      "iteration 26 current loss: 1.443426251411438 current acc: 0.156\n",
      "iteration 27 current loss: 1.5766316652297974 current acc: 0.161\n",
      "iteration 28 current loss: 1.5058209896087646 current acc: 0.1676\n",
      "iteration 29 current loss: 1.451744794845581 current acc: 0.174\n",
      "iteration 30 current loss: 1.4556959867477417 current acc: 0.18\n",
      "iteration 31 current loss: 1.3815845251083374 current acc: 0.1868\n",
      "iteration 32 current loss: 1.541675090789795 current acc: 0.1942\n",
      "iteration 33 current loss: 1.3067169189453125 current acc: 0.2014\n",
      "iteration 34 current loss: 1.312986135482788 current acc: 0.208\n",
      "iteration 35 current loss: 1.3836462497711182 current acc: 0.2138\n",
      "iteration 36 current loss: 1.5190902948379517 current acc: 0.2196\n",
      "iteration 37 current loss: 1.5141627788543701 current acc: 0.2252\n",
      "iteration 38 current loss: 1.5186786651611328 current acc: 0.231\n",
      "iteration 39 current loss: 1.8278725147247314 current acc: 0.2346\n",
      "iteration 40 current loss: 1.45591139793396 current acc: 0.2412\n",
      "iteration 41 current loss: 1.337360143661499 current acc: 0.2482\n",
      "iteration 42 current loss: 1.5162427425384521 current acc: 0.2532\n",
      "iteration 43 current loss: 1.434881567955017 current acc: 0.2586\n",
      "iteration 44 current loss: 1.7375110387802124 current acc: 0.2626\n",
      "iteration 45 current loss: 1.198314905166626 current acc: 0.27\n",
      "iteration 46 current loss: 1.459065318107605 current acc: 0.276\n",
      "iteration 47 current loss: 1.553875207901001 current acc: 0.2826\n",
      "iteration 48 current loss: 1.5250030755996704 current acc: 0.2888\n",
      "iteration 49 current loss: 1.5613343715667725 current acc: 0.2942\n",
      "iteration 50 current loss: 1.4436604976654053 current acc: 0.2992\n",
      "iteration 51 current loss: 1.6485645771026611 current acc: 0.3044\n",
      "iteration 52 current loss: 1.4099454879760742 current acc: 0.3104\n",
      "iteration 53 current loss: 1.5727455615997314 current acc: 0.316\n",
      "iteration 54 current loss: 1.639736533164978 current acc: 0.3212\n",
      "iteration 55 current loss: 1.3843045234680176 current acc: 0.3276\n",
      "iteration 56 current loss: 1.3475828170776367 current acc: 0.3344\n",
      "iteration 57 current loss: 1.5081971883773804 current acc: 0.3402\n",
      "iteration 58 current loss: 1.38961923122406 current acc: 0.3468\n",
      "iteration 59 current loss: 1.4112458229064941 current acc: 0.3534\n",
      "iteration 60 current loss: 1.6192963123321533 current acc: 0.359\n",
      "iteration 61 current loss: 1.53483247756958 current acc: 0.364\n",
      "iteration 62 current loss: 1.4271563291549683 current acc: 0.3698\n",
      "iteration 63 current loss: 1.1941337585449219 current acc: 0.3768\n",
      "iteration 64 current loss: 1.5254576206207275 current acc: 0.3814\n",
      "iteration 65 current loss: 1.2957035303115845 current acc: 0.3878\n",
      "iteration 66 current loss: 1.432270884513855 current acc: 0.393\n",
      "iteration 67 current loss: 1.3366432189941406 current acc: 0.3994\n",
      "iteration 68 current loss: 1.2503442764282227 current acc: 0.4056\n",
      "iteration 69 current loss: 1.5890032052993774 current acc: 0.4112\n",
      "iteration 70 current loss: 1.584202527999878 current acc: 0.4166\n",
      "iteration 71 current loss: 1.5350710153579712 current acc: 0.4218\n",
      "iteration 72 current loss: 1.3880330324172974 current acc: 0.4278\n",
      "iteration 73 current loss: 1.591030478477478 current acc: 0.4334\n",
      "iteration 74 current loss: 1.5436476469039917 current acc: 0.4394\n",
      "iteration 75 current loss: 1.5063273906707764 current acc: 0.4446\n",
      "iteration 76 current loss: 1.6158649921417236 current acc: 0.4502\n",
      "iteration 77 current loss: 1.5079725980758667 current acc: 0.4554\n",
      "iteration 78 current loss: 2.08864426612854 current acc: 0.456\n",
      "\t\tTrain Epoch 9/100,Train Accuracy: 0.456, Train Loss: 1.5031228669082062.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 9/100, Validation Accuracy: 0.417125, Validation Loss: 1.6700437459945678\n",
      "best loss 1.5031228669082062\n",
      "iteration 0 current loss: 1.4327714443206787 current acc: 0.0062\n",
      "iteration 1 current loss: 1.6120836734771729 current acc: 0.0126\n",
      "iteration 2 current loss: 1.412251353263855 current acc: 0.02\n",
      "iteration 3 current loss: 1.4661834239959717 current acc: 0.0258\n",
      "iteration 4 current loss: 1.412571907043457 current acc: 0.0328\n",
      "iteration 5 current loss: 1.252638339996338 current acc: 0.0398\n",
      "iteration 6 current loss: 1.4937878847122192 current acc: 0.046\n",
      "iteration 7 current loss: 1.5643813610076904 current acc: 0.0514\n",
      "iteration 8 current loss: 1.353963851928711 current acc: 0.0576\n",
      "iteration 9 current loss: 1.3152967691421509 current acc: 0.0634\n",
      "iteration 10 current loss: 1.4823554754257202 current acc: 0.0692\n",
      "iteration 11 current loss: 1.620763897895813 current acc: 0.075\n",
      "iteration 12 current loss: 1.6350131034851074 current acc: 0.0804\n",
      "iteration 13 current loss: 1.3835368156433105 current acc: 0.0864\n",
      "iteration 14 current loss: 1.1088194847106934 current acc: 0.0942\n",
      "iteration 15 current loss: 1.2244912385940552 current acc: 0.1004\n",
      "iteration 16 current loss: 1.553063988685608 current acc: 0.1064\n",
      "iteration 17 current loss: 1.2261666059494019 current acc: 0.1132\n",
      "iteration 18 current loss: 1.3751035928726196 current acc: 0.1188\n",
      "iteration 19 current loss: 1.815627098083496 current acc: 0.1236\n",
      "iteration 20 current loss: 1.4728728532791138 current acc: 0.13\n",
      "iteration 21 current loss: 1.3601542711257935 current acc: 0.137\n",
      "iteration 22 current loss: 0.9712546467781067 current acc: 0.1456\n",
      "iteration 23 current loss: 1.518707275390625 current acc: 0.1518\n",
      "iteration 24 current loss: 1.4154393672943115 current acc: 0.1582\n",
      "iteration 25 current loss: 1.3734667301177979 current acc: 0.1644\n",
      "iteration 26 current loss: 1.451454997062683 current acc: 0.1706\n",
      "iteration 27 current loss: 1.3229153156280518 current acc: 0.1772\n",
      "iteration 28 current loss: 1.1584328413009644 current acc: 0.185\n",
      "iteration 29 current loss: 1.58502197265625 current acc: 0.1914\n",
      "iteration 30 current loss: 1.4748210906982422 current acc: 0.1972\n",
      "iteration 31 current loss: 1.5481212139129639 current acc: 0.2038\n",
      "iteration 32 current loss: 1.3488572835922241 current acc: 0.2098\n",
      "iteration 33 current loss: 1.4303414821624756 current acc: 0.216\n",
      "iteration 34 current loss: 1.4436120986938477 current acc: 0.2218\n",
      "iteration 35 current loss: 1.737542986869812 current acc: 0.2264\n",
      "iteration 36 current loss: 1.3522852659225464 current acc: 0.2332\n",
      "iteration 37 current loss: 1.2571865320205688 current acc: 0.2406\n",
      "iteration 38 current loss: 1.5214298963546753 current acc: 0.247\n",
      "iteration 39 current loss: 1.4171861410140991 current acc: 0.2528\n",
      "iteration 40 current loss: 1.384060263633728 current acc: 0.2596\n",
      "iteration 41 current loss: 1.685158371925354 current acc: 0.2646\n",
      "iteration 42 current loss: 1.372679591178894 current acc: 0.2718\n",
      "iteration 43 current loss: 1.2364200353622437 current acc: 0.2782\n",
      "iteration 44 current loss: 1.347633957862854 current acc: 0.2846\n",
      "iteration 45 current loss: 1.5879639387130737 current acc: 0.2898\n",
      "iteration 46 current loss: 1.5125874280929565 current acc: 0.2956\n",
      "iteration 47 current loss: 1.3965286016464233 current acc: 0.3018\n",
      "iteration 48 current loss: 1.468312382698059 current acc: 0.3072\n",
      "iteration 49 current loss: 1.4006575345993042 current acc: 0.3126\n",
      "iteration 50 current loss: 1.1223567724227905 current acc: 0.3198\n",
      "iteration 51 current loss: 1.3153291940689087 current acc: 0.3268\n",
      "iteration 52 current loss: 1.4079571962356567 current acc: 0.3336\n",
      "iteration 53 current loss: 1.0178966522216797 current acc: 0.3414\n",
      "iteration 54 current loss: 1.507464051246643 current acc: 0.3476\n",
      "iteration 55 current loss: 1.2390681505203247 current acc: 0.3548\n",
      "iteration 56 current loss: 1.29083251953125 current acc: 0.3612\n",
      "iteration 57 current loss: 1.266793966293335 current acc: 0.3682\n",
      "iteration 58 current loss: 1.3798015117645264 current acc: 0.375\n",
      "iteration 59 current loss: 1.6705021858215332 current acc: 0.3804\n",
      "iteration 60 current loss: 1.3630284070968628 current acc: 0.3862\n",
      "iteration 61 current loss: 1.6181854009628296 current acc: 0.3916\n",
      "iteration 62 current loss: 1.5183979272842407 current acc: 0.3976\n",
      "iteration 63 current loss: 1.3815674781799316 current acc: 0.4038\n",
      "iteration 64 current loss: 1.2789002656936646 current acc: 0.411\n",
      "iteration 65 current loss: 1.5241758823394775 current acc: 0.4166\n",
      "iteration 66 current loss: 1.4423598051071167 current acc: 0.423\n",
      "iteration 67 current loss: 1.3819003105163574 current acc: 0.4292\n",
      "iteration 68 current loss: 1.6268165111541748 current acc: 0.4354\n",
      "iteration 69 current loss: 1.1765060424804688 current acc: 0.443\n",
      "iteration 70 current loss: 1.5469813346862793 current acc: 0.449\n",
      "iteration 71 current loss: 1.400199055671692 current acc: 0.455\n",
      "iteration 72 current loss: 1.4413645267486572 current acc: 0.461\n",
      "iteration 73 current loss: 1.3474797010421753 current acc: 0.4676\n",
      "iteration 74 current loss: 1.2595608234405518 current acc: 0.4746\n",
      "iteration 75 current loss: 1.248316764831543 current acc: 0.4816\n",
      "iteration 76 current loss: 1.5340152978897095 current acc: 0.4878\n",
      "iteration 77 current loss: 1.1896042823791504 current acc: 0.4956\n",
      "iteration 78 current loss: 2.4986536502838135 current acc: 0.4962\n",
      "\t\tTrain Epoch 10/100,Train Accuracy: 0.4962, Train Loss: 1.4213923208321197.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 10/100, Validation Accuracy: 0.41725, Validation Loss: 1.7374926700592042\n",
      "best loss 1.4213923208321197\n",
      "iteration 0 current loss: 1.3594820499420166 current acc: 0.0064\n",
      "iteration 1 current loss: 1.3532798290252686 current acc: 0.0134\n",
      "iteration 2 current loss: 1.4126125574111938 current acc: 0.0208\n",
      "iteration 3 current loss: 1.716610074043274 current acc: 0.0258\n",
      "iteration 4 current loss: 1.5866827964782715 current acc: 0.0318\n",
      "iteration 5 current loss: 1.3315752744674683 current acc: 0.0386\n",
      "iteration 6 current loss: 1.3736125230789185 current acc: 0.0456\n",
      "iteration 7 current loss: 1.4570335149765015 current acc: 0.0518\n",
      "iteration 8 current loss: 1.520892858505249 current acc: 0.057\n",
      "iteration 9 current loss: 1.607195258140564 current acc: 0.0626\n",
      "iteration 10 current loss: 1.5654504299163818 current acc: 0.0688\n",
      "iteration 11 current loss: 1.3130688667297363 current acc: 0.0758\n",
      "iteration 12 current loss: 1.3917895555496216 current acc: 0.0822\n",
      "iteration 13 current loss: 1.271293044090271 current acc: 0.09\n",
      "iteration 14 current loss: 1.381137490272522 current acc: 0.0968\n",
      "iteration 15 current loss: 1.439534306526184 current acc: 0.104\n",
      "iteration 16 current loss: 1.2926703691482544 current acc: 0.1114\n",
      "iteration 17 current loss: 1.5486243963241577 current acc: 0.1164\n",
      "iteration 18 current loss: 1.236938714981079 current acc: 0.1236\n",
      "iteration 19 current loss: 1.5380727052688599 current acc: 0.13\n",
      "iteration 20 current loss: 1.3221708536148071 current acc: 0.1376\n",
      "iteration 21 current loss: 1.4570202827453613 current acc: 0.1442\n",
      "iteration 22 current loss: 1.3820024728775024 current acc: 0.1504\n",
      "iteration 23 current loss: 1.4186540842056274 current acc: 0.1566\n",
      "iteration 24 current loss: 1.3775211572647095 current acc: 0.162\n",
      "iteration 25 current loss: 1.5102920532226562 current acc: 0.1672\n",
      "iteration 26 current loss: 1.462985634803772 current acc: 0.1732\n",
      "iteration 27 current loss: 1.3225047588348389 current acc: 0.1796\n",
      "iteration 28 current loss: 1.202953815460205 current acc: 0.1866\n",
      "iteration 29 current loss: 1.2476962804794312 current acc: 0.1928\n",
      "iteration 30 current loss: 1.2702739238739014 current acc: 0.1996\n",
      "iteration 31 current loss: 1.5809357166290283 current acc: 0.2054\n",
      "iteration 32 current loss: 1.4247238636016846 current acc: 0.2116\n",
      "iteration 33 current loss: 1.4167429208755493 current acc: 0.2186\n",
      "iteration 34 current loss: 1.392467975616455 current acc: 0.2246\n",
      "iteration 35 current loss: 1.3279234170913696 current acc: 0.2308\n",
      "iteration 36 current loss: 1.384090781211853 current acc: 0.2362\n",
      "iteration 37 current loss: 1.3494899272918701 current acc: 0.2428\n",
      "iteration 38 current loss: 1.0851985216140747 current acc: 0.2496\n",
      "iteration 39 current loss: 1.3420685529708862 current acc: 0.2552\n",
      "iteration 40 current loss: 1.2203153371810913 current acc: 0.2624\n",
      "iteration 41 current loss: 1.3018466234207153 current acc: 0.2698\n",
      "iteration 42 current loss: 1.285851001739502 current acc: 0.2764\n",
      "iteration 43 current loss: 1.3071719408035278 current acc: 0.2824\n",
      "iteration 44 current loss: 1.2426708936691284 current acc: 0.2888\n",
      "iteration 45 current loss: 1.1770507097244263 current acc: 0.2958\n",
      "iteration 46 current loss: 1.3710776567459106 current acc: 0.3024\n",
      "iteration 47 current loss: 1.481126070022583 current acc: 0.309\n",
      "iteration 48 current loss: 1.4609140157699585 current acc: 0.316\n",
      "iteration 49 current loss: 1.1832433938980103 current acc: 0.3232\n",
      "iteration 50 current loss: 1.398374080657959 current acc: 0.3288\n",
      "iteration 51 current loss: 1.391150951385498 current acc: 0.3356\n",
      "iteration 52 current loss: 1.5620698928833008 current acc: 0.3404\n",
      "iteration 53 current loss: 1.0962525606155396 current acc: 0.3484\n",
      "iteration 54 current loss: 1.3484888076782227 current acc: 0.355\n",
      "iteration 55 current loss: 1.1815420389175415 current acc: 0.363\n",
      "iteration 56 current loss: 1.1675242185592651 current acc: 0.3704\n",
      "iteration 57 current loss: 1.4982125759124756 current acc: 0.377\n",
      "iteration 58 current loss: 1.3973013162612915 current acc: 0.384\n",
      "iteration 59 current loss: 1.44350266456604 current acc: 0.3914\n",
      "iteration 60 current loss: 1.2672371864318848 current acc: 0.3982\n",
      "iteration 61 current loss: 1.3637655973434448 current acc: 0.405\n",
      "iteration 62 current loss: 1.3976023197174072 current acc: 0.4116\n",
      "iteration 63 current loss: 1.2630318403244019 current acc: 0.4192\n",
      "iteration 64 current loss: 1.4554507732391357 current acc: 0.4246\n",
      "iteration 65 current loss: 1.5003639459609985 current acc: 0.4296\n",
      "iteration 66 current loss: 1.1398760080337524 current acc: 0.4366\n",
      "iteration 67 current loss: 1.3589473962783813 current acc: 0.4442\n",
      "iteration 68 current loss: 1.2184776067733765 current acc: 0.451\n",
      "iteration 69 current loss: 1.6393581628799438 current acc: 0.456\n",
      "iteration 70 current loss: 1.4004398584365845 current acc: 0.4624\n",
      "iteration 71 current loss: 1.324230670928955 current acc: 0.4694\n",
      "iteration 72 current loss: 1.4852685928344727 current acc: 0.4752\n",
      "iteration 73 current loss: 1.380591869354248 current acc: 0.4814\n",
      "iteration 74 current loss: 1.5834659337997437 current acc: 0.487\n",
      "iteration 75 current loss: 1.3510693311691284 current acc: 0.4928\n",
      "iteration 76 current loss: 1.5148346424102783 current acc: 0.4986\n",
      "iteration 77 current loss: 1.4233753681182861 current acc: 0.505\n",
      "iteration 78 current loss: 0.792910635471344 current acc: 0.506\n",
      "\t\tTrain Epoch 11/100,Train Accuracy: 0.506, Train Loss: 1.3715349126465712.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 11/100, Validation Accuracy: 0.43325, Validation Loss: 1.600292594909668\n",
      "best loss 1.3715349126465712\n",
      "iteration 0 current loss: 1.2010365724563599 current acc: 0.0074\n",
      "iteration 1 current loss: 1.3294734954833984 current acc: 0.0136\n",
      "iteration 2 current loss: 1.2513065338134766 current acc: 0.0206\n",
      "iteration 3 current loss: 1.3958650827407837 current acc: 0.0274\n",
      "iteration 4 current loss: 1.4536066055297852 current acc: 0.0334\n",
      "iteration 5 current loss: 1.1350748538970947 current acc: 0.0404\n",
      "iteration 6 current loss: 1.1405436992645264 current acc: 0.0482\n",
      "iteration 7 current loss: 0.9253233075141907 current acc: 0.0566\n",
      "iteration 8 current loss: 1.153391718864441 current acc: 0.0638\n",
      "iteration 9 current loss: 1.135170340538025 current acc: 0.0708\n",
      "iteration 10 current loss: 1.4201085567474365 current acc: 0.0778\n",
      "iteration 11 current loss: 1.2161478996276855 current acc: 0.0848\n",
      "iteration 12 current loss: 1.1586672067642212 current acc: 0.0924\n",
      "iteration 13 current loss: 1.2830314636230469 current acc: 0.0996\n",
      "iteration 14 current loss: 1.1896297931671143 current acc: 0.1068\n",
      "iteration 15 current loss: 1.1529910564422607 current acc: 0.114\n",
      "iteration 16 current loss: 1.314112901687622 current acc: 0.1208\n",
      "iteration 17 current loss: 1.1228761672973633 current acc: 0.1276\n",
      "iteration 18 current loss: 1.2521464824676514 current acc: 0.1352\n",
      "iteration 19 current loss: 1.4081875085830688 current acc: 0.1422\n",
      "iteration 20 current loss: 1.188958764076233 current acc: 0.1498\n",
      "iteration 21 current loss: 1.4679218530654907 current acc: 0.1564\n",
      "iteration 22 current loss: 1.5263639688491821 current acc: 0.1626\n",
      "iteration 23 current loss: 1.0865644216537476 current acc: 0.17\n",
      "iteration 24 current loss: 1.3154575824737549 current acc: 0.1772\n",
      "iteration 25 current loss: 1.271483063697815 current acc: 0.1844\n",
      "iteration 26 current loss: 1.322534203529358 current acc: 0.1902\n",
      "iteration 27 current loss: 1.1225489377975464 current acc: 0.1972\n",
      "iteration 28 current loss: 1.1856427192687988 current acc: 0.2038\n",
      "iteration 29 current loss: 1.5276464223861694 current acc: 0.2094\n",
      "iteration 30 current loss: 1.4144748449325562 current acc: 0.2158\n",
      "iteration 31 current loss: 1.3443375825881958 current acc: 0.2222\n",
      "iteration 32 current loss: 1.2366890907287598 current acc: 0.2298\n",
      "iteration 33 current loss: 1.363749384880066 current acc: 0.236\n",
      "iteration 34 current loss: 1.2640289068222046 current acc: 0.2426\n",
      "iteration 35 current loss: 1.2454190254211426 current acc: 0.2496\n",
      "iteration 36 current loss: 1.4522134065628052 current acc: 0.2562\n",
      "iteration 37 current loss: 1.4958760738372803 current acc: 0.2626\n",
      "iteration 38 current loss: 1.153823971748352 current acc: 0.2706\n",
      "iteration 39 current loss: 1.449676752090454 current acc: 0.277\n",
      "iteration 40 current loss: 1.3041911125183105 current acc: 0.2834\n",
      "iteration 41 current loss: 1.227195143699646 current acc: 0.291\n",
      "iteration 42 current loss: 1.3502336740493774 current acc: 0.298\n",
      "iteration 43 current loss: 1.3743447065353394 current acc: 0.3034\n",
      "iteration 44 current loss: 1.560914397239685 current acc: 0.3098\n",
      "iteration 45 current loss: 1.382817268371582 current acc: 0.3156\n",
      "iteration 46 current loss: 1.2168443202972412 current acc: 0.3232\n",
      "iteration 47 current loss: 1.2705049514770508 current acc: 0.3304\n",
      "iteration 48 current loss: 1.3627647161483765 current acc: 0.3374\n",
      "iteration 49 current loss: 1.1565918922424316 current acc: 0.3454\n",
      "iteration 50 current loss: 1.4504625797271729 current acc: 0.352\n",
      "iteration 51 current loss: 1.511719822883606 current acc: 0.3584\n",
      "iteration 52 current loss: 1.358017086982727 current acc: 0.365\n",
      "iteration 53 current loss: 1.2859541177749634 current acc: 0.3714\n",
      "iteration 54 current loss: 1.1871509552001953 current acc: 0.3788\n",
      "iteration 55 current loss: 1.2705607414245605 current acc: 0.386\n",
      "iteration 56 current loss: 0.9282128214836121 current acc: 0.394\n",
      "iteration 57 current loss: 1.5446654558181763 current acc: 0.4004\n",
      "iteration 58 current loss: 1.2546542882919312 current acc: 0.408\n",
      "iteration 59 current loss: 1.2252299785614014 current acc: 0.4144\n",
      "iteration 60 current loss: 1.3991889953613281 current acc: 0.421\n",
      "iteration 61 current loss: 1.212205410003662 current acc: 0.4292\n",
      "iteration 62 current loss: 1.5167627334594727 current acc: 0.4344\n",
      "iteration 63 current loss: 1.599261999130249 current acc: 0.4402\n",
      "iteration 64 current loss: 1.4398373365402222 current acc: 0.446\n",
      "iteration 65 current loss: 1.7326765060424805 current acc: 0.4504\n",
      "iteration 66 current loss: 1.4399346113204956 current acc: 0.456\n",
      "iteration 67 current loss: 1.1547579765319824 current acc: 0.4628\n",
      "iteration 68 current loss: 1.3174980878829956 current acc: 0.4692\n",
      "iteration 69 current loss: 1.3380165100097656 current acc: 0.475\n",
      "iteration 70 current loss: 1.5107060670852661 current acc: 0.4806\n",
      "iteration 71 current loss: 1.211310625076294 current acc: 0.4872\n",
      "iteration 72 current loss: 1.6385198831558228 current acc: 0.4926\n",
      "iteration 73 current loss: 1.4562844038009644 current acc: 0.4984\n",
      "iteration 74 current loss: 1.4949442148208618 current acc: 0.5038\n",
      "iteration 75 current loss: 1.3164786100387573 current acc: 0.5102\n",
      "iteration 76 current loss: 1.2246756553649902 current acc: 0.5178\n",
      "iteration 77 current loss: 1.407457709312439 current acc: 0.5236\n",
      "iteration 78 current loss: 1.224161982536316 current acc: 0.5248\n",
      "\t\tTrain Epoch 12/100,Train Accuracy: 0.5248, Train Loss: 1.3155925515331799.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 12/100, Validation Accuracy: 0.422625, Validation Loss: 1.6734266576766967\n",
      "best loss 1.3155925515331799\n",
      "iteration 0 current loss: 1.16063392162323 current acc: 0.0064\n",
      "iteration 1 current loss: 1.3447901010513306 current acc: 0.0136\n",
      "iteration 2 current loss: 1.2012990713119507 current acc: 0.021\n",
      "iteration 3 current loss: 1.4351316690444946 current acc: 0.0276\n",
      "iteration 4 current loss: 1.3379895687103271 current acc: 0.0334\n",
      "iteration 5 current loss: 1.2591593265533447 current acc: 0.0396\n",
      "iteration 6 current loss: 1.3068166971206665 current acc: 0.0462\n",
      "iteration 7 current loss: 1.387206792831421 current acc: 0.0518\n",
      "iteration 8 current loss: 1.1128816604614258 current acc: 0.0596\n",
      "iteration 9 current loss: 1.421040415763855 current acc: 0.0656\n",
      "iteration 10 current loss: 1.1975765228271484 current acc: 0.073\n",
      "iteration 11 current loss: 1.2988553047180176 current acc: 0.0798\n",
      "iteration 12 current loss: 1.3947745561599731 current acc: 0.0866\n",
      "iteration 13 current loss: 1.6103945970535278 current acc: 0.0908\n",
      "iteration 14 current loss: 1.552286148071289 current acc: 0.0958\n",
      "iteration 15 current loss: 1.333040475845337 current acc: 0.1024\n",
      "iteration 16 current loss: 1.5755724906921387 current acc: 0.1084\n",
      "iteration 17 current loss: 1.1718016862869263 current acc: 0.1152\n",
      "iteration 18 current loss: 1.3588337898254395 current acc: 0.1212\n",
      "iteration 19 current loss: 1.655508041381836 current acc: 0.1258\n",
      "iteration 20 current loss: 1.1638263463974 current acc: 0.133\n",
      "iteration 21 current loss: 1.2423241138458252 current acc: 0.1398\n",
      "iteration 22 current loss: 1.3134498596191406 current acc: 0.1474\n",
      "iteration 23 current loss: 1.2557440996170044 current acc: 0.1548\n",
      "iteration 24 current loss: 1.0572822093963623 current acc: 0.1634\n",
      "iteration 25 current loss: 1.6109308004379272 current acc: 0.169\n",
      "iteration 26 current loss: 1.161913275718689 current acc: 0.1772\n",
      "iteration 27 current loss: 1.1409687995910645 current acc: 0.1848\n",
      "iteration 28 current loss: 1.4142149686813354 current acc: 0.1912\n",
      "iteration 29 current loss: 1.342494249343872 current acc: 0.198\n",
      "iteration 30 current loss: 1.0702329874038696 current acc: 0.2062\n",
      "iteration 31 current loss: 1.13400137424469 current acc: 0.2144\n",
      "iteration 32 current loss: 1.0997551679611206 current acc: 0.2218\n",
      "iteration 33 current loss: 1.1347891092300415 current acc: 0.2294\n",
      "iteration 34 current loss: 1.2592555284500122 current acc: 0.2362\n",
      "iteration 35 current loss: 1.2601172924041748 current acc: 0.244\n",
      "iteration 36 current loss: 1.1845232248306274 current acc: 0.2508\n",
      "iteration 37 current loss: 1.4078150987625122 current acc: 0.258\n",
      "iteration 38 current loss: 1.546976923942566 current acc: 0.2646\n",
      "iteration 39 current loss: 1.3722600936889648 current acc: 0.2718\n",
      "iteration 40 current loss: 1.1280186176300049 current acc: 0.2786\n",
      "iteration 41 current loss: 1.2147674560546875 current acc: 0.2858\n",
      "iteration 42 current loss: 1.4502174854278564 current acc: 0.292\n",
      "iteration 43 current loss: 1.2080061435699463 current acc: 0.2992\n",
      "iteration 44 current loss: 1.4647767543792725 current acc: 0.3044\n",
      "iteration 45 current loss: 1.1545441150665283 current acc: 0.3116\n",
      "iteration 46 current loss: 1.2612273693084717 current acc: 0.318\n",
      "iteration 47 current loss: 1.5838093757629395 current acc: 0.3234\n",
      "iteration 48 current loss: 1.3750988245010376 current acc: 0.3304\n",
      "iteration 49 current loss: 0.9889141917228699 current acc: 0.3384\n",
      "iteration 50 current loss: 1.3139231204986572 current acc: 0.3446\n",
      "iteration 51 current loss: 1.3961430788040161 current acc: 0.3512\n",
      "iteration 52 current loss: 1.3939640522003174 current acc: 0.3584\n",
      "iteration 53 current loss: 1.2397030591964722 current acc: 0.3652\n",
      "iteration 54 current loss: 1.2997950315475464 current acc: 0.3708\n",
      "iteration 55 current loss: 1.0633457899093628 current acc: 0.379\n",
      "iteration 56 current loss: 1.4615048170089722 current acc: 0.3848\n",
      "iteration 57 current loss: 1.263122797012329 current acc: 0.3916\n",
      "iteration 58 current loss: 1.2095859050750732 current acc: 0.3976\n",
      "iteration 59 current loss: 1.7920182943344116 current acc: 0.4016\n",
      "iteration 60 current loss: 1.2982791662216187 current acc: 0.409\n",
      "iteration 61 current loss: 1.6327682733535767 current acc: 0.4142\n",
      "iteration 62 current loss: 1.257318377494812 current acc: 0.4206\n",
      "iteration 63 current loss: 1.2336407899856567 current acc: 0.4278\n",
      "iteration 64 current loss: 1.189326524734497 current acc: 0.435\n",
      "iteration 65 current loss: 1.3348015546798706 current acc: 0.4408\n",
      "iteration 66 current loss: 1.2331546545028687 current acc: 0.4468\n",
      "iteration 67 current loss: 1.2274545431137085 current acc: 0.4536\n",
      "iteration 68 current loss: 1.1980191469192505 current acc: 0.4618\n",
      "iteration 69 current loss: 1.71015465259552 current acc: 0.4672\n",
      "iteration 70 current loss: 1.61760675907135 current acc: 0.4726\n",
      "iteration 71 current loss: 1.2210640907287598 current acc: 0.48\n",
      "iteration 72 current loss: 1.1738947629928589 current acc: 0.4876\n",
      "iteration 73 current loss: 1.4031051397323608 current acc: 0.494\n",
      "iteration 74 current loss: 1.1609814167022705 current acc: 0.5018\n",
      "iteration 75 current loss: 1.3619909286499023 current acc: 0.5086\n",
      "iteration 76 current loss: 1.4214826822280884 current acc: 0.5142\n",
      "iteration 77 current loss: 1.5338245630264282 current acc: 0.5202\n",
      "iteration 78 current loss: 1.721407413482666 current acc: 0.5212\n",
      "\t\tTrain Epoch 13/100,Train Accuracy: 0.5212, Train Loss: 1.3225472162041483.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 13/100, Validation Accuracy: 0.444875, Validation Loss: 1.5642422008514405\n",
      "iteration 0 current loss: 1.2113242149353027 current acc: 0.0072\n",
      "iteration 1 current loss: 1.3686894178390503 current acc: 0.0124\n",
      "iteration 2 current loss: 1.064256191253662 current acc: 0.0198\n",
      "iteration 3 current loss: 1.3097679615020752 current acc: 0.0268\n",
      "iteration 4 current loss: 1.2780673503875732 current acc: 0.0336\n",
      "iteration 5 current loss: 1.489200234413147 current acc: 0.038\n",
      "iteration 6 current loss: 1.5737940073013306 current acc: 0.0438\n",
      "iteration 7 current loss: 1.4239534139633179 current acc: 0.0494\n",
      "iteration 8 current loss: 1.2642511129379272 current acc: 0.0572\n",
      "iteration 9 current loss: 1.108918309211731 current acc: 0.065\n",
      "iteration 10 current loss: 1.4621951580047607 current acc: 0.0704\n",
      "iteration 11 current loss: 1.214953064918518 current acc: 0.077\n",
      "iteration 12 current loss: 1.1327717304229736 current acc: 0.0852\n",
      "iteration 13 current loss: 1.1939109563827515 current acc: 0.0932\n",
      "iteration 14 current loss: 1.33632230758667 current acc: 0.1\n",
      "iteration 15 current loss: 1.5321037769317627 current acc: 0.1046\n",
      "iteration 16 current loss: 1.1768473386764526 current acc: 0.1132\n",
      "iteration 17 current loss: 1.1672245264053345 current acc: 0.1202\n",
      "iteration 18 current loss: 1.0722781419754028 current acc: 0.1286\n",
      "iteration 19 current loss: 1.3858436346054077 current acc: 0.1356\n",
      "iteration 20 current loss: 1.2824894189834595 current acc: 0.1426\n",
      "iteration 21 current loss: 1.1648924350738525 current acc: 0.15\n",
      "iteration 22 current loss: 1.2310867309570312 current acc: 0.1572\n",
      "iteration 23 current loss: 1.4037686586380005 current acc: 0.1638\n",
      "iteration 24 current loss: 1.1208609342575073 current acc: 0.1722\n",
      "iteration 25 current loss: 1.08856201171875 current acc: 0.1806\n",
      "iteration 26 current loss: 1.3674170970916748 current acc: 0.1878\n",
      "iteration 27 current loss: 1.3212957382202148 current acc: 0.1942\n",
      "iteration 28 current loss: 1.2946184873580933 current acc: 0.2014\n",
      "iteration 29 current loss: 1.2086553573608398 current acc: 0.2088\n",
      "iteration 30 current loss: 1.2021903991699219 current acc: 0.2162\n",
      "iteration 31 current loss: 1.382886290550232 current acc: 0.2226\n",
      "iteration 32 current loss: 1.3091309070587158 current acc: 0.229\n",
      "iteration 33 current loss: 1.439160704612732 current acc: 0.2344\n",
      "iteration 34 current loss: 1.2646336555480957 current acc: 0.2416\n",
      "iteration 35 current loss: 1.5604525804519653 current acc: 0.2484\n",
      "iteration 36 current loss: 1.1161259412765503 current acc: 0.2566\n",
      "iteration 37 current loss: 1.136625051498413 current acc: 0.2636\n",
      "iteration 38 current loss: 1.2933701276779175 current acc: 0.2704\n",
      "iteration 39 current loss: 1.1415812969207764 current acc: 0.2784\n",
      "iteration 40 current loss: 1.4000747203826904 current acc: 0.2844\n",
      "iteration 41 current loss: 1.2602216005325317 current acc: 0.2906\n",
      "iteration 42 current loss: 1.4138126373291016 current acc: 0.2966\n",
      "iteration 43 current loss: 1.0620999336242676 current acc: 0.3042\n",
      "iteration 44 current loss: 1.1801962852478027 current acc: 0.3106\n",
      "iteration 45 current loss: 1.3631207942962646 current acc: 0.3168\n",
      "iteration 46 current loss: 1.4259521961212158 current acc: 0.3236\n",
      "iteration 47 current loss: 1.3058162927627563 current acc: 0.3306\n",
      "iteration 48 current loss: 1.1243250370025635 current acc: 0.3376\n",
      "iteration 49 current loss: 1.364054799079895 current acc: 0.3444\n",
      "iteration 50 current loss: 1.324782133102417 current acc: 0.3512\n",
      "iteration 51 current loss: 1.1814790964126587 current acc: 0.359\n",
      "iteration 52 current loss: 1.025286078453064 current acc: 0.3678\n",
      "iteration 53 current loss: 1.2765848636627197 current acc: 0.3748\n",
      "iteration 54 current loss: 1.552288293838501 current acc: 0.3802\n",
      "iteration 55 current loss: 1.3177008628845215 current acc: 0.3868\n",
      "iteration 56 current loss: 1.1240837574005127 current acc: 0.3944\n",
      "iteration 57 current loss: 1.3718472719192505 current acc: 0.4006\n",
      "iteration 58 current loss: 1.3217562437057495 current acc: 0.4076\n",
      "iteration 59 current loss: 1.2566640377044678 current acc: 0.4146\n",
      "iteration 60 current loss: 1.4078289270401 current acc: 0.4218\n",
      "iteration 61 current loss: 1.2568856477737427 current acc: 0.429\n",
      "iteration 62 current loss: 1.2294694185256958 current acc: 0.436\n",
      "iteration 63 current loss: 1.1862940788269043 current acc: 0.4434\n",
      "iteration 64 current loss: 1.2580804824829102 current acc: 0.4496\n",
      "iteration 65 current loss: 1.1191308498382568 current acc: 0.4576\n",
      "iteration 66 current loss: 1.460256576538086 current acc: 0.4636\n",
      "iteration 67 current loss: 1.390097975730896 current acc: 0.4696\n",
      "iteration 68 current loss: 1.5754878520965576 current acc: 0.4758\n",
      "iteration 69 current loss: 1.4511728286743164 current acc: 0.4836\n",
      "iteration 70 current loss: 1.337218165397644 current acc: 0.4906\n",
      "iteration 71 current loss: 1.4649888277053833 current acc: 0.4968\n",
      "iteration 72 current loss: 1.07370126247406 current acc: 0.5036\n",
      "iteration 73 current loss: 1.2943029403686523 current acc: 0.5104\n",
      "iteration 74 current loss: 1.3473471403121948 current acc: 0.5172\n",
      "iteration 75 current loss: 1.6362701654434204 current acc: 0.5226\n",
      "iteration 76 current loss: 1.294445514678955 current acc: 0.5292\n",
      "iteration 77 current loss: 1.5669053792953491 current acc: 0.5358\n",
      "iteration 78 current loss: 1.3645217418670654 current acc: 0.5368\n",
      "\t\tTrain Epoch 14/100,Train Accuracy: 0.5368, Train Loss: 1.297000371957127.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 14/100, Validation Accuracy: 0.424875, Validation Loss: 1.65855331325531\n",
      "best loss 1.297000371957127\n",
      "iteration 0 current loss: 1.434265375137329 current acc: 0.006\n",
      "iteration 1 current loss: 1.153408408164978 current acc: 0.0132\n",
      "iteration 2 current loss: 1.4014747142791748 current acc: 0.0206\n",
      "iteration 3 current loss: 1.1894797086715698 current acc: 0.028\n",
      "iteration 4 current loss: 1.2530289888381958 current acc: 0.0352\n",
      "iteration 5 current loss: 0.9576452970504761 current acc: 0.0436\n",
      "iteration 6 current loss: 1.5235304832458496 current acc: 0.0496\n",
      "iteration 7 current loss: 1.682527780532837 current acc: 0.0554\n",
      "iteration 8 current loss: 1.5335185527801514 current acc: 0.0606\n",
      "iteration 9 current loss: 1.2777708768844604 current acc: 0.0674\n",
      "iteration 10 current loss: 1.0787354707717896 current acc: 0.0744\n",
      "iteration 11 current loss: 1.3781170845031738 current acc: 0.0802\n",
      "iteration 12 current loss: 1.6116522550582886 current acc: 0.0858\n",
      "iteration 13 current loss: 1.7322241067886353 current acc: 0.092\n",
      "iteration 14 current loss: 1.497077465057373 current acc: 0.097\n",
      "iteration 15 current loss: 0.9285554885864258 current acc: 0.1056\n",
      "iteration 16 current loss: 1.0835226774215698 current acc: 0.1134\n",
      "iteration 17 current loss: 1.0770009756088257 current acc: 0.1216\n",
      "iteration 18 current loss: 1.5323851108551025 current acc: 0.1278\n",
      "iteration 19 current loss: 1.320261836051941 current acc: 0.1344\n",
      "iteration 20 current loss: 1.340131163597107 current acc: 0.1414\n",
      "iteration 21 current loss: 1.285629153251648 current acc: 0.1486\n",
      "iteration 22 current loss: 1.2274061441421509 current acc: 0.1556\n",
      "iteration 23 current loss: 1.236928105354309 current acc: 0.1626\n",
      "iteration 24 current loss: 1.1440030336380005 current acc: 0.1696\n",
      "iteration 25 current loss: 1.059478998184204 current acc: 0.1782\n",
      "iteration 26 current loss: 1.1817458868026733 current acc: 0.186\n",
      "iteration 27 current loss: 1.149788498878479 current acc: 0.193\n",
      "iteration 28 current loss: 1.202772855758667 current acc: 0.2\n",
      "iteration 29 current loss: 1.357426404953003 current acc: 0.2064\n",
      "iteration 30 current loss: 1.2317967414855957 current acc: 0.2126\n",
      "iteration 31 current loss: 1.077669620513916 current acc: 0.2206\n",
      "iteration 32 current loss: 1.3026165962219238 current acc: 0.227\n",
      "iteration 33 current loss: 1.2769505977630615 current acc: 0.2336\n",
      "iteration 34 current loss: 1.1411958932876587 current acc: 0.2412\n",
      "iteration 35 current loss: 1.4558912515640259 current acc: 0.2472\n",
      "iteration 36 current loss: 1.0215247869491577 current acc: 0.255\n",
      "iteration 37 current loss: 1.2986552715301514 current acc: 0.2624\n",
      "iteration 38 current loss: 1.4443496465682983 current acc: 0.2698\n",
      "iteration 39 current loss: 1.3452672958374023 current acc: 0.2762\n",
      "iteration 40 current loss: 0.957751989364624 current acc: 0.2854\n",
      "iteration 41 current loss: 1.0669732093811035 current acc: 0.2918\n",
      "iteration 42 current loss: 1.1992789506912231 current acc: 0.2994\n",
      "iteration 43 current loss: 1.319459080696106 current acc: 0.3064\n",
      "iteration 44 current loss: 1.2737146615982056 current acc: 0.3132\n",
      "iteration 45 current loss: 1.0753252506256104 current acc: 0.3206\n",
      "iteration 46 current loss: 1.117329716682434 current acc: 0.3274\n",
      "iteration 47 current loss: 1.3484141826629639 current acc: 0.3342\n",
      "iteration 48 current loss: 1.5209228992462158 current acc: 0.3402\n",
      "iteration 49 current loss: 1.4734344482421875 current acc: 0.3468\n",
      "iteration 50 current loss: 1.2539172172546387 current acc: 0.3536\n",
      "iteration 51 current loss: 1.2579089403152466 current acc: 0.3606\n",
      "iteration 52 current loss: 1.3036165237426758 current acc: 0.3668\n",
      "iteration 53 current loss: 1.2198115587234497 current acc: 0.374\n",
      "iteration 54 current loss: 1.0266026258468628 current acc: 0.3828\n",
      "iteration 55 current loss: 1.0465126037597656 current acc: 0.3908\n",
      "iteration 56 current loss: 1.6424328088760376 current acc: 0.3962\n",
      "iteration 57 current loss: 1.2825326919555664 current acc: 0.4032\n",
      "iteration 58 current loss: 1.1457326412200928 current acc: 0.4102\n",
      "iteration 59 current loss: 1.0438177585601807 current acc: 0.4178\n",
      "iteration 60 current loss: 1.183807134628296 current acc: 0.425\n",
      "iteration 61 current loss: 1.155369758605957 current acc: 0.4326\n",
      "iteration 62 current loss: 1.238830327987671 current acc: 0.4394\n",
      "iteration 63 current loss: 1.3224387168884277 current acc: 0.4456\n",
      "iteration 64 current loss: 1.423991322517395 current acc: 0.4522\n",
      "iteration 65 current loss: 1.0604816675186157 current acc: 0.4604\n",
      "iteration 66 current loss: 1.1110771894454956 current acc: 0.4682\n",
      "iteration 67 current loss: 1.2365885972976685 current acc: 0.4754\n",
      "iteration 68 current loss: 1.445505976676941 current acc: 0.4824\n",
      "iteration 69 current loss: 1.413501262664795 current acc: 0.4898\n",
      "iteration 70 current loss: 1.214396595954895 current acc: 0.4964\n",
      "iteration 71 current loss: 1.397213339805603 current acc: 0.502\n",
      "iteration 72 current loss: 1.018035888671875 current acc: 0.5106\n",
      "iteration 73 current loss: 1.6056346893310547 current acc: 0.5162\n",
      "iteration 74 current loss: 1.502360224723816 current acc: 0.523\n",
      "iteration 75 current loss: 1.0597983598709106 current acc: 0.5306\n",
      "iteration 76 current loss: 1.0730613470077515 current acc: 0.538\n",
      "iteration 77 current loss: 1.0259772539138794 current acc: 0.5462\n",
      "iteration 78 current loss: 0.6930049061775208 current acc: 0.5476\n",
      "\t\tTrain Epoch 15/100,Train Accuracy: 0.5476, Train Loss: 1.2554933534392827.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 15/100, Validation Accuracy: 0.45325, Validation Loss: 1.5464451122283935\n",
      "best loss 1.2554933534392827\n",
      "iteration 0 current loss: 1.3327522277832031 current acc: 0.0076\n",
      "iteration 1 current loss: 1.4056764841079712 current acc: 0.0148\n",
      "iteration 2 current loss: 0.9963907599449158 current acc: 0.0234\n",
      "iteration 3 current loss: 0.9000391960144043 current acc: 0.0326\n",
      "iteration 4 current loss: 1.1134991645812988 current acc: 0.04\n",
      "iteration 5 current loss: 1.1270263195037842 current acc: 0.0474\n",
      "iteration 6 current loss: 1.351758360862732 current acc: 0.0538\n",
      "iteration 7 current loss: 1.313342809677124 current acc: 0.061\n",
      "iteration 8 current loss: 1.0848889350891113 current acc: 0.0684\n",
      "iteration 9 current loss: 1.325790286064148 current acc: 0.0744\n",
      "iteration 10 current loss: 1.1837239265441895 current acc: 0.0816\n",
      "iteration 11 current loss: 0.9826380610466003 current acc: 0.0898\n",
      "iteration 12 current loss: 1.218732237815857 current acc: 0.096\n",
      "iteration 13 current loss: 1.3422760963439941 current acc: 0.1028\n",
      "iteration 14 current loss: 1.2368220090866089 current acc: 0.1102\n",
      "iteration 15 current loss: 1.1673038005828857 current acc: 0.1172\n",
      "iteration 16 current loss: 0.9153186678886414 current acc: 0.1258\n",
      "iteration 17 current loss: 1.2770041227340698 current acc: 0.1324\n",
      "iteration 18 current loss: 1.0448869466781616 current acc: 0.14\n",
      "iteration 19 current loss: 1.1256635189056396 current acc: 0.1482\n",
      "iteration 20 current loss: 1.166357159614563 current acc: 0.1552\n",
      "iteration 21 current loss: 1.2006134986877441 current acc: 0.1628\n",
      "iteration 22 current loss: 1.2347854375839233 current acc: 0.1702\n",
      "iteration 23 current loss: 1.2733571529388428 current acc: 0.177\n",
      "iteration 24 current loss: 1.2342720031738281 current acc: 0.1838\n",
      "iteration 25 current loss: 1.0039478540420532 current acc: 0.192\n",
      "iteration 26 current loss: 1.423124074935913 current acc: 0.1986\n",
      "iteration 27 current loss: 1.1262286901474 current acc: 0.2062\n",
      "iteration 28 current loss: 1.2884206771850586 current acc: 0.2128\n",
      "iteration 29 current loss: 1.2885462045669556 current acc: 0.2194\n",
      "iteration 30 current loss: 1.2485227584838867 current acc: 0.2268\n",
      "iteration 31 current loss: 1.0602020025253296 current acc: 0.2352\n",
      "iteration 32 current loss: 1.1905299425125122 current acc: 0.243\n",
      "iteration 33 current loss: 1.2631990909576416 current acc: 0.2494\n",
      "iteration 34 current loss: 1.5044647455215454 current acc: 0.2562\n",
      "iteration 35 current loss: 1.19584059715271 current acc: 0.2632\n",
      "iteration 36 current loss: 1.3339287042617798 current acc: 0.2692\n",
      "iteration 37 current loss: 1.1112245321273804 current acc: 0.2772\n",
      "iteration 38 current loss: 1.2275583744049072 current acc: 0.2844\n",
      "iteration 39 current loss: 1.3226854801177979 current acc: 0.292\n",
      "iteration 40 current loss: 0.9864816069602966 current acc: 0.3002\n",
      "iteration 41 current loss: 1.0008665323257446 current acc: 0.308\n",
      "iteration 42 current loss: 1.1551545858383179 current acc: 0.3144\n",
      "iteration 43 current loss: 1.1694096326828003 current acc: 0.322\n",
      "iteration 44 current loss: 1.4556766748428345 current acc: 0.3276\n",
      "iteration 45 current loss: 1.2188727855682373 current acc: 0.3348\n",
      "iteration 46 current loss: 1.1935449838638306 current acc: 0.3412\n",
      "iteration 47 current loss: 0.930458664894104 current acc: 0.3498\n",
      "iteration 48 current loss: 1.054192304611206 current acc: 0.3576\n",
      "iteration 49 current loss: 1.3525828123092651 current acc: 0.3648\n",
      "iteration 50 current loss: 0.7654649019241333 current acc: 0.3738\n",
      "iteration 51 current loss: 1.2612355947494507 current acc: 0.381\n",
      "iteration 52 current loss: 1.1142162084579468 current acc: 0.3888\n",
      "iteration 53 current loss: 1.2009694576263428 current acc: 0.3958\n",
      "iteration 54 current loss: 1.3193955421447754 current acc: 0.4028\n",
      "iteration 55 current loss: 1.0658752918243408 current acc: 0.411\n",
      "iteration 56 current loss: 1.1054942607879639 current acc: 0.4192\n",
      "iteration 57 current loss: 1.0862162113189697 current acc: 0.4268\n",
      "iteration 58 current loss: 1.3495137691497803 current acc: 0.4328\n",
      "iteration 59 current loss: 1.2586925029754639 current acc: 0.4402\n",
      "iteration 60 current loss: 0.9515128135681152 current acc: 0.4482\n",
      "iteration 61 current loss: 1.4751001596450806 current acc: 0.4534\n",
      "iteration 62 current loss: 1.2163995504379272 current acc: 0.4612\n",
      "iteration 63 current loss: 1.0814439058303833 current acc: 0.4688\n",
      "iteration 64 current loss: 0.9958459138870239 current acc: 0.4768\n",
      "iteration 65 current loss: 1.4281585216522217 current acc: 0.4838\n",
      "iteration 66 current loss: 1.1276226043701172 current acc: 0.4912\n",
      "iteration 67 current loss: 1.075478196144104 current acc: 0.4994\n",
      "iteration 68 current loss: 1.1456652879714966 current acc: 0.507\n",
      "iteration 69 current loss: 1.5477323532104492 current acc: 0.5122\n",
      "iteration 70 current loss: 1.259558081626892 current acc: 0.5206\n",
      "iteration 71 current loss: 1.3273659944534302 current acc: 0.5268\n",
      "iteration 72 current loss: 1.1195563077926636 current acc: 0.5344\n",
      "iteration 73 current loss: 1.1396273374557495 current acc: 0.5422\n",
      "iteration 74 current loss: 1.354864478111267 current acc: 0.5486\n",
      "iteration 75 current loss: 1.2149940729141235 current acc: 0.555\n",
      "iteration 76 current loss: 1.139787197113037 current acc: 0.5628\n",
      "iteration 77 current loss: 1.323646068572998 current acc: 0.5692\n",
      "iteration 78 current loss: 1.3352316617965698 current acc: 0.5702\n",
      "\t\tTrain Epoch 16/100,Train Accuracy: 0.5702, Train Loss: 1.19553478156464.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 16/100, Validation Accuracy: 0.45625, Validation Loss: 1.5393707656860351\n",
      "best loss 1.19553478156464\n",
      "iteration 0 current loss: 1.3289039134979248 current acc: 0.0068\n",
      "iteration 1 current loss: 1.020464301109314 current acc: 0.015\n",
      "iteration 2 current loss: 1.1396763324737549 current acc: 0.0226\n",
      "iteration 3 current loss: 1.2107877731323242 current acc: 0.0294\n",
      "iteration 4 current loss: 1.1836280822753906 current acc: 0.0364\n",
      "iteration 5 current loss: 1.044032096862793 current acc: 0.0442\n",
      "iteration 6 current loss: 1.141886830329895 current acc: 0.0512\n",
      "iteration 7 current loss: 1.4244322776794434 current acc: 0.0572\n",
      "iteration 8 current loss: 1.2730991840362549 current acc: 0.0646\n",
      "iteration 9 current loss: 1.3735888004302979 current acc: 0.0714\n",
      "iteration 10 current loss: 1.0179026126861572 current acc: 0.0786\n",
      "iteration 11 current loss: 1.1829160451889038 current acc: 0.0854\n",
      "iteration 12 current loss: 1.085268259048462 current acc: 0.0934\n",
      "iteration 13 current loss: 1.0908515453338623 current acc: 0.101\n",
      "iteration 14 current loss: 1.127928376197815 current acc: 0.1084\n",
      "iteration 15 current loss: 1.1437060832977295 current acc: 0.1166\n",
      "iteration 16 current loss: 1.1421774625778198 current acc: 0.1238\n",
      "iteration 17 current loss: 1.1167328357696533 current acc: 0.1316\n",
      "iteration 18 current loss: 0.9733576774597168 current acc: 0.14\n",
      "iteration 19 current loss: 1.4280451536178589 current acc: 0.1456\n",
      "iteration 20 current loss: 1.0937135219573975 current acc: 0.1534\n",
      "iteration 21 current loss: 1.4368884563446045 current acc: 0.1594\n",
      "iteration 22 current loss: 1.2383400201797485 current acc: 0.1662\n",
      "iteration 23 current loss: 1.1985751390457153 current acc: 0.174\n",
      "iteration 24 current loss: 1.1531833410263062 current acc: 0.1816\n",
      "iteration 25 current loss: 1.2174700498580933 current acc: 0.1884\n",
      "iteration 26 current loss: 1.204585075378418 current acc: 0.1948\n",
      "iteration 27 current loss: 1.4269824028015137 current acc: 0.2008\n",
      "iteration 28 current loss: 0.9584590196609497 current acc: 0.2098\n",
      "iteration 29 current loss: 1.3131839036941528 current acc: 0.2166\n",
      "iteration 30 current loss: 1.3457274436950684 current acc: 0.2222\n",
      "iteration 31 current loss: 0.9666284322738647 current acc: 0.2304\n",
      "iteration 32 current loss: 1.140872836112976 current acc: 0.2374\n",
      "iteration 33 current loss: 1.3173370361328125 current acc: 0.2438\n",
      "iteration 34 current loss: 1.0162297487258911 current acc: 0.2522\n",
      "iteration 35 current loss: 1.1516575813293457 current acc: 0.2598\n",
      "iteration 36 current loss: 1.1181949377059937 current acc: 0.268\n",
      "iteration 37 current loss: 1.2231887578964233 current acc: 0.2754\n",
      "iteration 38 current loss: 1.3208788633346558 current acc: 0.282\n",
      "iteration 39 current loss: 1.184088945388794 current acc: 0.2896\n",
      "iteration 40 current loss: 1.3647096157073975 current acc: 0.296\n",
      "iteration 41 current loss: 1.2949919700622559 current acc: 0.3024\n",
      "iteration 42 current loss: 1.013981580734253 current acc: 0.3104\n",
      "iteration 43 current loss: 1.1567269563674927 current acc: 0.3176\n",
      "iteration 44 current loss: 1.393923044204712 current acc: 0.3242\n",
      "iteration 45 current loss: 1.1591360569000244 current acc: 0.331\n",
      "iteration 46 current loss: 1.3941293954849243 current acc: 0.3372\n",
      "iteration 47 current loss: 1.3369237184524536 current acc: 0.3442\n",
      "iteration 48 current loss: 1.19577956199646 current acc: 0.3512\n",
      "iteration 49 current loss: 1.1864696741104126 current acc: 0.3578\n",
      "iteration 50 current loss: 1.1510896682739258 current acc: 0.3656\n",
      "iteration 51 current loss: 1.2826505899429321 current acc: 0.372\n",
      "iteration 52 current loss: 1.1313893795013428 current acc: 0.3798\n",
      "iteration 53 current loss: 1.0875884294509888 current acc: 0.3874\n",
      "iteration 54 current loss: 1.2009435892105103 current acc: 0.3954\n",
      "iteration 55 current loss: 1.1614261865615845 current acc: 0.4022\n",
      "iteration 56 current loss: 1.3198754787445068 current acc: 0.4092\n",
      "iteration 57 current loss: 1.2710037231445312 current acc: 0.4158\n",
      "iteration 58 current loss: 1.0788084268569946 current acc: 0.4242\n",
      "iteration 59 current loss: 1.3244147300720215 current acc: 0.4294\n",
      "iteration 60 current loss: 0.9211902618408203 current acc: 0.438\n",
      "iteration 61 current loss: 1.6351211071014404 current acc: 0.4438\n",
      "iteration 62 current loss: 1.139551043510437 current acc: 0.4504\n",
      "iteration 63 current loss: 1.0819181203842163 current acc: 0.4582\n",
      "iteration 64 current loss: 1.1682193279266357 current acc: 0.4662\n",
      "iteration 65 current loss: 1.20375657081604 current acc: 0.4734\n",
      "iteration 66 current loss: 1.0654915571212769 current acc: 0.4814\n",
      "iteration 67 current loss: 1.1151773929595947 current acc: 0.4892\n",
      "iteration 68 current loss: 1.2007464170455933 current acc: 0.497\n",
      "iteration 69 current loss: 1.2179094552993774 current acc: 0.5046\n",
      "iteration 70 current loss: 1.0014616250991821 current acc: 0.5118\n",
      "iteration 71 current loss: 1.25494384765625 current acc: 0.5188\n",
      "iteration 72 current loss: 0.9541245698928833 current acc: 0.5272\n",
      "iteration 73 current loss: 1.2414886951446533 current acc: 0.5344\n",
      "iteration 74 current loss: 1.3374474048614502 current acc: 0.5406\n",
      "iteration 75 current loss: 1.3612390756607056 current acc: 0.5472\n",
      "iteration 76 current loss: 1.320391297340393 current acc: 0.5536\n",
      "iteration 77 current loss: 1.3621249198913574 current acc: 0.56\n",
      "iteration 78 current loss: 0.9846841096878052 current acc: 0.561\n",
      "\t\tTrain Epoch 17/100,Train Accuracy: 0.561, Train Loss: 1.196816705450227.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 17/100, Validation Accuracy: 0.470375, Validation Loss: 1.563471185684204\n",
      "iteration 0 current loss: 1.0086793899536133 current acc: 0.0074\n",
      "iteration 1 current loss: 1.2998837232589722 current acc: 0.0136\n",
      "iteration 2 current loss: 1.248034954071045 current acc: 0.0208\n",
      "iteration 3 current loss: 1.1294853687286377 current acc: 0.0278\n",
      "iteration 4 current loss: 1.1167747974395752 current acc: 0.0358\n",
      "iteration 5 current loss: 1.040920376777649 current acc: 0.0434\n",
      "iteration 6 current loss: 0.9551495909690857 current acc: 0.0516\n",
      "iteration 7 current loss: 1.1841014623641968 current acc: 0.0596\n",
      "iteration 8 current loss: 1.2643691301345825 current acc: 0.067\n",
      "iteration 9 current loss: 0.9351887702941895 current acc: 0.0756\n",
      "iteration 10 current loss: 1.0773816108703613 current acc: 0.0836\n",
      "iteration 11 current loss: 1.2193995714187622 current acc: 0.0902\n",
      "iteration 12 current loss: 1.1597563028335571 current acc: 0.0978\n",
      "iteration 13 current loss: 0.7728770971298218 current acc: 0.1072\n",
      "iteration 14 current loss: 1.140715479850769 current acc: 0.1156\n",
      "iteration 15 current loss: 1.1716220378875732 current acc: 0.123\n",
      "iteration 16 current loss: 1.0492823123931885 current acc: 0.1316\n",
      "iteration 17 current loss: 1.114933967590332 current acc: 0.1394\n",
      "iteration 18 current loss: 1.3726176023483276 current acc: 0.1458\n",
      "iteration 19 current loss: 1.0352193117141724 current acc: 0.1542\n",
      "iteration 20 current loss: 1.4243884086608887 current acc: 0.1602\n",
      "iteration 21 current loss: 0.9916872978210449 current acc: 0.1682\n",
      "iteration 22 current loss: 1.1715571880340576 current acc: 0.1752\n",
      "iteration 23 current loss: 0.9892526865005493 current acc: 0.1836\n",
      "iteration 24 current loss: 1.0813050270080566 current acc: 0.1914\n",
      "iteration 25 current loss: 0.973678708076477 current acc: 0.1998\n",
      "iteration 26 current loss: 1.008939266204834 current acc: 0.2084\n",
      "iteration 27 current loss: 1.2742186784744263 current acc: 0.2152\n",
      "iteration 28 current loss: 1.0984079837799072 current acc: 0.2226\n",
      "iteration 29 current loss: 1.2137541770935059 current acc: 0.2292\n",
      "iteration 30 current loss: 1.168760061264038 current acc: 0.2356\n",
      "iteration 31 current loss: 1.135923981666565 current acc: 0.2436\n",
      "iteration 32 current loss: 1.0611673593521118 current acc: 0.2514\n",
      "iteration 33 current loss: 0.8373149037361145 current acc: 0.2614\n",
      "iteration 34 current loss: 1.3027565479278564 current acc: 0.2684\n",
      "iteration 35 current loss: 1.2460083961486816 current acc: 0.274\n",
      "iteration 36 current loss: 1.109399676322937 current acc: 0.2816\n",
      "iteration 37 current loss: 1.3946032524108887 current acc: 0.288\n",
      "iteration 38 current loss: 1.043649673461914 current acc: 0.2956\n",
      "iteration 39 current loss: 1.5879648923873901 current acc: 0.3012\n",
      "iteration 40 current loss: 1.098954439163208 current acc: 0.3082\n",
      "iteration 41 current loss: 1.123594045639038 current acc: 0.3168\n",
      "iteration 42 current loss: 0.9374459385871887 current acc: 0.3248\n",
      "iteration 43 current loss: 1.2388538122177124 current acc: 0.333\n",
      "iteration 44 current loss: 1.2509217262268066 current acc: 0.3394\n",
      "iteration 45 current loss: 1.061314582824707 current acc: 0.3468\n",
      "iteration 46 current loss: 1.1643013954162598 current acc: 0.3548\n",
      "iteration 47 current loss: 1.2239878177642822 current acc: 0.3614\n",
      "iteration 48 current loss: 1.159942626953125 current acc: 0.369\n",
      "iteration 49 current loss: 1.2939690351486206 current acc: 0.3762\n",
      "iteration 50 current loss: 1.0197333097457886 current acc: 0.3846\n",
      "iteration 51 current loss: 1.1451441049575806 current acc: 0.3928\n",
      "iteration 52 current loss: 1.0809839963912964 current acc: 0.4008\n",
      "iteration 53 current loss: 0.9952095746994019 current acc: 0.4092\n",
      "iteration 54 current loss: 1.1518210172653198 current acc: 0.417\n",
      "iteration 55 current loss: 1.036110520362854 current acc: 0.425\n",
      "iteration 56 current loss: 1.1022807359695435 current acc: 0.433\n",
      "iteration 57 current loss: 1.0735254287719727 current acc: 0.441\n",
      "iteration 58 current loss: 1.386651873588562 current acc: 0.4478\n",
      "iteration 59 current loss: 1.152146339416504 current acc: 0.4546\n",
      "iteration 60 current loss: 1.0094683170318604 current acc: 0.4622\n",
      "iteration 61 current loss: 1.0976346731185913 current acc: 0.4704\n",
      "iteration 62 current loss: 0.9086713194847107 current acc: 0.4796\n",
      "iteration 63 current loss: 1.011924147605896 current acc: 0.488\n",
      "iteration 64 current loss: 1.1734802722930908 current acc: 0.4944\n",
      "iteration 65 current loss: 1.3113600015640259 current acc: 0.5008\n",
      "iteration 66 current loss: 1.0190649032592773 current acc: 0.5088\n",
      "iteration 67 current loss: 1.0887144804000854 current acc: 0.5158\n",
      "iteration 68 current loss: 1.333485722541809 current acc: 0.5226\n",
      "iteration 69 current loss: 1.290239691734314 current acc: 0.5302\n",
      "iteration 70 current loss: 1.159814715385437 current acc: 0.5378\n",
      "iteration 71 current loss: 1.1068718433380127 current acc: 0.5454\n",
      "iteration 72 current loss: 0.9608846306800842 current acc: 0.5546\n",
      "iteration 73 current loss: 1.1099815368652344 current acc: 0.5622\n",
      "iteration 74 current loss: 1.364730715751648 current acc: 0.5688\n",
      "iteration 75 current loss: 1.1196650266647339 current acc: 0.577\n",
      "iteration 76 current loss: 1.0616055727005005 current acc: 0.5844\n",
      "iteration 77 current loss: 1.1279326677322388 current acc: 0.591\n",
      "iteration 78 current loss: 1.0809682607650757 current acc: 0.592\n",
      "\t\tTrain Epoch 18/100,Train Accuracy: 0.592, Train Loss: 1.1322091372707221.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 18/100, Validation Accuracy: 0.46325, Validation Loss: 1.5460687141418457\n",
      "best loss 1.1322091372707221\n",
      "iteration 0 current loss: 0.9888419508934021 current acc: 0.0082\n",
      "iteration 1 current loss: 1.0897753238677979 current acc: 0.0156\n",
      "iteration 2 current loss: 1.1717536449432373 current acc: 0.0232\n",
      "iteration 3 current loss: 1.0983654260635376 current acc: 0.0302\n",
      "iteration 4 current loss: 1.2859153747558594 current acc: 0.0368\n",
      "iteration 5 current loss: 1.2125738859176636 current acc: 0.044\n",
      "iteration 6 current loss: 1.0305920839309692 current acc: 0.0518\n",
      "iteration 7 current loss: 1.0468568801879883 current acc: 0.0596\n",
      "iteration 8 current loss: 1.012527585029602 current acc: 0.0678\n",
      "iteration 9 current loss: 1.1389936208724976 current acc: 0.0748\n",
      "iteration 10 current loss: 1.247298240661621 current acc: 0.0818\n",
      "iteration 11 current loss: 0.9133962988853455 current acc: 0.0902\n",
      "iteration 12 current loss: 1.1446874141693115 current acc: 0.0978\n",
      "iteration 13 current loss: 0.9557615518569946 current acc: 0.1062\n",
      "iteration 14 current loss: 1.065477967262268 current acc: 0.114\n",
      "iteration 15 current loss: 0.9853227734565735 current acc: 0.1228\n",
      "iteration 16 current loss: 1.053351640701294 current acc: 0.1304\n",
      "iteration 17 current loss: 1.3705520629882812 current acc: 0.1366\n",
      "iteration 18 current loss: 0.9282610416412354 current acc: 0.1452\n",
      "iteration 19 current loss: 1.3809939622879028 current acc: 0.152\n",
      "iteration 20 current loss: 1.032548427581787 current acc: 0.1612\n",
      "iteration 21 current loss: 1.2843087911605835 current acc: 0.1674\n",
      "iteration 22 current loss: 1.2575955390930176 current acc: 0.1738\n",
      "iteration 23 current loss: 1.00222647190094 current acc: 0.1826\n",
      "iteration 24 current loss: 1.2118310928344727 current acc: 0.1902\n",
      "iteration 25 current loss: 1.1915490627288818 current acc: 0.1982\n",
      "iteration 26 current loss: 1.1883844137191772 current acc: 0.205\n",
      "iteration 27 current loss: 1.1409153938293457 current acc: 0.2126\n",
      "iteration 28 current loss: 1.1870330572128296 current acc: 0.2202\n",
      "iteration 29 current loss: 1.1260899305343628 current acc: 0.228\n",
      "iteration 30 current loss: 1.592423677444458 current acc: 0.2344\n",
      "iteration 31 current loss: 1.1322789192199707 current acc: 0.2416\n",
      "iteration 32 current loss: 1.3216769695281982 current acc: 0.2478\n",
      "iteration 33 current loss: 1.4858933687210083 current acc: 0.2546\n",
      "iteration 34 current loss: 1.19008207321167 current acc: 0.2626\n",
      "iteration 35 current loss: 0.9547103643417358 current acc: 0.2704\n",
      "iteration 36 current loss: 1.0436978340148926 current acc: 0.2782\n",
      "iteration 37 current loss: 1.396716594696045 current acc: 0.2848\n",
      "iteration 38 current loss: 0.9748737215995789 current acc: 0.2936\n",
      "iteration 39 current loss: 0.9212242364883423 current acc: 0.3014\n",
      "iteration 40 current loss: 0.9644686579704285 current acc: 0.3096\n",
      "iteration 41 current loss: 1.110315203666687 current acc: 0.3174\n",
      "iteration 42 current loss: 0.8066827058792114 current acc: 0.3264\n",
      "iteration 43 current loss: 0.9261215329170227 current acc: 0.3342\n",
      "iteration 44 current loss: 0.9893978834152222 current acc: 0.3432\n",
      "iteration 45 current loss: 1.02390718460083 current acc: 0.352\n",
      "iteration 46 current loss: 1.388026237487793 current acc: 0.3578\n",
      "iteration 47 current loss: 0.9753225445747375 current acc: 0.3668\n",
      "iteration 48 current loss: 1.245259404182434 current acc: 0.3744\n",
      "iteration 49 current loss: 1.1739102602005005 current acc: 0.3816\n",
      "iteration 50 current loss: 1.110514521598816 current acc: 0.3894\n",
      "iteration 51 current loss: 1.1066616773605347 current acc: 0.3964\n",
      "iteration 52 current loss: 1.1280949115753174 current acc: 0.4042\n",
      "iteration 53 current loss: 1.0934245586395264 current acc: 0.4122\n",
      "iteration 54 current loss: 0.8927302956581116 current acc: 0.4208\n",
      "iteration 55 current loss: 1.0500408411026 current acc: 0.4282\n",
      "iteration 56 current loss: 1.2471773624420166 current acc: 0.4352\n",
      "iteration 57 current loss: 1.513259768486023 current acc: 0.4408\n",
      "iteration 58 current loss: 0.9947052597999573 current acc: 0.4494\n",
      "iteration 59 current loss: 1.3073861598968506 current acc: 0.4548\n",
      "iteration 60 current loss: 1.2628734111785889 current acc: 0.4622\n",
      "iteration 61 current loss: 1.1935561895370483 current acc: 0.47\n",
      "iteration 62 current loss: 0.9499607682228088 current acc: 0.478\n",
      "iteration 63 current loss: 0.862730085849762 current acc: 0.4874\n",
      "iteration 64 current loss: 1.4217488765716553 current acc: 0.4932\n",
      "iteration 65 current loss: 1.339509129524231 current acc: 0.4992\n",
      "iteration 66 current loss: 1.1491450071334839 current acc: 0.5072\n",
      "iteration 67 current loss: 1.323090672492981 current acc: 0.5136\n",
      "iteration 68 current loss: 1.3866496086120605 current acc: 0.5206\n",
      "iteration 69 current loss: 1.1186375617980957 current acc: 0.5286\n",
      "iteration 70 current loss: 1.1102542877197266 current acc: 0.5356\n",
      "iteration 71 current loss: 1.2615011930465698 current acc: 0.5424\n",
      "iteration 72 current loss: 1.3447002172470093 current acc: 0.5482\n",
      "iteration 73 current loss: 1.1905559301376343 current acc: 0.5552\n",
      "iteration 74 current loss: 1.431957483291626 current acc: 0.561\n",
      "iteration 75 current loss: 1.3290135860443115 current acc: 0.5678\n",
      "iteration 76 current loss: 1.2152767181396484 current acc: 0.5744\n",
      "iteration 77 current loss: 1.292971134185791 current acc: 0.5816\n",
      "iteration 78 current loss: 2.640145778656006 current acc: 0.582\n",
      "\t\tTrain Epoch 19/100,Train Accuracy: 0.582, Train Loss: 1.1725195352035234.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 19/100, Validation Accuracy: 0.489375, Validation Loss: 1.4810963764190674\n",
      "iteration 0 current loss: 1.14506995677948 current acc: 0.0076\n",
      "iteration 1 current loss: 1.0756491422653198 current acc: 0.0148\n",
      "iteration 2 current loss: 0.9550013542175293 current acc: 0.0228\n",
      "iteration 3 current loss: 1.1661545038223267 current acc: 0.0292\n",
      "iteration 4 current loss: 1.187515377998352 current acc: 0.0364\n",
      "iteration 5 current loss: 0.984939694404602 current acc: 0.0448\n",
      "iteration 6 current loss: 0.9440943002700806 current acc: 0.0538\n",
      "iteration 7 current loss: 0.9267081022262573 current acc: 0.063\n",
      "iteration 8 current loss: 1.3175610303878784 current acc: 0.0702\n",
      "iteration 9 current loss: 1.0581730604171753 current acc: 0.0774\n",
      "iteration 10 current loss: 0.7421395778656006 current acc: 0.0872\n",
      "iteration 11 current loss: 0.8665220141410828 current acc: 0.0962\n",
      "iteration 12 current loss: 1.3450754880905151 current acc: 0.1028\n",
      "iteration 13 current loss: 1.0358422994613647 current acc: 0.1108\n",
      "iteration 14 current loss: 1.1123578548431396 current acc: 0.119\n",
      "iteration 15 current loss: 1.5949095487594604 current acc: 0.1242\n",
      "iteration 16 current loss: 0.969416081905365 current acc: 0.1328\n",
      "iteration 17 current loss: 1.0867788791656494 current acc: 0.1414\n",
      "iteration 18 current loss: 1.02497398853302 current acc: 0.1494\n",
      "iteration 19 current loss: 1.3337297439575195 current acc: 0.1558\n",
      "iteration 20 current loss: 1.3364169597625732 current acc: 0.1634\n",
      "iteration 21 current loss: 1.4525277614593506 current acc: 0.1694\n",
      "iteration 22 current loss: 1.060477614402771 current acc: 0.1778\n",
      "iteration 23 current loss: 1.0941660404205322 current acc: 0.1858\n",
      "iteration 24 current loss: 1.3349990844726562 current acc: 0.1914\n",
      "iteration 25 current loss: 1.1259061098098755 current acc: 0.1984\n",
      "iteration 26 current loss: 1.01799476146698 current acc: 0.2058\n",
      "iteration 27 current loss: 1.3700662851333618 current acc: 0.2124\n",
      "iteration 28 current loss: 1.096384882926941 current acc: 0.2204\n",
      "iteration 29 current loss: 1.1892672777175903 current acc: 0.2278\n",
      "iteration 30 current loss: 0.9240120053291321 current acc: 0.237\n",
      "iteration 31 current loss: 1.083191156387329 current acc: 0.245\n",
      "iteration 32 current loss: 1.0045056343078613 current acc: 0.2536\n",
      "iteration 33 current loss: 1.0100512504577637 current acc: 0.2614\n",
      "iteration 34 current loss: 1.1829490661621094 current acc: 0.2694\n",
      "iteration 35 current loss: 1.3401432037353516 current acc: 0.2764\n",
      "iteration 36 current loss: 1.0743870735168457 current acc: 0.2838\n",
      "iteration 37 current loss: 1.0895862579345703 current acc: 0.292\n",
      "iteration 38 current loss: 1.12335205078125 current acc: 0.2992\n",
      "iteration 39 current loss: 1.026196002960205 current acc: 0.3066\n",
      "iteration 40 current loss: 1.0489391088485718 current acc: 0.315\n",
      "iteration 41 current loss: 1.0243899822235107 current acc: 0.3234\n",
      "iteration 42 current loss: 1.0689976215362549 current acc: 0.331\n",
      "iteration 43 current loss: 1.1532795429229736 current acc: 0.339\n",
      "iteration 44 current loss: 1.2915159463882446 current acc: 0.3466\n",
      "iteration 45 current loss: 1.063341736793518 current acc: 0.354\n",
      "iteration 46 current loss: 0.9014671444892883 current acc: 0.3634\n",
      "iteration 47 current loss: 1.1967668533325195 current acc: 0.37\n",
      "iteration 48 current loss: 1.4204280376434326 current acc: 0.3766\n",
      "iteration 49 current loss: 1.3290283679962158 current acc: 0.3828\n",
      "iteration 50 current loss: 1.0223946571350098 current acc: 0.3906\n",
      "iteration 51 current loss: 1.1456594467163086 current acc: 0.3988\n",
      "iteration 52 current loss: 1.2635915279388428 current acc: 0.4062\n",
      "iteration 53 current loss: 1.3769468069076538 current acc: 0.4134\n",
      "iteration 54 current loss: 1.3393633365631104 current acc: 0.4204\n",
      "iteration 55 current loss: 1.1639024019241333 current acc: 0.4274\n",
      "iteration 56 current loss: 0.9647940993309021 current acc: 0.435\n",
      "iteration 57 current loss: 1.2826958894729614 current acc: 0.443\n",
      "iteration 58 current loss: 1.2027958631515503 current acc: 0.4502\n",
      "iteration 59 current loss: 1.0929747819900513 current acc: 0.4578\n",
      "iteration 60 current loss: 1.2813644409179688 current acc: 0.4636\n",
      "iteration 61 current loss: 1.1264210939407349 current acc: 0.4706\n",
      "iteration 62 current loss: 0.8600760698318481 current acc: 0.479\n",
      "iteration 63 current loss: 1.0787957906723022 current acc: 0.4868\n",
      "iteration 64 current loss: 1.0346800088882446 current acc: 0.4952\n",
      "iteration 65 current loss: 0.8798078298568726 current acc: 0.5034\n",
      "iteration 66 current loss: 1.1132107973098755 current acc: 0.5114\n",
      "iteration 67 current loss: 1.1681240797042847 current acc: 0.519\n",
      "iteration 68 current loss: 1.1355763673782349 current acc: 0.5268\n",
      "iteration 69 current loss: 1.4967477321624756 current acc: 0.5318\n",
      "iteration 70 current loss: 1.2506085634231567 current acc: 0.5392\n",
      "iteration 71 current loss: 1.2119576930999756 current acc: 0.5456\n",
      "iteration 72 current loss: 0.8818937540054321 current acc: 0.5542\n",
      "iteration 73 current loss: 1.2359001636505127 current acc: 0.5616\n",
      "iteration 74 current loss: 0.9265967011451721 current acc: 0.57\n",
      "iteration 75 current loss: 1.3124655485153198 current acc: 0.5762\n",
      "iteration 76 current loss: 1.0717204809188843 current acc: 0.5834\n",
      "iteration 77 current loss: 1.2665684223175049 current acc: 0.59\n",
      "iteration 78 current loss: 0.9539014101028442 current acc: 0.5912\n",
      "\t\tTrain Epoch 20/100,Train Accuracy: 0.5912, Train Loss: 1.1322643364532083.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 20/100, Validation Accuracy: 0.487, Validation Loss: 1.4699299259185792\n",
      "iteration 0 current loss: 1.0674885511398315 current acc: 0.008\n",
      "iteration 1 current loss: 1.0729268789291382 current acc: 0.016\n",
      "iteration 2 current loss: 1.0922836065292358 current acc: 0.0238\n",
      "iteration 3 current loss: 0.9343804121017456 current acc: 0.0324\n",
      "iteration 4 current loss: 1.4206842184066772 current acc: 0.039\n",
      "iteration 5 current loss: 1.097216010093689 current acc: 0.0468\n",
      "iteration 6 current loss: 1.000360131263733 current acc: 0.0552\n",
      "iteration 7 current loss: 1.1445162296295166 current acc: 0.0624\n",
      "iteration 8 current loss: 1.081123948097229 current acc: 0.0702\n",
      "iteration 9 current loss: 1.1701716184616089 current acc: 0.0778\n",
      "iteration 10 current loss: 1.1746913194656372 current acc: 0.085\n",
      "iteration 11 current loss: 1.308346152305603 current acc: 0.0916\n",
      "iteration 12 current loss: 0.8440433144569397 current acc: 0.101\n",
      "iteration 13 current loss: 1.1544984579086304 current acc: 0.1086\n",
      "iteration 14 current loss: 0.9752452373504639 current acc: 0.1164\n",
      "iteration 15 current loss: 1.0508267879486084 current acc: 0.1238\n",
      "iteration 16 current loss: 1.0789439678192139 current acc: 0.1318\n",
      "iteration 17 current loss: 0.9207704067230225 current acc: 0.14\n",
      "iteration 18 current loss: 1.1017574071884155 current acc: 0.1486\n",
      "iteration 19 current loss: 1.091076374053955 current acc: 0.156\n",
      "iteration 20 current loss: 0.9494215250015259 current acc: 0.1638\n",
      "iteration 21 current loss: 0.9882586598396301 current acc: 0.1722\n",
      "iteration 22 current loss: 1.264660120010376 current acc: 0.1804\n",
      "iteration 23 current loss: 1.1319893598556519 current acc: 0.188\n",
      "iteration 24 current loss: 0.8701269626617432 current acc: 0.197\n",
      "iteration 25 current loss: 0.9461105465888977 current acc: 0.2054\n",
      "iteration 26 current loss: 1.442287564277649 current acc: 0.2112\n",
      "iteration 27 current loss: 1.069777250289917 current acc: 0.219\n",
      "iteration 28 current loss: 0.9207139015197754 current acc: 0.2274\n",
      "iteration 29 current loss: 1.249471664428711 current acc: 0.2342\n",
      "iteration 30 current loss: 0.9881373643875122 current acc: 0.2426\n",
      "iteration 31 current loss: 1.3658360242843628 current acc: 0.249\n",
      "iteration 32 current loss: 1.2525286674499512 current acc: 0.2564\n",
      "iteration 33 current loss: 1.0547099113464355 current acc: 0.2634\n",
      "iteration 34 current loss: 1.2114742994308472 current acc: 0.2712\n",
      "iteration 35 current loss: 1.144443154335022 current acc: 0.2784\n",
      "iteration 36 current loss: 0.9460681080818176 current acc: 0.2868\n",
      "iteration 37 current loss: 0.9618374705314636 current acc: 0.2948\n",
      "iteration 38 current loss: 0.9620330929756165 current acc: 0.3024\n",
      "iteration 39 current loss: 0.8332350850105286 current acc: 0.3106\n",
      "iteration 40 current loss: 1.1747463941574097 current acc: 0.3182\n",
      "iteration 41 current loss: 0.989698052406311 current acc: 0.3272\n",
      "iteration 42 current loss: 0.9269252419471741 current acc: 0.3358\n",
      "iteration 43 current loss: 1.1165797710418701 current acc: 0.343\n",
      "iteration 44 current loss: 0.9763819575309753 current acc: 0.3516\n",
      "iteration 45 current loss: 1.0246943235397339 current acc: 0.3588\n",
      "iteration 46 current loss: 1.0957074165344238 current acc: 0.3668\n",
      "iteration 47 current loss: 1.046286702156067 current acc: 0.3752\n",
      "iteration 48 current loss: 1.0226125717163086 current acc: 0.3836\n",
      "iteration 49 current loss: 1.2210476398468018 current acc: 0.3906\n",
      "iteration 50 current loss: 1.0328196287155151 current acc: 0.3984\n",
      "iteration 51 current loss: 1.0025594234466553 current acc: 0.4068\n",
      "iteration 52 current loss: 1.2629942893981934 current acc: 0.4134\n",
      "iteration 53 current loss: 1.0295637845993042 current acc: 0.4216\n",
      "iteration 54 current loss: 1.0415955781936646 current acc: 0.4294\n",
      "iteration 55 current loss: 0.9878925681114197 current acc: 0.4378\n",
      "iteration 56 current loss: 0.8948058485984802 current acc: 0.446\n",
      "iteration 57 current loss: 1.1042473316192627 current acc: 0.4536\n",
      "iteration 58 current loss: 1.0312649011611938 current acc: 0.462\n",
      "iteration 59 current loss: 1.1725702285766602 current acc: 0.4696\n",
      "iteration 60 current loss: 1.0464245080947876 current acc: 0.4774\n",
      "iteration 61 current loss: 1.2983572483062744 current acc: 0.4842\n",
      "iteration 62 current loss: 1.1694260835647583 current acc: 0.4918\n",
      "iteration 63 current loss: 0.9513130187988281 current acc: 0.5004\n",
      "iteration 64 current loss: 1.2661619186401367 current acc: 0.5066\n",
      "iteration 65 current loss: 1.2901697158813477 current acc: 0.514\n",
      "iteration 66 current loss: 1.3005598783493042 current acc: 0.5212\n",
      "iteration 67 current loss: 1.2517350912094116 current acc: 0.528\n",
      "iteration 68 current loss: 1.378389835357666 current acc: 0.5352\n",
      "iteration 69 current loss: 1.2249114513397217 current acc: 0.5416\n",
      "iteration 70 current loss: 1.0240106582641602 current acc: 0.5496\n",
      "iteration 71 current loss: 1.2429449558258057 current acc: 0.5558\n",
      "iteration 72 current loss: 1.0735375881195068 current acc: 0.5634\n",
      "iteration 73 current loss: 1.249740719795227 current acc: 0.5692\n",
      "iteration 74 current loss: 1.1467928886413574 current acc: 0.5772\n",
      "iteration 75 current loss: 1.3422666788101196 current acc: 0.5836\n",
      "iteration 76 current loss: 1.2084150314331055 current acc: 0.5908\n",
      "iteration 77 current loss: 1.318433165550232 current acc: 0.5978\n",
      "iteration 78 current loss: 1.4875175952911377 current acc: 0.5986\n",
      "\t\tTrain Epoch 21/100,Train Accuracy: 0.5986, Train Loss: 1.1108933347689955.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 21/100, Validation Accuracy: 0.475625, Validation Loss: 1.5093644542694091\n",
      "best loss 1.1108933347689955\n",
      "iteration 0 current loss: 1.0697177648544312 current acc: 0.0072\n",
      "iteration 1 current loss: 0.8968992233276367 current acc: 0.0162\n",
      "iteration 2 current loss: 1.3854049444198608 current acc: 0.0226\n",
      "iteration 3 current loss: 1.1835871934890747 current acc: 0.0308\n",
      "iteration 4 current loss: 1.1920727491378784 current acc: 0.0378\n",
      "iteration 5 current loss: 1.0889747142791748 current acc: 0.0456\n",
      "iteration 6 current loss: 1.302200198173523 current acc: 0.0522\n",
      "iteration 7 current loss: 1.068772315979004 current acc: 0.0606\n",
      "iteration 8 current loss: 1.007811188697815 current acc: 0.0686\n",
      "iteration 9 current loss: 1.146312952041626 current acc: 0.0766\n",
      "iteration 10 current loss: 1.1549299955368042 current acc: 0.0842\n",
      "iteration 11 current loss: 1.3156498670578003 current acc: 0.0908\n",
      "iteration 12 current loss: 1.2057780027389526 current acc: 0.0974\n",
      "iteration 13 current loss: 0.9640951156616211 current acc: 0.106\n",
      "iteration 14 current loss: 1.0844160318374634 current acc: 0.1134\n",
      "iteration 15 current loss: 1.2713594436645508 current acc: 0.1194\n",
      "iteration 16 current loss: 1.022249698638916 current acc: 0.1276\n",
      "iteration 17 current loss: 1.0473641157150269 current acc: 0.1356\n",
      "iteration 18 current loss: 0.9957354664802551 current acc: 0.1434\n",
      "iteration 19 current loss: 1.2587616443634033 current acc: 0.1508\n",
      "iteration 20 current loss: 0.9329823851585388 current acc: 0.1594\n",
      "iteration 21 current loss: 1.0146470069885254 current acc: 0.1664\n",
      "iteration 22 current loss: 0.8860599994659424 current acc: 0.1756\n",
      "iteration 23 current loss: 1.084314227104187 current acc: 0.1834\n",
      "iteration 24 current loss: 1.019369125366211 current acc: 0.1912\n",
      "iteration 25 current loss: 1.143329381942749 current acc: 0.1984\n",
      "iteration 26 current loss: 1.2115230560302734 current acc: 0.2056\n",
      "iteration 27 current loss: 1.4111754894256592 current acc: 0.212\n",
      "iteration 28 current loss: 1.1470752954483032 current acc: 0.219\n",
      "iteration 29 current loss: 1.1554874181747437 current acc: 0.2266\n",
      "iteration 30 current loss: 1.2442721128463745 current acc: 0.2332\n",
      "iteration 31 current loss: 1.0288180112838745 current acc: 0.2412\n",
      "iteration 32 current loss: 0.9970536828041077 current acc: 0.2496\n",
      "iteration 33 current loss: 1.2825075387954712 current acc: 0.2572\n",
      "iteration 34 current loss: 1.1239674091339111 current acc: 0.265\n",
      "iteration 35 current loss: 1.1320909261703491 current acc: 0.2716\n",
      "iteration 36 current loss: 1.050916314125061 current acc: 0.2796\n",
      "iteration 37 current loss: 1.1679576635360718 current acc: 0.2876\n",
      "iteration 38 current loss: 1.074600100517273 current acc: 0.2952\n",
      "iteration 39 current loss: 1.1055963039398193 current acc: 0.303\n",
      "iteration 40 current loss: 1.1018507480621338 current acc: 0.3118\n",
      "iteration 41 current loss: 1.1511136293411255 current acc: 0.319\n",
      "iteration 42 current loss: 0.9979670643806458 current acc: 0.327\n",
      "iteration 43 current loss: 1.2070682048797607 current acc: 0.3352\n",
      "iteration 44 current loss: 1.3104243278503418 current acc: 0.3422\n",
      "iteration 45 current loss: 0.994111955165863 current acc: 0.3498\n",
      "iteration 46 current loss: 0.8797937631607056 current acc: 0.358\n",
      "iteration 47 current loss: 1.1630005836486816 current acc: 0.365\n",
      "iteration 48 current loss: 1.0943292379379272 current acc: 0.373\n",
      "iteration 49 current loss: 1.1857730150222778 current acc: 0.3802\n",
      "iteration 50 current loss: 0.9400569200515747 current acc: 0.389\n",
      "iteration 51 current loss: 1.2493573427200317 current acc: 0.3958\n",
      "iteration 52 current loss: 1.2473427057266235 current acc: 0.4034\n",
      "iteration 53 current loss: 1.0041285753250122 current acc: 0.4118\n",
      "iteration 54 current loss: 0.9789879322052002 current acc: 0.4198\n",
      "iteration 55 current loss: 1.2060128450393677 current acc: 0.4278\n",
      "iteration 56 current loss: 1.0379939079284668 current acc: 0.436\n",
      "iteration 57 current loss: 1.113197922706604 current acc: 0.4434\n",
      "iteration 58 current loss: 0.9760164022445679 current acc: 0.4528\n",
      "iteration 59 current loss: 1.032750129699707 current acc: 0.4616\n",
      "iteration 60 current loss: 1.169929027557373 current acc: 0.469\n",
      "iteration 61 current loss: 1.1183890104293823 current acc: 0.477\n",
      "iteration 62 current loss: 0.9089895486831665 current acc: 0.4858\n",
      "iteration 63 current loss: 1.1530840396881104 current acc: 0.493\n",
      "iteration 64 current loss: 0.8808582425117493 current acc: 0.5022\n",
      "iteration 65 current loss: 1.411995768547058 current acc: 0.5086\n",
      "iteration 66 current loss: 0.9156932830810547 current acc: 0.5176\n",
      "iteration 67 current loss: 0.9471307396888733 current acc: 0.5258\n",
      "iteration 68 current loss: 1.0428129434585571 current acc: 0.5334\n",
      "iteration 69 current loss: 1.0132899284362793 current acc: 0.5418\n",
      "iteration 70 current loss: 1.0598342418670654 current acc: 0.5504\n",
      "iteration 71 current loss: 1.1606779098510742 current acc: 0.5566\n",
      "iteration 72 current loss: 1.1891870498657227 current acc: 0.5632\n",
      "iteration 73 current loss: 1.2229565382003784 current acc: 0.5704\n",
      "iteration 74 current loss: 1.0297646522521973 current acc: 0.578\n",
      "iteration 75 current loss: 1.0671170949935913 current acc: 0.5866\n",
      "iteration 76 current loss: 1.3322217464447021 current acc: 0.5936\n",
      "iteration 77 current loss: 1.051701307296753 current acc: 0.6012\n",
      "iteration 78 current loss: 0.7306742668151855 current acc: 0.6026\n",
      "\t\tTrain Epoch 22/100,Train Accuracy: 0.6026, Train Loss: 1.1032078560394576.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 22/100, Validation Accuracy: 0.50725, Validation Loss: 1.386270586013794\n",
      "best loss 1.1032078560394576\n",
      "iteration 0 current loss: 0.938977837562561 current acc: 0.0086\n",
      "iteration 1 current loss: 0.8379470109939575 current acc: 0.018\n",
      "iteration 2 current loss: 0.8058465123176575 current acc: 0.0272\n",
      "iteration 3 current loss: 1.142083764076233 current acc: 0.0352\n",
      "iteration 4 current loss: 0.9583818912506104 current acc: 0.0436\n",
      "iteration 5 current loss: 0.8569594621658325 current acc: 0.0526\n",
      "iteration 6 current loss: 0.8200609087944031 current acc: 0.062\n",
      "iteration 7 current loss: 1.0001450777053833 current acc: 0.0698\n",
      "iteration 8 current loss: 0.7515632510185242 current acc: 0.0798\n",
      "iteration 9 current loss: 1.055928349494934 current acc: 0.0886\n",
      "iteration 10 current loss: 0.9713732600212097 current acc: 0.0976\n",
      "iteration 11 current loss: 1.1558045148849487 current acc: 0.1058\n",
      "iteration 12 current loss: 1.1892181634902954 current acc: 0.1128\n",
      "iteration 13 current loss: 1.1631993055343628 current acc: 0.1214\n",
      "iteration 14 current loss: 0.9471951127052307 current acc: 0.1288\n",
      "iteration 15 current loss: 1.1997135877609253 current acc: 0.1372\n",
      "iteration 16 current loss: 0.8997613191604614 current acc: 0.1452\n",
      "iteration 17 current loss: 0.9562010169029236 current acc: 0.1532\n",
      "iteration 18 current loss: 0.9718579053878784 current acc: 0.1616\n",
      "iteration 19 current loss: 0.9344525933265686 current acc: 0.1704\n",
      "iteration 20 current loss: 1.125227928161621 current acc: 0.1778\n",
      "iteration 21 current loss: 1.2403182983398438 current acc: 0.1848\n",
      "iteration 22 current loss: 0.9244721531867981 current acc: 0.1946\n",
      "iteration 23 current loss: 1.190773844718933 current acc: 0.2012\n",
      "iteration 24 current loss: 1.0645780563354492 current acc: 0.209\n",
      "iteration 25 current loss: 1.3831144571304321 current acc: 0.2164\n",
      "iteration 26 current loss: 1.0025618076324463 current acc: 0.2248\n",
      "iteration 27 current loss: 1.2092640399932861 current acc: 0.2326\n",
      "iteration 28 current loss: 0.850242555141449 current acc: 0.2416\n",
      "iteration 29 current loss: 0.9992944598197937 current acc: 0.25\n",
      "iteration 30 current loss: 0.9813048243522644 current acc: 0.2582\n",
      "iteration 31 current loss: 0.9648913741111755 current acc: 0.2662\n",
      "iteration 32 current loss: 1.2198536396026611 current acc: 0.2738\n",
      "iteration 33 current loss: 1.08402681350708 current acc: 0.2818\n",
      "iteration 34 current loss: 1.0604588985443115 current acc: 0.289\n",
      "iteration 35 current loss: 1.2274659872055054 current acc: 0.2956\n",
      "iteration 36 current loss: 1.1656179428100586 current acc: 0.3026\n",
      "iteration 37 current loss: 0.8613702654838562 current acc: 0.3112\n",
      "iteration 38 current loss: 1.0831750631332397 current acc: 0.3196\n",
      "iteration 39 current loss: 1.0179616212844849 current acc: 0.3274\n",
      "iteration 40 current loss: 1.2019579410552979 current acc: 0.3346\n",
      "iteration 41 current loss: 0.8971975445747375 current acc: 0.3426\n",
      "iteration 42 current loss: 1.1031603813171387 current acc: 0.3508\n",
      "iteration 43 current loss: 1.314045786857605 current acc: 0.357\n",
      "iteration 44 current loss: 1.2731819152832031 current acc: 0.3634\n",
      "iteration 45 current loss: 0.9926324486732483 current acc: 0.3718\n",
      "iteration 46 current loss: 1.1049867868423462 current acc: 0.3796\n",
      "iteration 47 current loss: 0.996441125869751 current acc: 0.3874\n",
      "iteration 48 current loss: 1.1536298990249634 current acc: 0.3952\n",
      "iteration 49 current loss: 1.0396355390548706 current acc: 0.4028\n",
      "iteration 50 current loss: 1.1019020080566406 current acc: 0.4104\n",
      "iteration 51 current loss: 0.9900651574134827 current acc: 0.4196\n",
      "iteration 52 current loss: 1.2793611288070679 current acc: 0.4266\n",
      "iteration 53 current loss: 1.1211732625961304 current acc: 0.4346\n",
      "iteration 54 current loss: 1.2722036838531494 current acc: 0.4416\n",
      "iteration 55 current loss: 1.270967721939087 current acc: 0.4484\n",
      "iteration 56 current loss: 0.9994426965713501 current acc: 0.457\n",
      "iteration 57 current loss: 1.04499089717865 current acc: 0.4646\n",
      "iteration 58 current loss: 0.8892219066619873 current acc: 0.4726\n",
      "iteration 59 current loss: 1.1363943815231323 current acc: 0.4804\n",
      "iteration 60 current loss: 1.1119418144226074 current acc: 0.4882\n",
      "iteration 61 current loss: 1.0301551818847656 current acc: 0.496\n",
      "iteration 62 current loss: 1.0916297435760498 current acc: 0.5032\n",
      "iteration 63 current loss: 1.1070448160171509 current acc: 0.5106\n",
      "iteration 64 current loss: 1.2164329290390015 current acc: 0.5176\n",
      "iteration 65 current loss: 1.0551623106002808 current acc: 0.5254\n",
      "iteration 66 current loss: 1.0132851600646973 current acc: 0.5324\n",
      "iteration 67 current loss: 0.9181613922119141 current acc: 0.5408\n",
      "iteration 68 current loss: 0.9795163869857788 current acc: 0.5488\n",
      "iteration 69 current loss: 0.9181544780731201 current acc: 0.5574\n",
      "iteration 70 current loss: 1.4077608585357666 current acc: 0.5626\n",
      "iteration 71 current loss: 1.2851536273956299 current acc: 0.5694\n",
      "iteration 72 current loss: 1.1414434909820557 current acc: 0.5764\n",
      "iteration 73 current loss: 1.1911801099777222 current acc: 0.583\n",
      "iteration 74 current loss: 1.2091079950332642 current acc: 0.5904\n",
      "iteration 75 current loss: 1.0658912658691406 current acc: 0.5984\n",
      "iteration 76 current loss: 1.208799958229065 current acc: 0.6054\n",
      "iteration 77 current loss: 1.2436765432357788 current acc: 0.612\n",
      "iteration 78 current loss: 1.5505261421203613 current acc: 0.6128\n",
      "\t\tTrain Epoch 23/100,Train Accuracy: 0.6128, Train Loss: 1.077724826486805.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 23/100, Validation Accuracy: 0.470875, Validation Loss: 1.484808843612671\n",
      "best loss 1.077724826486805\n",
      "iteration 0 current loss: 1.1868867874145508 current acc: 0.0078\n",
      "iteration 1 current loss: 1.0323741436004639 current acc: 0.016\n",
      "iteration 2 current loss: 1.082900047302246 current acc: 0.0236\n",
      "iteration 3 current loss: 0.9664100408554077 current acc: 0.0318\n",
      "iteration 4 current loss: 0.993569016456604 current acc: 0.0402\n",
      "iteration 5 current loss: 1.0389995574951172 current acc: 0.0484\n",
      "iteration 6 current loss: 1.0987963676452637 current acc: 0.056\n",
      "iteration 7 current loss: 1.13032865524292 current acc: 0.0634\n",
      "iteration 8 current loss: 1.1798505783081055 current acc: 0.071\n",
      "iteration 9 current loss: 1.0998377799987793 current acc: 0.0786\n",
      "iteration 10 current loss: 1.0451905727386475 current acc: 0.0866\n",
      "iteration 11 current loss: 0.9169523119926453 current acc: 0.0954\n",
      "iteration 12 current loss: 0.8739302754402161 current acc: 0.104\n",
      "iteration 13 current loss: 1.0268566608428955 current acc: 0.1116\n",
      "iteration 14 current loss: 1.0721051692962646 current acc: 0.12\n",
      "iteration 15 current loss: 1.0393205881118774 current acc: 0.1276\n",
      "iteration 16 current loss: 1.036746859550476 current acc: 0.1366\n",
      "iteration 17 current loss: 1.2957196235656738 current acc: 0.1432\n",
      "iteration 18 current loss: 1.1116979122161865 current acc: 0.1508\n",
      "iteration 19 current loss: 1.1904066801071167 current acc: 0.1584\n",
      "iteration 20 current loss: 1.0194289684295654 current acc: 0.1658\n",
      "iteration 21 current loss: 1.0729514360427856 current acc: 0.1728\n",
      "iteration 22 current loss: 1.165962815284729 current acc: 0.1804\n",
      "iteration 23 current loss: 0.9843396544456482 current acc: 0.1884\n",
      "iteration 24 current loss: 1.289703130722046 current acc: 0.1936\n",
      "iteration 25 current loss: 1.0423943996429443 current acc: 0.2014\n",
      "iteration 26 current loss: 1.1322293281555176 current acc: 0.2098\n",
      "iteration 27 current loss: 1.2035231590270996 current acc: 0.2178\n",
      "iteration 28 current loss: 0.9617308974266052 current acc: 0.2262\n",
      "iteration 29 current loss: 1.0615910291671753 current acc: 0.2346\n",
      "iteration 30 current loss: 1.0224202871322632 current acc: 0.2428\n",
      "iteration 31 current loss: 1.2357478141784668 current acc: 0.25\n",
      "iteration 32 current loss: 1.0926865339279175 current acc: 0.2574\n",
      "iteration 33 current loss: 0.9022229313850403 current acc: 0.2662\n",
      "iteration 34 current loss: 1.024016261100769 current acc: 0.2744\n",
      "iteration 35 current loss: 1.1905524730682373 current acc: 0.281\n",
      "iteration 36 current loss: 1.041935682296753 current acc: 0.2894\n",
      "iteration 37 current loss: 0.9862114787101746 current acc: 0.2976\n",
      "iteration 38 current loss: 1.0681953430175781 current acc: 0.3054\n",
      "iteration 39 current loss: 1.105719804763794 current acc: 0.3124\n",
      "iteration 40 current loss: 1.0452961921691895 current acc: 0.3204\n",
      "iteration 41 current loss: 0.9249883890151978 current acc: 0.3294\n",
      "iteration 42 current loss: 1.0213323831558228 current acc: 0.3376\n",
      "iteration 43 current loss: 1.050303339958191 current acc: 0.346\n",
      "iteration 44 current loss: 1.1463626623153687 current acc: 0.3536\n",
      "iteration 45 current loss: 1.1645265817642212 current acc: 0.3616\n",
      "iteration 46 current loss: 1.0644208192825317 current acc: 0.369\n",
      "iteration 47 current loss: 1.0470404624938965 current acc: 0.376\n",
      "iteration 48 current loss: 1.0196775197982788 current acc: 0.3838\n",
      "iteration 49 current loss: 1.1542540788650513 current acc: 0.3908\n",
      "iteration 50 current loss: 1.2991876602172852 current acc: 0.3974\n",
      "iteration 51 current loss: 0.8783518671989441 current acc: 0.4068\n",
      "iteration 52 current loss: 1.0384730100631714 current acc: 0.415\n",
      "iteration 53 current loss: 1.0943137407302856 current acc: 0.4226\n",
      "iteration 54 current loss: 1.0694324970245361 current acc: 0.4306\n",
      "iteration 55 current loss: 1.0134682655334473 current acc: 0.4382\n",
      "iteration 56 current loss: 0.7989395260810852 current acc: 0.4478\n",
      "iteration 57 current loss: 1.0049197673797607 current acc: 0.4564\n",
      "iteration 58 current loss: 1.1829643249511719 current acc: 0.4642\n",
      "iteration 59 current loss: 0.8590088486671448 current acc: 0.4726\n",
      "iteration 60 current loss: 1.0643900632858276 current acc: 0.4802\n",
      "iteration 61 current loss: 0.8788127303123474 current acc: 0.489\n",
      "iteration 62 current loss: 1.2524423599243164 current acc: 0.4954\n",
      "iteration 63 current loss: 1.0644667148590088 current acc: 0.5034\n",
      "iteration 64 current loss: 0.9357129335403442 current acc: 0.5112\n",
      "iteration 65 current loss: 0.8905368447303772 current acc: 0.5198\n",
      "iteration 66 current loss: 1.0766026973724365 current acc: 0.5274\n",
      "iteration 67 current loss: 1.1749980449676514 current acc: 0.5344\n",
      "iteration 68 current loss: 1.006912350654602 current acc: 0.5426\n",
      "iteration 69 current loss: 0.9403192400932312 current acc: 0.5514\n",
      "iteration 70 current loss: 1.0685292482376099 current acc: 0.5596\n",
      "iteration 71 current loss: 0.936072826385498 current acc: 0.5678\n",
      "iteration 72 current loss: 1.0018969774246216 current acc: 0.5764\n",
      "iteration 73 current loss: 1.0208377838134766 current acc: 0.5842\n",
      "iteration 74 current loss: 1.1432981491088867 current acc: 0.5916\n",
      "iteration 75 current loss: 1.070026159286499 current acc: 0.5986\n",
      "iteration 76 current loss: 1.2219657897949219 current acc: 0.6062\n",
      "iteration 77 current loss: 1.1103178262710571 current acc: 0.6138\n",
      "iteration 78 current loss: 0.6930108070373535 current acc: 0.6148\n",
      "\t\tTrain Epoch 24/100,Train Accuracy: 0.6148, Train Loss: 1.0563399308844457.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 24/100, Validation Accuracy: 0.49625, Validation Loss: 1.4187437248229982\n",
      "best loss 1.0563399308844457\n",
      "iteration 0 current loss: 0.9799982905387878 current acc: 0.0088\n",
      "iteration 1 current loss: 1.014587163925171 current acc: 0.017\n",
      "iteration 2 current loss: 0.9395771622657776 current acc: 0.0262\n",
      "iteration 3 current loss: 0.9221609234809875 current acc: 0.0354\n",
      "iteration 4 current loss: 1.1889888048171997 current acc: 0.0432\n",
      "iteration 5 current loss: 1.0601409673690796 current acc: 0.0506\n",
      "iteration 6 current loss: 1.2135525941848755 current acc: 0.0582\n",
      "iteration 7 current loss: 1.0166071653366089 current acc: 0.0666\n",
      "iteration 8 current loss: 1.0267075300216675 current acc: 0.0746\n",
      "iteration 9 current loss: 1.0770609378814697 current acc: 0.082\n",
      "iteration 10 current loss: 0.8772218227386475 current acc: 0.0904\n",
      "iteration 11 current loss: 0.9558979272842407 current acc: 0.098\n",
      "iteration 12 current loss: 1.2280046939849854 current acc: 0.105\n",
      "iteration 13 current loss: 0.8547261953353882 current acc: 0.1136\n",
      "iteration 14 current loss: 0.8498473763465881 current acc: 0.1228\n",
      "iteration 15 current loss: 1.1064952611923218 current acc: 0.1306\n",
      "iteration 16 current loss: 1.0193696022033691 current acc: 0.1378\n",
      "iteration 17 current loss: 0.9900287985801697 current acc: 0.1464\n",
      "iteration 18 current loss: 0.8169063925743103 current acc: 0.155\n",
      "iteration 19 current loss: 1.1365981101989746 current acc: 0.1638\n",
      "iteration 20 current loss: 0.9712533354759216 current acc: 0.1714\n",
      "iteration 21 current loss: 0.8643112182617188 current acc: 0.1802\n",
      "iteration 22 current loss: 0.8724979162216187 current acc: 0.189\n",
      "iteration 23 current loss: 0.9235334992408752 current acc: 0.198\n",
      "iteration 24 current loss: 1.1146392822265625 current acc: 0.2056\n",
      "iteration 25 current loss: 0.9709609746932983 current acc: 0.214\n",
      "iteration 26 current loss: 1.2029812335968018 current acc: 0.221\n",
      "iteration 27 current loss: 1.1226707696914673 current acc: 0.2284\n",
      "iteration 28 current loss: 1.080228328704834 current acc: 0.2364\n",
      "iteration 29 current loss: 0.7882596254348755 current acc: 0.2466\n",
      "iteration 30 current loss: 1.2035844326019287 current acc: 0.2536\n",
      "iteration 31 current loss: 0.8024179339408875 current acc: 0.263\n",
      "iteration 32 current loss: 1.018314242362976 current acc: 0.2708\n",
      "iteration 33 current loss: 1.4027631282806396 current acc: 0.277\n",
      "iteration 34 current loss: 1.1360464096069336 current acc: 0.2846\n",
      "iteration 35 current loss: 1.0738787651062012 current acc: 0.2924\n",
      "iteration 36 current loss: 0.9981528520584106 current acc: 0.3002\n",
      "iteration 37 current loss: 0.8948822617530823 current acc: 0.3088\n",
      "iteration 38 current loss: 1.0739060640335083 current acc: 0.3168\n",
      "iteration 39 current loss: 0.9741255044937134 current acc: 0.325\n",
      "iteration 40 current loss: 1.0005208253860474 current acc: 0.3328\n",
      "iteration 41 current loss: 1.021234393119812 current acc: 0.341\n",
      "iteration 42 current loss: 1.092481255531311 current acc: 0.3488\n",
      "iteration 43 current loss: 0.8704139590263367 current acc: 0.3574\n",
      "iteration 44 current loss: 1.0444645881652832 current acc: 0.3656\n",
      "iteration 45 current loss: 0.9337175488471985 current acc: 0.3752\n",
      "iteration 46 current loss: 0.7484043836593628 current acc: 0.385\n",
      "iteration 47 current loss: 1.0026967525482178 current acc: 0.3938\n",
      "iteration 48 current loss: 1.0249260663986206 current acc: 0.4016\n",
      "iteration 49 current loss: 0.9797595143318176 current acc: 0.4098\n",
      "iteration 50 current loss: 1.146480679512024 current acc: 0.4174\n",
      "iteration 51 current loss: 1.0612379312515259 current acc: 0.4254\n",
      "iteration 52 current loss: 0.9750440120697021 current acc: 0.4336\n",
      "iteration 53 current loss: 1.1894201040267944 current acc: 0.441\n",
      "iteration 54 current loss: 0.9441145062446594 current acc: 0.4496\n",
      "iteration 55 current loss: 1.0751302242279053 current acc: 0.4568\n",
      "iteration 56 current loss: 1.020943522453308 current acc: 0.4648\n",
      "iteration 57 current loss: 1.2706494331359863 current acc: 0.4716\n",
      "iteration 58 current loss: 1.227877140045166 current acc: 0.4798\n",
      "iteration 59 current loss: 1.135125994682312 current acc: 0.487\n",
      "iteration 60 current loss: 1.2490158081054688 current acc: 0.4936\n",
      "iteration 61 current loss: 1.0874667167663574 current acc: 0.5016\n",
      "iteration 62 current loss: 1.1334892511367798 current acc: 0.5086\n",
      "iteration 63 current loss: 1.002140760421753 current acc: 0.5164\n",
      "iteration 64 current loss: 0.9838776588439941 current acc: 0.5246\n",
      "iteration 65 current loss: 1.2486532926559448 current acc: 0.532\n",
      "iteration 66 current loss: 0.90786212682724 current acc: 0.5406\n",
      "iteration 67 current loss: 1.1901534795761108 current acc: 0.5484\n",
      "iteration 68 current loss: 1.1083130836486816 current acc: 0.5562\n",
      "iteration 69 current loss: 1.0173882246017456 current acc: 0.563\n",
      "iteration 70 current loss: 0.9206148386001587 current acc: 0.5718\n",
      "iteration 71 current loss: 1.0672096014022827 current acc: 0.5798\n",
      "iteration 72 current loss: 1.0284416675567627 current acc: 0.5876\n",
      "iteration 73 current loss: 1.002243995666504 current acc: 0.5958\n",
      "iteration 74 current loss: 1.0786919593811035 current acc: 0.6024\n",
      "iteration 75 current loss: 1.1186059713363647 current acc: 0.6108\n",
      "iteration 76 current loss: 1.0275318622589111 current acc: 0.6188\n",
      "iteration 77 current loss: 1.1686618328094482 current acc: 0.6268\n",
      "iteration 78 current loss: 1.5346128940582275 current acc: 0.6276\n",
      "\t\tTrain Epoch 25/100,Train Accuracy: 0.6276, Train Loss: 1.0430823209919506.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 25/100, Validation Accuracy: 0.51225, Validation Loss: 1.3843507843017577\n",
      "best loss 1.0430823209919506\n",
      "iteration 0 current loss: 0.8682780265808105 current acc: 0.0086\n",
      "iteration 1 current loss: 0.9920262098312378 current acc: 0.016\n",
      "iteration 2 current loss: 0.9517937898635864 current acc: 0.0242\n",
      "iteration 3 current loss: 1.0790395736694336 current acc: 0.032\n",
      "iteration 4 current loss: 1.0448707342147827 current acc: 0.04\n",
      "iteration 5 current loss: 1.1346849203109741 current acc: 0.0474\n",
      "iteration 6 current loss: 1.071520209312439 current acc: 0.055\n",
      "iteration 7 current loss: 1.093024730682373 current acc: 0.0632\n",
      "iteration 8 current loss: 0.98858642578125 current acc: 0.0712\n",
      "iteration 9 current loss: 0.9133914113044739 current acc: 0.0798\n",
      "iteration 10 current loss: 0.9907138347625732 current acc: 0.0886\n",
      "iteration 11 current loss: 1.016587734222412 current acc: 0.0964\n",
      "iteration 12 current loss: 0.8718374967575073 current acc: 0.1052\n",
      "iteration 13 current loss: 0.9405540823936462 current acc: 0.1134\n",
      "iteration 14 current loss: 1.007082223892212 current acc: 0.1216\n",
      "iteration 15 current loss: 0.9612436294555664 current acc: 0.1302\n",
      "iteration 16 current loss: 0.9347721934318542 current acc: 0.139\n",
      "iteration 17 current loss: 0.9061547517776489 current acc: 0.1488\n",
      "iteration 18 current loss: 0.7250694036483765 current acc: 0.1586\n",
      "iteration 19 current loss: 1.0905356407165527 current acc: 0.1672\n",
      "iteration 20 current loss: 1.0017204284667969 current acc: 0.175\n",
      "iteration 21 current loss: 0.9376736879348755 current acc: 0.1826\n",
      "iteration 22 current loss: 0.9596012234687805 current acc: 0.191\n",
      "iteration 23 current loss: 0.9579781889915466 current acc: 0.1996\n",
      "iteration 24 current loss: 0.9151666164398193 current acc: 0.208\n",
      "iteration 25 current loss: 1.025039553642273 current acc: 0.216\n",
      "iteration 26 current loss: 1.3179842233657837 current acc: 0.2236\n",
      "iteration 27 current loss: 1.0493712425231934 current acc: 0.231\n",
      "iteration 28 current loss: 1.0499849319458008 current acc: 0.2386\n",
      "iteration 29 current loss: 1.1909724473953247 current acc: 0.246\n",
      "iteration 30 current loss: 1.0493117570877075 current acc: 0.2534\n",
      "iteration 31 current loss: 0.829118013381958 current acc: 0.2622\n",
      "iteration 32 current loss: 0.9682116508483887 current acc: 0.2702\n",
      "iteration 33 current loss: 1.0260810852050781 current acc: 0.2776\n",
      "iteration 34 current loss: 0.8433725237846375 current acc: 0.286\n",
      "iteration 35 current loss: 1.0801440477371216 current acc: 0.2948\n",
      "iteration 36 current loss: 1.0873512029647827 current acc: 0.3018\n",
      "iteration 37 current loss: 1.2617474794387817 current acc: 0.3082\n",
      "iteration 38 current loss: 1.0218638181686401 current acc: 0.3164\n",
      "iteration 39 current loss: 0.9151766300201416 current acc: 0.325\n",
      "iteration 40 current loss: 0.9659032821655273 current acc: 0.333\n",
      "iteration 41 current loss: 1.1563421487808228 current acc: 0.3402\n",
      "iteration 42 current loss: 0.8434691429138184 current acc: 0.3496\n",
      "iteration 43 current loss: 0.8498889207839966 current acc: 0.3588\n",
      "iteration 44 current loss: 0.8690864443778992 current acc: 0.3676\n",
      "iteration 45 current loss: 1.154180645942688 current acc: 0.3752\n",
      "iteration 46 current loss: 0.9582745432853699 current acc: 0.3828\n",
      "iteration 47 current loss: 1.1688979864120483 current acc: 0.3902\n",
      "iteration 48 current loss: 1.0650967359542847 current acc: 0.3976\n",
      "iteration 49 current loss: 0.8542916178703308 current acc: 0.4076\n",
      "iteration 50 current loss: 1.072505235671997 current acc: 0.4158\n",
      "iteration 51 current loss: 1.0886262655258179 current acc: 0.423\n",
      "iteration 52 current loss: 0.9378319382667542 current acc: 0.4312\n",
      "iteration 53 current loss: 1.0133394002914429 current acc: 0.44\n",
      "iteration 54 current loss: 0.9284440875053406 current acc: 0.4492\n",
      "iteration 55 current loss: 1.2766547203063965 current acc: 0.4564\n",
      "iteration 56 current loss: 1.1649668216705322 current acc: 0.4634\n",
      "iteration 57 current loss: 0.9410795569419861 current acc: 0.4714\n",
      "iteration 58 current loss: 1.2626745700836182 current acc: 0.4788\n",
      "iteration 59 current loss: 1.057439923286438 current acc: 0.4862\n",
      "iteration 60 current loss: 1.048055648803711 current acc: 0.4938\n",
      "iteration 61 current loss: 0.974923849105835 current acc: 0.5026\n",
      "iteration 62 current loss: 1.0595413446426392 current acc: 0.5102\n",
      "iteration 63 current loss: 0.8855412602424622 current acc: 0.518\n",
      "iteration 64 current loss: 1.025297999382019 current acc: 0.5256\n",
      "iteration 65 current loss: 1.1300585269927979 current acc: 0.5328\n",
      "iteration 66 current loss: 0.9971401691436768 current acc: 0.5406\n",
      "iteration 67 current loss: 1.148543357849121 current acc: 0.5482\n",
      "iteration 68 current loss: 0.8610209822654724 current acc: 0.5566\n",
      "iteration 69 current loss: 1.173032283782959 current acc: 0.5642\n",
      "iteration 70 current loss: 1.2631852626800537 current acc: 0.571\n",
      "iteration 71 current loss: 1.1017743349075317 current acc: 0.5788\n",
      "iteration 72 current loss: 1.0122625827789307 current acc: 0.5864\n",
      "iteration 73 current loss: 1.1666796207427979 current acc: 0.5944\n",
      "iteration 74 current loss: 1.0856908559799194 current acc: 0.6024\n",
      "iteration 75 current loss: 1.1343159675598145 current acc: 0.61\n",
      "iteration 76 current loss: 1.1301193237304688 current acc: 0.6172\n",
      "iteration 77 current loss: 1.2594921588897705 current acc: 0.6238\n",
      "iteration 78 current loss: 0.8267378211021423 current acc: 0.6248\n",
      "\t\tTrain Epoch 26/100,Train Accuracy: 0.6248, Train Loss: 1.0250891786587388.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 26/100, Validation Accuracy: 0.513, Validation Loss: 1.368196270942688\n",
      "best loss 1.0250891786587388\n",
      "iteration 0 current loss: 0.8156073093414307 current acc: 0.0102\n",
      "iteration 1 current loss: 0.8660560846328735 current acc: 0.0196\n",
      "iteration 2 current loss: 0.9035425186157227 current acc: 0.0286\n",
      "iteration 3 current loss: 0.7745491862297058 current acc: 0.0384\n",
      "iteration 4 current loss: 0.9398567080497742 current acc: 0.0474\n",
      "iteration 5 current loss: 1.0101500749588013 current acc: 0.0558\n",
      "iteration 6 current loss: 0.9543455243110657 current acc: 0.0636\n",
      "iteration 7 current loss: 0.9027736186981201 current acc: 0.0716\n",
      "iteration 8 current loss: 1.022349238395691 current acc: 0.0794\n",
      "iteration 9 current loss: 1.2497429847717285 current acc: 0.087\n",
      "iteration 10 current loss: 0.7700719833374023 current acc: 0.0964\n",
      "iteration 11 current loss: 0.8548276424407959 current acc: 0.1054\n",
      "iteration 12 current loss: 0.9124625325202942 current acc: 0.1136\n",
      "iteration 13 current loss: 0.8814091086387634 current acc: 0.1222\n",
      "iteration 14 current loss: 1.0228646993637085 current acc: 0.1298\n",
      "iteration 15 current loss: 1.1310150623321533 current acc: 0.1378\n",
      "iteration 16 current loss: 1.2556161880493164 current acc: 0.1452\n",
      "iteration 17 current loss: 1.0568357706069946 current acc: 0.1528\n",
      "iteration 18 current loss: 0.7566515803337097 current acc: 0.162\n",
      "iteration 19 current loss: 0.9397692084312439 current acc: 0.1704\n",
      "iteration 20 current loss: 0.8786061406135559 current acc: 0.1794\n",
      "iteration 21 current loss: 0.9756700396537781 current acc: 0.188\n",
      "iteration 22 current loss: 0.9578827619552612 current acc: 0.1968\n",
      "iteration 23 current loss: 0.9427270293235779 current acc: 0.2042\n",
      "iteration 24 current loss: 0.9597331285476685 current acc: 0.2124\n",
      "iteration 25 current loss: 0.9076750874519348 current acc: 0.2216\n",
      "iteration 26 current loss: 0.9602077007293701 current acc: 0.23\n",
      "iteration 27 current loss: 1.1542434692382812 current acc: 0.238\n",
      "iteration 28 current loss: 1.008481502532959 current acc: 0.2454\n",
      "iteration 29 current loss: 1.1820954084396362 current acc: 0.2532\n",
      "iteration 30 current loss: 0.8510197401046753 current acc: 0.262\n",
      "iteration 31 current loss: 1.1805680990219116 current acc: 0.27\n",
      "iteration 32 current loss: 1.0618683099746704 current acc: 0.2778\n",
      "iteration 33 current loss: 0.9205623269081116 current acc: 0.2856\n",
      "iteration 34 current loss: 1.1780966520309448 current acc: 0.2926\n",
      "iteration 35 current loss: 0.9807841777801514 current acc: 0.3016\n",
      "iteration 36 current loss: 1.093212604522705 current acc: 0.3094\n",
      "iteration 37 current loss: 1.1855570077896118 current acc: 0.3166\n",
      "iteration 38 current loss: 1.4223731756210327 current acc: 0.3234\n",
      "iteration 39 current loss: 1.0689889192581177 current acc: 0.3312\n",
      "iteration 40 current loss: 0.8748133182525635 current acc: 0.3404\n",
      "iteration 41 current loss: 1.246589183807373 current acc: 0.3472\n",
      "iteration 42 current loss: 0.9383501410484314 current acc: 0.3556\n",
      "iteration 43 current loss: 0.8636720776557922 current acc: 0.3646\n",
      "iteration 44 current loss: 0.865145742893219 current acc: 0.3738\n",
      "iteration 45 current loss: 0.8680002689361572 current acc: 0.3834\n",
      "iteration 46 current loss: 1.0068353414535522 current acc: 0.3918\n",
      "iteration 47 current loss: 0.7915141582489014 current acc: 0.4012\n",
      "iteration 48 current loss: 1.087508201599121 current acc: 0.4096\n",
      "iteration 49 current loss: 0.9633508920669556 current acc: 0.418\n",
      "iteration 50 current loss: 1.5577688217163086 current acc: 0.4238\n",
      "iteration 51 current loss: 1.0151783227920532 current acc: 0.4322\n",
      "iteration 52 current loss: 1.18282151222229 current acc: 0.44\n",
      "iteration 53 current loss: 0.9595842957496643 current acc: 0.448\n",
      "iteration 54 current loss: 1.1023733615875244 current acc: 0.456\n",
      "iteration 55 current loss: 1.0090354681015015 current acc: 0.463\n",
      "iteration 56 current loss: 1.0164977312088013 current acc: 0.4704\n",
      "iteration 57 current loss: 1.166094422340393 current acc: 0.4786\n",
      "iteration 58 current loss: 1.0569720268249512 current acc: 0.4862\n",
      "iteration 59 current loss: 1.0417500734329224 current acc: 0.4942\n",
      "iteration 60 current loss: 1.211247205734253 current acc: 0.5012\n",
      "iteration 61 current loss: 1.0532786846160889 current acc: 0.509\n",
      "iteration 62 current loss: 0.9896143674850464 current acc: 0.5176\n",
      "iteration 63 current loss: 1.2209067344665527 current acc: 0.524\n",
      "iteration 64 current loss: 0.8670744895935059 current acc: 0.5332\n",
      "iteration 65 current loss: 1.2149006128311157 current acc: 0.5412\n",
      "iteration 66 current loss: 1.1856439113616943 current acc: 0.5478\n",
      "iteration 67 current loss: 1.0432322025299072 current acc: 0.5554\n",
      "iteration 68 current loss: 1.3461503982543945 current acc: 0.5618\n",
      "iteration 69 current loss: 1.1883697509765625 current acc: 0.5692\n",
      "iteration 70 current loss: 1.0629996061325073 current acc: 0.5776\n",
      "iteration 71 current loss: 0.9424884915351868 current acc: 0.5852\n",
      "iteration 72 current loss: 1.0219976902008057 current acc: 0.5932\n",
      "iteration 73 current loss: 1.0031111240386963 current acc: 0.6014\n",
      "iteration 74 current loss: 0.9790381789207458 current acc: 0.61\n",
      "iteration 75 current loss: 1.2893608808517456 current acc: 0.617\n",
      "iteration 76 current loss: 0.9989698529243469 current acc: 0.6252\n",
      "iteration 77 current loss: 0.858546793460846 current acc: 0.6344\n",
      "iteration 78 current loss: 1.389001488685608 current acc: 0.6354\n",
      "\t\tTrain Epoch 27/100,Train Accuracy: 0.6354, Train Loss: 1.03000807762146.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 27/100, Validation Accuracy: 0.50825, Validation Loss: 1.3689689345359801\n",
      "iteration 0 current loss: 0.8239918947219849 current acc: 0.01\n",
      "iteration 1 current loss: 0.8065659999847412 current acc: 0.0202\n",
      "iteration 2 current loss: 1.0676242113113403 current acc: 0.0282\n",
      "iteration 3 current loss: 1.1466224193572998 current acc: 0.0356\n",
      "iteration 4 current loss: 0.9246992468833923 current acc: 0.044\n",
      "iteration 5 current loss: 1.0366132259368896 current acc: 0.0518\n",
      "iteration 6 current loss: 0.9624844789505005 current acc: 0.0594\n",
      "iteration 7 current loss: 1.1123465299606323 current acc: 0.0672\n",
      "iteration 8 current loss: 1.0177390575408936 current acc: 0.075\n",
      "iteration 9 current loss: 1.0757595300674438 current acc: 0.0822\n",
      "iteration 10 current loss: 0.9054077863693237 current acc: 0.0906\n",
      "iteration 11 current loss: 0.9185630679130554 current acc: 0.099\n",
      "iteration 12 current loss: 1.0639963150024414 current acc: 0.1068\n",
      "iteration 13 current loss: 1.1628180742263794 current acc: 0.1142\n",
      "iteration 14 current loss: 1.0615153312683105 current acc: 0.1224\n",
      "iteration 15 current loss: 1.04154634475708 current acc: 0.1302\n",
      "iteration 16 current loss: 0.9805023670196533 current acc: 0.1382\n",
      "iteration 17 current loss: 1.0360145568847656 current acc: 0.146\n",
      "iteration 18 current loss: 0.9146240949630737 current acc: 0.1544\n",
      "iteration 19 current loss: 1.310066819190979 current acc: 0.1618\n",
      "iteration 20 current loss: 1.1398186683654785 current acc: 0.1696\n",
      "iteration 21 current loss: 0.9593312740325928 current acc: 0.1786\n",
      "iteration 22 current loss: 0.9179372787475586 current acc: 0.187\n",
      "iteration 23 current loss: 0.7425709366798401 current acc: 0.1966\n",
      "iteration 24 current loss: 1.1775208711624146 current acc: 0.2044\n",
      "iteration 25 current loss: 0.8997108340263367 current acc: 0.2136\n",
      "iteration 26 current loss: 1.0409340858459473 current acc: 0.222\n",
      "iteration 27 current loss: 1.17261803150177 current acc: 0.2286\n",
      "iteration 28 current loss: 1.145779013633728 current acc: 0.2364\n",
      "iteration 29 current loss: 0.9112008213996887 current acc: 0.2446\n",
      "iteration 30 current loss: 1.221483826637268 current acc: 0.2506\n",
      "iteration 31 current loss: 0.9758047461509705 current acc: 0.259\n",
      "iteration 32 current loss: 0.999791145324707 current acc: 0.2668\n",
      "iteration 33 current loss: 0.8032588958740234 current acc: 0.2764\n",
      "iteration 34 current loss: 1.2225854396820068 current acc: 0.2838\n",
      "iteration 35 current loss: 0.9866713881492615 current acc: 0.292\n",
      "iteration 36 current loss: 1.0912208557128906 current acc: 0.3\n",
      "iteration 37 current loss: 1.094397783279419 current acc: 0.3082\n",
      "iteration 38 current loss: 1.050897240638733 current acc: 0.3162\n",
      "iteration 39 current loss: 0.8562248945236206 current acc: 0.3248\n",
      "iteration 40 current loss: 1.0703132152557373 current acc: 0.3316\n",
      "iteration 41 current loss: 0.9132378101348877 current acc: 0.3406\n",
      "iteration 42 current loss: 0.926179051399231 current acc: 0.3486\n",
      "iteration 43 current loss: 0.9617802500724792 current acc: 0.357\n",
      "iteration 44 current loss: 0.9084908962249756 current acc: 0.366\n",
      "iteration 45 current loss: 0.7969884276390076 current acc: 0.3754\n",
      "iteration 46 current loss: 0.9693354964256287 current acc: 0.3848\n",
      "iteration 47 current loss: 0.9288282990455627 current acc: 0.3934\n",
      "iteration 48 current loss: 0.9602235555648804 current acc: 0.4014\n",
      "iteration 49 current loss: 1.067281723022461 current acc: 0.409\n",
      "iteration 50 current loss: 1.0828503370285034 current acc: 0.4166\n",
      "iteration 51 current loss: 1.011513590812683 current acc: 0.4256\n",
      "iteration 52 current loss: 1.0172514915466309 current acc: 0.4338\n",
      "iteration 53 current loss: 1.0743072032928467 current acc: 0.4422\n",
      "iteration 54 current loss: 0.9847548007965088 current acc: 0.4508\n",
      "iteration 55 current loss: 0.8622860312461853 current acc: 0.46\n",
      "iteration 56 current loss: 1.0418752431869507 current acc: 0.4682\n",
      "iteration 57 current loss: 1.0314542055130005 current acc: 0.4764\n",
      "iteration 58 current loss: 1.1292119026184082 current acc: 0.4842\n",
      "iteration 59 current loss: 0.9689050316810608 current acc: 0.4928\n",
      "iteration 60 current loss: 1.2462385892868042 current acc: 0.4992\n",
      "iteration 61 current loss: 0.7712814211845398 current acc: 0.5086\n",
      "iteration 62 current loss: 0.9943923354148865 current acc: 0.5176\n",
      "iteration 63 current loss: 1.0597984790802002 current acc: 0.525\n",
      "iteration 64 current loss: 0.9024524092674255 current acc: 0.5332\n",
      "iteration 65 current loss: 1.087649941444397 current acc: 0.541\n",
      "iteration 66 current loss: 0.9478070735931396 current acc: 0.55\n",
      "iteration 67 current loss: 1.147708535194397 current acc: 0.558\n",
      "iteration 68 current loss: 1.0883963108062744 current acc: 0.5658\n",
      "iteration 69 current loss: 1.1753544807434082 current acc: 0.574\n",
      "iteration 70 current loss: 0.8922863006591797 current acc: 0.5818\n",
      "iteration 71 current loss: 1.1126518249511719 current acc: 0.5904\n",
      "iteration 72 current loss: 1.1588637828826904 current acc: 0.5972\n",
      "iteration 73 current loss: 0.8775894045829773 current acc: 0.6066\n",
      "iteration 74 current loss: 1.1368669271469116 current acc: 0.6136\n",
      "iteration 75 current loss: 0.96004319190979 current acc: 0.6224\n",
      "iteration 76 current loss: 0.8732180595397949 current acc: 0.6312\n",
      "iteration 77 current loss: 0.984457790851593 current acc: 0.6394\n",
      "iteration 78 current loss: 1.3253029584884644 current acc: 0.6402\n",
      "\t\tTrain Epoch 28/100,Train Accuracy: 0.6402, Train Loss: 1.0155530096609382.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 28/100, Validation Accuracy: 0.492875, Validation Loss: 1.4372464857101441\n",
      "best loss 1.0155530096609382\n",
      "iteration 0 current loss: 0.8592461943626404 current acc: 0.009\n",
      "iteration 1 current loss: 1.073618769645691 current acc: 0.016\n",
      "iteration 2 current loss: 1.0476340055465698 current acc: 0.0234\n",
      "iteration 3 current loss: 1.1241698265075684 current acc: 0.0306\n",
      "iteration 4 current loss: 1.0294181108474731 current acc: 0.0386\n",
      "iteration 5 current loss: 1.2827309370040894 current acc: 0.0456\n",
      "iteration 6 current loss: 1.1015704870224 current acc: 0.053\n",
      "iteration 7 current loss: 0.9985008239746094 current acc: 0.0608\n",
      "iteration 8 current loss: 0.8554506301879883 current acc: 0.0696\n",
      "iteration 9 current loss: 0.73585444688797 current acc: 0.0792\n",
      "iteration 10 current loss: 0.9769806265830994 current acc: 0.0872\n",
      "iteration 11 current loss: 1.121901273727417 current acc: 0.0958\n",
      "iteration 12 current loss: 1.168224573135376 current acc: 0.1032\n",
      "iteration 13 current loss: 1.2313761711120605 current acc: 0.1106\n",
      "iteration 14 current loss: 1.130513310432434 current acc: 0.118\n",
      "iteration 15 current loss: 0.9158607721328735 current acc: 0.1272\n",
      "iteration 16 current loss: 1.0250579118728638 current acc: 0.1346\n",
      "iteration 17 current loss: 0.9765521883964539 current acc: 0.1434\n",
      "iteration 18 current loss: 0.8061800003051758 current acc: 0.1528\n",
      "iteration 19 current loss: 0.9797777533531189 current acc: 0.1608\n",
      "iteration 20 current loss: 1.0104409456253052 current acc: 0.1688\n",
      "iteration 21 current loss: 1.0428389310836792 current acc: 0.177\n",
      "iteration 22 current loss: 0.9517725706100464 current acc: 0.1856\n",
      "iteration 23 current loss: 1.0610697269439697 current acc: 0.1938\n",
      "iteration 24 current loss: 0.8630227446556091 current acc: 0.2032\n",
      "iteration 25 current loss: 1.068658709526062 current acc: 0.2108\n",
      "iteration 26 current loss: 0.9848640561103821 current acc: 0.2194\n",
      "iteration 27 current loss: 1.0120737552642822 current acc: 0.2268\n",
      "iteration 28 current loss: 1.1596678495407104 current acc: 0.2346\n",
      "iteration 29 current loss: 0.826120138168335 current acc: 0.244\n",
      "iteration 30 current loss: 1.1661676168441772 current acc: 0.2514\n",
      "iteration 31 current loss: 1.0665514469146729 current acc: 0.2596\n",
      "iteration 32 current loss: 0.8896207809448242 current acc: 0.2688\n",
      "iteration 33 current loss: 1.1622782945632935 current acc: 0.2766\n",
      "iteration 34 current loss: 1.1233676671981812 current acc: 0.2844\n",
      "iteration 35 current loss: 0.9930718541145325 current acc: 0.2928\n",
      "iteration 36 current loss: 1.0478945970535278 current acc: 0.3008\n",
      "iteration 37 current loss: 0.9262990355491638 current acc: 0.3094\n",
      "iteration 38 current loss: 1.0987236499786377 current acc: 0.3176\n",
      "iteration 39 current loss: 1.0497814416885376 current acc: 0.3268\n",
      "iteration 40 current loss: 1.1917086839675903 current acc: 0.334\n",
      "iteration 41 current loss: 1.0987330675125122 current acc: 0.3418\n",
      "iteration 42 current loss: 1.0943719148635864 current acc: 0.3504\n",
      "iteration 43 current loss: 1.0549219846725464 current acc: 0.3586\n",
      "iteration 44 current loss: 0.9095261096954346 current acc: 0.3672\n",
      "iteration 45 current loss: 0.9103792905807495 current acc: 0.3762\n",
      "iteration 46 current loss: 1.1014792919158936 current acc: 0.384\n",
      "iteration 47 current loss: 0.8344701528549194 current acc: 0.3932\n",
      "iteration 48 current loss: 1.0453168153762817 current acc: 0.4012\n",
      "iteration 49 current loss: 1.2990244626998901 current acc: 0.4074\n",
      "iteration 50 current loss: 1.1787798404693604 current acc: 0.4148\n",
      "iteration 51 current loss: 1.0770692825317383 current acc: 0.4232\n",
      "iteration 52 current loss: 1.0739842653274536 current acc: 0.4306\n",
      "iteration 53 current loss: 1.002015233039856 current acc: 0.4392\n",
      "iteration 54 current loss: 0.8684285879135132 current acc: 0.4486\n",
      "iteration 55 current loss: 0.7868338823318481 current acc: 0.4582\n",
      "iteration 56 current loss: 0.935268223285675 current acc: 0.4666\n",
      "iteration 57 current loss: 1.5003920793533325 current acc: 0.4736\n",
      "iteration 58 current loss: 1.043573260307312 current acc: 0.4804\n",
      "iteration 59 current loss: 1.1716463565826416 current acc: 0.4886\n",
      "iteration 60 current loss: 1.0541105270385742 current acc: 0.4958\n",
      "iteration 61 current loss: 1.066359281539917 current acc: 0.5038\n",
      "iteration 62 current loss: 1.0453742742538452 current acc: 0.512\n",
      "iteration 63 current loss: 0.9605416059494019 current acc: 0.5204\n",
      "iteration 64 current loss: 1.1854740381240845 current acc: 0.528\n",
      "iteration 65 current loss: 0.9187921285629272 current acc: 0.5372\n",
      "iteration 66 current loss: 1.015011191368103 current acc: 0.546\n",
      "iteration 67 current loss: 1.1550679206848145 current acc: 0.5534\n",
      "iteration 68 current loss: 1.1300146579742432 current acc: 0.561\n",
      "iteration 69 current loss: 0.828507661819458 current acc: 0.57\n",
      "iteration 70 current loss: 0.9134033918380737 current acc: 0.579\n",
      "iteration 71 current loss: 0.977785587310791 current acc: 0.588\n",
      "iteration 72 current loss: 1.2304295301437378 current acc: 0.5952\n",
      "iteration 73 current loss: 1.1692711114883423 current acc: 0.6026\n",
      "iteration 74 current loss: 1.2254507541656494 current acc: 0.6094\n",
      "iteration 75 current loss: 1.045400857925415 current acc: 0.6168\n",
      "iteration 76 current loss: 1.0229333639144897 current acc: 0.6242\n",
      "iteration 77 current loss: 0.9349671602249146 current acc: 0.6334\n",
      "iteration 78 current loss: 1.30730140209198 current acc: 0.634\n",
      "\t\tTrain Epoch 29/100,Train Accuracy: 0.634, Train Loss: 1.0418863779381862.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 29/100, Validation Accuracy: 0.503, Validation Loss: 1.3734980754852295\n",
      "iteration 0 current loss: 0.8973996639251709 current acc: 0.0092\n",
      "iteration 1 current loss: 1.062984824180603 current acc: 0.0172\n",
      "iteration 2 current loss: 0.9329291582107544 current acc: 0.0256\n",
      "iteration 3 current loss: 0.819218099117279 current acc: 0.0338\n",
      "iteration 4 current loss: 1.1593976020812988 current acc: 0.041\n",
      "iteration 5 current loss: 1.1635737419128418 current acc: 0.0484\n",
      "iteration 6 current loss: 1.0741791725158691 current acc: 0.0568\n",
      "iteration 7 current loss: 0.9594401121139526 current acc: 0.0654\n",
      "iteration 8 current loss: 1.1143932342529297 current acc: 0.0732\n",
      "iteration 9 current loss: 1.1956212520599365 current acc: 0.081\n",
      "iteration 10 current loss: 1.11056387424469 current acc: 0.0888\n",
      "iteration 11 current loss: 1.096550703048706 current acc: 0.0964\n",
      "iteration 12 current loss: 0.913094699382782 current acc: 0.1054\n",
      "iteration 13 current loss: 0.8956543803215027 current acc: 0.1132\n",
      "iteration 14 current loss: 0.9817310571670532 current acc: 0.1214\n",
      "iteration 15 current loss: 0.9595568180084229 current acc: 0.1298\n",
      "iteration 16 current loss: 1.1874654293060303 current acc: 0.1374\n",
      "iteration 17 current loss: 0.9765082597732544 current acc: 0.1466\n",
      "iteration 18 current loss: 0.9914151430130005 current acc: 0.1548\n",
      "iteration 19 current loss: 0.8475833535194397 current acc: 0.1642\n",
      "iteration 20 current loss: 1.0174397230148315 current acc: 0.1718\n",
      "iteration 21 current loss: 0.8908912539482117 current acc: 0.1806\n",
      "iteration 22 current loss: 0.9255301356315613 current acc: 0.1894\n",
      "iteration 23 current loss: 1.1099658012390137 current acc: 0.1972\n",
      "iteration 24 current loss: 0.8615789413452148 current acc: 0.2062\n",
      "iteration 25 current loss: 1.0303902626037598 current acc: 0.2142\n",
      "iteration 26 current loss: 0.913777232170105 current acc: 0.2228\n",
      "iteration 27 current loss: 0.9968245029449463 current acc: 0.2306\n",
      "iteration 28 current loss: 0.8621540665626526 current acc: 0.2392\n",
      "iteration 29 current loss: 1.0206565856933594 current acc: 0.2472\n",
      "iteration 30 current loss: 1.0791386365890503 current acc: 0.254\n",
      "iteration 31 current loss: 1.0536137819290161 current acc: 0.262\n",
      "iteration 32 current loss: 1.062417984008789 current acc: 0.2698\n",
      "iteration 33 current loss: 1.029844880104065 current acc: 0.2778\n",
      "iteration 34 current loss: 0.8204342722892761 current acc: 0.2866\n",
      "iteration 35 current loss: 0.9806579947471619 current acc: 0.295\n",
      "iteration 36 current loss: 1.1233282089233398 current acc: 0.3022\n",
      "iteration 37 current loss: 0.9107036590576172 current acc: 0.3104\n",
      "iteration 38 current loss: 1.1347968578338623 current acc: 0.3192\n",
      "iteration 39 current loss: 1.0223243236541748 current acc: 0.3272\n",
      "iteration 40 current loss: 1.0121709108352661 current acc: 0.336\n",
      "iteration 41 current loss: 1.1837483644485474 current acc: 0.3438\n",
      "iteration 42 current loss: 0.9920145869255066 current acc: 0.353\n",
      "iteration 43 current loss: 0.9952030181884766 current acc: 0.3614\n",
      "iteration 44 current loss: 0.8590795993804932 current acc: 0.3702\n",
      "iteration 45 current loss: 1.0250848531723022 current acc: 0.378\n",
      "iteration 46 current loss: 1.029307246208191 current acc: 0.3858\n",
      "iteration 47 current loss: 0.8441110849380493 current acc: 0.3946\n",
      "iteration 48 current loss: 1.0897215604782104 current acc: 0.4024\n",
      "iteration 49 current loss: 0.7878860831260681 current acc: 0.4118\n",
      "iteration 50 current loss: 1.0584250688552856 current acc: 0.4194\n",
      "iteration 51 current loss: 1.199540376663208 current acc: 0.426\n",
      "iteration 52 current loss: 0.9915645122528076 current acc: 0.4332\n",
      "iteration 53 current loss: 1.4073976278305054 current acc: 0.4398\n",
      "iteration 54 current loss: 0.9783072471618652 current acc: 0.4476\n",
      "iteration 55 current loss: 0.9897909760475159 current acc: 0.4566\n",
      "iteration 56 current loss: 0.9013940691947937 current acc: 0.465\n",
      "iteration 57 current loss: 1.0184377431869507 current acc: 0.4724\n",
      "iteration 58 current loss: 0.9803460836410522 current acc: 0.4802\n",
      "iteration 59 current loss: 0.9813006520271301 current acc: 0.4896\n",
      "iteration 60 current loss: 1.062374234199524 current acc: 0.498\n",
      "iteration 61 current loss: 1.1106014251708984 current acc: 0.5054\n",
      "iteration 62 current loss: 1.1216869354248047 current acc: 0.5126\n",
      "iteration 63 current loss: 1.0162714719772339 current acc: 0.5208\n",
      "iteration 64 current loss: 1.0841565132141113 current acc: 0.5282\n",
      "iteration 65 current loss: 0.87565016746521 current acc: 0.536\n",
      "iteration 66 current loss: 0.9547173976898193 current acc: 0.544\n",
      "iteration 67 current loss: 0.8402183055877686 current acc: 0.5522\n",
      "iteration 68 current loss: 1.0490628480911255 current acc: 0.5604\n",
      "iteration 69 current loss: 1.0707602500915527 current acc: 0.5682\n",
      "iteration 70 current loss: 1.0997569561004639 current acc: 0.5756\n",
      "iteration 71 current loss: 0.974814772605896 current acc: 0.5842\n",
      "iteration 72 current loss: 1.0097007751464844 current acc: 0.5922\n",
      "iteration 73 current loss: 0.9931421875953674 current acc: 0.6006\n",
      "iteration 74 current loss: 1.0182429552078247 current acc: 0.6086\n",
      "iteration 75 current loss: 0.9999286532402039 current acc: 0.6162\n",
      "iteration 76 current loss: 1.130334496498108 current acc: 0.624\n",
      "iteration 77 current loss: 0.9878201484680176 current acc: 0.6326\n",
      "iteration 78 current loss: 1.3004298210144043 current acc: 0.6334\n",
      "\t\tTrain Epoch 30/100,Train Accuracy: 0.6334, Train Loss: 1.0153193881240072.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 30/100, Validation Accuracy: 0.517875, Validation Loss: 1.3387401685714722\n",
      "best loss 1.0153193881240072\n",
      "iteration 0 current loss: 0.7241227030754089 current acc: 0.0094\n",
      "iteration 1 current loss: 0.7770793437957764 current acc: 0.0188\n",
      "iteration 2 current loss: 1.0210586786270142 current acc: 0.026\n",
      "iteration 3 current loss: 1.3425604104995728 current acc: 0.0322\n",
      "iteration 4 current loss: 1.0089539289474487 current acc: 0.0398\n",
      "iteration 5 current loss: 0.9829365015029907 current acc: 0.0474\n",
      "iteration 6 current loss: 1.0285724401474 current acc: 0.0562\n",
      "iteration 7 current loss: 0.9956502914428711 current acc: 0.0642\n",
      "iteration 8 current loss: 1.0068693161010742 current acc: 0.072\n",
      "iteration 9 current loss: 1.2070879936218262 current acc: 0.0794\n",
      "iteration 10 current loss: 1.058226227760315 current acc: 0.0868\n",
      "iteration 11 current loss: 0.9550440907478333 current acc: 0.095\n",
      "iteration 12 current loss: 1.0159640312194824 current acc: 0.1028\n",
      "iteration 13 current loss: 1.085691213607788 current acc: 0.1112\n",
      "iteration 14 current loss: 1.126623511314392 current acc: 0.119\n",
      "iteration 15 current loss: 1.1749964952468872 current acc: 0.1258\n",
      "iteration 16 current loss: 1.0172045230865479 current acc: 0.1334\n",
      "iteration 17 current loss: 1.0791300535202026 current acc: 0.1412\n",
      "iteration 18 current loss: 0.9374136328697205 current acc: 0.15\n",
      "iteration 19 current loss: 0.7515201568603516 current acc: 0.1596\n",
      "iteration 20 current loss: 0.939140796661377 current acc: 0.1682\n",
      "iteration 21 current loss: 1.0584821701049805 current acc: 0.1766\n",
      "iteration 22 current loss: 0.9690213203430176 current acc: 0.185\n",
      "iteration 23 current loss: 0.8810940384864807 current acc: 0.1936\n",
      "iteration 24 current loss: 1.2422747611999512 current acc: 0.2012\n",
      "iteration 25 current loss: 1.097545862197876 current acc: 0.209\n",
      "iteration 26 current loss: 0.8464354872703552 current acc: 0.2184\n",
      "iteration 27 current loss: 0.8457272052764893 current acc: 0.228\n",
      "iteration 28 current loss: 0.8479070663452148 current acc: 0.2374\n",
      "iteration 29 current loss: 1.1232430934906006 current acc: 0.2446\n",
      "iteration 30 current loss: 0.9570662975311279 current acc: 0.253\n",
      "iteration 31 current loss: 1.0994303226470947 current acc: 0.2612\n",
      "iteration 32 current loss: 0.9677400588989258 current acc: 0.2694\n",
      "iteration 33 current loss: 1.094115972518921 current acc: 0.277\n",
      "iteration 34 current loss: 0.8590705990791321 current acc: 0.2856\n",
      "iteration 35 current loss: 0.8767158389091492 current acc: 0.2944\n",
      "iteration 36 current loss: 1.1760311126708984 current acc: 0.3008\n",
      "iteration 37 current loss: 0.8685562610626221 current acc: 0.31\n",
      "iteration 38 current loss: 0.9180514216423035 current acc: 0.3186\n",
      "iteration 39 current loss: 1.1062231063842773 current acc: 0.3258\n",
      "iteration 40 current loss: 1.0928122997283936 current acc: 0.3336\n",
      "iteration 41 current loss: 1.1892731189727783 current acc: 0.3416\n",
      "iteration 42 current loss: 0.9624358415603638 current acc: 0.3496\n",
      "iteration 43 current loss: 0.8648470640182495 current acc: 0.3588\n",
      "iteration 44 current loss: 0.951075553894043 current acc: 0.3666\n",
      "iteration 45 current loss: 0.8416256904602051 current acc: 0.376\n",
      "iteration 46 current loss: 0.9290811419487 current acc: 0.384\n",
      "iteration 47 current loss: 1.1269148588180542 current acc: 0.3914\n",
      "iteration 48 current loss: 0.7469801306724548 current acc: 0.4022\n",
      "iteration 49 current loss: 0.9033179879188538 current acc: 0.4104\n",
      "iteration 50 current loss: 1.0305089950561523 current acc: 0.4184\n",
      "iteration 51 current loss: 1.233892560005188 current acc: 0.425\n",
      "iteration 52 current loss: 0.9633834958076477 current acc: 0.4342\n",
      "iteration 53 current loss: 0.9144824743270874 current acc: 0.4436\n",
      "iteration 54 current loss: 1.0092387199401855 current acc: 0.4514\n",
      "iteration 55 current loss: 1.0339313745498657 current acc: 0.46\n",
      "iteration 56 current loss: 0.9212714433670044 current acc: 0.4688\n",
      "iteration 57 current loss: 0.951181173324585 current acc: 0.4772\n",
      "iteration 58 current loss: 0.9863951802253723 current acc: 0.4852\n",
      "iteration 59 current loss: 0.8550930619239807 current acc: 0.495\n",
      "iteration 60 current loss: 0.8026772737503052 current acc: 0.5044\n",
      "iteration 61 current loss: 1.0947701930999756 current acc: 0.5124\n",
      "iteration 62 current loss: 1.1887420415878296 current acc: 0.5192\n",
      "iteration 63 current loss: 0.9994416236877441 current acc: 0.5276\n",
      "iteration 64 current loss: 0.9754432439804077 current acc: 0.5358\n",
      "iteration 65 current loss: 1.2374361753463745 current acc: 0.5424\n",
      "iteration 66 current loss: 0.8007301092147827 current acc: 0.5518\n",
      "iteration 67 current loss: 1.1257555484771729 current acc: 0.5604\n",
      "iteration 68 current loss: 0.9756153225898743 current acc: 0.5686\n",
      "iteration 69 current loss: 0.746324360370636 current acc: 0.5794\n",
      "iteration 70 current loss: 1.2301548719406128 current acc: 0.5876\n",
      "iteration 71 current loss: 0.8759155869483948 current acc: 0.5962\n",
      "iteration 72 current loss: 1.0572483539581299 current acc: 0.6038\n",
      "iteration 73 current loss: 0.9459388852119446 current acc: 0.6132\n",
      "iteration 74 current loss: 0.8433336615562439 current acc: 0.6224\n",
      "iteration 75 current loss: 1.2742940187454224 current acc: 0.629\n",
      "iteration 76 current loss: 1.041704535484314 current acc: 0.6368\n",
      "iteration 77 current loss: 1.1605654954910278 current acc: 0.6434\n",
      "iteration 78 current loss: 0.7333500385284424 current acc: 0.6444\n",
      "\t\tTrain Epoch 31/100,Train Accuracy: 0.6444, Train Loss: 0.9960476182684114.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 31/100, Validation Accuracy: 0.523625, Validation Loss: 1.358809645652771\n",
      "best loss 0.9960476182684114\n",
      "iteration 0 current loss: 0.9691518545150757 current acc: 0.0088\n",
      "iteration 1 current loss: 0.8767442107200623 current acc: 0.0174\n",
      "iteration 2 current loss: 0.9945039749145508 current acc: 0.0256\n",
      "iteration 3 current loss: 1.154553771018982 current acc: 0.032\n",
      "iteration 4 current loss: 1.012630581855774 current acc: 0.0402\n",
      "iteration 5 current loss: 1.1277860403060913 current acc: 0.0478\n",
      "iteration 6 current loss: 1.0163856744766235 current acc: 0.0562\n",
      "iteration 7 current loss: 0.8816384077072144 current acc: 0.0652\n",
      "iteration 8 current loss: 1.1469892263412476 current acc: 0.0728\n",
      "iteration 9 current loss: 1.1251232624053955 current acc: 0.0808\n",
      "iteration 10 current loss: 1.1046385765075684 current acc: 0.0884\n",
      "iteration 11 current loss: 1.1241353750228882 current acc: 0.0962\n",
      "iteration 12 current loss: 1.0111377239227295 current acc: 0.1044\n",
      "iteration 13 current loss: 0.7593521475791931 current acc: 0.114\n",
      "iteration 14 current loss: 1.0674313306808472 current acc: 0.1226\n",
      "iteration 15 current loss: 0.801472008228302 current acc: 0.1314\n",
      "iteration 16 current loss: 0.9950494170188904 current acc: 0.139\n",
      "iteration 17 current loss: 0.8172776103019714 current acc: 0.148\n",
      "iteration 18 current loss: 0.8450241088867188 current acc: 0.1566\n",
      "iteration 19 current loss: 0.9274885654449463 current acc: 0.1648\n",
      "iteration 20 current loss: 1.0480455160140991 current acc: 0.1724\n",
      "iteration 21 current loss: 0.9010871648788452 current acc: 0.1808\n",
      "iteration 22 current loss: 0.812023401260376 current acc: 0.1898\n",
      "iteration 23 current loss: 1.160433292388916 current acc: 0.1966\n",
      "iteration 24 current loss: 1.0508880615234375 current acc: 0.2042\n",
      "iteration 25 current loss: 0.9241387844085693 current acc: 0.2136\n",
      "iteration 26 current loss: 0.8299607038497925 current acc: 0.223\n",
      "iteration 27 current loss: 0.9007825255393982 current acc: 0.2322\n",
      "iteration 28 current loss: 0.99642413854599 current acc: 0.24\n",
      "iteration 29 current loss: 1.2279748916625977 current acc: 0.2468\n",
      "iteration 30 current loss: 0.8821386098861694 current acc: 0.256\n",
      "iteration 31 current loss: 0.896287739276886 current acc: 0.265\n",
      "iteration 32 current loss: 0.9850202798843384 current acc: 0.2734\n",
      "iteration 33 current loss: 1.1252012252807617 current acc: 0.2814\n",
      "iteration 34 current loss: 0.896169900894165 current acc: 0.29\n",
      "iteration 35 current loss: 1.0002222061157227 current acc: 0.2982\n",
      "iteration 36 current loss: 1.2716041803359985 current acc: 0.3054\n",
      "iteration 37 current loss: 1.0215355157852173 current acc: 0.313\n",
      "iteration 38 current loss: 1.0786446332931519 current acc: 0.3206\n",
      "iteration 39 current loss: 1.0816655158996582 current acc: 0.3278\n",
      "iteration 40 current loss: 1.2696964740753174 current acc: 0.334\n",
      "iteration 41 current loss: 1.0756791830062866 current acc: 0.3428\n",
      "iteration 42 current loss: 0.9388262629508972 current acc: 0.3522\n",
      "iteration 43 current loss: 0.9401111006736755 current acc: 0.3606\n",
      "iteration 44 current loss: 1.0549635887145996 current acc: 0.3682\n",
      "iteration 45 current loss: 0.9052217602729797 current acc: 0.3768\n",
      "iteration 46 current loss: 0.9775102734565735 current acc: 0.3852\n",
      "iteration 47 current loss: 0.9012700319290161 current acc: 0.394\n",
      "iteration 48 current loss: 1.2004356384277344 current acc: 0.4012\n",
      "iteration 49 current loss: 0.7364499568939209 current acc: 0.4106\n",
      "iteration 50 current loss: 0.9487709403038025 current acc: 0.419\n",
      "iteration 51 current loss: 1.0674738883972168 current acc: 0.4272\n",
      "iteration 52 current loss: 0.9573364853858948 current acc: 0.4348\n",
      "iteration 53 current loss: 0.9774649739265442 current acc: 0.4434\n",
      "iteration 54 current loss: 1.0547934770584106 current acc: 0.4504\n",
      "iteration 55 current loss: 0.8575186133384705 current acc: 0.4584\n",
      "iteration 56 current loss: 1.212217092514038 current acc: 0.4652\n",
      "iteration 57 current loss: 1.1964375972747803 current acc: 0.4722\n",
      "iteration 58 current loss: 0.8615326881408691 current acc: 0.4804\n",
      "iteration 59 current loss: 0.8876717686653137 current acc: 0.4898\n",
      "iteration 60 current loss: 0.9505857229232788 current acc: 0.4986\n",
      "iteration 61 current loss: 1.0783241987228394 current acc: 0.5068\n",
      "iteration 62 current loss: 0.8836132287979126 current acc: 0.5164\n",
      "iteration 63 current loss: 1.0103832483291626 current acc: 0.524\n",
      "iteration 64 current loss: 0.9081853628158569 current acc: 0.5332\n",
      "iteration 65 current loss: 0.8041819930076599 current acc: 0.542\n",
      "iteration 66 current loss: 1.3058909177780151 current acc: 0.548\n",
      "iteration 67 current loss: 1.1110782623291016 current acc: 0.5558\n",
      "iteration 68 current loss: 1.0132863521575928 current acc: 0.564\n",
      "iteration 69 current loss: 0.9023581147193909 current acc: 0.5724\n",
      "iteration 70 current loss: 1.107951045036316 current acc: 0.5806\n",
      "iteration 71 current loss: 1.2396163940429688 current acc: 0.5882\n",
      "iteration 72 current loss: 0.9031984806060791 current acc: 0.5972\n",
      "iteration 73 current loss: 0.9212138652801514 current acc: 0.6058\n",
      "iteration 74 current loss: 1.1534414291381836 current acc: 0.6138\n",
      "iteration 75 current loss: 1.1585187911987305 current acc: 0.6216\n",
      "iteration 76 current loss: 1.0015625953674316 current acc: 0.63\n",
      "iteration 77 current loss: 0.625106155872345 current acc: 0.6408\n",
      "iteration 78 current loss: 1.0937933921813965 current acc: 0.6416\n",
      "\t\tTrain Epoch 32/100,Train Accuracy: 0.6416, Train Loss: 1.0005371329150623.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 32/100, Validation Accuracy: 0.538, Validation Loss: 1.2939801158905029\n",
      "iteration 0 current loss: 0.8385505676269531 current acc: 0.0092\n",
      "iteration 1 current loss: 1.0773972272872925 current acc: 0.0174\n",
      "iteration 2 current loss: 0.8478268384933472 current acc: 0.0272\n",
      "iteration 3 current loss: 0.8526412844657898 current acc: 0.0356\n",
      "iteration 4 current loss: 0.812631368637085 current acc: 0.0452\n",
      "iteration 5 current loss: 1.0828883647918701 current acc: 0.0534\n",
      "iteration 6 current loss: 0.7543874979019165 current acc: 0.063\n",
      "iteration 7 current loss: 1.062634825706482 current acc: 0.0714\n",
      "iteration 8 current loss: 0.962772011756897 current acc: 0.0802\n",
      "iteration 9 current loss: 1.0154982805252075 current acc: 0.0878\n",
      "iteration 10 current loss: 0.8901234865188599 current acc: 0.0966\n",
      "iteration 11 current loss: 1.0232142210006714 current acc: 0.1042\n",
      "iteration 12 current loss: 1.0378870964050293 current acc: 0.1124\n",
      "iteration 13 current loss: 1.1444307565689087 current acc: 0.1196\n",
      "iteration 14 current loss: 0.9895787835121155 current acc: 0.1278\n",
      "iteration 15 current loss: 0.8726440668106079 current acc: 0.1366\n",
      "iteration 16 current loss: 0.8800114989280701 current acc: 0.1456\n",
      "iteration 17 current loss: 0.9313071966171265 current acc: 0.1542\n",
      "iteration 18 current loss: 0.8541355133056641 current acc: 0.1628\n",
      "iteration 19 current loss: 1.0966297388076782 current acc: 0.1706\n",
      "iteration 20 current loss: 1.0769273042678833 current acc: 0.1788\n",
      "iteration 21 current loss: 0.8663508892059326 current acc: 0.1872\n",
      "iteration 22 current loss: 1.073149561882019 current acc: 0.1952\n",
      "iteration 23 current loss: 0.9572300910949707 current acc: 0.2034\n",
      "iteration 24 current loss: 1.1877846717834473 current acc: 0.21\n",
      "iteration 25 current loss: 1.0037769079208374 current acc: 0.2186\n",
      "iteration 26 current loss: 0.9385064840316772 current acc: 0.2266\n",
      "iteration 27 current loss: 0.9234740734100342 current acc: 0.2354\n",
      "iteration 28 current loss: 0.8320317268371582 current acc: 0.2442\n",
      "iteration 29 current loss: 0.8077215552330017 current acc: 0.254\n",
      "iteration 30 current loss: 0.7764108180999756 current acc: 0.2634\n",
      "iteration 31 current loss: 0.9324328899383545 current acc: 0.2718\n",
      "iteration 32 current loss: 1.1809427738189697 current acc: 0.2794\n",
      "iteration 33 current loss: 1.1235970258712769 current acc: 0.2882\n",
      "iteration 34 current loss: 1.2088943719863892 current acc: 0.2962\n",
      "iteration 35 current loss: 1.0673633813858032 current acc: 0.3034\n",
      "iteration 36 current loss: 0.8273167610168457 current acc: 0.313\n",
      "iteration 37 current loss: 0.9202949404716492 current acc: 0.3232\n",
      "iteration 38 current loss: 0.930770993232727 current acc: 0.3324\n",
      "iteration 39 current loss: 1.0548361539840698 current acc: 0.3402\n",
      "iteration 40 current loss: 1.4258077144622803 current acc: 0.347\n",
      "iteration 41 current loss: 0.9910492300987244 current acc: 0.3552\n",
      "iteration 42 current loss: 1.028849720954895 current acc: 0.3624\n",
      "iteration 43 current loss: 1.1849119663238525 current acc: 0.3698\n",
      "iteration 44 current loss: 1.1285614967346191 current acc: 0.3776\n",
      "iteration 45 current loss: 1.1024171113967896 current acc: 0.3852\n",
      "iteration 46 current loss: 1.087777018547058 current acc: 0.3926\n",
      "iteration 47 current loss: 1.0235224962234497 current acc: 0.4006\n",
      "iteration 48 current loss: 1.0496069192886353 current acc: 0.4084\n",
      "iteration 49 current loss: 1.0214552879333496 current acc: 0.417\n",
      "iteration 50 current loss: 0.8704301118850708 current acc: 0.4256\n",
      "iteration 51 current loss: 0.9807359576225281 current acc: 0.4338\n",
      "iteration 52 current loss: 0.8618937134742737 current acc: 0.4422\n",
      "iteration 53 current loss: 1.040269374847412 current acc: 0.45\n",
      "iteration 54 current loss: 0.9270687103271484 current acc: 0.4586\n",
      "iteration 55 current loss: 0.9249287843704224 current acc: 0.468\n",
      "iteration 56 current loss: 1.0431486368179321 current acc: 0.4764\n",
      "iteration 57 current loss: 0.9145328402519226 current acc: 0.4846\n",
      "iteration 58 current loss: 1.026384949684143 current acc: 0.4918\n",
      "iteration 59 current loss: 1.0877829790115356 current acc: 0.4992\n",
      "iteration 60 current loss: 1.1573572158813477 current acc: 0.5062\n",
      "iteration 61 current loss: 0.9092551469802856 current acc: 0.515\n",
      "iteration 62 current loss: 0.9549141526222229 current acc: 0.524\n",
      "iteration 63 current loss: 1.03432035446167 current acc: 0.5316\n",
      "iteration 64 current loss: 1.2148343324661255 current acc: 0.539\n",
      "iteration 65 current loss: 0.8612658977508545 current acc: 0.548\n",
      "iteration 66 current loss: 1.2035101652145386 current acc: 0.555\n",
      "iteration 67 current loss: 0.9711514711380005 current acc: 0.5632\n",
      "iteration 68 current loss: 1.1765402555465698 current acc: 0.5698\n",
      "iteration 69 current loss: 0.9893937706947327 current acc: 0.5782\n",
      "iteration 70 current loss: 1.1378164291381836 current acc: 0.5858\n",
      "iteration 71 current loss: 1.1996368169784546 current acc: 0.5928\n",
      "iteration 72 current loss: 0.8807157278060913 current acc: 0.6012\n",
      "iteration 73 current loss: 0.813180685043335 current acc: 0.61\n",
      "iteration 74 current loss: 0.9539622068405151 current acc: 0.6184\n",
      "iteration 75 current loss: 1.1032947301864624 current acc: 0.626\n",
      "iteration 76 current loss: 1.3577874898910522 current acc: 0.6316\n",
      "iteration 77 current loss: 1.1180510520935059 current acc: 0.6396\n",
      "iteration 78 current loss: 1.4271122217178345 current acc: 0.6404\n",
      "\t\tTrain Epoch 33/100,Train Accuracy: 0.6404, Train Loss: 1.0114207486563092.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 33/100, Validation Accuracy: 0.514, Validation Loss: 1.3302571659088134\n",
      "iteration 0 current loss: 0.8428308367729187 current acc: 0.0092\n",
      "iteration 1 current loss: 1.1233030557632446 current acc: 0.0168\n",
      "iteration 2 current loss: 1.1589897871017456 current acc: 0.0236\n",
      "iteration 3 current loss: 0.991196870803833 current acc: 0.0312\n",
      "iteration 4 current loss: 1.0833414793014526 current acc: 0.0386\n",
      "iteration 5 current loss: 1.0121463537216187 current acc: 0.047\n",
      "iteration 6 current loss: 0.9376645088195801 current acc: 0.0554\n",
      "iteration 7 current loss: 1.0122754573822021 current acc: 0.0638\n",
      "iteration 8 current loss: 1.099083423614502 current acc: 0.072\n",
      "iteration 9 current loss: 0.8288986682891846 current acc: 0.081\n",
      "iteration 10 current loss: 0.9363047480583191 current acc: 0.0894\n",
      "iteration 11 current loss: 0.9890223145484924 current acc: 0.0978\n",
      "iteration 12 current loss: 1.1721214056015015 current acc: 0.1048\n",
      "iteration 13 current loss: 1.0155248641967773 current acc: 0.113\n",
      "iteration 14 current loss: 1.0024352073669434 current acc: 0.1206\n",
      "iteration 15 current loss: 1.1299724578857422 current acc: 0.1282\n",
      "iteration 16 current loss: 0.8809811472892761 current acc: 0.1366\n",
      "iteration 17 current loss: 1.043373942375183 current acc: 0.1444\n",
      "iteration 18 current loss: 1.0770134925842285 current acc: 0.1534\n",
      "iteration 19 current loss: 1.144415259361267 current acc: 0.1602\n",
      "iteration 20 current loss: 1.0701028108596802 current acc: 0.1674\n",
      "iteration 21 current loss: 1.1026660203933716 current acc: 0.1756\n",
      "iteration 22 current loss: 1.0781651735305786 current acc: 0.1832\n",
      "iteration 23 current loss: 1.0872098207473755 current acc: 0.191\n",
      "iteration 24 current loss: 1.03638756275177 current acc: 0.199\n",
      "iteration 25 current loss: 0.7580224871635437 current acc: 0.2082\n",
      "iteration 26 current loss: 1.0478389263153076 current acc: 0.216\n",
      "iteration 27 current loss: 1.027116060256958 current acc: 0.224\n",
      "iteration 28 current loss: 0.9356482625007629 current acc: 0.2324\n",
      "iteration 29 current loss: 1.0157166719436646 current acc: 0.242\n",
      "iteration 30 current loss: 0.8804478049278259 current acc: 0.2506\n",
      "iteration 31 current loss: 0.7457019090652466 current acc: 0.2598\n",
      "iteration 32 current loss: 1.0369104146957397 current acc: 0.2682\n",
      "iteration 33 current loss: 0.947632372379303 current acc: 0.2764\n",
      "iteration 34 current loss: 0.9311609268188477 current acc: 0.2854\n",
      "iteration 35 current loss: 1.00601065158844 current acc: 0.2934\n",
      "iteration 36 current loss: 1.062158465385437 current acc: 0.301\n",
      "iteration 37 current loss: 0.8767489790916443 current acc: 0.3096\n",
      "iteration 38 current loss: 0.8780169486999512 current acc: 0.3184\n",
      "iteration 39 current loss: 0.9768585562705994 current acc: 0.327\n",
      "iteration 40 current loss: 0.959782063961029 current acc: 0.335\n",
      "iteration 41 current loss: 0.8160684704780579 current acc: 0.344\n",
      "iteration 42 current loss: 1.0027774572372437 current acc: 0.3528\n",
      "iteration 43 current loss: 0.980154812335968 current acc: 0.3622\n",
      "iteration 44 current loss: 0.9121868014335632 current acc: 0.3718\n",
      "iteration 45 current loss: 1.1826153993606567 current acc: 0.3788\n",
      "iteration 46 current loss: 0.9434525966644287 current acc: 0.387\n",
      "iteration 47 current loss: 0.9664368629455566 current acc: 0.3956\n",
      "iteration 48 current loss: 0.916541337966919 current acc: 0.4042\n",
      "iteration 49 current loss: 1.064576268196106 current acc: 0.4122\n",
      "iteration 50 current loss: 0.7293345928192139 current acc: 0.4218\n",
      "iteration 51 current loss: 1.0951298475265503 current acc: 0.4304\n",
      "iteration 52 current loss: 0.9152241945266724 current acc: 0.4388\n",
      "iteration 53 current loss: 1.0372309684753418 current acc: 0.4474\n",
      "iteration 54 current loss: 1.032104253768921 current acc: 0.4554\n",
      "iteration 55 current loss: 1.0403145551681519 current acc: 0.4632\n",
      "iteration 56 current loss: 1.021579384803772 current acc: 0.4712\n",
      "iteration 57 current loss: 1.0101841688156128 current acc: 0.4794\n",
      "iteration 58 current loss: 1.0721547603607178 current acc: 0.487\n",
      "iteration 59 current loss: 0.9954250454902649 current acc: 0.4956\n",
      "iteration 60 current loss: 0.8848379254341125 current acc: 0.5048\n",
      "iteration 61 current loss: 0.9751091003417969 current acc: 0.5128\n",
      "iteration 62 current loss: 1.0966001749038696 current acc: 0.5204\n",
      "iteration 63 current loss: 0.9724829792976379 current acc: 0.5286\n",
      "iteration 64 current loss: 0.8642770051956177 current acc: 0.5378\n",
      "iteration 65 current loss: 0.9292057752609253 current acc: 0.5456\n",
      "iteration 66 current loss: 1.1624082326889038 current acc: 0.5526\n",
      "iteration 67 current loss: 1.030160665512085 current acc: 0.561\n",
      "iteration 68 current loss: 0.9120932221412659 current acc: 0.5692\n",
      "iteration 69 current loss: 1.051312804222107 current acc: 0.5768\n",
      "iteration 70 current loss: 0.8510107398033142 current acc: 0.5858\n",
      "iteration 71 current loss: 1.1023069620132446 current acc: 0.5942\n",
      "iteration 72 current loss: 0.9576855897903442 current acc: 0.6024\n",
      "iteration 73 current loss: 0.9438477158546448 current acc: 0.6102\n",
      "iteration 74 current loss: 1.1331071853637695 current acc: 0.6174\n",
      "iteration 75 current loss: 1.0559217929840088 current acc: 0.626\n",
      "iteration 76 current loss: 0.9806886315345764 current acc: 0.634\n",
      "iteration 77 current loss: 0.9831024408340454 current acc: 0.6428\n",
      "iteration 78 current loss: 1.1587071418762207 current acc: 0.6438\n",
      "\t\tTrain Epoch 34/100,Train Accuracy: 0.6438, Train Loss: 0.9964500763748265.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 34/100, Validation Accuracy: 0.539, Validation Loss: 1.3065724291801453\n",
      "iteration 0 current loss: 0.9480901956558228 current acc: 0.0088\n",
      "iteration 1 current loss: 0.9316728711128235 current acc: 0.0172\n",
      "iteration 2 current loss: 0.7678090929985046 current acc: 0.0262\n",
      "iteration 3 current loss: 1.143945574760437 current acc: 0.0332\n",
      "iteration 4 current loss: 0.8665978908538818 current acc: 0.0422\n",
      "iteration 5 current loss: 0.950934112071991 current acc: 0.0514\n",
      "iteration 6 current loss: 0.8832679986953735 current acc: 0.0598\n",
      "iteration 7 current loss: 0.9577471613883972 current acc: 0.068\n",
      "iteration 8 current loss: 1.1232998371124268 current acc: 0.075\n",
      "iteration 9 current loss: 0.7473497986793518 current acc: 0.085\n",
      "iteration 10 current loss: 0.9205304980278015 current acc: 0.0924\n",
      "iteration 11 current loss: 0.782622754573822 current acc: 0.102\n",
      "iteration 12 current loss: 1.0547678470611572 current acc: 0.11\n",
      "iteration 13 current loss: 0.8310946822166443 current acc: 0.1192\n",
      "iteration 14 current loss: 0.6737459301948547 current acc: 0.1292\n",
      "iteration 15 current loss: 1.1550062894821167 current acc: 0.1362\n",
      "iteration 16 current loss: 1.350020170211792 current acc: 0.1434\n",
      "iteration 17 current loss: 0.9494502544403076 current acc: 0.1524\n",
      "iteration 18 current loss: 1.0086379051208496 current acc: 0.1602\n",
      "iteration 19 current loss: 0.8398150205612183 current acc: 0.1694\n",
      "iteration 20 current loss: 0.9853343367576599 current acc: 0.1774\n",
      "iteration 21 current loss: 0.9011938571929932 current acc: 0.1862\n",
      "iteration 22 current loss: 1.0740201473236084 current acc: 0.1938\n",
      "iteration 23 current loss: 1.2424448728561401 current acc: 0.2004\n",
      "iteration 24 current loss: 0.9223386645317078 current acc: 0.2096\n",
      "iteration 25 current loss: 0.8743130564689636 current acc: 0.2186\n",
      "iteration 26 current loss: 1.0534799098968506 current acc: 0.226\n",
      "iteration 27 current loss: 1.156543493270874 current acc: 0.2334\n",
      "iteration 28 current loss: 0.7356410026550293 current acc: 0.2432\n",
      "iteration 29 current loss: 0.9072962999343872 current acc: 0.2524\n",
      "iteration 30 current loss: 0.9107879400253296 current acc: 0.2608\n",
      "iteration 31 current loss: 0.8899891376495361 current acc: 0.2692\n",
      "iteration 32 current loss: 0.9298705458641052 current acc: 0.2782\n",
      "iteration 33 current loss: 1.0746333599090576 current acc: 0.2862\n",
      "iteration 34 current loss: 1.0637593269348145 current acc: 0.2944\n",
      "iteration 35 current loss: 0.9864392280578613 current acc: 0.3018\n",
      "iteration 36 current loss: 0.9758545756340027 current acc: 0.3102\n",
      "iteration 37 current loss: 1.0518912076950073 current acc: 0.3182\n",
      "iteration 38 current loss: 0.9885612726211548 current acc: 0.3268\n",
      "iteration 39 current loss: 0.8568667769432068 current acc: 0.3354\n",
      "iteration 40 current loss: 1.027901530265808 current acc: 0.3432\n",
      "iteration 41 current loss: 1.0130497217178345 current acc: 0.3518\n",
      "iteration 42 current loss: 1.107865333557129 current acc: 0.3588\n",
      "iteration 43 current loss: 1.093155860900879 current acc: 0.3668\n",
      "iteration 44 current loss: 1.3905510902404785 current acc: 0.3732\n",
      "iteration 45 current loss: 1.0029412508010864 current acc: 0.3812\n",
      "iteration 46 current loss: 0.9334067702293396 current acc: 0.3902\n",
      "iteration 47 current loss: 0.9017065167427063 current acc: 0.3988\n",
      "iteration 48 current loss: 0.8550390601158142 current acc: 0.408\n",
      "iteration 49 current loss: 0.8207685947418213 current acc: 0.4172\n",
      "iteration 50 current loss: 0.9179375171661377 current acc: 0.4254\n",
      "iteration 51 current loss: 0.951675534248352 current acc: 0.4332\n",
      "iteration 52 current loss: 0.9128395915031433 current acc: 0.4422\n",
      "iteration 53 current loss: 1.4062697887420654 current acc: 0.4472\n",
      "iteration 54 current loss: 0.8105731010437012 current acc: 0.457\n",
      "iteration 55 current loss: 1.1933497190475464 current acc: 0.4642\n",
      "iteration 56 current loss: 0.9890917539596558 current acc: 0.473\n",
      "iteration 57 current loss: 1.0699471235275269 current acc: 0.4806\n",
      "iteration 58 current loss: 1.0374352931976318 current acc: 0.4888\n",
      "iteration 59 current loss: 1.0993293523788452 current acc: 0.496\n",
      "iteration 60 current loss: 1.041372299194336 current acc: 0.5036\n",
      "iteration 61 current loss: 1.1462502479553223 current acc: 0.5108\n",
      "iteration 62 current loss: 1.0065057277679443 current acc: 0.5188\n",
      "iteration 63 current loss: 0.9352484941482544 current acc: 0.5278\n",
      "iteration 64 current loss: 1.0167251825332642 current acc: 0.5358\n",
      "iteration 65 current loss: 0.9560542106628418 current acc: 0.5446\n",
      "iteration 66 current loss: 0.7822332382202148 current acc: 0.5528\n",
      "iteration 67 current loss: 0.9648181796073914 current acc: 0.5608\n",
      "iteration 68 current loss: 1.0742334127426147 current acc: 0.5686\n",
      "iteration 69 current loss: 1.022326111793518 current acc: 0.5756\n",
      "iteration 70 current loss: 0.9094843864440918 current acc: 0.5838\n",
      "iteration 71 current loss: 0.9688655734062195 current acc: 0.5922\n",
      "iteration 72 current loss: 0.9678394198417664 current acc: 0.6\n",
      "iteration 73 current loss: 0.9164552092552185 current acc: 0.6086\n",
      "iteration 74 current loss: 0.9663335680961609 current acc: 0.616\n",
      "iteration 75 current loss: 1.0987650156021118 current acc: 0.6234\n",
      "iteration 76 current loss: 0.902423083782196 current acc: 0.6326\n",
      "iteration 77 current loss: 1.1196907758712769 current acc: 0.6392\n",
      "iteration 78 current loss: 1.4217183589935303 current acc: 0.64\n",
      "\t\tTrain Epoch 35/100,Train Accuracy: 0.64, Train Loss: 0.9901245809808562.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 35/100, Validation Accuracy: 0.501375, Validation Loss: 1.4216541805267333\n",
      "best loss 0.9901245809808562\n",
      "iteration 0 current loss: 1.04300057888031 current acc: 0.0078\n",
      "iteration 1 current loss: 1.0025272369384766 current acc: 0.0168\n",
      "iteration 2 current loss: 1.104853630065918 current acc: 0.0248\n",
      "iteration 3 current loss: 1.2665598392486572 current acc: 0.0306\n",
      "iteration 4 current loss: 1.2426536083221436 current acc: 0.0382\n",
      "iteration 5 current loss: 1.2817730903625488 current acc: 0.0448\n",
      "iteration 6 current loss: 1.01664137840271 current acc: 0.0534\n",
      "iteration 7 current loss: 0.9371121525764465 current acc: 0.0622\n",
      "iteration 8 current loss: 1.1553539037704468 current acc: 0.0696\n",
      "iteration 9 current loss: 1.027312159538269 current acc: 0.0782\n",
      "iteration 10 current loss: 1.0187585353851318 current acc: 0.0866\n",
      "iteration 11 current loss: 0.9895650148391724 current acc: 0.0944\n",
      "iteration 12 current loss: 0.8301981687545776 current acc: 0.1038\n",
      "iteration 13 current loss: 1.0912350416183472 current acc: 0.1118\n",
      "iteration 14 current loss: 1.0626416206359863 current acc: 0.12\n",
      "iteration 15 current loss: 1.0679365396499634 current acc: 0.128\n",
      "iteration 16 current loss: 0.9646614789962769 current acc: 0.1358\n",
      "iteration 17 current loss: 0.8601706624031067 current acc: 0.1448\n",
      "iteration 18 current loss: 1.033315658569336 current acc: 0.1532\n",
      "iteration 19 current loss: 0.896416425704956 current acc: 0.163\n",
      "iteration 20 current loss: 0.8950111865997314 current acc: 0.1714\n",
      "iteration 21 current loss: 1.0866641998291016 current acc: 0.1792\n",
      "iteration 22 current loss: 0.9629724025726318 current acc: 0.1876\n",
      "iteration 23 current loss: 0.9248411655426025 current acc: 0.1962\n",
      "iteration 24 current loss: 0.8852479457855225 current acc: 0.2048\n",
      "iteration 25 current loss: 0.9984302520751953 current acc: 0.2124\n",
      "iteration 26 current loss: 0.7912817597389221 current acc: 0.2226\n",
      "iteration 27 current loss: 1.0744887590408325 current acc: 0.23\n",
      "iteration 28 current loss: 1.2093340158462524 current acc: 0.237\n",
      "iteration 29 current loss: 1.108737826347351 current acc: 0.2442\n",
      "iteration 30 current loss: 1.1029133796691895 current acc: 0.2518\n",
      "iteration 31 current loss: 1.0183196067810059 current acc: 0.2594\n",
      "iteration 32 current loss: 0.7980313301086426 current acc: 0.2686\n",
      "iteration 33 current loss: 1.0682299137115479 current acc: 0.2762\n",
      "iteration 34 current loss: 1.0474255084991455 current acc: 0.2844\n",
      "iteration 35 current loss: 1.0014348030090332 current acc: 0.2928\n",
      "iteration 36 current loss: 0.9269885420799255 current acc: 0.3016\n",
      "iteration 37 current loss: 0.9739482402801514 current acc: 0.3104\n",
      "iteration 38 current loss: 1.1383872032165527 current acc: 0.3182\n",
      "iteration 39 current loss: 1.2131524085998535 current acc: 0.3256\n",
      "iteration 40 current loss: 1.148374080657959 current acc: 0.3336\n",
      "iteration 41 current loss: 0.9498569965362549 current acc: 0.3422\n",
      "iteration 42 current loss: 0.9554405808448792 current acc: 0.351\n",
      "iteration 43 current loss: 1.2229028940200806 current acc: 0.3578\n",
      "iteration 44 current loss: 0.7868499755859375 current acc: 0.367\n",
      "iteration 45 current loss: 0.9550638198852539 current acc: 0.3754\n",
      "iteration 46 current loss: 1.1631547212600708 current acc: 0.3826\n",
      "iteration 47 current loss: 1.1163064241409302 current acc: 0.3902\n",
      "iteration 48 current loss: 1.0045820474624634 current acc: 0.398\n",
      "iteration 49 current loss: 0.9726994037628174 current acc: 0.4066\n",
      "iteration 50 current loss: 1.0259239673614502 current acc: 0.4148\n",
      "iteration 51 current loss: 0.9694229364395142 current acc: 0.423\n",
      "iteration 52 current loss: 0.933795690536499 current acc: 0.4312\n",
      "iteration 53 current loss: 0.9140316247940063 current acc: 0.4398\n",
      "iteration 54 current loss: 0.971694827079773 current acc: 0.448\n",
      "iteration 55 current loss: 0.8920661807060242 current acc: 0.4568\n",
      "iteration 56 current loss: 0.9124836325645447 current acc: 0.466\n",
      "iteration 57 current loss: 1.1244136095046997 current acc: 0.4736\n",
      "iteration 58 current loss: 1.0938597917556763 current acc: 0.4814\n",
      "iteration 59 current loss: 0.9274939894676208 current acc: 0.4904\n",
      "iteration 60 current loss: 0.9444365501403809 current acc: 0.4982\n",
      "iteration 61 current loss: 1.021513819694519 current acc: 0.5054\n",
      "iteration 62 current loss: 1.0677955150604248 current acc: 0.5136\n",
      "iteration 63 current loss: 1.1262876987457275 current acc: 0.5206\n",
      "iteration 64 current loss: 1.0793485641479492 current acc: 0.529\n",
      "iteration 65 current loss: 1.009052038192749 current acc: 0.5376\n",
      "iteration 66 current loss: 0.9687713384628296 current acc: 0.5464\n",
      "iteration 67 current loss: 0.9200707674026489 current acc: 0.5554\n",
      "iteration 68 current loss: 0.9006458520889282 current acc: 0.5646\n",
      "iteration 69 current loss: 0.8681635856628418 current acc: 0.5734\n",
      "iteration 70 current loss: 1.0311459302902222 current acc: 0.5812\n",
      "iteration 71 current loss: 1.0990862846374512 current acc: 0.5894\n",
      "iteration 72 current loss: 1.1387003660202026 current acc: 0.5968\n",
      "iteration 73 current loss: 0.8459045886993408 current acc: 0.6058\n",
      "iteration 74 current loss: 0.9017432928085327 current acc: 0.6142\n",
      "iteration 75 current loss: 0.9224497079849243 current acc: 0.622\n",
      "iteration 76 current loss: 0.9671714901924133 current acc: 0.63\n",
      "iteration 77 current loss: 0.9930756092071533 current acc: 0.6386\n",
      "iteration 78 current loss: 0.8950580358505249 current acc: 0.6398\n",
      "\t\tTrain Epoch 36/100,Train Accuracy: 0.6398, Train Loss: 1.010878043084205.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 36/100, Validation Accuracy: 0.544125, Validation Loss: 1.2538748083114624\n",
      "iteration 0 current loss: 0.6887795329093933 current acc: 0.0102\n",
      "iteration 1 current loss: 0.8888713121414185 current acc: 0.0188\n",
      "iteration 2 current loss: 0.7907379865646362 current acc: 0.0288\n",
      "iteration 3 current loss: 0.7492061853408813 current acc: 0.0392\n",
      "iteration 4 current loss: 0.749958336353302 current acc: 0.0478\n",
      "iteration 5 current loss: 0.9344788789749146 current acc: 0.0562\n",
      "iteration 6 current loss: 1.2006068229675293 current acc: 0.0626\n",
      "iteration 7 current loss: 0.9242494106292725 current acc: 0.0712\n",
      "iteration 8 current loss: 0.8442158699035645 current acc: 0.0802\n",
      "iteration 9 current loss: 0.882763683795929 current acc: 0.0892\n",
      "iteration 10 current loss: 1.1185778379440308 current acc: 0.096\n",
      "iteration 11 current loss: 1.0692884922027588 current acc: 0.1038\n",
      "iteration 12 current loss: 0.7297319173812866 current acc: 0.114\n",
      "iteration 13 current loss: 0.8475505113601685 current acc: 0.1226\n",
      "iteration 14 current loss: 0.7762542963027954 current acc: 0.132\n",
      "iteration 15 current loss: 1.031445026397705 current acc: 0.14\n",
      "iteration 16 current loss: 0.8104706406593323 current acc: 0.149\n",
      "iteration 17 current loss: 1.0653300285339355 current acc: 0.157\n",
      "iteration 18 current loss: 0.7799909710884094 current acc: 0.1666\n",
      "iteration 19 current loss: 0.8218093514442444 current acc: 0.1754\n",
      "iteration 20 current loss: 0.8025215864181519 current acc: 0.185\n",
      "iteration 21 current loss: 0.9640197157859802 current acc: 0.1944\n",
      "iteration 22 current loss: 0.9337736964225769 current acc: 0.203\n",
      "iteration 23 current loss: 0.8554717302322388 current acc: 0.2118\n",
      "iteration 24 current loss: 0.9208287596702576 current acc: 0.2202\n",
      "iteration 25 current loss: 1.0146576166152954 current acc: 0.228\n",
      "iteration 26 current loss: 0.7600205540657043 current acc: 0.2374\n",
      "iteration 27 current loss: 0.8223779797554016 current acc: 0.247\n",
      "iteration 28 current loss: 1.1826457977294922 current acc: 0.254\n",
      "iteration 29 current loss: 0.8673863410949707 current acc: 0.2622\n",
      "iteration 30 current loss: 0.8596993684768677 current acc: 0.2712\n",
      "iteration 31 current loss: 1.098230242729187 current acc: 0.2786\n",
      "iteration 32 current loss: 1.0307193994522095 current acc: 0.287\n",
      "iteration 33 current loss: 0.927753746509552 current acc: 0.2952\n",
      "iteration 34 current loss: 1.0101484060287476 current acc: 0.3034\n",
      "iteration 35 current loss: 1.0270535945892334 current acc: 0.3104\n",
      "iteration 36 current loss: 1.0211135149002075 current acc: 0.318\n",
      "iteration 37 current loss: 1.0235062837600708 current acc: 0.3266\n",
      "iteration 38 current loss: 1.0545685291290283 current acc: 0.3352\n",
      "iteration 39 current loss: 1.1187554597854614 current acc: 0.3432\n",
      "iteration 40 current loss: 0.744234025478363 current acc: 0.3524\n",
      "iteration 41 current loss: 1.0882084369659424 current acc: 0.3598\n",
      "iteration 42 current loss: 1.1093803644180298 current acc: 0.367\n",
      "iteration 43 current loss: 0.919736385345459 current acc: 0.3756\n",
      "iteration 44 current loss: 1.201658844947815 current acc: 0.3826\n",
      "iteration 45 current loss: 0.897235095500946 current acc: 0.391\n",
      "iteration 46 current loss: 1.0186681747436523 current acc: 0.3992\n",
      "iteration 47 current loss: 0.9702733159065247 current acc: 0.4086\n",
      "iteration 48 current loss: 1.0831100940704346 current acc: 0.416\n",
      "iteration 49 current loss: 1.0165144205093384 current acc: 0.4244\n",
      "iteration 50 current loss: 0.9262964725494385 current acc: 0.4334\n",
      "iteration 51 current loss: 1.037003755569458 current acc: 0.4412\n",
      "iteration 52 current loss: 0.9884399175643921 current acc: 0.4494\n",
      "iteration 53 current loss: 1.258449912071228 current acc: 0.4566\n",
      "iteration 54 current loss: 1.0270558595657349 current acc: 0.4648\n",
      "iteration 55 current loss: 0.9144437313079834 current acc: 0.4732\n",
      "iteration 56 current loss: 0.8966779708862305 current acc: 0.4822\n",
      "iteration 57 current loss: 1.391019344329834 current acc: 0.4888\n",
      "iteration 58 current loss: 1.026885747909546 current acc: 0.4964\n",
      "iteration 59 current loss: 0.8559805154800415 current acc: 0.5048\n",
      "iteration 60 current loss: 1.1164071559906006 current acc: 0.512\n",
      "iteration 61 current loss: 0.8365470170974731 current acc: 0.5208\n",
      "iteration 62 current loss: 1.0355784893035889 current acc: 0.5286\n",
      "iteration 63 current loss: 1.058242917060852 current acc: 0.5372\n",
      "iteration 64 current loss: 1.040886640548706 current acc: 0.546\n",
      "iteration 65 current loss: 0.9679816961288452 current acc: 0.554\n",
      "iteration 66 current loss: 0.991485059261322 current acc: 0.562\n",
      "iteration 67 current loss: 1.3211898803710938 current acc: 0.5686\n",
      "iteration 68 current loss: 0.9276618361473083 current acc: 0.5776\n",
      "iteration 69 current loss: 0.9201772212982178 current acc: 0.5862\n",
      "iteration 70 current loss: 1.0928515195846558 current acc: 0.5934\n",
      "iteration 71 current loss: 1.050418734550476 current acc: 0.6018\n",
      "iteration 72 current loss: 0.7789389491081238 current acc: 0.6114\n",
      "iteration 73 current loss: 0.8931739330291748 current acc: 0.6204\n",
      "iteration 74 current loss: 1.183296799659729 current acc: 0.6272\n",
      "iteration 75 current loss: 1.1428242921829224 current acc: 0.6356\n",
      "iteration 76 current loss: 0.8785632252693176 current acc: 0.645\n",
      "iteration 77 current loss: 0.9474940299987793 current acc: 0.6534\n",
      "iteration 78 current loss: 0.8983330726623535 current acc: 0.6544\n",
      "\t\tTrain Epoch 37/100,Train Accuracy: 0.6544, Train Loss: 0.9677581553217731.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 37/100, Validation Accuracy: 0.526375, Validation Loss: 1.3355663318634032\n",
      "best loss 0.9677581553217731\n",
      "iteration 0 current loss: 0.7906581163406372 current acc: 0.0096\n",
      "iteration 1 current loss: 0.8662548065185547 current acc: 0.0186\n",
      "iteration 2 current loss: 1.0387169122695923 current acc: 0.0272\n",
      "iteration 3 current loss: 1.0620636940002441 current acc: 0.0352\n",
      "iteration 4 current loss: 1.008976697921753 current acc: 0.0434\n",
      "iteration 5 current loss: 1.0836966037750244 current acc: 0.051\n",
      "iteration 6 current loss: 0.7812931537628174 current acc: 0.0606\n",
      "iteration 7 current loss: 1.0187844038009644 current acc: 0.0684\n",
      "iteration 8 current loss: 0.8698793053627014 current acc: 0.0776\n",
      "iteration 9 current loss: 0.8333715200424194 current acc: 0.087\n",
      "iteration 10 current loss: 0.7823225855827332 current acc: 0.0964\n",
      "iteration 11 current loss: 0.9696524143218994 current acc: 0.1052\n",
      "iteration 12 current loss: 0.9958203434944153 current acc: 0.113\n",
      "iteration 13 current loss: 1.0422289371490479 current acc: 0.1212\n",
      "iteration 14 current loss: 0.9929037094116211 current acc: 0.1298\n",
      "iteration 15 current loss: 0.9006124138832092 current acc: 0.1384\n",
      "iteration 16 current loss: 0.910460352897644 current acc: 0.1464\n",
      "iteration 17 current loss: 0.9498914480209351 current acc: 0.1546\n",
      "iteration 18 current loss: 0.9832930564880371 current acc: 0.1632\n",
      "iteration 19 current loss: 1.1696680784225464 current acc: 0.1712\n",
      "iteration 20 current loss: 0.9631804823875427 current acc: 0.18\n",
      "iteration 21 current loss: 0.762421190738678 current acc: 0.1894\n",
      "iteration 22 current loss: 1.0069185495376587 current acc: 0.1976\n",
      "iteration 23 current loss: 0.9769363403320312 current acc: 0.2064\n",
      "iteration 24 current loss: 1.0925990343093872 current acc: 0.2142\n",
      "iteration 25 current loss: 0.9475119113922119 current acc: 0.2216\n",
      "iteration 26 current loss: 1.029466152191162 current acc: 0.2294\n",
      "iteration 27 current loss: 0.8246381878852844 current acc: 0.2378\n",
      "iteration 28 current loss: 1.1947226524353027 current acc: 0.2452\n",
      "iteration 29 current loss: 0.8974472284317017 current acc: 0.254\n",
      "iteration 30 current loss: 1.1149592399597168 current acc: 0.2618\n",
      "iteration 31 current loss: 0.9021267294883728 current acc: 0.2708\n",
      "iteration 32 current loss: 0.8445038199424744 current acc: 0.2802\n",
      "iteration 33 current loss: 0.9637906551361084 current acc: 0.2886\n",
      "iteration 34 current loss: 1.1217823028564453 current acc: 0.2966\n",
      "iteration 35 current loss: 0.7785007953643799 current acc: 0.306\n",
      "iteration 36 current loss: 0.9940885901451111 current acc: 0.3136\n",
      "iteration 37 current loss: 0.8666568398475647 current acc: 0.3218\n",
      "iteration 38 current loss: 0.9878346920013428 current acc: 0.331\n",
      "iteration 39 current loss: 0.9766361713409424 current acc: 0.3396\n",
      "iteration 40 current loss: 0.897903323173523 current acc: 0.3478\n",
      "iteration 41 current loss: 0.8524664640426636 current acc: 0.3564\n",
      "iteration 42 current loss: 1.0608203411102295 current acc: 0.3636\n",
      "iteration 43 current loss: 1.0076342821121216 current acc: 0.3724\n",
      "iteration 44 current loss: 0.9725221395492554 current acc: 0.3818\n",
      "iteration 45 current loss: 1.0919839143753052 current acc: 0.3898\n",
      "iteration 46 current loss: 1.0247939825057983 current acc: 0.3966\n",
      "iteration 47 current loss: 1.1744023561477661 current acc: 0.4036\n",
      "iteration 48 current loss: 0.8973779678344727 current acc: 0.4128\n",
      "iteration 49 current loss: 0.9011865258216858 current acc: 0.4216\n",
      "iteration 50 current loss: 1.0902663469314575 current acc: 0.4294\n",
      "iteration 51 current loss: 0.7475528120994568 current acc: 0.4394\n",
      "iteration 52 current loss: 0.9418425559997559 current acc: 0.448\n",
      "iteration 53 current loss: 0.8685302734375 current acc: 0.4564\n",
      "iteration 54 current loss: 1.0057766437530518 current acc: 0.464\n",
      "iteration 55 current loss: 1.0353586673736572 current acc: 0.4724\n",
      "iteration 56 current loss: 0.9792826175689697 current acc: 0.4806\n",
      "iteration 57 current loss: 1.1099529266357422 current acc: 0.488\n",
      "iteration 58 current loss: 0.8802204728126526 current acc: 0.497\n",
      "iteration 59 current loss: 1.0430538654327393 current acc: 0.5056\n",
      "iteration 60 current loss: 0.8314405083656311 current acc: 0.515\n",
      "iteration 61 current loss: 1.0009419918060303 current acc: 0.5228\n",
      "iteration 62 current loss: 1.1652168035507202 current acc: 0.5304\n",
      "iteration 63 current loss: 0.8822566270828247 current acc: 0.5398\n",
      "iteration 64 current loss: 0.9314122796058655 current acc: 0.548\n",
      "iteration 65 current loss: 0.9756836295127869 current acc: 0.556\n",
      "iteration 66 current loss: 0.852083683013916 current acc: 0.5646\n",
      "iteration 67 current loss: 0.8277332782745361 current acc: 0.5732\n",
      "iteration 68 current loss: 1.0706373453140259 current acc: 0.581\n",
      "iteration 69 current loss: 0.9518569707870483 current acc: 0.5882\n",
      "iteration 70 current loss: 0.7651745676994324 current acc: 0.5972\n",
      "iteration 71 current loss: 0.985841691493988 current acc: 0.605\n",
      "iteration 72 current loss: 1.0259506702423096 current acc: 0.613\n",
      "iteration 73 current loss: 0.9678571820259094 current acc: 0.6214\n",
      "iteration 74 current loss: 1.0655159950256348 current acc: 0.6292\n",
      "iteration 75 current loss: 0.8260844945907593 current acc: 0.6376\n",
      "iteration 76 current loss: 0.8740953803062439 current acc: 0.6466\n",
      "iteration 77 current loss: 1.2638441324234009 current acc: 0.6534\n",
      "iteration 78 current loss: 0.7806652784347534 current acc: 0.6546\n",
      "\t\tTrain Epoch 38/100,Train Accuracy: 0.6546, Train Loss: 0.9610695076894157.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 38/100, Validation Accuracy: 0.54875, Validation Loss: 1.2566380438804627\n",
      "best loss 0.9610695076894157\n",
      "iteration 0 current loss: 0.7754234671592712 current acc: 0.0092\n",
      "iteration 1 current loss: 1.108376145362854 current acc: 0.016\n",
      "iteration 2 current loss: 1.147475242614746 current acc: 0.023\n",
      "iteration 3 current loss: 0.9595387578010559 current acc: 0.0316\n",
      "iteration 4 current loss: 0.7931796908378601 current acc: 0.0402\n",
      "iteration 5 current loss: 0.9499885439872742 current acc: 0.0494\n",
      "iteration 6 current loss: 1.0588586330413818 current acc: 0.057\n",
      "iteration 7 current loss: 0.9291849732398987 current acc: 0.066\n",
      "iteration 8 current loss: 0.9092218279838562 current acc: 0.0748\n",
      "iteration 9 current loss: 1.0795884132385254 current acc: 0.0828\n",
      "iteration 10 current loss: 0.9239044189453125 current acc: 0.0914\n",
      "iteration 11 current loss: 1.0275315046310425 current acc: 0.0988\n",
      "iteration 12 current loss: 0.9310557246208191 current acc: 0.1076\n",
      "iteration 13 current loss: 1.019926905632019 current acc: 0.1156\n",
      "iteration 14 current loss: 0.7517405152320862 current acc: 0.1256\n",
      "iteration 15 current loss: 0.7989699244499207 current acc: 0.1352\n",
      "iteration 16 current loss: 1.0362063646316528 current acc: 0.1444\n",
      "iteration 17 current loss: 1.0106289386749268 current acc: 0.1518\n",
      "iteration 18 current loss: 1.0483866930007935 current acc: 0.1606\n",
      "iteration 19 current loss: 0.7369964122772217 current acc: 0.1704\n",
      "iteration 20 current loss: 0.9094950556755066 current acc: 0.1782\n",
      "iteration 21 current loss: 0.9085904359817505 current acc: 0.1868\n",
      "iteration 22 current loss: 1.0042109489440918 current acc: 0.1948\n",
      "iteration 23 current loss: 0.786496639251709 current acc: 0.2038\n",
      "iteration 24 current loss: 0.803297758102417 current acc: 0.2136\n",
      "iteration 25 current loss: 0.7983030080795288 current acc: 0.223\n",
      "iteration 26 current loss: 0.8730058073997498 current acc: 0.2318\n",
      "iteration 27 current loss: 0.8737837672233582 current acc: 0.2408\n",
      "iteration 28 current loss: 0.8547464609146118 current acc: 0.2498\n",
      "iteration 29 current loss: 0.9494493007659912 current acc: 0.258\n",
      "iteration 30 current loss: 0.8778560161590576 current acc: 0.2672\n",
      "iteration 31 current loss: 0.8516443967819214 current acc: 0.2756\n",
      "iteration 32 current loss: 0.9412671327590942 current acc: 0.2848\n",
      "iteration 33 current loss: 1.1338763236999512 current acc: 0.2916\n",
      "iteration 34 current loss: 0.9565312266349792 current acc: 0.2996\n",
      "iteration 35 current loss: 1.0951919555664062 current acc: 0.3068\n",
      "iteration 36 current loss: 0.9953227639198303 current acc: 0.3152\n",
      "iteration 37 current loss: 1.0293718576431274 current acc: 0.3236\n",
      "iteration 38 current loss: 0.7981367111206055 current acc: 0.3334\n",
      "iteration 39 current loss: 0.9247104525566101 current acc: 0.3418\n",
      "iteration 40 current loss: 0.7470771074295044 current acc: 0.3514\n",
      "iteration 41 current loss: 1.069929838180542 current acc: 0.3598\n",
      "iteration 42 current loss: 0.9224992990493774 current acc: 0.3692\n",
      "iteration 43 current loss: 1.0010355710983276 current acc: 0.3778\n",
      "iteration 44 current loss: 1.0812222957611084 current acc: 0.3856\n",
      "iteration 45 current loss: 0.956979513168335 current acc: 0.395\n",
      "iteration 46 current loss: 1.032396912574768 current acc: 0.4028\n",
      "iteration 47 current loss: 1.1346145868301392 current acc: 0.4106\n",
      "iteration 48 current loss: 1.0352489948272705 current acc: 0.4188\n",
      "iteration 49 current loss: 1.065927267074585 current acc: 0.4264\n",
      "iteration 50 current loss: 0.8432864546775818 current acc: 0.4354\n",
      "iteration 51 current loss: 0.8953606486320496 current acc: 0.4436\n",
      "iteration 52 current loss: 0.8854678869247437 current acc: 0.4516\n",
      "iteration 53 current loss: 0.8442179560661316 current acc: 0.4606\n",
      "iteration 54 current loss: 0.9051790833473206 current acc: 0.4692\n",
      "iteration 55 current loss: 0.9818260669708252 current acc: 0.4772\n",
      "iteration 56 current loss: 1.0381544828414917 current acc: 0.485\n",
      "iteration 57 current loss: 0.9062698483467102 current acc: 0.494\n",
      "iteration 58 current loss: 0.8201256990432739 current acc: 0.5034\n",
      "iteration 59 current loss: 1.1638835668563843 current acc: 0.5106\n",
      "iteration 60 current loss: 1.117870569229126 current acc: 0.5182\n",
      "iteration 61 current loss: 0.8353458642959595 current acc: 0.5276\n",
      "iteration 62 current loss: 0.8626699447631836 current acc: 0.5366\n",
      "iteration 63 current loss: 1.1171393394470215 current acc: 0.5442\n",
      "iteration 64 current loss: 1.046960711479187 current acc: 0.5524\n",
      "iteration 65 current loss: 1.2008330821990967 current acc: 0.56\n",
      "iteration 66 current loss: 0.8846826553344727 current acc: 0.5686\n",
      "iteration 67 current loss: 0.9759389758110046 current acc: 0.5768\n",
      "iteration 68 current loss: 1.105647087097168 current acc: 0.5836\n",
      "iteration 69 current loss: 0.9923433065414429 current acc: 0.5912\n",
      "iteration 70 current loss: 0.9143638610839844 current acc: 0.5998\n",
      "iteration 71 current loss: 1.0735787153244019 current acc: 0.608\n",
      "iteration 72 current loss: 1.0513676404953003 current acc: 0.6162\n",
      "iteration 73 current loss: 1.1634931564331055 current acc: 0.6242\n",
      "iteration 74 current loss: 0.9695372581481934 current acc: 0.6328\n",
      "iteration 75 current loss: 1.1129776239395142 current acc: 0.6414\n",
      "iteration 76 current loss: 1.012068748474121 current acc: 0.6492\n",
      "iteration 77 current loss: 1.0148732662200928 current acc: 0.657\n",
      "iteration 78 current loss: 1.3166946172714233 current acc: 0.6576\n",
      "\t\tTrain Epoch 39/100,Train Accuracy: 0.6576, Train Loss: 0.9678551977193808.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 39/100, Validation Accuracy: 0.519375, Validation Loss: 1.3335471959114076\n",
      "iteration 0 current loss: 0.9796987175941467 current acc: 0.0088\n",
      "iteration 1 current loss: 0.9327008724212646 current acc: 0.0172\n",
      "iteration 2 current loss: 0.832852303981781 current acc: 0.0264\n",
      "iteration 3 current loss: 0.9527756571769714 current acc: 0.035\n",
      "iteration 4 current loss: 0.9837628602981567 current acc: 0.0432\n",
      "iteration 5 current loss: 1.2398438453674316 current acc: 0.051\n",
      "iteration 6 current loss: 1.025491714477539 current acc: 0.0588\n",
      "iteration 7 current loss: 0.7622140645980835 current acc: 0.0682\n",
      "iteration 8 current loss: 0.9362615942955017 current acc: 0.0768\n",
      "iteration 9 current loss: 0.85943603515625 current acc: 0.0858\n",
      "iteration 10 current loss: 0.943764328956604 current acc: 0.0946\n",
      "iteration 11 current loss: 1.100099802017212 current acc: 0.102\n",
      "iteration 12 current loss: 0.9710736274719238 current acc: 0.1106\n",
      "iteration 13 current loss: 0.9373186826705933 current acc: 0.1184\n",
      "iteration 14 current loss: 1.104843020439148 current acc: 0.1268\n",
      "iteration 15 current loss: 0.855783998966217 current acc: 0.135\n",
      "iteration 16 current loss: 0.9426631331443787 current acc: 0.1434\n",
      "iteration 17 current loss: 1.009379267692566 current acc: 0.152\n",
      "iteration 18 current loss: 1.0361748933792114 current acc: 0.1598\n",
      "iteration 19 current loss: 0.9424630403518677 current acc: 0.1672\n",
      "iteration 20 current loss: 1.07545804977417 current acc: 0.175\n",
      "iteration 21 current loss: 0.9792382717132568 current acc: 0.1838\n",
      "iteration 22 current loss: 0.9094310402870178 current acc: 0.1926\n",
      "iteration 23 current loss: 0.852599024772644 current acc: 0.2014\n",
      "iteration 24 current loss: 0.945615828037262 current acc: 0.2096\n",
      "iteration 25 current loss: 0.9465588331222534 current acc: 0.2176\n",
      "iteration 26 current loss: 0.8794062733650208 current acc: 0.2252\n",
      "iteration 27 current loss: 0.8572735786437988 current acc: 0.2338\n",
      "iteration 28 current loss: 1.077481746673584 current acc: 0.2412\n",
      "iteration 29 current loss: 1.0258710384368896 current acc: 0.2492\n",
      "iteration 30 current loss: 0.97573322057724 current acc: 0.2572\n",
      "iteration 31 current loss: 1.015497088432312 current acc: 0.2656\n",
      "iteration 32 current loss: 0.895403265953064 current acc: 0.2742\n",
      "iteration 33 current loss: 1.0867027044296265 current acc: 0.2828\n",
      "iteration 34 current loss: 0.7679803371429443 current acc: 0.2918\n",
      "iteration 35 current loss: 1.134474277496338 current acc: 0.2996\n",
      "iteration 36 current loss: 1.072845220565796 current acc: 0.307\n",
      "iteration 37 current loss: 0.9607492089271545 current acc: 0.3162\n",
      "iteration 38 current loss: 1.0513544082641602 current acc: 0.3246\n",
      "iteration 39 current loss: 1.1004745960235596 current acc: 0.3336\n",
      "iteration 40 current loss: 0.9181996583938599 current acc: 0.3424\n",
      "iteration 41 current loss: 0.8410494923591614 current acc: 0.3512\n",
      "iteration 42 current loss: 0.8591083288192749 current acc: 0.3602\n",
      "iteration 43 current loss: 1.0472463369369507 current acc: 0.368\n",
      "iteration 44 current loss: 0.9660429358482361 current acc: 0.3768\n",
      "iteration 45 current loss: 0.8213311433792114 current acc: 0.3858\n",
      "iteration 46 current loss: 0.9950790405273438 current acc: 0.3942\n",
      "iteration 47 current loss: 1.0126150846481323 current acc: 0.4034\n",
      "iteration 48 current loss: 1.0116918087005615 current acc: 0.4118\n",
      "iteration 49 current loss: 0.9601848721504211 current acc: 0.4206\n",
      "iteration 50 current loss: 0.9960465431213379 current acc: 0.4292\n",
      "iteration 51 current loss: 1.0088672637939453 current acc: 0.4374\n",
      "iteration 52 current loss: 0.8814201354980469 current acc: 0.4468\n",
      "iteration 53 current loss: 0.8542071580886841 current acc: 0.456\n",
      "iteration 54 current loss: 0.9556291699409485 current acc: 0.4646\n",
      "iteration 55 current loss: 1.0326035022735596 current acc: 0.4724\n",
      "iteration 56 current loss: 0.8461000919342041 current acc: 0.482\n",
      "iteration 57 current loss: 0.7871387004852295 current acc: 0.4916\n",
      "iteration 58 current loss: 1.0773651599884033 current acc: 0.4994\n",
      "iteration 59 current loss: 0.8862488865852356 current acc: 0.5082\n",
      "iteration 60 current loss: 1.115557312965393 current acc: 0.5152\n",
      "iteration 61 current loss: 0.9849938154220581 current acc: 0.5236\n",
      "iteration 62 current loss: 0.9618697762489319 current acc: 0.5316\n",
      "iteration 63 current loss: 0.8141345381736755 current acc: 0.54\n",
      "iteration 64 current loss: 0.9380605220794678 current acc: 0.5486\n",
      "iteration 65 current loss: 0.8867363929748535 current acc: 0.557\n",
      "iteration 66 current loss: 1.046110987663269 current acc: 0.5656\n",
      "iteration 67 current loss: 0.8789356350898743 current acc: 0.575\n",
      "iteration 68 current loss: 1.2094407081604004 current acc: 0.5828\n",
      "iteration 69 current loss: 0.8691440224647522 current acc: 0.5912\n",
      "iteration 70 current loss: 0.9607029557228088 current acc: 0.5996\n",
      "iteration 71 current loss: 1.0807795524597168 current acc: 0.607\n",
      "iteration 72 current loss: 1.0578207969665527 current acc: 0.6148\n",
      "iteration 73 current loss: 1.1255537271499634 current acc: 0.6218\n",
      "iteration 74 current loss: 1.2160950899124146 current acc: 0.6286\n",
      "iteration 75 current loss: 0.9917361736297607 current acc: 0.6368\n",
      "iteration 76 current loss: 0.9581586122512817 current acc: 0.6458\n",
      "iteration 77 current loss: 0.9340787529945374 current acc: 0.6538\n",
      "iteration 78 current loss: 0.9458701610565186 current acc: 0.6552\n",
      "\t\tTrain Epoch 40/100,Train Accuracy: 0.6552, Train Loss: 0.970793725569037.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 40/100, Validation Accuracy: 0.54175, Validation Loss: 1.30993709897995\n",
      "iteration 0 current loss: 1.0308332443237305 current acc: 0.0082\n",
      "iteration 1 current loss: 0.9158827662467957 current acc: 0.0174\n",
      "iteration 2 current loss: 0.9786316156387329 current acc: 0.0254\n",
      "iteration 3 current loss: 0.9451967477798462 current acc: 0.0338\n",
      "iteration 4 current loss: 0.7177826762199402 current acc: 0.0438\n",
      "iteration 5 current loss: 0.9247235059738159 current acc: 0.0526\n",
      "iteration 6 current loss: 0.9437253475189209 current acc: 0.061\n",
      "iteration 7 current loss: 0.8368623852729797 current acc: 0.0702\n",
      "iteration 8 current loss: 0.9815142154693604 current acc: 0.0776\n",
      "iteration 9 current loss: 1.1972190141677856 current acc: 0.0842\n",
      "iteration 10 current loss: 1.1787570714950562 current acc: 0.0912\n",
      "iteration 11 current loss: 0.9531388878822327 current acc: 0.0988\n",
      "iteration 12 current loss: 0.9736676216125488 current acc: 0.1078\n",
      "iteration 13 current loss: 0.7505013346672058 current acc: 0.1166\n",
      "iteration 14 current loss: 0.8820841312408447 current acc: 0.1252\n",
      "iteration 15 current loss: 1.098599910736084 current acc: 0.1326\n",
      "iteration 16 current loss: 0.9276272058486938 current acc: 0.1408\n",
      "iteration 17 current loss: 1.187641978263855 current acc: 0.148\n",
      "iteration 18 current loss: 1.088014006614685 current acc: 0.1552\n",
      "iteration 19 current loss: 0.7874947190284729 current acc: 0.1642\n",
      "iteration 20 current loss: 0.9522387981414795 current acc: 0.172\n",
      "iteration 21 current loss: 1.0068937540054321 current acc: 0.1804\n",
      "iteration 22 current loss: 1.0089751482009888 current acc: 0.188\n",
      "iteration 23 current loss: 1.1222790479660034 current acc: 0.1958\n",
      "iteration 24 current loss: 1.1972324848175049 current acc: 0.2038\n",
      "iteration 25 current loss: 0.9325240254402161 current acc: 0.2118\n",
      "iteration 26 current loss: 0.9846881031990051 current acc: 0.22\n",
      "iteration 27 current loss: 1.1788992881774902 current acc: 0.2278\n",
      "iteration 28 current loss: 0.9418259263038635 current acc: 0.2368\n",
      "iteration 29 current loss: 0.8830058574676514 current acc: 0.2454\n",
      "iteration 30 current loss: 0.9975914359092712 current acc: 0.2532\n",
      "iteration 31 current loss: 0.7641777396202087 current acc: 0.2632\n",
      "iteration 32 current loss: 0.9316439628601074 current acc: 0.272\n",
      "iteration 33 current loss: 0.9663470387458801 current acc: 0.2804\n",
      "iteration 34 current loss: 0.923366367816925 current acc: 0.2888\n",
      "iteration 35 current loss: 0.898070752620697 current acc: 0.297\n",
      "iteration 36 current loss: 0.7970291376113892 current acc: 0.3056\n",
      "iteration 37 current loss: 0.7470877766609192 current acc: 0.315\n",
      "iteration 38 current loss: 1.0546751022338867 current acc: 0.3226\n",
      "iteration 39 current loss: 0.9476998448371887 current acc: 0.331\n",
      "iteration 40 current loss: 0.8352475166320801 current acc: 0.34\n",
      "iteration 41 current loss: 1.0180842876434326 current acc: 0.348\n",
      "iteration 42 current loss: 0.9387813210487366 current acc: 0.3566\n",
      "iteration 43 current loss: 1.0799100399017334 current acc: 0.3648\n",
      "iteration 44 current loss: 0.8532682061195374 current acc: 0.3736\n",
      "iteration 45 current loss: 0.992865264415741 current acc: 0.3824\n",
      "iteration 46 current loss: 1.0411620140075684 current acc: 0.3904\n",
      "iteration 47 current loss: 0.9187168478965759 current acc: 0.399\n",
      "iteration 48 current loss: 0.8879188299179077 current acc: 0.4074\n",
      "iteration 49 current loss: 0.8368457555770874 current acc: 0.4162\n",
      "iteration 50 current loss: 0.7953398823738098 current acc: 0.4258\n",
      "iteration 51 current loss: 0.9510485529899597 current acc: 0.4348\n",
      "iteration 52 current loss: 0.8788624405860901 current acc: 0.4436\n",
      "iteration 53 current loss: 0.863533616065979 current acc: 0.4528\n",
      "iteration 54 current loss: 0.8189583420753479 current acc: 0.4616\n",
      "iteration 55 current loss: 1.0660899877548218 current acc: 0.4696\n",
      "iteration 56 current loss: 1.0139228105545044 current acc: 0.4782\n",
      "iteration 57 current loss: 1.118377685546875 current acc: 0.486\n",
      "iteration 58 current loss: 1.0129750967025757 current acc: 0.494\n",
      "iteration 59 current loss: 1.1434872150421143 current acc: 0.5016\n",
      "iteration 60 current loss: 0.9886977672576904 current acc: 0.5096\n",
      "iteration 61 current loss: 0.8474459052085876 current acc: 0.5186\n",
      "iteration 62 current loss: 0.9537595510482788 current acc: 0.5274\n",
      "iteration 63 current loss: 0.9898129105567932 current acc: 0.5356\n",
      "iteration 64 current loss: 1.0749982595443726 current acc: 0.5432\n",
      "iteration 65 current loss: 0.9770810008049011 current acc: 0.5502\n",
      "iteration 66 current loss: 0.8830059766769409 current acc: 0.5592\n",
      "iteration 67 current loss: 0.8133231401443481 current acc: 0.5686\n",
      "iteration 68 current loss: 1.017419695854187 current acc: 0.5764\n",
      "iteration 69 current loss: 1.0909336805343628 current acc: 0.5836\n",
      "iteration 70 current loss: 1.0468711853027344 current acc: 0.5912\n",
      "iteration 71 current loss: 0.9381828308105469 current acc: 0.599\n",
      "iteration 72 current loss: 0.7472453713417053 current acc: 0.6086\n",
      "iteration 73 current loss: 0.9565622210502625 current acc: 0.6168\n",
      "iteration 74 current loss: 0.9106646776199341 current acc: 0.6256\n",
      "iteration 75 current loss: 0.7309203147888184 current acc: 0.636\n",
      "iteration 76 current loss: 0.9568936824798584 current acc: 0.6454\n",
      "iteration 77 current loss: 0.963710606098175 current acc: 0.6536\n",
      "iteration 78 current loss: 1.6197426319122314 current acc: 0.6542\n",
      "\t\tTrain Epoch 41/100,Train Accuracy: 0.6542, Train Loss: 0.9634027481079102.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 41/100, Validation Accuracy: 0.553375, Validation Loss: 1.2509336652755738\n",
      "iteration 0 current loss: 0.8608405590057373 current acc: 0.0096\n",
      "iteration 1 current loss: 1.0130932331085205 current acc: 0.0182\n",
      "iteration 2 current loss: 0.7911081910133362 current acc: 0.0278\n",
      "iteration 3 current loss: 1.0601310729980469 current acc: 0.0354\n",
      "iteration 4 current loss: 1.0545903444290161 current acc: 0.0432\n",
      "iteration 5 current loss: 1.354317545890808 current acc: 0.0488\n",
      "iteration 6 current loss: 1.263817310333252 current acc: 0.0556\n",
      "iteration 7 current loss: 1.1114768981933594 current acc: 0.0634\n",
      "iteration 8 current loss: 1.0443949699401855 current acc: 0.0712\n",
      "iteration 9 current loss: 0.8116421103477478 current acc: 0.0802\n",
      "iteration 10 current loss: 1.0105358362197876 current acc: 0.089\n",
      "iteration 11 current loss: 0.6975266933441162 current acc: 0.099\n",
      "iteration 12 current loss: 0.7003698945045471 current acc: 0.1084\n",
      "iteration 13 current loss: 1.0720305442810059 current acc: 0.1156\n",
      "iteration 14 current loss: 0.9197596907615662 current acc: 0.1236\n",
      "iteration 15 current loss: 0.9499781727790833 current acc: 0.1318\n",
      "iteration 16 current loss: 1.0120447874069214 current acc: 0.1398\n",
      "iteration 17 current loss: 0.8897237777709961 current acc: 0.1488\n",
      "iteration 18 current loss: 1.005121111869812 current acc: 0.1572\n",
      "iteration 19 current loss: 0.9483832716941833 current acc: 0.1656\n",
      "iteration 20 current loss: 0.9216704368591309 current acc: 0.1736\n",
      "iteration 21 current loss: 0.8317461013793945 current acc: 0.1826\n",
      "iteration 22 current loss: 1.1108229160308838 current acc: 0.191\n",
      "iteration 23 current loss: 1.0271883010864258 current acc: 0.1994\n",
      "iteration 24 current loss: 0.8098036050796509 current acc: 0.208\n",
      "iteration 25 current loss: 0.990190863609314 current acc: 0.2166\n",
      "iteration 26 current loss: 0.9114483594894409 current acc: 0.2258\n",
      "iteration 27 current loss: 0.865998387336731 current acc: 0.2346\n",
      "iteration 28 current loss: 0.8993154764175415 current acc: 0.243\n",
      "iteration 29 current loss: 0.9261070489883423 current acc: 0.2512\n",
      "iteration 30 current loss: 0.8536937832832336 current acc: 0.2608\n",
      "iteration 31 current loss: 1.086312174797058 current acc: 0.269\n",
      "iteration 32 current loss: 0.689153254032135 current acc: 0.2792\n",
      "iteration 33 current loss: 1.286003589630127 current acc: 0.286\n",
      "iteration 34 current loss: 0.9309350848197937 current acc: 0.2948\n",
      "iteration 35 current loss: 1.0493565797805786 current acc: 0.3036\n",
      "iteration 36 current loss: 1.0757341384887695 current acc: 0.3116\n",
      "iteration 37 current loss: 1.1639409065246582 current acc: 0.3194\n",
      "iteration 38 current loss: 0.8803939819335938 current acc: 0.3284\n",
      "iteration 39 current loss: 1.0006314516067505 current acc: 0.3366\n",
      "iteration 40 current loss: 1.0299772024154663 current acc: 0.344\n",
      "iteration 41 current loss: 1.0547343492507935 current acc: 0.352\n",
      "iteration 42 current loss: 0.9701951146125793 current acc: 0.3606\n",
      "iteration 43 current loss: 1.057673454284668 current acc: 0.3686\n",
      "iteration 44 current loss: 0.9617801904678345 current acc: 0.3774\n",
      "iteration 45 current loss: 0.9226313829421997 current acc: 0.3862\n",
      "iteration 46 current loss: 0.9508413672447205 current acc: 0.3952\n",
      "iteration 47 current loss: 0.8213936686515808 current acc: 0.405\n",
      "iteration 48 current loss: 1.0478683710098267 current acc: 0.4126\n",
      "iteration 49 current loss: 0.8900406360626221 current acc: 0.4218\n",
      "iteration 50 current loss: 0.9646994471549988 current acc: 0.4302\n",
      "iteration 51 current loss: 0.9482134580612183 current acc: 0.439\n",
      "iteration 52 current loss: 1.258078694343567 current acc: 0.4458\n",
      "iteration 53 current loss: 1.0911855697631836 current acc: 0.454\n",
      "iteration 54 current loss: 0.961162805557251 current acc: 0.4616\n",
      "iteration 55 current loss: 0.9931334853172302 current acc: 0.4696\n",
      "iteration 56 current loss: 1.3730483055114746 current acc: 0.4754\n",
      "iteration 57 current loss: 1.056546926498413 current acc: 0.4838\n",
      "iteration 58 current loss: 1.092624306678772 current acc: 0.491\n",
      "iteration 59 current loss: 1.2077503204345703 current acc: 0.499\n",
      "iteration 60 current loss: 0.8268360495567322 current acc: 0.508\n",
      "iteration 61 current loss: 1.050356149673462 current acc: 0.5164\n",
      "iteration 62 current loss: 0.8363552093505859 current acc: 0.5252\n",
      "iteration 63 current loss: 0.8965575098991394 current acc: 0.5336\n",
      "iteration 64 current loss: 1.0318865776062012 current acc: 0.5424\n",
      "iteration 65 current loss: 0.9886351823806763 current acc: 0.551\n",
      "iteration 66 current loss: 1.1802700757980347 current acc: 0.5586\n",
      "iteration 67 current loss: 0.9308146238327026 current acc: 0.5666\n",
      "iteration 68 current loss: 0.8137878179550171 current acc: 0.576\n",
      "iteration 69 current loss: 0.9788506031036377 current acc: 0.584\n",
      "iteration 70 current loss: 0.9746041893959045 current acc: 0.5928\n",
      "iteration 71 current loss: 1.17002272605896 current acc: 0.6018\n",
      "iteration 72 current loss: 0.8125136494636536 current acc: 0.6116\n",
      "iteration 73 current loss: 1.0184978246688843 current acc: 0.6194\n",
      "iteration 74 current loss: 1.0139745473861694 current acc: 0.6278\n",
      "iteration 75 current loss: 0.9124054312705994 current acc: 0.6366\n",
      "iteration 76 current loss: 0.8329328298568726 current acc: 0.6458\n",
      "iteration 77 current loss: 0.9590038657188416 current acc: 0.654\n",
      "iteration 78 current loss: 1.0967859029769897 current acc: 0.655\n",
      "\t\tTrain Epoch 42/100,Train Accuracy: 0.655, Train Loss: 0.9855696491048306.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 42/100, Validation Accuracy: 0.541125, Validation Loss: 1.2817538709640504\n",
      "iteration 0 current loss: 0.8908356428146362 current acc: 0.0082\n",
      "iteration 1 current loss: 0.8892521262168884 current acc: 0.017\n",
      "iteration 2 current loss: 1.0654419660568237 current acc: 0.0248\n",
      "iteration 3 current loss: 1.174795389175415 current acc: 0.0314\n",
      "iteration 4 current loss: 1.0164040327072144 current acc: 0.04\n",
      "iteration 5 current loss: 1.011223316192627 current acc: 0.049\n",
      "iteration 6 current loss: 1.0938351154327393 current acc: 0.0572\n",
      "iteration 7 current loss: 0.9127374291419983 current acc: 0.066\n",
      "iteration 8 current loss: 1.035215139389038 current acc: 0.074\n",
      "iteration 9 current loss: 1.1304783821105957 current acc: 0.0816\n",
      "iteration 10 current loss: 1.0427957773208618 current acc: 0.0886\n",
      "iteration 11 current loss: 0.8657031059265137 current acc: 0.0966\n",
      "iteration 12 current loss: 0.8905448913574219 current acc: 0.1056\n",
      "iteration 13 current loss: 0.9578419923782349 current acc: 0.1136\n",
      "iteration 14 current loss: 0.9919261932373047 current acc: 0.1218\n",
      "iteration 15 current loss: 1.019373893737793 current acc: 0.1302\n",
      "iteration 16 current loss: 0.765055239200592 current acc: 0.139\n",
      "iteration 17 current loss: 0.9568990468978882 current acc: 0.1484\n",
      "iteration 18 current loss: 0.8088645935058594 current acc: 0.1576\n",
      "iteration 19 current loss: 0.9705110788345337 current acc: 0.1662\n",
      "iteration 20 current loss: 1.1155848503112793 current acc: 0.1734\n",
      "iteration 21 current loss: 1.0631569623947144 current acc: 0.1812\n",
      "iteration 22 current loss: 0.9239419102668762 current acc: 0.1898\n",
      "iteration 23 current loss: 1.0329841375350952 current acc: 0.1984\n",
      "iteration 24 current loss: 0.6226261854171753 current acc: 0.2088\n",
      "iteration 25 current loss: 1.0653237104415894 current acc: 0.2158\n",
      "iteration 26 current loss: 0.9772953987121582 current acc: 0.2234\n",
      "iteration 27 current loss: 0.7979831695556641 current acc: 0.233\n",
      "iteration 28 current loss: 0.9391096830368042 current acc: 0.242\n",
      "iteration 29 current loss: 0.9525336623191833 current acc: 0.2502\n",
      "iteration 30 current loss: 0.9728266000747681 current acc: 0.2586\n",
      "iteration 31 current loss: 0.9922126531600952 current acc: 0.2668\n",
      "iteration 32 current loss: 1.0430587530136108 current acc: 0.2746\n",
      "iteration 33 current loss: 1.055999755859375 current acc: 0.2828\n",
      "iteration 34 current loss: 0.7274425625801086 current acc: 0.2924\n",
      "iteration 35 current loss: 0.8076297640800476 current acc: 0.3022\n",
      "iteration 36 current loss: 1.034698724746704 current acc: 0.3098\n",
      "iteration 37 current loss: 0.8090658783912659 current acc: 0.3194\n",
      "iteration 38 current loss: 0.8785669207572937 current acc: 0.3284\n",
      "iteration 39 current loss: 1.1832597255706787 current acc: 0.3358\n",
      "iteration 40 current loss: 0.8236316442489624 current acc: 0.3452\n",
      "iteration 41 current loss: 1.0884027481079102 current acc: 0.353\n",
      "iteration 42 current loss: 0.9649660587310791 current acc: 0.3614\n",
      "iteration 43 current loss: 1.0531107187271118 current acc: 0.369\n",
      "iteration 44 current loss: 1.2104071378707886 current acc: 0.3762\n",
      "iteration 45 current loss: 0.8401579856872559 current acc: 0.385\n",
      "iteration 46 current loss: 0.9279664158821106 current acc: 0.394\n",
      "iteration 47 current loss: 0.9443572163581848 current acc: 0.4018\n",
      "iteration 48 current loss: 0.883624255657196 current acc: 0.4112\n",
      "iteration 49 current loss: 0.9377509951591492 current acc: 0.42\n",
      "iteration 50 current loss: 0.7989298701286316 current acc: 0.4292\n",
      "iteration 51 current loss: 0.796858549118042 current acc: 0.4386\n",
      "iteration 52 current loss: 0.8908211588859558 current acc: 0.447\n",
      "iteration 53 current loss: 0.865385115146637 current acc: 0.4552\n",
      "iteration 54 current loss: 0.8553142547607422 current acc: 0.4638\n",
      "iteration 55 current loss: 1.17629873752594 current acc: 0.4708\n",
      "iteration 56 current loss: 0.9599320888519287 current acc: 0.4784\n",
      "iteration 57 current loss: 0.8957128524780273 current acc: 0.4868\n",
      "iteration 58 current loss: 0.9561319351196289 current acc: 0.4956\n",
      "iteration 59 current loss: 1.0111125707626343 current acc: 0.5036\n",
      "iteration 60 current loss: 0.8426105380058289 current acc: 0.5124\n",
      "iteration 61 current loss: 1.1089023351669312 current acc: 0.5206\n",
      "iteration 62 current loss: 0.8327747583389282 current acc: 0.5294\n",
      "iteration 63 current loss: 0.8509868383407593 current acc: 0.5376\n",
      "iteration 64 current loss: 1.0272434949874878 current acc: 0.5456\n",
      "iteration 65 current loss: 1.0634979009628296 current acc: 0.5534\n",
      "iteration 66 current loss: 1.1432538032531738 current acc: 0.5604\n",
      "iteration 67 current loss: 0.860443115234375 current acc: 0.569\n",
      "iteration 68 current loss: 0.9901375770568848 current acc: 0.5772\n",
      "iteration 69 current loss: 0.9995366930961609 current acc: 0.5846\n",
      "iteration 70 current loss: 0.8620749115943909 current acc: 0.594\n",
      "iteration 71 current loss: 1.0035654306411743 current acc: 0.6022\n",
      "iteration 72 current loss: 0.7134284973144531 current acc: 0.612\n",
      "iteration 73 current loss: 0.9528867602348328 current acc: 0.6206\n",
      "iteration 74 current loss: 0.857272744178772 current acc: 0.628\n",
      "iteration 75 current loss: 1.0880197286605835 current acc: 0.6358\n",
      "iteration 76 current loss: 1.2322359085083008 current acc: 0.6434\n",
      "iteration 77 current loss: 1.0260846614837646 current acc: 0.6514\n",
      "iteration 78 current loss: 0.9084925055503845 current acc: 0.6526\n",
      "\t\tTrain Epoch 43/100,Train Accuracy: 0.6526, Train Loss: 0.958599441413638.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 43/100, Validation Accuracy: 0.5535, Validation Loss: 1.2411520142555237\n",
      "best loss 0.958599441413638\n",
      "iteration 0 current loss: 0.8459643125534058 current acc: 0.0086\n",
      "iteration 1 current loss: 0.8252500295639038 current acc: 0.0176\n",
      "iteration 2 current loss: 1.0123724937438965 current acc: 0.0262\n",
      "iteration 3 current loss: 0.8840492367744446 current acc: 0.0354\n",
      "iteration 4 current loss: 0.7142736315727234 current acc: 0.045\n",
      "iteration 5 current loss: 0.900917649269104 current acc: 0.054\n",
      "iteration 6 current loss: 1.051544427871704 current acc: 0.0622\n",
      "iteration 7 current loss: 0.7606733441352844 current acc: 0.0716\n",
      "iteration 8 current loss: 1.1591647863388062 current acc: 0.0792\n",
      "iteration 9 current loss: 0.9801688194274902 current acc: 0.0868\n",
      "iteration 10 current loss: 0.752933919429779 current acc: 0.0972\n",
      "iteration 11 current loss: 1.0235083103179932 current acc: 0.105\n",
      "iteration 12 current loss: 0.8157145977020264 current acc: 0.1138\n",
      "iteration 13 current loss: 0.8979727625846863 current acc: 0.1228\n",
      "iteration 14 current loss: 1.0074825286865234 current acc: 0.131\n",
      "iteration 15 current loss: 0.8444833755493164 current acc: 0.1398\n",
      "iteration 16 current loss: 0.7865468859672546 current acc: 0.149\n",
      "iteration 17 current loss: 1.0391680002212524 current acc: 0.157\n",
      "iteration 18 current loss: 1.2372123003005981 current acc: 0.1644\n",
      "iteration 19 current loss: 1.049660325050354 current acc: 0.173\n",
      "iteration 20 current loss: 0.989849328994751 current acc: 0.182\n",
      "iteration 21 current loss: 0.9623714089393616 current acc: 0.191\n",
      "iteration 22 current loss: 0.8204347491264343 current acc: 0.2\n",
      "iteration 23 current loss: 0.8669756054878235 current acc: 0.2088\n",
      "iteration 24 current loss: 0.8934549689292908 current acc: 0.2172\n",
      "iteration 25 current loss: 0.8478935956954956 current acc: 0.2264\n",
      "iteration 26 current loss: 1.0159538984298706 current acc: 0.2344\n",
      "iteration 27 current loss: 1.033488154411316 current acc: 0.243\n",
      "iteration 28 current loss: 1.1236344575881958 current acc: 0.2504\n",
      "iteration 29 current loss: 0.8298154473304749 current acc: 0.2586\n",
      "iteration 30 current loss: 0.9274768233299255 current acc: 0.2678\n",
      "iteration 31 current loss: 1.023803472518921 current acc: 0.2754\n",
      "iteration 32 current loss: 0.7960604429244995 current acc: 0.2842\n",
      "iteration 33 current loss: 0.9826576709747314 current acc: 0.2928\n",
      "iteration 34 current loss: 0.8923316597938538 current acc: 0.3016\n",
      "iteration 35 current loss: 0.8996036052703857 current acc: 0.3104\n",
      "iteration 36 current loss: 0.8923872709274292 current acc: 0.3186\n",
      "iteration 37 current loss: 0.9245469570159912 current acc: 0.3268\n",
      "iteration 38 current loss: 1.2000406980514526 current acc: 0.3336\n",
      "iteration 39 current loss: 0.825741708278656 current acc: 0.3426\n",
      "iteration 40 current loss: 0.994790256023407 current acc: 0.351\n",
      "iteration 41 current loss: 0.9318139553070068 current acc: 0.359\n",
      "iteration 42 current loss: 0.9409571290016174 current acc: 0.3676\n",
      "iteration 43 current loss: 0.944951593875885 current acc: 0.3762\n",
      "iteration 44 current loss: 0.885223925113678 current acc: 0.385\n",
      "iteration 45 current loss: 0.9321277737617493 current acc: 0.394\n",
      "iteration 46 current loss: 1.1624921560287476 current acc: 0.4016\n",
      "iteration 47 current loss: 0.8843516111373901 current acc: 0.4108\n",
      "iteration 48 current loss: 1.0051432847976685 current acc: 0.419\n",
      "iteration 49 current loss: 1.0301616191864014 current acc: 0.4272\n",
      "iteration 50 current loss: 1.2124218940734863 current acc: 0.4346\n",
      "iteration 51 current loss: 0.9507570266723633 current acc: 0.4434\n",
      "iteration 52 current loss: 0.8326800465583801 current acc: 0.4522\n",
      "iteration 53 current loss: 0.9881580471992493 current acc: 0.461\n",
      "iteration 54 current loss: 1.0155271291732788 current acc: 0.4686\n",
      "iteration 55 current loss: 1.0055967569351196 current acc: 0.4764\n",
      "iteration 56 current loss: 1.0742748975753784 current acc: 0.4846\n",
      "iteration 57 current loss: 1.1083073616027832 current acc: 0.4926\n",
      "iteration 58 current loss: 1.1017125844955444 current acc: 0.501\n",
      "iteration 59 current loss: 0.8966141939163208 current acc: 0.5094\n",
      "iteration 60 current loss: 1.2900967597961426 current acc: 0.5168\n",
      "iteration 61 current loss: 1.178025484085083 current acc: 0.5238\n",
      "iteration 62 current loss: 1.1286441087722778 current acc: 0.5314\n",
      "iteration 63 current loss: 1.0177571773529053 current acc: 0.5398\n",
      "iteration 64 current loss: 0.9864848256111145 current acc: 0.5484\n",
      "iteration 65 current loss: 0.7958816289901733 current acc: 0.5578\n",
      "iteration 66 current loss: 0.9597573280334473 current acc: 0.5656\n",
      "iteration 67 current loss: 0.9809451103210449 current acc: 0.574\n",
      "iteration 68 current loss: 1.04349684715271 current acc: 0.5812\n",
      "iteration 69 current loss: 0.993864893913269 current acc: 0.5894\n",
      "iteration 70 current loss: 0.9684972763061523 current acc: 0.5974\n",
      "iteration 71 current loss: 0.9941819906234741 current acc: 0.605\n",
      "iteration 72 current loss: 0.8178306818008423 current acc: 0.6144\n",
      "iteration 73 current loss: 1.0804815292358398 current acc: 0.6222\n",
      "iteration 74 current loss: 0.8819202184677124 current acc: 0.631\n",
      "iteration 75 current loss: 0.9739536046981812 current acc: 0.6398\n",
      "iteration 76 current loss: 0.9834473133087158 current acc: 0.6478\n",
      "iteration 77 current loss: 1.1554665565490723 current acc: 0.655\n",
      "iteration 78 current loss: 0.9568382501602173 current acc: 0.6562\n",
      "\t\tTrain Epoch 44/100,Train Accuracy: 0.6562, Train Loss: 0.9673825125151043.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 44/100, Validation Accuracy: 0.541875, Validation Loss: 1.305542013168335\n",
      "iteration 0 current loss: 0.8739880919456482 current acc: 0.0094\n",
      "iteration 1 current loss: 0.9758189916610718 current acc: 0.0178\n",
      "iteration 2 current loss: 0.7095697522163391 current acc: 0.0276\n",
      "iteration 3 current loss: 1.0111900568008423 current acc: 0.0352\n",
      "iteration 4 current loss: 1.1615928411483765 current acc: 0.0424\n",
      "iteration 5 current loss: 1.0878167152404785 current acc: 0.05\n",
      "iteration 6 current loss: 0.7899419665336609 current acc: 0.0594\n",
      "iteration 7 current loss: 1.1826770305633545 current acc: 0.0672\n",
      "iteration 8 current loss: 0.9371251463890076 current acc: 0.0756\n",
      "iteration 9 current loss: 0.9167812466621399 current acc: 0.084\n",
      "iteration 10 current loss: 0.934883177280426 current acc: 0.0924\n",
      "iteration 11 current loss: 1.0830668210983276 current acc: 0.1004\n",
      "iteration 12 current loss: 0.9708458185195923 current acc: 0.1086\n",
      "iteration 13 current loss: 0.8643369674682617 current acc: 0.1174\n",
      "iteration 14 current loss: 0.9133601188659668 current acc: 0.1264\n",
      "iteration 15 current loss: 1.0246695280075073 current acc: 0.1348\n",
      "iteration 16 current loss: 0.9091736674308777 current acc: 0.1442\n",
      "iteration 17 current loss: 0.6981387138366699 current acc: 0.1538\n",
      "iteration 18 current loss: 1.1188180446624756 current acc: 0.1614\n",
      "iteration 19 current loss: 0.8533632755279541 current acc: 0.1706\n",
      "iteration 20 current loss: 1.0821053981781006 current acc: 0.1784\n",
      "iteration 21 current loss: 1.0197734832763672 current acc: 0.1862\n",
      "iteration 22 current loss: 1.0861371755599976 current acc: 0.194\n",
      "iteration 23 current loss: 0.9211108684539795 current acc: 0.2028\n",
      "iteration 24 current loss: 0.9744464159011841 current acc: 0.2112\n",
      "iteration 25 current loss: 0.9452497363090515 current acc: 0.22\n",
      "iteration 26 current loss: 0.9622718691825867 current acc: 0.2282\n",
      "iteration 27 current loss: 1.0499366521835327 current acc: 0.236\n",
      "iteration 28 current loss: 0.9262609481811523 current acc: 0.2452\n",
      "iteration 29 current loss: 0.7666142582893372 current acc: 0.2552\n",
      "iteration 30 current loss: 0.911331295967102 current acc: 0.2638\n",
      "iteration 31 current loss: 0.8570835590362549 current acc: 0.273\n",
      "iteration 32 current loss: 0.9972718954086304 current acc: 0.281\n",
      "iteration 33 current loss: 1.003274917602539 current acc: 0.289\n",
      "iteration 34 current loss: 0.8433231115341187 current acc: 0.2978\n",
      "iteration 35 current loss: 0.9866573810577393 current acc: 0.3064\n",
      "iteration 36 current loss: 0.955197811126709 current acc: 0.3144\n",
      "iteration 37 current loss: 0.8153089284896851 current acc: 0.323\n",
      "iteration 38 current loss: 1.1548805236816406 current acc: 0.331\n",
      "iteration 39 current loss: 0.9034690260887146 current acc: 0.3386\n",
      "iteration 40 current loss: 0.9468181729316711 current acc: 0.3472\n",
      "iteration 41 current loss: 0.9742907881736755 current acc: 0.3554\n",
      "iteration 42 current loss: 0.8013959527015686 current acc: 0.3646\n",
      "iteration 43 current loss: 1.0859415531158447 current acc: 0.3716\n",
      "iteration 44 current loss: 0.9767056107521057 current acc: 0.3798\n",
      "iteration 45 current loss: 0.9439005851745605 current acc: 0.3878\n",
      "iteration 46 current loss: 0.9835259914398193 current acc: 0.3962\n",
      "iteration 47 current loss: 0.8990010023117065 current acc: 0.4044\n",
      "iteration 48 current loss: 1.0310765504837036 current acc: 0.4122\n",
      "iteration 49 current loss: 1.0347938537597656 current acc: 0.4188\n",
      "iteration 50 current loss: 0.7682520747184753 current acc: 0.4268\n",
      "iteration 51 current loss: 1.0062698125839233 current acc: 0.4354\n",
      "iteration 52 current loss: 1.1836674213409424 current acc: 0.443\n",
      "iteration 53 current loss: 1.1433286666870117 current acc: 0.4506\n",
      "iteration 54 current loss: 1.008749008178711 current acc: 0.458\n",
      "iteration 55 current loss: 1.0668213367462158 current acc: 0.466\n",
      "iteration 56 current loss: 1.054684042930603 current acc: 0.4742\n",
      "iteration 57 current loss: 1.0479296445846558 current acc: 0.4814\n",
      "iteration 58 current loss: 1.050963282585144 current acc: 0.4894\n",
      "iteration 59 current loss: 0.8965323567390442 current acc: 0.498\n",
      "iteration 60 current loss: 0.8663958311080933 current acc: 0.5072\n",
      "iteration 61 current loss: 0.7416207790374756 current acc: 0.5168\n",
      "iteration 62 current loss: 1.1653311252593994 current acc: 0.5242\n",
      "iteration 63 current loss: 0.8647466897964478 current acc: 0.533\n",
      "iteration 64 current loss: 0.9531756639480591 current acc: 0.541\n",
      "iteration 65 current loss: 0.8913300037384033 current acc: 0.549\n",
      "iteration 66 current loss: 0.8760725259780884 current acc: 0.5576\n",
      "iteration 67 current loss: 0.7668586373329163 current acc: 0.5672\n",
      "iteration 68 current loss: 0.833658754825592 current acc: 0.5764\n",
      "iteration 69 current loss: 1.1609156131744385 current acc: 0.5838\n",
      "iteration 70 current loss: 1.0835918188095093 current acc: 0.5922\n",
      "iteration 71 current loss: 0.9944931864738464 current acc: 0.601\n",
      "iteration 72 current loss: 0.9201863408088684 current acc: 0.61\n",
      "iteration 73 current loss: 0.9282377362251282 current acc: 0.6182\n",
      "iteration 74 current loss: 0.8588599562644958 current acc: 0.6264\n",
      "iteration 75 current loss: 0.8475424647331238 current acc: 0.6348\n",
      "iteration 76 current loss: 1.0157921314239502 current acc: 0.643\n",
      "iteration 77 current loss: 1.1016881465911865 current acc: 0.6506\n",
      "iteration 78 current loss: 0.8134803175926208 current acc: 0.6518\n",
      "\t\tTrain Epoch 45/100,Train Accuracy: 0.6518, Train Loss: 0.9590821349168126.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 45/100, Validation Accuracy: 0.551875, Validation Loss: 1.2568224439620972\n",
      "iteration 0 current loss: 0.9164938926696777 current acc: 0.0084\n",
      "iteration 1 current loss: 0.9641782641410828 current acc: 0.0172\n",
      "iteration 2 current loss: 0.8153865933418274 current acc: 0.027\n",
      "iteration 3 current loss: 0.9525253176689148 current acc: 0.0352\n",
      "iteration 4 current loss: 1.0375213623046875 current acc: 0.0424\n",
      "iteration 5 current loss: 0.8289722204208374 current acc: 0.0518\n",
      "iteration 6 current loss: 0.8414686918258667 current acc: 0.0606\n",
      "iteration 7 current loss: 1.0405113697052002 current acc: 0.0682\n",
      "iteration 8 current loss: 0.8417186737060547 current acc: 0.0776\n",
      "iteration 9 current loss: 0.9185623526573181 current acc: 0.086\n",
      "iteration 10 current loss: 0.9822627902030945 current acc: 0.094\n",
      "iteration 11 current loss: 1.183448076248169 current acc: 0.1022\n",
      "iteration 12 current loss: 0.880322277545929 current acc: 0.1112\n",
      "iteration 13 current loss: 0.685681164264679 current acc: 0.122\n",
      "iteration 14 current loss: 0.7101759910583496 current acc: 0.132\n",
      "iteration 15 current loss: 0.9883302450180054 current acc: 0.139\n",
      "iteration 16 current loss: 1.1485744714736938 current acc: 0.146\n",
      "iteration 17 current loss: 1.0242449045181274 current acc: 0.1544\n",
      "iteration 18 current loss: 1.129380464553833 current acc: 0.162\n",
      "iteration 19 current loss: 0.9498838782310486 current acc: 0.1704\n",
      "iteration 20 current loss: 0.864020824432373 current acc: 0.1794\n",
      "iteration 21 current loss: 0.9366632103919983 current acc: 0.1876\n",
      "iteration 22 current loss: 0.9736887216567993 current acc: 0.1962\n",
      "iteration 23 current loss: 0.8849626779556274 current acc: 0.206\n",
      "iteration 24 current loss: 0.829143762588501 current acc: 0.2146\n",
      "iteration 25 current loss: 0.7784934043884277 current acc: 0.2236\n",
      "iteration 26 current loss: 0.8932994604110718 current acc: 0.2322\n",
      "iteration 27 current loss: 1.1673061847686768 current acc: 0.2394\n",
      "iteration 28 current loss: 0.862769365310669 current acc: 0.2486\n",
      "iteration 29 current loss: 0.9840899109840393 current acc: 0.2578\n",
      "iteration 30 current loss: 1.01759934425354 current acc: 0.2656\n",
      "iteration 31 current loss: 0.9769973754882812 current acc: 0.273\n",
      "iteration 32 current loss: 0.9862562417984009 current acc: 0.2814\n",
      "iteration 33 current loss: 1.0340269804000854 current acc: 0.2896\n",
      "iteration 34 current loss: 1.0564488172531128 current acc: 0.2982\n",
      "iteration 35 current loss: 0.8436132669448853 current acc: 0.3074\n",
      "iteration 36 current loss: 0.7966528534889221 current acc: 0.3178\n",
      "iteration 37 current loss: 0.8736198544502258 current acc: 0.327\n",
      "iteration 38 current loss: 0.9941437840461731 current acc: 0.3352\n",
      "iteration 39 current loss: 1.090226411819458 current acc: 0.3424\n",
      "iteration 40 current loss: 1.108642339706421 current acc: 0.35\n",
      "iteration 41 current loss: 0.8899244666099548 current acc: 0.359\n",
      "iteration 42 current loss: 0.8321804404258728 current acc: 0.3684\n",
      "iteration 43 current loss: 0.8814163208007812 current acc: 0.3766\n",
      "iteration 44 current loss: 1.041107177734375 current acc: 0.3842\n",
      "iteration 45 current loss: 0.8906986713409424 current acc: 0.3934\n",
      "iteration 46 current loss: 1.041594386100769 current acc: 0.4\n",
      "iteration 47 current loss: 1.0576835870742798 current acc: 0.4078\n",
      "iteration 48 current loss: 0.8909253478050232 current acc: 0.4164\n",
      "iteration 49 current loss: 0.9374790191650391 current acc: 0.4258\n",
      "iteration 50 current loss: 0.7280565500259399 current acc: 0.4358\n",
      "iteration 51 current loss: 0.905099630355835 current acc: 0.4448\n",
      "iteration 52 current loss: 0.8435023427009583 current acc: 0.4534\n",
      "iteration 53 current loss: 0.9733346104621887 current acc: 0.4622\n",
      "iteration 54 current loss: 1.0111775398254395 current acc: 0.4706\n",
      "iteration 55 current loss: 1.0266423225402832 current acc: 0.4788\n",
      "iteration 56 current loss: 0.9749270081520081 current acc: 0.4864\n",
      "iteration 57 current loss: 0.9527361392974854 current acc: 0.4958\n",
      "iteration 58 current loss: 0.8019459247589111 current acc: 0.505\n",
      "iteration 59 current loss: 1.0700610876083374 current acc: 0.5124\n",
      "iteration 60 current loss: 0.9417417049407959 current acc: 0.5206\n",
      "iteration 61 current loss: 1.0474815368652344 current acc: 0.528\n",
      "iteration 62 current loss: 1.123010516166687 current acc: 0.5356\n",
      "iteration 63 current loss: 0.8944927453994751 current acc: 0.5442\n",
      "iteration 64 current loss: 1.073998212814331 current acc: 0.5518\n",
      "iteration 65 current loss: 0.9725503921508789 current acc: 0.5602\n",
      "iteration 66 current loss: 1.1288267374038696 current acc: 0.5678\n",
      "iteration 67 current loss: 0.9958802461624146 current acc: 0.5752\n",
      "iteration 68 current loss: 0.9637904167175293 current acc: 0.5836\n",
      "iteration 69 current loss: 1.1580551862716675 current acc: 0.5912\n",
      "iteration 70 current loss: 0.9886406064033508 current acc: 0.5984\n",
      "iteration 71 current loss: 0.8976843357086182 current acc: 0.6074\n",
      "iteration 72 current loss: 1.0501337051391602 current acc: 0.6158\n",
      "iteration 73 current loss: 0.9054335355758667 current acc: 0.6242\n",
      "iteration 74 current loss: 0.817684531211853 current acc: 0.6334\n",
      "iteration 75 current loss: 1.1861969232559204 current acc: 0.6404\n",
      "iteration 76 current loss: 0.883253276348114 current acc: 0.6496\n",
      "iteration 77 current loss: 0.8037202954292297 current acc: 0.6586\n",
      "iteration 78 current loss: 0.9724398255348206 current acc: 0.6594\n",
      "\t\tTrain Epoch 46/100,Train Accuracy: 0.6594, Train Loss: 0.9537698113465611.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 46/100, Validation Accuracy: 0.53975, Validation Loss: 1.2945726647377014\n",
      "best loss 0.9537698113465611\n",
      "iteration 0 current loss: 0.8934200406074524 current acc: 0.009\n",
      "iteration 1 current loss: 0.8167341351509094 current acc: 0.0174\n",
      "iteration 2 current loss: 0.9346590042114258 current acc: 0.0262\n",
      "iteration 3 current loss: 1.0498417615890503 current acc: 0.0332\n",
      "iteration 4 current loss: 0.8168155550956726 current acc: 0.0418\n",
      "iteration 5 current loss: 1.0049681663513184 current acc: 0.0492\n",
      "iteration 6 current loss: 1.230252742767334 current acc: 0.0564\n",
      "iteration 7 current loss: 0.8201021552085876 current acc: 0.0662\n",
      "iteration 8 current loss: 0.8519366979598999 current acc: 0.0748\n",
      "iteration 9 current loss: 0.8777128458023071 current acc: 0.0838\n",
      "iteration 10 current loss: 0.8585558533668518 current acc: 0.0928\n",
      "iteration 11 current loss: 0.8977662920951843 current acc: 0.1014\n",
      "iteration 12 current loss: 0.9038434624671936 current acc: 0.1108\n",
      "iteration 13 current loss: 1.0859843492507935 current acc: 0.1176\n",
      "iteration 14 current loss: 1.0506935119628906 current acc: 0.1248\n",
      "iteration 15 current loss: 0.8244121074676514 current acc: 0.1342\n",
      "iteration 16 current loss: 0.9534541964530945 current acc: 0.1428\n",
      "iteration 17 current loss: 0.8540729284286499 current acc: 0.1518\n",
      "iteration 18 current loss: 1.0697400569915771 current acc: 0.1596\n",
      "iteration 19 current loss: 0.9825010299682617 current acc: 0.1674\n",
      "iteration 20 current loss: 0.9989617466926575 current acc: 0.1752\n",
      "iteration 21 current loss: 0.8855702877044678 current acc: 0.1834\n",
      "iteration 22 current loss: 0.912575900554657 current acc: 0.1926\n",
      "iteration 23 current loss: 0.9801196455955505 current acc: 0.201\n",
      "iteration 24 current loss: 0.962946355342865 current acc: 0.2088\n",
      "iteration 25 current loss: 0.9212734699249268 current acc: 0.2176\n",
      "iteration 26 current loss: 0.9559341073036194 current acc: 0.2264\n",
      "iteration 27 current loss: 0.9891718626022339 current acc: 0.2344\n",
      "iteration 28 current loss: 1.1604000329971313 current acc: 0.2416\n",
      "iteration 29 current loss: 0.9959849119186401 current acc: 0.249\n",
      "iteration 30 current loss: 0.9142987132072449 current acc: 0.2582\n",
      "iteration 31 current loss: 1.1916080713272095 current acc: 0.264\n",
      "iteration 32 current loss: 0.987074077129364 current acc: 0.2726\n",
      "iteration 33 current loss: 1.007289171218872 current acc: 0.2802\n",
      "iteration 34 current loss: 0.9391674399375916 current acc: 0.2884\n",
      "iteration 35 current loss: 1.3405965566635132 current acc: 0.295\n",
      "iteration 36 current loss: 1.0590872764587402 current acc: 0.3026\n",
      "iteration 37 current loss: 0.9866165518760681 current acc: 0.3108\n",
      "iteration 38 current loss: 0.9134300947189331 current acc: 0.319\n",
      "iteration 39 current loss: 1.028754711151123 current acc: 0.3268\n",
      "iteration 40 current loss: 0.8661371469497681 current acc: 0.3362\n",
      "iteration 41 current loss: 1.0891340970993042 current acc: 0.344\n",
      "iteration 42 current loss: 0.965709388256073 current acc: 0.3524\n",
      "iteration 43 current loss: 0.9837197065353394 current acc: 0.3596\n",
      "iteration 44 current loss: 0.8508058190345764 current acc: 0.3682\n",
      "iteration 45 current loss: 0.8210683465003967 current acc: 0.3776\n",
      "iteration 46 current loss: 0.9847200512886047 current acc: 0.3866\n",
      "iteration 47 current loss: 0.9991821646690369 current acc: 0.395\n",
      "iteration 48 current loss: 0.872955322265625 current acc: 0.4034\n",
      "iteration 49 current loss: 0.8910374045372009 current acc: 0.4126\n",
      "iteration 50 current loss: 1.0622689723968506 current acc: 0.42\n",
      "iteration 51 current loss: 0.6808347105979919 current acc: 0.4288\n",
      "iteration 52 current loss: 0.881273090839386 current acc: 0.437\n",
      "iteration 53 current loss: 0.8674015402793884 current acc: 0.4456\n",
      "iteration 54 current loss: 0.799547016620636 current acc: 0.455\n",
      "iteration 55 current loss: 0.931858479976654 current acc: 0.4634\n",
      "iteration 56 current loss: 0.9269765019416809 current acc: 0.4722\n",
      "iteration 57 current loss: 0.850670337677002 current acc: 0.4816\n",
      "iteration 58 current loss: 1.0207390785217285 current acc: 0.4898\n",
      "iteration 59 current loss: 0.9635165929794312 current acc: 0.4978\n",
      "iteration 60 current loss: 0.8933488726615906 current acc: 0.5064\n",
      "iteration 61 current loss: 0.8895570635795593 current acc: 0.5144\n",
      "iteration 62 current loss: 0.894036054611206 current acc: 0.5224\n",
      "iteration 63 current loss: 1.0646686553955078 current acc: 0.5308\n",
      "iteration 64 current loss: 0.7606679201126099 current acc: 0.5406\n",
      "iteration 65 current loss: 1.0283887386322021 current acc: 0.5498\n",
      "iteration 66 current loss: 0.9733163118362427 current acc: 0.5586\n",
      "iteration 67 current loss: 0.8051970601081848 current acc: 0.5676\n",
      "iteration 68 current loss: 0.8207271695137024 current acc: 0.577\n",
      "iteration 69 current loss: 0.9703350067138672 current acc: 0.5854\n",
      "iteration 70 current loss: 0.8004248738288879 current acc: 0.5942\n",
      "iteration 71 current loss: 0.9565757513046265 current acc: 0.6024\n",
      "iteration 72 current loss: 1.0632315874099731 current acc: 0.6104\n",
      "iteration 73 current loss: 0.9851524829864502 current acc: 0.618\n",
      "iteration 74 current loss: 1.1774808168411255 current acc: 0.6256\n",
      "iteration 75 current loss: 0.9794288277626038 current acc: 0.6336\n",
      "iteration 76 current loss: 1.0508480072021484 current acc: 0.6424\n",
      "iteration 77 current loss: 1.0987869501113892 current acc: 0.6506\n",
      "iteration 78 current loss: 0.8335059285163879 current acc: 0.6516\n",
      "\t\tTrain Epoch 47/100,Train Accuracy: 0.6516, Train Loss: 0.9527033129824868.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 47/100, Validation Accuracy: 0.523875, Validation Loss: 1.3261141757965087\n",
      "best loss 0.9527033129824868\n",
      "iteration 0 current loss: 0.7900277972221375 current acc: 0.009\n",
      "iteration 1 current loss: 0.9334902167320251 current acc: 0.0174\n",
      "iteration 2 current loss: 1.1724439859390259 current acc: 0.025\n",
      "iteration 3 current loss: 0.9946557879447937 current acc: 0.0336\n",
      "iteration 4 current loss: 1.1558204889297485 current acc: 0.0406\n",
      "iteration 5 current loss: 0.9547414183616638 current acc: 0.0492\n",
      "iteration 6 current loss: 0.783932626247406 current acc: 0.0594\n",
      "iteration 7 current loss: 0.7050237655639648 current acc: 0.0696\n",
      "iteration 8 current loss: 1.0547226667404175 current acc: 0.0782\n",
      "iteration 9 current loss: 0.8732620477676392 current acc: 0.0866\n",
      "iteration 10 current loss: 0.9608116745948792 current acc: 0.0954\n",
      "iteration 11 current loss: 0.845231831073761 current acc: 0.1042\n",
      "iteration 12 current loss: 0.9213449954986572 current acc: 0.113\n",
      "iteration 13 current loss: 0.8155921697616577 current acc: 0.1224\n",
      "iteration 14 current loss: 0.920673668384552 current acc: 0.1304\n",
      "iteration 15 current loss: 1.0281263589859009 current acc: 0.1392\n",
      "iteration 16 current loss: 0.9526350498199463 current acc: 0.1474\n",
      "iteration 17 current loss: 0.8855815529823303 current acc: 0.1564\n",
      "iteration 18 current loss: 1.0797561407089233 current acc: 0.164\n",
      "iteration 19 current loss: 0.8609045743942261 current acc: 0.1726\n",
      "iteration 20 current loss: 0.8015215992927551 current acc: 0.182\n",
      "iteration 21 current loss: 1.0525299310684204 current acc: 0.1902\n",
      "iteration 22 current loss: 0.8038873076438904 current acc: 0.1998\n",
      "iteration 23 current loss: 0.7362012267112732 current acc: 0.2094\n",
      "iteration 24 current loss: 0.7627987265586853 current acc: 0.2186\n",
      "iteration 25 current loss: 1.0569483041763306 current acc: 0.226\n",
      "iteration 26 current loss: 1.0401943922042847 current acc: 0.2338\n",
      "iteration 27 current loss: 0.7981922030448914 current acc: 0.2438\n",
      "iteration 28 current loss: 0.986788272857666 current acc: 0.2522\n",
      "iteration 29 current loss: 0.9497880339622498 current acc: 0.26\n",
      "iteration 30 current loss: 1.0054502487182617 current acc: 0.2686\n",
      "iteration 31 current loss: 0.8877699971199036 current acc: 0.2782\n",
      "iteration 32 current loss: 0.8864043354988098 current acc: 0.2868\n",
      "iteration 33 current loss: 1.0046526193618774 current acc: 0.2946\n",
      "iteration 34 current loss: 0.8594484925270081 current acc: 0.3032\n",
      "iteration 35 current loss: 1.017243504524231 current acc: 0.3118\n",
      "iteration 36 current loss: 0.6855562329292297 current acc: 0.3222\n",
      "iteration 37 current loss: 1.0432170629501343 current acc: 0.3304\n",
      "iteration 38 current loss: 0.7490785717964172 current acc: 0.3398\n",
      "iteration 39 current loss: 0.7678053379058838 current acc: 0.349\n",
      "iteration 40 current loss: 0.9766599535942078 current acc: 0.3568\n",
      "iteration 41 current loss: 1.0739915370941162 current acc: 0.3644\n",
      "iteration 42 current loss: 0.9009791016578674 current acc: 0.3732\n",
      "iteration 43 current loss: 0.9590489864349365 current acc: 0.3812\n",
      "iteration 44 current loss: 0.7214763760566711 current acc: 0.3898\n",
      "iteration 45 current loss: 0.8834641575813293 current acc: 0.3988\n",
      "iteration 46 current loss: 0.9390157461166382 current acc: 0.4076\n",
      "iteration 47 current loss: 0.985041081905365 current acc: 0.416\n",
      "iteration 48 current loss: 0.7824586033821106 current acc: 0.4252\n",
      "iteration 49 current loss: 0.9844802618026733 current acc: 0.4338\n",
      "iteration 50 current loss: 0.8932739496231079 current acc: 0.4426\n",
      "iteration 51 current loss: 0.9929854273796082 current acc: 0.4512\n",
      "iteration 52 current loss: 0.9931785464286804 current acc: 0.4592\n",
      "iteration 53 current loss: 1.1040782928466797 current acc: 0.4668\n",
      "iteration 54 current loss: 0.7192229628562927 current acc: 0.4764\n",
      "iteration 55 current loss: 0.977340579032898 current acc: 0.4846\n",
      "iteration 56 current loss: 0.906598687171936 current acc: 0.4938\n",
      "iteration 57 current loss: 1.157207727432251 current acc: 0.5018\n",
      "iteration 58 current loss: 1.0277975797653198 current acc: 0.5098\n",
      "iteration 59 current loss: 1.0470850467681885 current acc: 0.5188\n",
      "iteration 60 current loss: 0.9140111804008484 current acc: 0.5282\n",
      "iteration 61 current loss: 0.9256962537765503 current acc: 0.5364\n",
      "iteration 62 current loss: 1.0723081827163696 current acc: 0.5444\n",
      "iteration 63 current loss: 0.92262202501297 current acc: 0.5534\n",
      "iteration 64 current loss: 1.074568271636963 current acc: 0.561\n",
      "iteration 65 current loss: 1.254547357559204 current acc: 0.568\n",
      "iteration 66 current loss: 1.0036818981170654 current acc: 0.577\n",
      "iteration 67 current loss: 0.8925356864929199 current acc: 0.5854\n",
      "iteration 68 current loss: 1.061444640159607 current acc: 0.5928\n",
      "iteration 69 current loss: 1.007789134979248 current acc: 0.6004\n",
      "iteration 70 current loss: 1.032275676727295 current acc: 0.6088\n",
      "iteration 71 current loss: 0.9911080598831177 current acc: 0.6168\n",
      "iteration 72 current loss: 0.9309456944465637 current acc: 0.6258\n",
      "iteration 73 current loss: 0.8747109770774841 current acc: 0.634\n",
      "iteration 74 current loss: 0.9283591508865356 current acc: 0.6426\n",
      "iteration 75 current loss: 0.9668930768966675 current acc: 0.652\n",
      "iteration 76 current loss: 1.1076797246932983 current acc: 0.6592\n",
      "iteration 77 current loss: 1.2042067050933838 current acc: 0.6664\n",
      "iteration 78 current loss: 1.0187406539916992 current acc: 0.6674\n",
      "\t\tTrain Epoch 48/100,Train Accuracy: 0.6674, Train Loss: 0.9467565085314498.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 48/100, Validation Accuracy: 0.54875, Validation Loss: 1.2747178869247437\n",
      "best loss 0.9467565085314498\n",
      "iteration 0 current loss: 1.0330698490142822 current acc: 0.0076\n",
      "iteration 1 current loss: 0.9730273485183716 current acc: 0.0158\n",
      "iteration 2 current loss: 0.7699010372161865 current acc: 0.0254\n",
      "iteration 3 current loss: 1.0430396795272827 current acc: 0.0336\n",
      "iteration 4 current loss: 0.731071412563324 current acc: 0.0426\n",
      "iteration 5 current loss: 0.8691636323928833 current acc: 0.0516\n",
      "iteration 6 current loss: 1.0835676193237305 current acc: 0.0594\n",
      "iteration 7 current loss: 0.8759862780570984 current acc: 0.0682\n",
      "iteration 8 current loss: 1.0393297672271729 current acc: 0.0766\n",
      "iteration 9 current loss: 0.9655101299285889 current acc: 0.0852\n",
      "iteration 10 current loss: 0.7933512330055237 current acc: 0.0946\n",
      "iteration 11 current loss: 0.8942887783050537 current acc: 0.1038\n",
      "iteration 12 current loss: 0.949348509311676 current acc: 0.1116\n",
      "iteration 13 current loss: 0.902714192867279 current acc: 0.1202\n",
      "iteration 14 current loss: 1.1368898153305054 current acc: 0.1282\n",
      "iteration 15 current loss: 0.8958840370178223 current acc: 0.137\n",
      "iteration 16 current loss: 0.810314416885376 current acc: 0.1462\n",
      "iteration 17 current loss: 1.2151048183441162 current acc: 0.1534\n",
      "iteration 18 current loss: 1.3019858598709106 current acc: 0.1608\n",
      "iteration 19 current loss: 0.8787798285484314 current acc: 0.1704\n",
      "iteration 20 current loss: 0.874411404132843 current acc: 0.1794\n",
      "iteration 21 current loss: 1.0337716341018677 current acc: 0.187\n",
      "iteration 22 current loss: 0.8001646995544434 current acc: 0.1962\n",
      "iteration 23 current loss: 0.7813672423362732 current acc: 0.2066\n",
      "iteration 24 current loss: 1.0053417682647705 current acc: 0.2156\n",
      "iteration 25 current loss: 0.9667249917984009 current acc: 0.2238\n",
      "iteration 26 current loss: 0.7290315628051758 current acc: 0.2326\n",
      "iteration 27 current loss: 0.9048733711242676 current acc: 0.2402\n",
      "iteration 28 current loss: 0.8324881792068481 current acc: 0.25\n",
      "iteration 29 current loss: 0.9946575164794922 current acc: 0.2584\n",
      "iteration 30 current loss: 0.7710898518562317 current acc: 0.268\n",
      "iteration 31 current loss: 0.9179107546806335 current acc: 0.2772\n",
      "iteration 32 current loss: 1.1239163875579834 current acc: 0.284\n",
      "iteration 33 current loss: 0.9315198063850403 current acc: 0.2918\n",
      "iteration 34 current loss: 0.8620507717132568 current acc: 0.3016\n",
      "iteration 35 current loss: 0.7686329483985901 current acc: 0.3112\n",
      "iteration 36 current loss: 0.8615372180938721 current acc: 0.32\n",
      "iteration 37 current loss: 1.1798802614212036 current acc: 0.3278\n",
      "iteration 38 current loss: 1.0648655891418457 current acc: 0.3358\n",
      "iteration 39 current loss: 0.9602628350257874 current acc: 0.3448\n",
      "iteration 40 current loss: 1.0277225971221924 current acc: 0.3512\n",
      "iteration 41 current loss: 0.9974422454833984 current acc: 0.3592\n",
      "iteration 42 current loss: 0.8804506659507751 current acc: 0.3688\n",
      "iteration 43 current loss: 0.9903125166893005 current acc: 0.3768\n",
      "iteration 44 current loss: 0.8507066369056702 current acc: 0.3856\n",
      "iteration 45 current loss: 0.7631666660308838 current acc: 0.3946\n",
      "iteration 46 current loss: 0.864470899105072 current acc: 0.404\n",
      "iteration 47 current loss: 1.0115519762039185 current acc: 0.4128\n",
      "iteration 48 current loss: 1.0270748138427734 current acc: 0.4208\n",
      "iteration 49 current loss: 0.8924610614776611 current acc: 0.4298\n",
      "iteration 50 current loss: 0.8654910326004028 current acc: 0.4384\n",
      "iteration 51 current loss: 0.8487740159034729 current acc: 0.4478\n",
      "iteration 52 current loss: 1.0046204328536987 current acc: 0.4562\n",
      "iteration 53 current loss: 0.9375251531600952 current acc: 0.4648\n",
      "iteration 54 current loss: 0.8978480100631714 current acc: 0.474\n",
      "iteration 55 current loss: 0.8762804865837097 current acc: 0.4838\n",
      "iteration 56 current loss: 0.7265852689743042 current acc: 0.4942\n",
      "iteration 57 current loss: 0.8830761909484863 current acc: 0.5032\n",
      "iteration 58 current loss: 0.8665180802345276 current acc: 0.512\n",
      "iteration 59 current loss: 0.9180482625961304 current acc: 0.5202\n",
      "iteration 60 current loss: 0.8054969310760498 current acc: 0.5288\n",
      "iteration 61 current loss: 0.9554592370986938 current acc: 0.5372\n",
      "iteration 62 current loss: 0.8383246064186096 current acc: 0.5462\n",
      "iteration 63 current loss: 0.9056549072265625 current acc: 0.5542\n",
      "iteration 64 current loss: 1.1142828464508057 current acc: 0.5614\n",
      "iteration 65 current loss: 1.2111008167266846 current acc: 0.5698\n",
      "iteration 66 current loss: 1.0948716402053833 current acc: 0.5774\n",
      "iteration 67 current loss: 0.8682865500450134 current acc: 0.5864\n",
      "iteration 68 current loss: 1.0483160018920898 current acc: 0.5938\n",
      "iteration 69 current loss: 0.9067755341529846 current acc: 0.6022\n",
      "iteration 70 current loss: 1.0402690172195435 current acc: 0.6104\n",
      "iteration 71 current loss: 1.254223108291626 current acc: 0.6168\n",
      "iteration 72 current loss: 0.9145498871803284 current acc: 0.6256\n",
      "iteration 73 current loss: 0.982459545135498 current acc: 0.634\n",
      "iteration 74 current loss: 1.0596331357955933 current acc: 0.6418\n",
      "iteration 75 current loss: 1.1576025485992432 current acc: 0.6494\n",
      "iteration 76 current loss: 0.8849352598190308 current acc: 0.6584\n",
      "iteration 77 current loss: 1.1238712072372437 current acc: 0.6652\n",
      "iteration 78 current loss: 0.9700495004653931 current acc: 0.6664\n",
      "\t\tTrain Epoch 49/100,Train Accuracy: 0.6664, Train Loss: 0.9467483079886134.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 49/100, Validation Accuracy: 0.559375, Validation Loss: 1.21894464635849\n",
      "best loss 0.9467483079886134\n",
      "iteration 0 current loss: 0.7259843349456787 current acc: 0.0094\n",
      "iteration 1 current loss: 0.9178323745727539 current acc: 0.0176\n",
      "iteration 2 current loss: 0.8253399133682251 current acc: 0.0276\n",
      "iteration 3 current loss: 0.8031398057937622 current acc: 0.0372\n",
      "iteration 4 current loss: 0.8566605448722839 current acc: 0.0458\n",
      "iteration 5 current loss: 1.0380967855453491 current acc: 0.0544\n",
      "iteration 6 current loss: 0.897794783115387 current acc: 0.0632\n",
      "iteration 7 current loss: 0.8731768727302551 current acc: 0.0726\n",
      "iteration 8 current loss: 1.1392697095870972 current acc: 0.0806\n",
      "iteration 9 current loss: 1.1967118978500366 current acc: 0.0886\n",
      "iteration 10 current loss: 0.8427953720092773 current acc: 0.0976\n",
      "iteration 11 current loss: 0.9898788332939148 current acc: 0.1058\n",
      "iteration 12 current loss: 0.9676223993301392 current acc: 0.1144\n",
      "iteration 13 current loss: 0.8209012150764465 current acc: 0.1238\n",
      "iteration 14 current loss: 0.8096494078636169 current acc: 0.1334\n",
      "iteration 15 current loss: 0.9524418115615845 current acc: 0.1422\n",
      "iteration 16 current loss: 0.8242144584655762 current acc: 0.152\n",
      "iteration 17 current loss: 0.9731718301773071 current acc: 0.1602\n",
      "iteration 18 current loss: 0.927592396736145 current acc: 0.1692\n",
      "iteration 19 current loss: 0.7428933382034302 current acc: 0.1784\n",
      "iteration 20 current loss: 0.8468407988548279 current acc: 0.1872\n",
      "iteration 21 current loss: 0.987289547920227 current acc: 0.1958\n",
      "iteration 22 current loss: 0.9496520161628723 current acc: 0.2038\n",
      "iteration 23 current loss: 0.9341429471969604 current acc: 0.2126\n",
      "iteration 24 current loss: 1.0210007429122925 current acc: 0.2204\n",
      "iteration 25 current loss: 0.7553412914276123 current acc: 0.2304\n",
      "iteration 26 current loss: 0.9859691262245178 current acc: 0.2386\n",
      "iteration 27 current loss: 1.026001214981079 current acc: 0.247\n",
      "iteration 28 current loss: 0.8224482536315918 current acc: 0.2556\n",
      "iteration 29 current loss: 1.0110400915145874 current acc: 0.264\n",
      "iteration 30 current loss: 0.7924162745475769 current acc: 0.2736\n",
      "iteration 31 current loss: 0.9691470265388489 current acc: 0.282\n",
      "iteration 32 current loss: 1.2041761875152588 current acc: 0.2894\n",
      "iteration 33 current loss: 0.6480298638343811 current acc: 0.2988\n",
      "iteration 34 current loss: 0.8580906987190247 current acc: 0.3084\n",
      "iteration 35 current loss: 0.8254098296165466 current acc: 0.3188\n",
      "iteration 36 current loss: 0.809796154499054 current acc: 0.3288\n",
      "iteration 37 current loss: 1.126894235610962 current acc: 0.3362\n",
      "iteration 38 current loss: 1.0188188552856445 current acc: 0.3442\n",
      "iteration 39 current loss: 0.7962467670440674 current acc: 0.3534\n",
      "iteration 40 current loss: 0.9968684315681458 current acc: 0.361\n",
      "iteration 41 current loss: 0.86467444896698 current acc: 0.3696\n",
      "iteration 42 current loss: 0.9179123044013977 current acc: 0.3782\n",
      "iteration 43 current loss: 0.8688429594039917 current acc: 0.3876\n",
      "iteration 44 current loss: 0.9693387746810913 current acc: 0.396\n",
      "iteration 45 current loss: 0.9202249050140381 current acc: 0.4056\n",
      "iteration 46 current loss: 0.990041971206665 current acc: 0.4136\n",
      "iteration 47 current loss: 0.9376134276390076 current acc: 0.4216\n",
      "iteration 48 current loss: 0.8549771904945374 current acc: 0.43\n",
      "iteration 49 current loss: 0.9737874269485474 current acc: 0.438\n",
      "iteration 50 current loss: 0.9230590462684631 current acc: 0.446\n",
      "iteration 51 current loss: 1.028138518333435 current acc: 0.4538\n",
      "iteration 52 current loss: 0.7486610412597656 current acc: 0.4632\n",
      "iteration 53 current loss: 0.9140321016311646 current acc: 0.4718\n",
      "iteration 54 current loss: 0.8519026041030884 current acc: 0.4808\n",
      "iteration 55 current loss: 0.960527777671814 current acc: 0.4892\n",
      "iteration 56 current loss: 1.0612539052963257 current acc: 0.4978\n",
      "iteration 57 current loss: 1.0525590181350708 current acc: 0.5056\n",
      "iteration 58 current loss: 1.0680543184280396 current acc: 0.5136\n",
      "iteration 59 current loss: 0.9493142366409302 current acc: 0.5222\n",
      "iteration 60 current loss: 1.0643517971038818 current acc: 0.5302\n",
      "iteration 61 current loss: 0.7304445505142212 current acc: 0.5394\n",
      "iteration 62 current loss: 1.0032737255096436 current acc: 0.5466\n",
      "iteration 63 current loss: 1.011757493019104 current acc: 0.5556\n",
      "iteration 64 current loss: 0.9936497211456299 current acc: 0.5634\n",
      "iteration 65 current loss: 0.9271372556686401 current acc: 0.5722\n",
      "iteration 66 current loss: 1.150923490524292 current acc: 0.5798\n",
      "iteration 67 current loss: 0.8430636525154114 current acc: 0.588\n",
      "iteration 68 current loss: 1.0421943664550781 current acc: 0.5962\n",
      "iteration 69 current loss: 0.9414705038070679 current acc: 0.6044\n",
      "iteration 70 current loss: 1.1879597902297974 current acc: 0.6118\n",
      "iteration 71 current loss: 0.9074661731719971 current acc: 0.6204\n",
      "iteration 72 current loss: 0.8026393055915833 current acc: 0.63\n",
      "iteration 73 current loss: 0.9091622233390808 current acc: 0.6388\n",
      "iteration 74 current loss: 0.9708080887794495 current acc: 0.6476\n",
      "iteration 75 current loss: 1.259450078010559 current acc: 0.6552\n",
      "iteration 76 current loss: 0.9628605246543884 current acc: 0.6632\n",
      "iteration 77 current loss: 0.9851764440536499 current acc: 0.6716\n",
      "iteration 78 current loss: 0.9810123443603516 current acc: 0.6726\n",
      "\t\tTrain Epoch 50/100,Train Accuracy: 0.6726, Train Loss: 0.9372215942491459.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 50/100, Validation Accuracy: 0.555125, Validation Loss: 1.2567267827987672\n",
      "best loss 0.9372215942491459\n",
      "iteration 0 current loss: 0.8261250853538513 current acc: 0.0094\n",
      "iteration 1 current loss: 0.7555822134017944 current acc: 0.0192\n",
      "iteration 2 current loss: 0.8639156222343445 current acc: 0.0276\n",
      "iteration 3 current loss: 0.7098965048789978 current acc: 0.0374\n",
      "iteration 4 current loss: 1.1571699380874634 current acc: 0.0446\n",
      "iteration 5 current loss: 1.144253134727478 current acc: 0.052\n",
      "iteration 6 current loss: 1.1225868463516235 current acc: 0.0588\n",
      "iteration 7 current loss: 1.0105693340301514 current acc: 0.0668\n",
      "iteration 8 current loss: 1.0757817029953003 current acc: 0.0742\n",
      "iteration 9 current loss: 1.0585994720458984 current acc: 0.082\n",
      "iteration 10 current loss: 0.9419710636138916 current acc: 0.0908\n",
      "iteration 11 current loss: 1.0091122388839722 current acc: 0.0988\n",
      "iteration 12 current loss: 0.7922723889350891 current acc: 0.1086\n",
      "iteration 13 current loss: 0.8105650544166565 current acc: 0.1184\n",
      "iteration 14 current loss: 0.7154077291488647 current acc: 0.1276\n",
      "iteration 15 current loss: 0.9182538390159607 current acc: 0.1364\n",
      "iteration 16 current loss: 1.039947271347046 current acc: 0.144\n",
      "iteration 17 current loss: 0.9563392996788025 current acc: 0.1522\n",
      "iteration 18 current loss: 0.8633049726486206 current acc: 0.1612\n",
      "iteration 19 current loss: 0.8933277726173401 current acc: 0.17\n",
      "iteration 20 current loss: 1.1173714399337769 current acc: 0.1776\n",
      "iteration 21 current loss: 0.9585970044136047 current acc: 0.1864\n",
      "iteration 22 current loss: 0.8726347088813782 current acc: 0.1956\n",
      "iteration 23 current loss: 0.9957671165466309 current acc: 0.2046\n",
      "iteration 24 current loss: 0.8382430672645569 current acc: 0.2134\n",
      "iteration 25 current loss: 1.0530614852905273 current acc: 0.2226\n",
      "iteration 26 current loss: 0.7202277183532715 current acc: 0.2328\n",
      "iteration 27 current loss: 0.7830697894096375 current acc: 0.242\n",
      "iteration 28 current loss: 0.9081113934516907 current acc: 0.251\n",
      "iteration 29 current loss: 1.036133885383606 current acc: 0.259\n",
      "iteration 30 current loss: 0.8902685046195984 current acc: 0.268\n",
      "iteration 31 current loss: 0.8412442803382874 current acc: 0.2772\n",
      "iteration 32 current loss: 0.9703075289726257 current acc: 0.2856\n",
      "iteration 33 current loss: 1.058769702911377 current acc: 0.294\n",
      "iteration 34 current loss: 0.9695104956626892 current acc: 0.3018\n",
      "iteration 35 current loss: 1.073763132095337 current acc: 0.31\n",
      "iteration 36 current loss: 1.078853964805603 current acc: 0.3178\n",
      "iteration 37 current loss: 0.8788783550262451 current acc: 0.3266\n",
      "iteration 38 current loss: 0.906535804271698 current acc: 0.3354\n",
      "iteration 39 current loss: 0.9119778275489807 current acc: 0.3444\n",
      "iteration 40 current loss: 0.7473098635673523 current acc: 0.3542\n",
      "iteration 41 current loss: 0.8104583024978638 current acc: 0.3636\n",
      "iteration 42 current loss: 0.7922520637512207 current acc: 0.3726\n",
      "iteration 43 current loss: 1.1384766101837158 current acc: 0.3808\n",
      "iteration 44 current loss: 0.8600839376449585 current acc: 0.39\n",
      "iteration 45 current loss: 0.9417163729667664 current acc: 0.3974\n",
      "iteration 46 current loss: 0.8495711088180542 current acc: 0.4066\n",
      "iteration 47 current loss: 1.0121272802352905 current acc: 0.4142\n",
      "iteration 48 current loss: 0.927189290523529 current acc: 0.4226\n",
      "iteration 49 current loss: 1.1285513639450073 current acc: 0.4308\n",
      "iteration 50 current loss: 1.0111274719238281 current acc: 0.4392\n",
      "iteration 51 current loss: 0.8789399266242981 current acc: 0.4484\n",
      "iteration 52 current loss: 1.138249397277832 current acc: 0.4558\n",
      "iteration 53 current loss: 1.162546992301941 current acc: 0.4636\n",
      "iteration 54 current loss: 0.9178683757781982 current acc: 0.4724\n",
      "iteration 55 current loss: 0.940410852432251 current acc: 0.4814\n",
      "iteration 56 current loss: 1.0797854661941528 current acc: 0.4896\n",
      "iteration 57 current loss: 0.7198829650878906 current acc: 0.4986\n",
      "iteration 58 current loss: 0.9848310947418213 current acc: 0.5072\n",
      "iteration 59 current loss: 0.8871211409568787 current acc: 0.516\n",
      "iteration 60 current loss: 0.94865882396698 current acc: 0.5244\n",
      "iteration 61 current loss: 1.1672887802124023 current acc: 0.5326\n",
      "iteration 62 current loss: 1.0182764530181885 current acc: 0.541\n",
      "iteration 63 current loss: 1.0225231647491455 current acc: 0.549\n",
      "iteration 64 current loss: 0.8887427449226379 current acc: 0.558\n",
      "iteration 65 current loss: 0.8387673497200012 current acc: 0.5672\n",
      "iteration 66 current loss: 0.9362272620201111 current acc: 0.5762\n",
      "iteration 67 current loss: 0.9123721718788147 current acc: 0.5844\n",
      "iteration 68 current loss: 1.2491939067840576 current acc: 0.5914\n",
      "iteration 69 current loss: 0.8287317156791687 current acc: 0.6004\n",
      "iteration 70 current loss: 1.1078959703445435 current acc: 0.6076\n",
      "iteration 71 current loss: 0.9828354120254517 current acc: 0.6158\n",
      "iteration 72 current loss: 0.7948954701423645 current acc: 0.6254\n",
      "iteration 73 current loss: 0.9598981738090515 current acc: 0.634\n",
      "iteration 74 current loss: 0.9028016328811646 current acc: 0.6422\n",
      "iteration 75 current loss: 0.8433394432067871 current acc: 0.6514\n",
      "iteration 76 current loss: 0.8234561681747437 current acc: 0.6606\n",
      "iteration 77 current loss: 0.8658427000045776 current acc: 0.6694\n",
      "iteration 78 current loss: 0.8836711645126343 current acc: 0.6702\n",
      "\t\tTrain Epoch 51/100,Train Accuracy: 0.6702, Train Loss: 0.9421800895582272.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 51/100, Validation Accuracy: 0.55625, Validation Loss: 1.205050335407257\n",
      "iteration 0 current loss: 0.9246959686279297 current acc: 0.0078\n",
      "iteration 1 current loss: 0.662618100643158 current acc: 0.0178\n",
      "iteration 2 current loss: 0.6997726559638977 current acc: 0.0276\n",
      "iteration 3 current loss: 0.7410370707511902 current acc: 0.037\n",
      "iteration 4 current loss: 0.8974535465240479 current acc: 0.0464\n",
      "iteration 5 current loss: 1.0496225357055664 current acc: 0.0544\n",
      "iteration 6 current loss: 0.9505383372306824 current acc: 0.0626\n",
      "iteration 7 current loss: 1.206507921218872 current acc: 0.0704\n",
      "iteration 8 current loss: 1.0643954277038574 current acc: 0.0774\n",
      "iteration 9 current loss: 0.8618146777153015 current acc: 0.0864\n",
      "iteration 10 current loss: 0.915535032749176 current acc: 0.0946\n",
      "iteration 11 current loss: 0.7305846810340881 current acc: 0.105\n",
      "iteration 12 current loss: 0.9761112928390503 current acc: 0.113\n",
      "iteration 13 current loss: 0.9941644072532654 current acc: 0.1208\n",
      "iteration 14 current loss: 0.7205013632774353 current acc: 0.1306\n",
      "iteration 15 current loss: 1.2750537395477295 current acc: 0.1372\n",
      "iteration 16 current loss: 0.8506622910499573 current acc: 0.1458\n",
      "iteration 17 current loss: 0.9415003061294556 current acc: 0.1546\n",
      "iteration 18 current loss: 0.8402338027954102 current acc: 0.1634\n",
      "iteration 19 current loss: 0.7614725828170776 current acc: 0.1728\n",
      "iteration 20 current loss: 0.8789162635803223 current acc: 0.1812\n",
      "iteration 21 current loss: 0.9203739166259766 current acc: 0.1898\n",
      "iteration 22 current loss: 0.9821746945381165 current acc: 0.1984\n",
      "iteration 23 current loss: 1.1231361627578735 current acc: 0.206\n",
      "iteration 24 current loss: 0.9720287919044495 current acc: 0.2152\n",
      "iteration 25 current loss: 0.8015025854110718 current acc: 0.2248\n",
      "iteration 26 current loss: 0.7258264422416687 current acc: 0.2354\n",
      "iteration 27 current loss: 0.9645760655403137 current acc: 0.2442\n",
      "iteration 28 current loss: 1.002902626991272 current acc: 0.2524\n",
      "iteration 29 current loss: 1.0439910888671875 current acc: 0.2602\n",
      "iteration 30 current loss: 0.9582930207252502 current acc: 0.2688\n",
      "iteration 31 current loss: 0.8867140412330627 current acc: 0.2782\n",
      "iteration 32 current loss: 1.1159160137176514 current acc: 0.286\n",
      "iteration 33 current loss: 0.9948849081993103 current acc: 0.2938\n",
      "iteration 34 current loss: 0.9440173506736755 current acc: 0.3018\n",
      "iteration 35 current loss: 0.8866265416145325 current acc: 0.3106\n",
      "iteration 36 current loss: 0.693571925163269 current acc: 0.3204\n",
      "iteration 37 current loss: 0.8935298919677734 current acc: 0.3288\n",
      "iteration 38 current loss: 1.0734634399414062 current acc: 0.3372\n",
      "iteration 39 current loss: 0.872890055179596 current acc: 0.346\n",
      "iteration 40 current loss: 0.8990767598152161 current acc: 0.3548\n",
      "iteration 41 current loss: 0.766241729259491 current acc: 0.3638\n",
      "iteration 42 current loss: 0.9307088851928711 current acc: 0.3722\n",
      "iteration 43 current loss: 1.1446434259414673 current acc: 0.3802\n",
      "iteration 44 current loss: 1.1749836206436157 current acc: 0.3876\n",
      "iteration 45 current loss: 1.1763417720794678 current acc: 0.3954\n",
      "iteration 46 current loss: 0.9285961389541626 current acc: 0.404\n",
      "iteration 47 current loss: 0.8122838735580444 current acc: 0.4124\n",
      "iteration 48 current loss: 0.9552181363105774 current acc: 0.4206\n",
      "iteration 49 current loss: 0.9425230622291565 current acc: 0.4292\n",
      "iteration 50 current loss: 0.8576061129570007 current acc: 0.4378\n",
      "iteration 51 current loss: 1.3858067989349365 current acc: 0.4438\n",
      "iteration 52 current loss: 0.7481162548065186 current acc: 0.453\n",
      "iteration 53 current loss: 1.0591762065887451 current acc: 0.4606\n",
      "iteration 54 current loss: 0.8191565275192261 current acc: 0.4696\n",
      "iteration 55 current loss: 1.0508503913879395 current acc: 0.477\n",
      "iteration 56 current loss: 0.953924834728241 current acc: 0.4856\n",
      "iteration 57 current loss: 0.8114362955093384 current acc: 0.495\n",
      "iteration 58 current loss: 0.8653131723403931 current acc: 0.503\n",
      "iteration 59 current loss: 0.9553202390670776 current acc: 0.5114\n",
      "iteration 60 current loss: 1.1027930974960327 current acc: 0.5178\n",
      "iteration 61 current loss: 1.0293633937835693 current acc: 0.526\n",
      "iteration 62 current loss: 0.8512449264526367 current acc: 0.5358\n",
      "iteration 63 current loss: 1.2190393209457397 current acc: 0.5428\n",
      "iteration 64 current loss: 1.0133414268493652 current acc: 0.551\n",
      "iteration 65 current loss: 0.8272621035575867 current acc: 0.561\n",
      "iteration 66 current loss: 1.2005776166915894 current acc: 0.5682\n",
      "iteration 67 current loss: 0.9450786709785461 current acc: 0.5764\n",
      "iteration 68 current loss: 0.9055492877960205 current acc: 0.5854\n",
      "iteration 69 current loss: 1.0591259002685547 current acc: 0.5934\n",
      "iteration 70 current loss: 0.9887387752532959 current acc: 0.6022\n",
      "iteration 71 current loss: 1.0463807582855225 current acc: 0.6096\n",
      "iteration 72 current loss: 0.9850289821624756 current acc: 0.6174\n",
      "iteration 73 current loss: 1.0057291984558105 current acc: 0.6258\n",
      "iteration 74 current loss: 0.9606306552886963 current acc: 0.6344\n",
      "iteration 75 current loss: 0.9053520560264587 current acc: 0.6436\n",
      "iteration 76 current loss: 1.0144824981689453 current acc: 0.6516\n",
      "iteration 77 current loss: 0.8048452138900757 current acc: 0.6614\n",
      "iteration 78 current loss: 0.7659064531326294 current acc: 0.6626\n",
      "\t\tTrain Epoch 52/100,Train Accuracy: 0.6626, Train Loss: 0.9451570397690882.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 52/100, Validation Accuracy: 0.5525, Validation Loss: 1.23312468957901\n",
      "iteration 0 current loss: 1.0942310094833374 current acc: 0.007\n",
      "iteration 1 current loss: 1.1874518394470215 current acc: 0.0136\n",
      "iteration 2 current loss: 1.0256375074386597 current acc: 0.0216\n",
      "iteration 3 current loss: 1.0327690839767456 current acc: 0.0296\n",
      "iteration 4 current loss: 0.7396140694618225 current acc: 0.0386\n",
      "iteration 5 current loss: 0.8472983837127686 current acc: 0.0478\n",
      "iteration 6 current loss: 0.8467429876327515 current acc: 0.0568\n",
      "iteration 7 current loss: 0.982062816619873 current acc: 0.0642\n",
      "iteration 8 current loss: 0.8800386786460876 current acc: 0.0732\n",
      "iteration 9 current loss: 0.9247199296951294 current acc: 0.082\n",
      "iteration 10 current loss: 0.8454986810684204 current acc: 0.0916\n",
      "iteration 11 current loss: 0.9560936689376831 current acc: 0.1004\n",
      "iteration 12 current loss: 0.7447168827056885 current acc: 0.1098\n",
      "iteration 13 current loss: 0.9625781774520874 current acc: 0.118\n",
      "iteration 14 current loss: 0.966683030128479 current acc: 0.1262\n",
      "iteration 15 current loss: 1.1137754917144775 current acc: 0.1342\n",
      "iteration 16 current loss: 0.8389179706573486 current acc: 0.1432\n",
      "iteration 17 current loss: 0.9102060794830322 current acc: 0.1524\n",
      "iteration 18 current loss: 1.0149530172348022 current acc: 0.1608\n",
      "iteration 19 current loss: 0.8398975133895874 current acc: 0.1696\n",
      "iteration 20 current loss: 1.0159655809402466 current acc: 0.1782\n",
      "iteration 21 current loss: 1.0667117834091187 current acc: 0.1854\n",
      "iteration 22 current loss: 1.1548936367034912 current acc: 0.1928\n",
      "iteration 23 current loss: 0.823238730430603 current acc: 0.202\n",
      "iteration 24 current loss: 0.9869112968444824 current acc: 0.2102\n",
      "iteration 25 current loss: 0.888022243976593 current acc: 0.219\n",
      "iteration 26 current loss: 0.7828596234321594 current acc: 0.2286\n",
      "iteration 27 current loss: 0.9017992615699768 current acc: 0.2368\n",
      "iteration 28 current loss: 0.9236993789672852 current acc: 0.2454\n",
      "iteration 29 current loss: 0.7254498600959778 current acc: 0.2554\n",
      "iteration 30 current loss: 0.9792578220367432 current acc: 0.2632\n",
      "iteration 31 current loss: 0.9116438627243042 current acc: 0.2722\n",
      "iteration 32 current loss: 1.0927754640579224 current acc: 0.2796\n",
      "iteration 33 current loss: 0.8178264498710632 current acc: 0.2886\n",
      "iteration 34 current loss: 0.8043056726455688 current acc: 0.2976\n",
      "iteration 35 current loss: 0.9637230634689331 current acc: 0.306\n",
      "iteration 36 current loss: 0.9198575615882874 current acc: 0.3142\n",
      "iteration 37 current loss: 1.1070326566696167 current acc: 0.321\n",
      "iteration 38 current loss: 0.813817024230957 current acc: 0.3296\n",
      "iteration 39 current loss: 0.8757196664810181 current acc: 0.3388\n",
      "iteration 40 current loss: 0.9247215390205383 current acc: 0.3474\n",
      "iteration 41 current loss: 1.1425844430923462 current acc: 0.3548\n",
      "iteration 42 current loss: 0.9907711148262024 current acc: 0.3624\n",
      "iteration 43 current loss: 0.7456676959991455 current acc: 0.372\n",
      "iteration 44 current loss: 0.8861543536186218 current acc: 0.3804\n",
      "iteration 45 current loss: 0.9088151454925537 current acc: 0.3894\n",
      "iteration 46 current loss: 0.9788574576377869 current acc: 0.3974\n",
      "iteration 47 current loss: 0.9559306502342224 current acc: 0.4058\n",
      "iteration 48 current loss: 0.9937831163406372 current acc: 0.4138\n",
      "iteration 49 current loss: 0.6815808415412903 current acc: 0.4238\n",
      "iteration 50 current loss: 0.9380813837051392 current acc: 0.432\n",
      "iteration 51 current loss: 0.9810851812362671 current acc: 0.4404\n",
      "iteration 52 current loss: 1.1327826976776123 current acc: 0.4476\n",
      "iteration 53 current loss: 0.7796064019203186 current acc: 0.4564\n",
      "iteration 54 current loss: 0.9294478893280029 current acc: 0.4648\n",
      "iteration 55 current loss: 0.7751754522323608 current acc: 0.475\n",
      "iteration 56 current loss: 1.1161115169525146 current acc: 0.4822\n",
      "iteration 57 current loss: 0.973976731300354 current acc: 0.4912\n",
      "iteration 58 current loss: 0.9282618761062622 current acc: 0.5002\n",
      "iteration 59 current loss: 0.7618685960769653 current acc: 0.51\n",
      "iteration 60 current loss: 1.1620144844055176 current acc: 0.5166\n",
      "iteration 61 current loss: 0.8951292037963867 current acc: 0.5248\n",
      "iteration 62 current loss: 1.2461001873016357 current acc: 0.5324\n",
      "iteration 63 current loss: 1.076022982597351 current acc: 0.5404\n",
      "iteration 64 current loss: 0.8091340661048889 current acc: 0.5496\n",
      "iteration 65 current loss: 1.0809423923492432 current acc: 0.5584\n",
      "iteration 66 current loss: 0.8859898447990417 current acc: 0.5674\n",
      "iteration 67 current loss: 0.8819392919540405 current acc: 0.576\n",
      "iteration 68 current loss: 0.8837084174156189 current acc: 0.5842\n",
      "iteration 69 current loss: 0.8143507838249207 current acc: 0.5934\n",
      "iteration 70 current loss: 0.788216233253479 current acc: 0.6018\n",
      "iteration 71 current loss: 0.8818711042404175 current acc: 0.6104\n",
      "iteration 72 current loss: 0.9933964014053345 current acc: 0.6188\n",
      "iteration 73 current loss: 1.089306116104126 current acc: 0.626\n",
      "iteration 74 current loss: 1.1579433679580688 current acc: 0.6332\n",
      "iteration 75 current loss: 1.0403920412063599 current acc: 0.6412\n",
      "iteration 76 current loss: 0.929779052734375 current acc: 0.65\n",
      "iteration 77 current loss: 0.8689984083175659 current acc: 0.659\n",
      "iteration 78 current loss: 0.6443305015563965 current acc: 0.6602\n",
      "\t\tTrain Epoch 53/100,Train Accuracy: 0.6602, Train Loss: 0.9362560306923299.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 53/100, Validation Accuracy: 0.55225, Validation Loss: 1.2374957962036133\n",
      "best loss 0.9362560306923299\n",
      "iteration 0 current loss: 0.9091787934303284 current acc: 0.0084\n",
      "iteration 1 current loss: 0.789439857006073 current acc: 0.0178\n",
      "iteration 2 current loss: 0.9799671173095703 current acc: 0.0266\n",
      "iteration 3 current loss: 1.0248913764953613 current acc: 0.0346\n",
      "iteration 4 current loss: 0.8556474447250366 current acc: 0.0438\n",
      "iteration 5 current loss: 1.0193610191345215 current acc: 0.052\n",
      "iteration 6 current loss: 0.8515570163726807 current acc: 0.0604\n",
      "iteration 7 current loss: 1.0399765968322754 current acc: 0.0684\n",
      "iteration 8 current loss: 0.983336865901947 current acc: 0.0764\n",
      "iteration 9 current loss: 0.9173130393028259 current acc: 0.084\n",
      "iteration 10 current loss: 0.9046830534934998 current acc: 0.0922\n",
      "iteration 11 current loss: 0.7623642683029175 current acc: 0.1022\n",
      "iteration 12 current loss: 0.8665416836738586 current acc: 0.1112\n",
      "iteration 13 current loss: 0.9297011494636536 current acc: 0.1202\n",
      "iteration 14 current loss: 0.9204056859016418 current acc: 0.1288\n",
      "iteration 15 current loss: 1.1259994506835938 current acc: 0.1362\n",
      "iteration 16 current loss: 0.9574969410896301 current acc: 0.1444\n",
      "iteration 17 current loss: 1.0203884840011597 current acc: 0.1532\n",
      "iteration 18 current loss: 0.956146240234375 current acc: 0.162\n",
      "iteration 19 current loss: 0.9447586536407471 current acc: 0.1712\n",
      "iteration 20 current loss: 0.783033013343811 current acc: 0.1808\n",
      "iteration 21 current loss: 0.912039577960968 current acc: 0.1898\n",
      "iteration 22 current loss: 0.8985078930854797 current acc: 0.1984\n",
      "iteration 23 current loss: 0.9835095405578613 current acc: 0.2052\n",
      "iteration 24 current loss: 0.916947603225708 current acc: 0.2138\n",
      "iteration 25 current loss: 0.8160185813903809 current acc: 0.223\n",
      "iteration 26 current loss: 0.9047242403030396 current acc: 0.2322\n",
      "iteration 27 current loss: 0.749390721321106 current acc: 0.242\n",
      "iteration 28 current loss: 0.8600113391876221 current acc: 0.2516\n",
      "iteration 29 current loss: 0.7474714517593384 current acc: 0.2608\n",
      "iteration 30 current loss: 0.7873157262802124 current acc: 0.2706\n",
      "iteration 31 current loss: 0.8526042103767395 current acc: 0.279\n",
      "iteration 32 current loss: 0.9004743099212646 current acc: 0.288\n",
      "iteration 33 current loss: 0.831497073173523 current acc: 0.2972\n",
      "iteration 34 current loss: 1.002242922782898 current acc: 0.3056\n",
      "iteration 35 current loss: 0.8556259274482727 current acc: 0.3148\n",
      "iteration 36 current loss: 1.1800119876861572 current acc: 0.3224\n",
      "iteration 37 current loss: 1.0767120122909546 current acc: 0.3302\n",
      "iteration 38 current loss: 0.9991583824157715 current acc: 0.339\n",
      "iteration 39 current loss: 0.9620315432548523 current acc: 0.3474\n",
      "iteration 40 current loss: 1.0971605777740479 current acc: 0.3556\n",
      "iteration 41 current loss: 0.822688102722168 current acc: 0.3648\n",
      "iteration 42 current loss: 0.995415210723877 current acc: 0.3726\n",
      "iteration 43 current loss: 1.0139142274856567 current acc: 0.3808\n",
      "iteration 44 current loss: 0.8640581965446472 current acc: 0.391\n",
      "iteration 45 current loss: 0.927222728729248 current acc: 0.3994\n",
      "iteration 46 current loss: 0.8486784100532532 current acc: 0.4086\n",
      "iteration 47 current loss: 0.8583705425262451 current acc: 0.4184\n",
      "iteration 48 current loss: 0.9408641457557678 current acc: 0.4276\n",
      "iteration 49 current loss: 0.9101313948631287 current acc: 0.4356\n",
      "iteration 50 current loss: 1.1630257368087769 current acc: 0.444\n",
      "iteration 51 current loss: 0.840538501739502 current acc: 0.4528\n",
      "iteration 52 current loss: 0.7917893528938293 current acc: 0.4626\n",
      "iteration 53 current loss: 1.0321050882339478 current acc: 0.4702\n",
      "iteration 54 current loss: 0.839491605758667 current acc: 0.4788\n",
      "iteration 55 current loss: 0.9301266074180603 current acc: 0.4876\n",
      "iteration 56 current loss: 0.8700735569000244 current acc: 0.4962\n",
      "iteration 57 current loss: 1.059529185295105 current acc: 0.5042\n",
      "iteration 58 current loss: 0.8130484819412231 current acc: 0.513\n",
      "iteration 59 current loss: 0.798945963382721 current acc: 0.5224\n",
      "iteration 60 current loss: 0.9135584831237793 current acc: 0.5308\n",
      "iteration 61 current loss: 0.9007429480552673 current acc: 0.5398\n",
      "iteration 62 current loss: 0.751457691192627 current acc: 0.549\n",
      "iteration 63 current loss: 1.1742719411849976 current acc: 0.557\n",
      "iteration 64 current loss: 0.8490517139434814 current acc: 0.566\n",
      "iteration 65 current loss: 0.8405879139900208 current acc: 0.5748\n",
      "iteration 66 current loss: 1.2317923307418823 current acc: 0.581\n",
      "iteration 67 current loss: 1.1491576433181763 current acc: 0.5888\n",
      "iteration 68 current loss: 0.9729700088500977 current acc: 0.5974\n",
      "iteration 69 current loss: 0.7926837801933289 current acc: 0.6066\n",
      "iteration 70 current loss: 0.9154675006866455 current acc: 0.616\n",
      "iteration 71 current loss: 1.0322275161743164 current acc: 0.624\n",
      "iteration 72 current loss: 1.000128149986267 current acc: 0.6316\n",
      "iteration 73 current loss: 0.9710564017295837 current acc: 0.64\n",
      "iteration 74 current loss: 0.94793301820755 current acc: 0.6494\n",
      "iteration 75 current loss: 1.0012786388397217 current acc: 0.6578\n",
      "iteration 76 current loss: 0.8512781858444214 current acc: 0.6662\n",
      "iteration 77 current loss: 0.7020139694213867 current acc: 0.676\n",
      "iteration 78 current loss: 1.4218639135360718 current acc: 0.6766\n",
      "\t\tTrain Epoch 54/100,Train Accuracy: 0.6766, Train Loss: 0.9318377488776098.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 54/100, Validation Accuracy: 0.51875, Validation Loss: 1.3989604034423828\n",
      "best loss 0.9318377488776098\n",
      "iteration 0 current loss: 0.8876721858978271 current acc: 0.0078\n",
      "iteration 1 current loss: 0.8650159239768982 current acc: 0.017\n",
      "iteration 2 current loss: 0.9433259963989258 current acc: 0.0258\n",
      "iteration 3 current loss: 1.1545218229293823 current acc: 0.033\n",
      "iteration 4 current loss: 1.2691853046417236 current acc: 0.0408\n",
      "iteration 5 current loss: 1.2172932624816895 current acc: 0.0482\n",
      "iteration 6 current loss: 0.9048335552215576 current acc: 0.0564\n",
      "iteration 7 current loss: 0.7695326805114746 current acc: 0.0654\n",
      "iteration 8 current loss: 0.8737261295318604 current acc: 0.0742\n",
      "iteration 9 current loss: 0.9489222168922424 current acc: 0.083\n",
      "iteration 10 current loss: 0.9456890821456909 current acc: 0.092\n",
      "iteration 11 current loss: 1.1149075031280518 current acc: 0.1\n",
      "iteration 12 current loss: 1.2622954845428467 current acc: 0.1068\n",
      "iteration 13 current loss: 0.9597154855728149 current acc: 0.1156\n",
      "iteration 14 current loss: 0.9408618807792664 current acc: 0.124\n",
      "iteration 15 current loss: 0.9928898215293884 current acc: 0.1328\n",
      "iteration 16 current loss: 0.9127349853515625 current acc: 0.1416\n",
      "iteration 17 current loss: 1.0448458194732666 current acc: 0.149\n",
      "iteration 18 current loss: 0.8654892444610596 current acc: 0.1576\n",
      "iteration 19 current loss: 0.8527222871780396 current acc: 0.1668\n",
      "iteration 20 current loss: 1.2089688777923584 current acc: 0.1726\n",
      "iteration 21 current loss: 0.9691989421844482 current acc: 0.18\n",
      "iteration 22 current loss: 1.0176401138305664 current acc: 0.1878\n",
      "iteration 23 current loss: 1.0163511037826538 current acc: 0.1962\n",
      "iteration 24 current loss: 1.0128028392791748 current acc: 0.2046\n",
      "iteration 25 current loss: 1.0625622272491455 current acc: 0.2122\n",
      "iteration 26 current loss: 0.8503521680831909 current acc: 0.2212\n",
      "iteration 27 current loss: 0.9926740527153015 current acc: 0.2292\n",
      "iteration 28 current loss: 1.1007497310638428 current acc: 0.2368\n",
      "iteration 29 current loss: 1.0624539852142334 current acc: 0.2448\n",
      "iteration 30 current loss: 1.0243359804153442 current acc: 0.2524\n",
      "iteration 31 current loss: 0.9547592401504517 current acc: 0.2614\n",
      "iteration 32 current loss: 1.0151528120040894 current acc: 0.2692\n",
      "iteration 33 current loss: 0.9490076303482056 current acc: 0.2776\n",
      "iteration 34 current loss: 0.9522180557250977 current acc: 0.2862\n",
      "iteration 35 current loss: 0.9357810020446777 current acc: 0.2942\n",
      "iteration 36 current loss: 0.8998547792434692 current acc: 0.3028\n",
      "iteration 37 current loss: 0.8292127251625061 current acc: 0.3122\n",
      "iteration 38 current loss: 1.0995066165924072 current acc: 0.3204\n",
      "iteration 39 current loss: 1.0241634845733643 current acc: 0.3284\n",
      "iteration 40 current loss: 0.9193050861358643 current acc: 0.3374\n",
      "iteration 41 current loss: 1.1057100296020508 current acc: 0.3458\n",
      "iteration 42 current loss: 0.9741964340209961 current acc: 0.3542\n",
      "iteration 43 current loss: 1.025039792060852 current acc: 0.3626\n",
      "iteration 44 current loss: 1.0608516931533813 current acc: 0.3706\n",
      "iteration 45 current loss: 0.9339886903762817 current acc: 0.379\n",
      "iteration 46 current loss: 0.8267073035240173 current acc: 0.3878\n",
      "iteration 47 current loss: 0.8438045382499695 current acc: 0.3958\n",
      "iteration 48 current loss: 0.7875388264656067 current acc: 0.4052\n",
      "iteration 49 current loss: 1.2203913927078247 current acc: 0.4128\n",
      "iteration 50 current loss: 0.8314322829246521 current acc: 0.4226\n",
      "iteration 51 current loss: 0.8809968829154968 current acc: 0.4312\n",
      "iteration 52 current loss: 0.7251289486885071 current acc: 0.4412\n",
      "iteration 53 current loss: 1.1303211450576782 current acc: 0.4492\n",
      "iteration 54 current loss: 1.0791364908218384 current acc: 0.4576\n",
      "iteration 55 current loss: 1.0149410963058472 current acc: 0.4654\n",
      "iteration 56 current loss: 1.0867245197296143 current acc: 0.4726\n",
      "iteration 57 current loss: 0.9734318256378174 current acc: 0.4812\n",
      "iteration 58 current loss: 0.9693692922592163 current acc: 0.4886\n",
      "iteration 59 current loss: 0.9267471432685852 current acc: 0.4966\n",
      "iteration 60 current loss: 0.9173753261566162 current acc: 0.505\n",
      "iteration 61 current loss: 0.9645273685455322 current acc: 0.5136\n",
      "iteration 62 current loss: 0.9213425517082214 current acc: 0.5224\n",
      "iteration 63 current loss: 1.2799232006072998 current acc: 0.53\n",
      "iteration 64 current loss: 0.7692399024963379 current acc: 0.54\n",
      "iteration 65 current loss: 0.8320811986923218 current acc: 0.5492\n",
      "iteration 66 current loss: 1.019115686416626 current acc: 0.5574\n",
      "iteration 67 current loss: 1.113014578819275 current acc: 0.5648\n",
      "iteration 68 current loss: 0.9502020478248596 current acc: 0.573\n",
      "iteration 69 current loss: 0.9985668659210205 current acc: 0.581\n",
      "iteration 70 current loss: 0.9198476076126099 current acc: 0.5892\n",
      "iteration 71 current loss: 1.188334584236145 current acc: 0.597\n",
      "iteration 72 current loss: 1.0681787729263306 current acc: 0.6054\n",
      "iteration 73 current loss: 0.9182074069976807 current acc: 0.6144\n",
      "iteration 74 current loss: 0.8053042888641357 current acc: 0.6236\n",
      "iteration 75 current loss: 0.9163899421691895 current acc: 0.6326\n",
      "iteration 76 current loss: 1.140586018562317 current acc: 0.641\n",
      "iteration 77 current loss: 0.9498043060302734 current acc: 0.6484\n",
      "iteration 78 current loss: 0.8518404960632324 current acc: 0.6492\n",
      "\t\tTrain Epoch 55/100,Train Accuracy: 0.6492, Train Loss: 0.9815642101855218.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 55/100, Validation Accuracy: 0.54025, Validation Loss: 1.3175432500839233\n",
      "iteration 0 current loss: 1.0350908041000366 current acc: 0.008\n",
      "iteration 1 current loss: 0.9943627119064331 current acc: 0.0162\n",
      "iteration 2 current loss: 0.945853054523468 current acc: 0.0248\n",
      "iteration 3 current loss: 0.7325473427772522 current acc: 0.034\n",
      "iteration 4 current loss: 0.9996327757835388 current acc: 0.0412\n",
      "iteration 5 current loss: 1.065622091293335 current acc: 0.0492\n",
      "iteration 6 current loss: 0.9488707184791565 current acc: 0.0574\n",
      "iteration 7 current loss: 0.9272864460945129 current acc: 0.0658\n",
      "iteration 8 current loss: 0.7772006392478943 current acc: 0.0754\n",
      "iteration 9 current loss: 0.9142161011695862 current acc: 0.0834\n",
      "iteration 10 current loss: 0.9382641315460205 current acc: 0.0922\n",
      "iteration 11 current loss: 0.8427527546882629 current acc: 0.102\n",
      "iteration 12 current loss: 1.048532247543335 current acc: 0.1098\n",
      "iteration 13 current loss: 0.9151694178581238 current acc: 0.1192\n",
      "iteration 14 current loss: 0.693010151386261 current acc: 0.1286\n",
      "iteration 15 current loss: 0.9825921058654785 current acc: 0.137\n",
      "iteration 16 current loss: 0.8085430860519409 current acc: 0.1466\n",
      "iteration 17 current loss: 0.9826803803443909 current acc: 0.155\n",
      "iteration 18 current loss: 0.9307190775871277 current acc: 0.1638\n",
      "iteration 19 current loss: 0.9793477058410645 current acc: 0.1716\n",
      "iteration 20 current loss: 0.8747208118438721 current acc: 0.1808\n",
      "iteration 21 current loss: 0.8673386573791504 current acc: 0.1894\n",
      "iteration 22 current loss: 0.8865926265716553 current acc: 0.1984\n",
      "iteration 23 current loss: 0.9974979758262634 current acc: 0.2066\n",
      "iteration 24 current loss: 0.8648912310600281 current acc: 0.2152\n",
      "iteration 25 current loss: 0.7439064383506775 current acc: 0.226\n",
      "iteration 26 current loss: 0.8757550716400146 current acc: 0.2344\n",
      "iteration 27 current loss: 1.0539350509643555 current acc: 0.2418\n",
      "iteration 28 current loss: 0.8490803837776184 current acc: 0.2506\n",
      "iteration 29 current loss: 0.9051513671875 current acc: 0.2596\n",
      "iteration 30 current loss: 0.9691886305809021 current acc: 0.2674\n",
      "iteration 31 current loss: 0.9623964428901672 current acc: 0.276\n",
      "iteration 32 current loss: 1.2084447145462036 current acc: 0.2832\n",
      "iteration 33 current loss: 1.0494022369384766 current acc: 0.2902\n",
      "iteration 34 current loss: 0.9141743183135986 current acc: 0.2994\n",
      "iteration 35 current loss: 0.9328513741493225 current acc: 0.3076\n",
      "iteration 36 current loss: 0.8571202158927917 current acc: 0.3162\n",
      "iteration 37 current loss: 0.9689078330993652 current acc: 0.325\n",
      "iteration 38 current loss: 0.9648652076721191 current acc: 0.3334\n",
      "iteration 39 current loss: 1.0025885105133057 current acc: 0.3418\n",
      "iteration 40 current loss: 1.222427248954773 current acc: 0.3492\n",
      "iteration 41 current loss: 1.0529556274414062 current acc: 0.358\n",
      "iteration 42 current loss: 0.9378056526184082 current acc: 0.367\n",
      "iteration 43 current loss: 0.9863660931587219 current acc: 0.3752\n",
      "iteration 44 current loss: 1.0743199586868286 current acc: 0.3836\n",
      "iteration 45 current loss: 0.8547682762145996 current acc: 0.3922\n",
      "iteration 46 current loss: 0.9541366100311279 current acc: 0.4\n",
      "iteration 47 current loss: 0.7009944319725037 current acc: 0.4098\n",
      "iteration 48 current loss: 0.9178078174591064 current acc: 0.4184\n",
      "iteration 49 current loss: 0.9545479416847229 current acc: 0.4266\n",
      "iteration 50 current loss: 0.9100711941719055 current acc: 0.4346\n",
      "iteration 51 current loss: 1.0535427331924438 current acc: 0.4424\n",
      "iteration 52 current loss: 1.0807911157608032 current acc: 0.4508\n",
      "iteration 53 current loss: 0.8310544490814209 current acc: 0.4592\n",
      "iteration 54 current loss: 1.079445719718933 current acc: 0.4664\n",
      "iteration 55 current loss: 0.8869139552116394 current acc: 0.4748\n",
      "iteration 56 current loss: 0.846457839012146 current acc: 0.484\n",
      "iteration 57 current loss: 1.030099868774414 current acc: 0.4914\n",
      "iteration 58 current loss: 0.9936434030532837 current acc: 0.5\n",
      "iteration 59 current loss: 0.9325081706047058 current acc: 0.5078\n",
      "iteration 60 current loss: 0.7988725900650024 current acc: 0.5178\n",
      "iteration 61 current loss: 1.0123119354248047 current acc: 0.526\n",
      "iteration 62 current loss: 0.9522295594215393 current acc: 0.5344\n",
      "iteration 63 current loss: 1.0108988285064697 current acc: 0.5424\n",
      "iteration 64 current loss: 0.714688777923584 current acc: 0.5524\n",
      "iteration 65 current loss: 1.0790889263153076 current acc: 0.5604\n",
      "iteration 66 current loss: 0.9715439081192017 current acc: 0.5688\n",
      "iteration 67 current loss: 0.7953856587409973 current acc: 0.578\n",
      "iteration 68 current loss: 0.8789265155792236 current acc: 0.5872\n",
      "iteration 69 current loss: 0.944058895111084 current acc: 0.5958\n",
      "iteration 70 current loss: 1.0686842203140259 current acc: 0.604\n",
      "iteration 71 current loss: 0.9763166904449463 current acc: 0.612\n",
      "iteration 72 current loss: 1.018306016921997 current acc: 0.6212\n",
      "iteration 73 current loss: 0.9425511360168457 current acc: 0.6296\n",
      "iteration 74 current loss: 1.0127034187316895 current acc: 0.6378\n",
      "iteration 75 current loss: 1.1294000148773193 current acc: 0.6452\n",
      "iteration 76 current loss: 1.0781248807907104 current acc: 0.6528\n",
      "iteration 77 current loss: 1.1056104898452759 current acc: 0.661\n",
      "iteration 78 current loss: 1.0105371475219727 current acc: 0.662\n",
      "\t\tTrain Epoch 56/100,Train Accuracy: 0.662, Train Loss: 0.9495189944400063.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 56/100, Validation Accuracy: 0.544625, Validation Loss: 1.2529897646903991\n",
      "iteration 0 current loss: 0.7020915150642395 current acc: 0.01\n",
      "iteration 1 current loss: 0.8299464583396912 current acc: 0.019\n",
      "iteration 2 current loss: 0.7435773611068726 current acc: 0.0282\n",
      "iteration 3 current loss: 0.941165030002594 current acc: 0.0364\n",
      "iteration 4 current loss: 0.911634087562561 current acc: 0.0452\n",
      "iteration 5 current loss: 1.0124198198318481 current acc: 0.0524\n",
      "iteration 6 current loss: 0.9542067646980286 current acc: 0.0604\n",
      "iteration 7 current loss: 0.856196939945221 current acc: 0.0694\n",
      "iteration 8 current loss: 0.6825314164161682 current acc: 0.0786\n",
      "iteration 9 current loss: 0.6968125700950623 current acc: 0.0888\n",
      "iteration 10 current loss: 0.7530773878097534 current acc: 0.098\n",
      "iteration 11 current loss: 0.742015540599823 current acc: 0.1074\n",
      "iteration 12 current loss: 1.1031020879745483 current acc: 0.1154\n",
      "iteration 13 current loss: 1.13710355758667 current acc: 0.1224\n",
      "iteration 14 current loss: 0.8974564671516418 current acc: 0.1312\n",
      "iteration 15 current loss: 0.8201611042022705 current acc: 0.1404\n",
      "iteration 16 current loss: 1.0822583436965942 current acc: 0.1484\n",
      "iteration 17 current loss: 1.000745177268982 current acc: 0.1568\n",
      "iteration 18 current loss: 1.0788617134094238 current acc: 0.1646\n",
      "iteration 19 current loss: 1.0606975555419922 current acc: 0.1722\n",
      "iteration 20 current loss: 0.9895873665809631 current acc: 0.1796\n",
      "iteration 21 current loss: 0.9236717224121094 current acc: 0.188\n",
      "iteration 22 current loss: 0.9855338335037231 current acc: 0.196\n",
      "iteration 23 current loss: 1.008707880973816 current acc: 0.2042\n",
      "iteration 24 current loss: 1.202028751373291 current acc: 0.2114\n",
      "iteration 25 current loss: 1.1447631120681763 current acc: 0.218\n",
      "iteration 26 current loss: 0.940075695514679 current acc: 0.2256\n",
      "iteration 27 current loss: 0.837532103061676 current acc: 0.2354\n",
      "iteration 28 current loss: 1.021359920501709 current acc: 0.2436\n",
      "iteration 29 current loss: 0.8731660842895508 current acc: 0.2532\n",
      "iteration 30 current loss: 0.9550540447235107 current acc: 0.2614\n",
      "iteration 31 current loss: 0.9438098073005676 current acc: 0.2702\n",
      "iteration 32 current loss: 1.0506393909454346 current acc: 0.278\n",
      "iteration 33 current loss: 0.990359902381897 current acc: 0.2862\n",
      "iteration 34 current loss: 0.8928360939025879 current acc: 0.2946\n",
      "iteration 35 current loss: 0.9845737218856812 current acc: 0.3022\n",
      "iteration 36 current loss: 1.0116047859191895 current acc: 0.3096\n",
      "iteration 37 current loss: 1.135986328125 current acc: 0.317\n",
      "iteration 38 current loss: 0.8567090034484863 current acc: 0.326\n",
      "iteration 39 current loss: 0.9500864148139954 current acc: 0.334\n",
      "iteration 40 current loss: 0.8680465221405029 current acc: 0.3424\n",
      "iteration 41 current loss: 0.8522732257843018 current acc: 0.3506\n",
      "iteration 42 current loss: 0.7759585976600647 current acc: 0.3598\n",
      "iteration 43 current loss: 0.9189606308937073 current acc: 0.3686\n",
      "iteration 44 current loss: 0.8733900785446167 current acc: 0.3772\n",
      "iteration 45 current loss: 1.1650127172470093 current acc: 0.3844\n",
      "iteration 46 current loss: 0.9262210130691528 current acc: 0.3932\n",
      "iteration 47 current loss: 1.0098676681518555 current acc: 0.401\n",
      "iteration 48 current loss: 1.0435757637023926 current acc: 0.4088\n",
      "iteration 49 current loss: 1.076529622077942 current acc: 0.4168\n",
      "iteration 50 current loss: 0.8884229063987732 current acc: 0.4256\n",
      "iteration 51 current loss: 0.9495030641555786 current acc: 0.4346\n",
      "iteration 52 current loss: 0.8691138625144958 current acc: 0.4432\n",
      "iteration 53 current loss: 0.8921679258346558 current acc: 0.4518\n",
      "iteration 54 current loss: 1.0687713623046875 current acc: 0.4594\n",
      "iteration 55 current loss: 0.9094403982162476 current acc: 0.4686\n",
      "iteration 56 current loss: 0.8020614385604858 current acc: 0.4774\n",
      "iteration 57 current loss: 0.959433913230896 current acc: 0.486\n",
      "iteration 58 current loss: 1.0078762769699097 current acc: 0.4936\n",
      "iteration 59 current loss: 0.9826000928878784 current acc: 0.5018\n",
      "iteration 60 current loss: 0.8875991702079773 current acc: 0.5102\n",
      "iteration 61 current loss: 1.0691159963607788 current acc: 0.5172\n",
      "iteration 62 current loss: 0.8212803602218628 current acc: 0.5264\n",
      "iteration 63 current loss: 0.9030942320823669 current acc: 0.5356\n",
      "iteration 64 current loss: 1.1045479774475098 current acc: 0.5434\n",
      "iteration 65 current loss: 1.0802363157272339 current acc: 0.5512\n",
      "iteration 66 current loss: 0.7811775207519531 current acc: 0.5606\n",
      "iteration 67 current loss: 1.0213000774383545 current acc: 0.5686\n",
      "iteration 68 current loss: 1.0155444145202637 current acc: 0.577\n",
      "iteration 69 current loss: 0.8703035116195679 current acc: 0.586\n",
      "iteration 70 current loss: 0.9477135539054871 current acc: 0.594\n",
      "iteration 71 current loss: 0.7669530510902405 current acc: 0.6038\n",
      "iteration 72 current loss: 0.9009341597557068 current acc: 0.6124\n",
      "iteration 73 current loss: 0.9109312295913696 current acc: 0.6208\n",
      "iteration 74 current loss: 0.9688241481781006 current acc: 0.629\n",
      "iteration 75 current loss: 0.7772854566574097 current acc: 0.6374\n",
      "iteration 76 current loss: 0.8380264043807983 current acc: 0.647\n",
      "iteration 77 current loss: 1.042088270187378 current acc: 0.655\n",
      "iteration 78 current loss: 0.5952305197715759 current acc: 0.6562\n",
      "\t\tTrain Epoch 57/100,Train Accuracy: 0.6562, Train Loss: 0.9347539280034318.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 57/100, Validation Accuracy: 0.567, Validation Loss: 1.22163623380661\n",
      "iteration 0 current loss: 0.8708568811416626 current acc: 0.0086\n",
      "iteration 1 current loss: 0.9245995879173279 current acc: 0.0168\n",
      "iteration 2 current loss: 0.9603440761566162 current acc: 0.0252\n",
      "iteration 3 current loss: 1.0092365741729736 current acc: 0.0334\n",
      "iteration 4 current loss: 0.9034379720687866 current acc: 0.0424\n",
      "iteration 5 current loss: 0.7599713802337646 current acc: 0.0518\n",
      "iteration 6 current loss: 0.8777107000350952 current acc: 0.0602\n",
      "iteration 7 current loss: 0.9887202382087708 current acc: 0.0672\n",
      "iteration 8 current loss: 0.8985052704811096 current acc: 0.076\n",
      "iteration 9 current loss: 0.9283695816993713 current acc: 0.0842\n",
      "iteration 10 current loss: 0.9203197956085205 current acc: 0.093\n",
      "iteration 11 current loss: 0.8294452428817749 current acc: 0.1028\n",
      "iteration 12 current loss: 0.9099615216255188 current acc: 0.1112\n",
      "iteration 13 current loss: 0.9673873782157898 current acc: 0.1194\n",
      "iteration 14 current loss: 0.9249484539031982 current acc: 0.1276\n",
      "iteration 15 current loss: 0.7775639295578003 current acc: 0.1368\n",
      "iteration 16 current loss: 1.0489646196365356 current acc: 0.1442\n",
      "iteration 17 current loss: 0.6494867205619812 current acc: 0.1546\n",
      "iteration 18 current loss: 0.7573205828666687 current acc: 0.1642\n",
      "iteration 19 current loss: 0.808574914932251 current acc: 0.173\n",
      "iteration 20 current loss: 0.9025882482528687 current acc: 0.1812\n",
      "iteration 21 current loss: 0.9437786340713501 current acc: 0.1904\n",
      "iteration 22 current loss: 0.7729203701019287 current acc: 0.2004\n",
      "iteration 23 current loss: 0.9679242968559265 current acc: 0.2092\n",
      "iteration 24 current loss: 0.8660028576850891 current acc: 0.2178\n",
      "iteration 25 current loss: 0.8515164256095886 current acc: 0.2272\n",
      "iteration 26 current loss: 0.9363936185836792 current acc: 0.236\n",
      "iteration 27 current loss: 0.7489837408065796 current acc: 0.245\n",
      "iteration 28 current loss: 1.0209228992462158 current acc: 0.2524\n",
      "iteration 29 current loss: 0.7756685614585876 current acc: 0.262\n",
      "iteration 30 current loss: 0.8762887716293335 current acc: 0.2706\n",
      "iteration 31 current loss: 0.9263861179351807 current acc: 0.2796\n",
      "iteration 32 current loss: 0.9982707500457764 current acc: 0.2882\n",
      "iteration 33 current loss: 1.0819716453552246 current acc: 0.2962\n",
      "iteration 34 current loss: 0.8965277075767517 current acc: 0.3036\n",
      "iteration 35 current loss: 1.056072473526001 current acc: 0.3112\n",
      "iteration 36 current loss: 0.8369112610816956 current acc: 0.3206\n",
      "iteration 37 current loss: 0.9201294183731079 current acc: 0.3292\n",
      "iteration 38 current loss: 0.9590407609939575 current acc: 0.3372\n",
      "iteration 39 current loss: 1.0536375045776367 current acc: 0.3454\n",
      "iteration 40 current loss: 0.8502398133277893 current acc: 0.3544\n",
      "iteration 41 current loss: 1.0150495767593384 current acc: 0.3618\n",
      "iteration 42 current loss: 1.0261772871017456 current acc: 0.371\n",
      "iteration 43 current loss: 1.000475525856018 current acc: 0.3784\n",
      "iteration 44 current loss: 0.9551295638084412 current acc: 0.3878\n",
      "iteration 45 current loss: 1.2078471183776855 current acc: 0.3942\n",
      "iteration 46 current loss: 1.0386567115783691 current acc: 0.4028\n",
      "iteration 47 current loss: 0.7713086605072021 current acc: 0.4126\n",
      "iteration 48 current loss: 0.9772289395332336 current acc: 0.421\n",
      "iteration 49 current loss: 1.1005610227584839 current acc: 0.4292\n",
      "iteration 50 current loss: 1.0722754001617432 current acc: 0.4366\n",
      "iteration 51 current loss: 0.745913028717041 current acc: 0.446\n",
      "iteration 52 current loss: 0.8081770539283752 current acc: 0.4556\n",
      "iteration 53 current loss: 0.7704164385795593 current acc: 0.4654\n",
      "iteration 54 current loss: 0.8470331430435181 current acc: 0.475\n",
      "iteration 55 current loss: 1.0559241771697998 current acc: 0.4834\n",
      "iteration 56 current loss: 0.9173088669776917 current acc: 0.4924\n",
      "iteration 57 current loss: 0.9859402775764465 current acc: 0.5006\n",
      "iteration 58 current loss: 0.8802095055580139 current acc: 0.509\n",
      "iteration 59 current loss: 0.8333637118339539 current acc: 0.5174\n",
      "iteration 60 current loss: 0.8301538825035095 current acc: 0.5272\n",
      "iteration 61 current loss: 1.143174409866333 current acc: 0.5352\n",
      "iteration 62 current loss: 1.0482206344604492 current acc: 0.5434\n",
      "iteration 63 current loss: 0.8468637466430664 current acc: 0.5526\n",
      "iteration 64 current loss: 1.0086455345153809 current acc: 0.5608\n",
      "iteration 65 current loss: 0.9236299991607666 current acc: 0.5694\n",
      "iteration 66 current loss: 0.9210522174835205 current acc: 0.578\n",
      "iteration 67 current loss: 1.3003230094909668 current acc: 0.5844\n",
      "iteration 68 current loss: 1.0850900411605835 current acc: 0.5932\n",
      "iteration 69 current loss: 0.9086243510246277 current acc: 0.6018\n",
      "iteration 70 current loss: 0.6673681735992432 current acc: 0.612\n",
      "iteration 71 current loss: 0.7053636908531189 current acc: 0.622\n",
      "iteration 72 current loss: 0.7557135820388794 current acc: 0.6314\n",
      "iteration 73 current loss: 0.9091025590896606 current acc: 0.6402\n",
      "iteration 74 current loss: 1.1476384401321411 current acc: 0.6478\n",
      "iteration 75 current loss: 0.7535886168479919 current acc: 0.657\n",
      "iteration 76 current loss: 0.9830891489982605 current acc: 0.6648\n",
      "iteration 77 current loss: 0.7662971615791321 current acc: 0.6744\n",
      "iteration 78 current loss: 0.7447866797447205 current acc: 0.6758\n",
      "\t\tTrain Epoch 58/100,Train Accuracy: 0.6758, Train Loss: 0.9166534580761874.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 58/100, Validation Accuracy: 0.553125, Validation Loss: 1.2427435245513916\n",
      "best loss 0.9166534580761874\n",
      "iteration 0 current loss: 0.9120433330535889 current acc: 0.0086\n",
      "iteration 1 current loss: 0.8103642463684082 current acc: 0.018\n",
      "iteration 2 current loss: 0.8246860504150391 current acc: 0.0268\n",
      "iteration 3 current loss: 0.7992157936096191 current acc: 0.0346\n",
      "iteration 4 current loss: 0.8599392771720886 current acc: 0.0428\n",
      "iteration 5 current loss: 0.9306641221046448 current acc: 0.0508\n",
      "iteration 6 current loss: 1.0075173377990723 current acc: 0.06\n",
      "iteration 7 current loss: 1.11294686794281 current acc: 0.0676\n",
      "iteration 8 current loss: 0.9436129331588745 current acc: 0.0764\n",
      "iteration 9 current loss: 1.0077331066131592 current acc: 0.0842\n",
      "iteration 10 current loss: 0.7965460419654846 current acc: 0.0934\n",
      "iteration 11 current loss: 1.017853856086731 current acc: 0.1018\n",
      "iteration 12 current loss: 0.7746065258979797 current acc: 0.1116\n",
      "iteration 13 current loss: 0.9827436208724976 current acc: 0.1192\n",
      "iteration 14 current loss: 0.9455909132957458 current acc: 0.1264\n",
      "iteration 15 current loss: 1.0593922138214111 current acc: 0.1346\n",
      "iteration 16 current loss: 0.8169267177581787 current acc: 0.1432\n",
      "iteration 17 current loss: 0.68960040807724 current acc: 0.1524\n",
      "iteration 18 current loss: 1.0594522953033447 current acc: 0.159\n",
      "iteration 19 current loss: 0.9036819338798523 current acc: 0.1678\n",
      "iteration 20 current loss: 0.9145899415016174 current acc: 0.1764\n",
      "iteration 21 current loss: 0.8714247941970825 current acc: 0.1854\n",
      "iteration 22 current loss: 1.049448013305664 current acc: 0.1934\n",
      "iteration 23 current loss: 0.925760805606842 current acc: 0.202\n",
      "iteration 24 current loss: 0.7492415904998779 current acc: 0.2114\n",
      "iteration 25 current loss: 0.7928141951560974 current acc: 0.2206\n",
      "iteration 26 current loss: 0.9323989152908325 current acc: 0.2294\n",
      "iteration 27 current loss: 1.0040738582611084 current acc: 0.2372\n",
      "iteration 28 current loss: 0.8947994112968445 current acc: 0.245\n",
      "iteration 29 current loss: 0.8056356310844421 current acc: 0.2532\n",
      "iteration 30 current loss: 0.8776392340660095 current acc: 0.2622\n",
      "iteration 31 current loss: 0.9693042635917664 current acc: 0.2704\n",
      "iteration 32 current loss: 1.075615644454956 current acc: 0.2782\n",
      "iteration 33 current loss: 1.0275601148605347 current acc: 0.286\n",
      "iteration 34 current loss: 0.8897042870521545 current acc: 0.2954\n",
      "iteration 35 current loss: 1.0157852172851562 current acc: 0.303\n",
      "iteration 36 current loss: 1.0755831003189087 current acc: 0.3102\n",
      "iteration 37 current loss: 0.8644168972969055 current acc: 0.3192\n",
      "iteration 38 current loss: 1.1625003814697266 current acc: 0.326\n",
      "iteration 39 current loss: 0.9119715094566345 current acc: 0.3352\n",
      "iteration 40 current loss: 0.8308287262916565 current acc: 0.3442\n",
      "iteration 41 current loss: 0.8145471215248108 current acc: 0.3536\n",
      "iteration 42 current loss: 0.758247971534729 current acc: 0.3636\n",
      "iteration 43 current loss: 1.031758189201355 current acc: 0.3718\n",
      "iteration 44 current loss: 0.9425953030586243 current acc: 0.3792\n",
      "iteration 45 current loss: 0.9198580980300903 current acc: 0.3882\n",
      "iteration 46 current loss: 1.0977461338043213 current acc: 0.3958\n",
      "iteration 47 current loss: 0.8636101484298706 current acc: 0.4044\n",
      "iteration 48 current loss: 1.153029203414917 current acc: 0.4128\n",
      "iteration 49 current loss: 0.8350082039833069 current acc: 0.4218\n",
      "iteration 50 current loss: 0.7568393349647522 current acc: 0.4312\n",
      "iteration 51 current loss: 0.9391462206840515 current acc: 0.4388\n",
      "iteration 52 current loss: 0.9817896485328674 current acc: 0.4466\n",
      "iteration 53 current loss: 0.8284056186676025 current acc: 0.4562\n",
      "iteration 54 current loss: 0.9696893095970154 current acc: 0.4652\n",
      "iteration 55 current loss: 0.8982367515563965 current acc: 0.4742\n",
      "iteration 56 current loss: 0.8734649419784546 current acc: 0.4832\n",
      "iteration 57 current loss: 0.9627231359481812 current acc: 0.4924\n",
      "iteration 58 current loss: 0.8023871779441833 current acc: 0.502\n",
      "iteration 59 current loss: 1.3184244632720947 current acc: 0.5078\n",
      "iteration 60 current loss: 0.7040354609489441 current acc: 0.5176\n",
      "iteration 61 current loss: 0.7458336353302002 current acc: 0.5276\n",
      "iteration 62 current loss: 1.1694059371948242 current acc: 0.5356\n",
      "iteration 63 current loss: 1.0709701776504517 current acc: 0.5432\n",
      "iteration 64 current loss: 1.0801310539245605 current acc: 0.5514\n",
      "iteration 65 current loss: 0.8554903864860535 current acc: 0.5606\n",
      "iteration 66 current loss: 1.0166698694229126 current acc: 0.5684\n",
      "iteration 67 current loss: 0.8121801614761353 current acc: 0.5776\n",
      "iteration 68 current loss: 0.828916072845459 current acc: 0.5878\n",
      "iteration 69 current loss: 0.7877618670463562 current acc: 0.5968\n",
      "iteration 70 current loss: 0.7887060642242432 current acc: 0.6064\n",
      "iteration 71 current loss: 0.9117487668991089 current acc: 0.6142\n",
      "iteration 72 current loss: 0.9767812490463257 current acc: 0.6232\n",
      "iteration 73 current loss: 0.9575565457344055 current acc: 0.6318\n",
      "iteration 74 current loss: 0.9612866044044495 current acc: 0.6408\n",
      "iteration 75 current loss: 0.9312847256660461 current acc: 0.6496\n",
      "iteration 76 current loss: 1.070825219154358 current acc: 0.6574\n",
      "iteration 77 current loss: 0.8187738656997681 current acc: 0.6666\n",
      "iteration 78 current loss: 0.7424349188804626 current acc: 0.6678\n",
      "\t\tTrain Epoch 59/100,Train Accuracy: 0.6678, Train Loss: 0.922462198553206.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 59/100, Validation Accuracy: 0.550125, Validation Loss: 1.2380700063705445\n",
      "iteration 0 current loss: 0.8587309718132019 current acc: 0.008\n",
      "iteration 1 current loss: 0.7545008063316345 current acc: 0.0172\n",
      "iteration 2 current loss: 0.7059428691864014 current acc: 0.0268\n",
      "iteration 3 current loss: 0.8569245934486389 current acc: 0.0352\n",
      "iteration 4 current loss: 1.0652517080307007 current acc: 0.0426\n",
      "iteration 5 current loss: 0.8640475869178772 current acc: 0.0518\n",
      "iteration 6 current loss: 0.9828795790672302 current acc: 0.0596\n",
      "iteration 7 current loss: 0.7617881894111633 current acc: 0.0686\n",
      "iteration 8 current loss: 0.8882837891578674 current acc: 0.077\n",
      "iteration 9 current loss: 0.7094963192939758 current acc: 0.0866\n",
      "iteration 10 current loss: 1.1731631755828857 current acc: 0.0934\n",
      "iteration 11 current loss: 0.9352058172225952 current acc: 0.1024\n",
      "iteration 12 current loss: 0.7418190836906433 current acc: 0.1128\n",
      "iteration 13 current loss: 0.959015965461731 current acc: 0.1216\n",
      "iteration 14 current loss: 0.8847312331199646 current acc: 0.1302\n",
      "iteration 15 current loss: 0.8214119076728821 current acc: 0.1394\n",
      "iteration 16 current loss: 1.1597673892974854 current acc: 0.147\n",
      "iteration 17 current loss: 0.9772765040397644 current acc: 0.1554\n",
      "iteration 18 current loss: 0.7370771765708923 current acc: 0.1654\n",
      "iteration 19 current loss: 0.9969555139541626 current acc: 0.1742\n",
      "iteration 20 current loss: 0.9626864194869995 current acc: 0.183\n",
      "iteration 21 current loss: 0.8315476179122925 current acc: 0.1922\n",
      "iteration 22 current loss: 1.1759151220321655 current acc: 0.2\n",
      "iteration 23 current loss: 0.9395104646682739 current acc: 0.2084\n",
      "iteration 24 current loss: 0.8930618762969971 current acc: 0.2172\n",
      "iteration 25 current loss: 0.8906378746032715 current acc: 0.2256\n",
      "iteration 26 current loss: 0.9832828640937805 current acc: 0.2332\n",
      "iteration 27 current loss: 0.8907267451286316 current acc: 0.2422\n",
      "iteration 28 current loss: 0.7815197706222534 current acc: 0.2516\n",
      "iteration 29 current loss: 0.8022820949554443 current acc: 0.2608\n",
      "iteration 30 current loss: 0.9690646529197693 current acc: 0.2688\n",
      "iteration 31 current loss: 0.8035088181495667 current acc: 0.2786\n",
      "iteration 32 current loss: 0.9571806788444519 current acc: 0.2874\n",
      "iteration 33 current loss: 0.7120916247367859 current acc: 0.2972\n",
      "iteration 34 current loss: 0.9139088988304138 current acc: 0.3058\n",
      "iteration 35 current loss: 0.996097207069397 current acc: 0.314\n",
      "iteration 36 current loss: 1.1630265712738037 current acc: 0.3214\n",
      "iteration 37 current loss: 0.8633882999420166 current acc: 0.3304\n",
      "iteration 38 current loss: 1.0414146184921265 current acc: 0.338\n",
      "iteration 39 current loss: 0.8143707513809204 current acc: 0.3466\n",
      "iteration 40 current loss: 0.9355570673942566 current acc: 0.3554\n",
      "iteration 41 current loss: 0.836938738822937 current acc: 0.3642\n",
      "iteration 42 current loss: 0.8394583463668823 current acc: 0.3732\n",
      "iteration 43 current loss: 0.8831327557563782 current acc: 0.3822\n",
      "iteration 44 current loss: 0.958355724811554 current acc: 0.3904\n",
      "iteration 45 current loss: 0.9402611255645752 current acc: 0.3994\n",
      "iteration 46 current loss: 0.7482896447181702 current acc: 0.4092\n",
      "iteration 47 current loss: 0.99699866771698 current acc: 0.417\n",
      "iteration 48 current loss: 1.0188432931900024 current acc: 0.4256\n",
      "iteration 49 current loss: 0.9893023371696472 current acc: 0.4342\n",
      "iteration 50 current loss: 1.0205761194229126 current acc: 0.4426\n",
      "iteration 51 current loss: 0.8820443749427795 current acc: 0.4506\n",
      "iteration 52 current loss: 0.9812824726104736 current acc: 0.4588\n",
      "iteration 53 current loss: 1.0717387199401855 current acc: 0.4666\n",
      "iteration 54 current loss: 0.8541197776794434 current acc: 0.4756\n",
      "iteration 55 current loss: 1.2326607704162598 current acc: 0.4836\n",
      "iteration 56 current loss: 0.8892098069190979 current acc: 0.4926\n",
      "iteration 57 current loss: 0.9437774419784546 current acc: 0.5008\n",
      "iteration 58 current loss: 0.9396084547042847 current acc: 0.509\n",
      "iteration 59 current loss: 0.928884744644165 current acc: 0.5166\n",
      "iteration 60 current loss: 1.0615805387496948 current acc: 0.5246\n",
      "iteration 61 current loss: 1.1182621717453003 current acc: 0.5322\n",
      "iteration 62 current loss: 0.839154839515686 current acc: 0.5408\n",
      "iteration 63 current loss: 0.9195157289505005 current acc: 0.5492\n",
      "iteration 64 current loss: 1.1338956356048584 current acc: 0.5566\n",
      "iteration 65 current loss: 0.9240275621414185 current acc: 0.5642\n",
      "iteration 66 current loss: 0.9807568192481995 current acc: 0.5732\n",
      "iteration 67 current loss: 0.6882186532020569 current acc: 0.5834\n",
      "iteration 68 current loss: 0.7804080247879028 current acc: 0.5926\n",
      "iteration 69 current loss: 0.9702811241149902 current acc: 0.6012\n",
      "iteration 70 current loss: 1.0217398405075073 current acc: 0.6092\n",
      "iteration 71 current loss: 0.8485667705535889 current acc: 0.6182\n",
      "iteration 72 current loss: 0.9485405683517456 current acc: 0.627\n",
      "iteration 73 current loss: 0.8901400566101074 current acc: 0.6356\n",
      "iteration 74 current loss: 1.063240885734558 current acc: 0.6436\n",
      "iteration 75 current loss: 1.1513946056365967 current acc: 0.6522\n",
      "iteration 76 current loss: 0.6938295364379883 current acc: 0.662\n",
      "iteration 77 current loss: 0.9216427206993103 current acc: 0.6702\n",
      "iteration 78 current loss: 0.5768354535102844 current acc: 0.6714\n",
      "\t\tTrain Epoch 60/100,Train Accuracy: 0.6714, Train Loss: 0.9195261401466176.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 60/100, Validation Accuracy: 0.565625, Validation Loss: 1.2009236645698547\n",
      "iteration 0 current loss: 0.7819039821624756 current acc: 0.0084\n",
      "iteration 1 current loss: 0.84853595495224 current acc: 0.0172\n",
      "iteration 2 current loss: 0.8066287040710449 current acc: 0.0262\n",
      "iteration 3 current loss: 0.8688161373138428 current acc: 0.0356\n",
      "iteration 4 current loss: 1.171449065208435 current acc: 0.0424\n",
      "iteration 5 current loss: 0.702790379524231 current acc: 0.052\n",
      "iteration 6 current loss: 0.9608808755874634 current acc: 0.0606\n",
      "iteration 7 current loss: 1.0239238739013672 current acc: 0.0684\n",
      "iteration 8 current loss: 1.0255807638168335 current acc: 0.0776\n",
      "iteration 9 current loss: 0.8524494767189026 current acc: 0.0862\n",
      "iteration 10 current loss: 0.8790196180343628 current acc: 0.0962\n",
      "iteration 11 current loss: 0.8560653924942017 current acc: 0.1054\n",
      "iteration 12 current loss: 0.7507304549217224 current acc: 0.1148\n",
      "iteration 13 current loss: 1.0124176740646362 current acc: 0.1234\n",
      "iteration 14 current loss: 1.0034027099609375 current acc: 0.1312\n",
      "iteration 15 current loss: 0.9928423762321472 current acc: 0.1384\n",
      "iteration 16 current loss: 0.8436320424079895 current acc: 0.1478\n",
      "iteration 17 current loss: 0.97323077917099 current acc: 0.1566\n",
      "iteration 18 current loss: 0.8766276240348816 current acc: 0.1658\n",
      "iteration 19 current loss: 0.8561750054359436 current acc: 0.1752\n",
      "iteration 20 current loss: 0.8373057246208191 current acc: 0.1844\n",
      "iteration 21 current loss: 1.1430045366287231 current acc: 0.1916\n",
      "iteration 22 current loss: 0.8300418257713318 current acc: 0.2004\n",
      "iteration 23 current loss: 1.0671632289886475 current acc: 0.2084\n",
      "iteration 24 current loss: 0.8522160053253174 current acc: 0.2162\n",
      "iteration 25 current loss: 0.8924801349639893 current acc: 0.2246\n",
      "iteration 26 current loss: 0.7076618075370789 current acc: 0.2346\n",
      "iteration 27 current loss: 1.0393716096878052 current acc: 0.2416\n",
      "iteration 28 current loss: 0.8810438513755798 current acc: 0.2506\n",
      "iteration 29 current loss: 0.7761492729187012 current acc: 0.26\n",
      "iteration 30 current loss: 0.9894336462020874 current acc: 0.2686\n",
      "iteration 31 current loss: 0.8631118535995483 current acc: 0.2778\n",
      "iteration 32 current loss: 0.9145345091819763 current acc: 0.2864\n",
      "iteration 33 current loss: 0.8945255279541016 current acc: 0.296\n",
      "iteration 34 current loss: 0.9312780499458313 current acc: 0.3054\n",
      "iteration 35 current loss: 0.982473611831665 current acc: 0.3142\n",
      "iteration 36 current loss: 0.923100471496582 current acc: 0.3228\n",
      "iteration 37 current loss: 0.8863331079483032 current acc: 0.3314\n",
      "iteration 38 current loss: 0.8716006875038147 current acc: 0.3404\n",
      "iteration 39 current loss: 0.9695711135864258 current acc: 0.348\n",
      "iteration 40 current loss: 0.8822991847991943 current acc: 0.3566\n",
      "iteration 41 current loss: 0.9437234401702881 current acc: 0.3642\n",
      "iteration 42 current loss: 1.1189345121383667 current acc: 0.3716\n",
      "iteration 43 current loss: 0.8701169490814209 current acc: 0.38\n",
      "iteration 44 current loss: 0.7835325598716736 current acc: 0.3896\n",
      "iteration 45 current loss: 0.8655790686607361 current acc: 0.3988\n",
      "iteration 46 current loss: 0.7582216858863831 current acc: 0.4082\n",
      "iteration 47 current loss: 0.793766438961029 current acc: 0.417\n",
      "iteration 48 current loss: 0.9283799529075623 current acc: 0.4258\n",
      "iteration 49 current loss: 0.8449273109436035 current acc: 0.4354\n",
      "iteration 50 current loss: 0.8464885950088501 current acc: 0.4442\n",
      "iteration 51 current loss: 0.8662497997283936 current acc: 0.4534\n",
      "iteration 52 current loss: 1.1978973150253296 current acc: 0.4602\n",
      "iteration 53 current loss: 0.8647143840789795 current acc: 0.4696\n",
      "iteration 54 current loss: 0.7764892578125 current acc: 0.4794\n",
      "iteration 55 current loss: 0.886272668838501 current acc: 0.4882\n",
      "iteration 56 current loss: 1.0515811443328857 current acc: 0.4964\n",
      "iteration 57 current loss: 1.0170085430145264 current acc: 0.5052\n",
      "iteration 58 current loss: 1.1555670499801636 current acc: 0.5122\n",
      "iteration 59 current loss: 0.8473601341247559 current acc: 0.5204\n",
      "iteration 60 current loss: 0.9528284668922424 current acc: 0.5286\n",
      "iteration 61 current loss: 0.901727020740509 current acc: 0.5368\n",
      "iteration 62 current loss: 0.9784559011459351 current acc: 0.545\n",
      "iteration 63 current loss: 0.7787609696388245 current acc: 0.5548\n",
      "iteration 64 current loss: 1.0285474061965942 current acc: 0.5618\n",
      "iteration 65 current loss: 1.008821964263916 current acc: 0.5702\n",
      "iteration 66 current loss: 1.0959432125091553 current acc: 0.5782\n",
      "iteration 67 current loss: 0.8317261338233948 current acc: 0.5878\n",
      "iteration 68 current loss: 0.7844693660736084 current acc: 0.5968\n",
      "iteration 69 current loss: 0.9465041160583496 current acc: 0.6044\n",
      "iteration 70 current loss: 0.9328793883323669 current acc: 0.613\n",
      "iteration 71 current loss: 0.9961462616920471 current acc: 0.6218\n",
      "iteration 72 current loss: 0.9493345022201538 current acc: 0.631\n",
      "iteration 73 current loss: 0.9690004587173462 current acc: 0.639\n",
      "iteration 74 current loss: 0.900003969669342 current acc: 0.6466\n",
      "iteration 75 current loss: 0.9268991351127625 current acc: 0.6548\n",
      "iteration 76 current loss: 0.9903944134712219 current acc: 0.6632\n",
      "iteration 77 current loss: 0.9597070813179016 current acc: 0.6718\n",
      "iteration 78 current loss: 0.5419577360153198 current acc: 0.673\n",
      "\t\tTrain Epoch 61/100,Train Accuracy: 0.673, Train Loss: 0.9128192527384698.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 61/100, Validation Accuracy: 0.519625, Validation Loss: 1.3666903619766235\n",
      "best loss 0.9128192527384698\n",
      "iteration 0 current loss: 1.163779377937317 current acc: 0.0068\n",
      "iteration 1 current loss: 0.7928418517112732 current acc: 0.0166\n",
      "iteration 2 current loss: 0.9410971999168396 current acc: 0.0252\n",
      "iteration 3 current loss: 0.7434667348861694 current acc: 0.0352\n",
      "iteration 4 current loss: 0.8568238615989685 current acc: 0.043\n",
      "iteration 5 current loss: 1.0179804563522339 current acc: 0.0506\n",
      "iteration 6 current loss: 1.1259092092514038 current acc: 0.0578\n",
      "iteration 7 current loss: 1.0372064113616943 current acc: 0.0662\n",
      "iteration 8 current loss: 0.8120230436325073 current acc: 0.0752\n",
      "iteration 9 current loss: 0.7297329306602478 current acc: 0.0852\n",
      "iteration 10 current loss: 0.7350885272026062 current acc: 0.0958\n",
      "iteration 11 current loss: 0.7528253197669983 current acc: 0.1046\n",
      "iteration 12 current loss: 0.9945170879364014 current acc: 0.113\n",
      "iteration 13 current loss: 0.7848954796791077 current acc: 0.1224\n",
      "iteration 14 current loss: 0.9198933839797974 current acc: 0.1304\n",
      "iteration 15 current loss: 0.8087450861930847 current acc: 0.1394\n",
      "iteration 16 current loss: 0.899486780166626 current acc: 0.148\n",
      "iteration 17 current loss: 0.7627640962600708 current acc: 0.158\n",
      "iteration 18 current loss: 0.884646475315094 current acc: 0.1672\n",
      "iteration 19 current loss: 0.8028151988983154 current acc: 0.1762\n",
      "iteration 20 current loss: 0.9975600242614746 current acc: 0.184\n",
      "iteration 21 current loss: 0.9127410054206848 current acc: 0.1928\n",
      "iteration 22 current loss: 0.8355085849761963 current acc: 0.202\n",
      "iteration 23 current loss: 1.015332818031311 current acc: 0.21\n",
      "iteration 24 current loss: 1.0547075271606445 current acc: 0.2178\n",
      "iteration 25 current loss: 1.0760343074798584 current acc: 0.2258\n",
      "iteration 26 current loss: 0.9614869952201843 current acc: 0.2342\n",
      "iteration 27 current loss: 0.9676485657691956 current acc: 0.243\n",
      "iteration 28 current loss: 0.8848668336868286 current acc: 0.252\n",
      "iteration 29 current loss: 0.9232408404350281 current acc: 0.2602\n",
      "iteration 30 current loss: 0.9542283415794373 current acc: 0.2686\n",
      "iteration 31 current loss: 0.9683581590652466 current acc: 0.2768\n",
      "iteration 32 current loss: 0.8704518675804138 current acc: 0.2862\n",
      "iteration 33 current loss: 0.8872788548469543 current acc: 0.2958\n",
      "iteration 34 current loss: 1.0598931312561035 current acc: 0.3036\n",
      "iteration 35 current loss: 0.8372880816459656 current acc: 0.3126\n",
      "iteration 36 current loss: 0.9062086939811707 current acc: 0.3216\n",
      "iteration 37 current loss: 0.8701453804969788 current acc: 0.3296\n",
      "iteration 38 current loss: 0.8626264929771423 current acc: 0.3392\n",
      "iteration 39 current loss: 0.889551043510437 current acc: 0.3484\n",
      "iteration 40 current loss: 0.9911742806434631 current acc: 0.357\n",
      "iteration 41 current loss: 0.9243000745773315 current acc: 0.365\n",
      "iteration 42 current loss: 1.0021107196807861 current acc: 0.3728\n",
      "iteration 43 current loss: 0.9931267499923706 current acc: 0.3816\n",
      "iteration 44 current loss: 1.1015512943267822 current acc: 0.3888\n",
      "iteration 45 current loss: 1.0192954540252686 current acc: 0.3968\n",
      "iteration 46 current loss: 0.9205763936042786 current acc: 0.4056\n",
      "iteration 47 current loss: 1.0971014499664307 current acc: 0.4138\n",
      "iteration 48 current loss: 0.8909431099891663 current acc: 0.4228\n",
      "iteration 49 current loss: 0.8406059741973877 current acc: 0.4316\n",
      "iteration 50 current loss: 0.8779455423355103 current acc: 0.4412\n",
      "iteration 51 current loss: 0.8966188430786133 current acc: 0.45\n",
      "iteration 52 current loss: 1.1845623254776 current acc: 0.4576\n",
      "iteration 53 current loss: 0.8986639976501465 current acc: 0.4664\n",
      "iteration 54 current loss: 0.9014348387718201 current acc: 0.4748\n",
      "iteration 55 current loss: 0.9440088868141174 current acc: 0.4826\n",
      "iteration 56 current loss: 0.7744647860527039 current acc: 0.4924\n",
      "iteration 57 current loss: 0.886740505695343 current acc: 0.501\n",
      "iteration 58 current loss: 0.9007852077484131 current acc: 0.5096\n",
      "iteration 59 current loss: 0.8414489030838013 current acc: 0.5186\n",
      "iteration 60 current loss: 0.8813697099685669 current acc: 0.5278\n",
      "iteration 61 current loss: 0.9192962050437927 current acc: 0.5364\n",
      "iteration 62 current loss: 1.0488890409469604 current acc: 0.5438\n",
      "iteration 63 current loss: 0.8103137612342834 current acc: 0.5524\n",
      "iteration 64 current loss: 1.0583715438842773 current acc: 0.5594\n",
      "iteration 65 current loss: 0.8095661401748657 current acc: 0.5682\n",
      "iteration 66 current loss: 1.0020349025726318 current acc: 0.5764\n",
      "iteration 67 current loss: 1.1308625936508179 current acc: 0.5838\n",
      "iteration 68 current loss: 1.0651252269744873 current acc: 0.5914\n",
      "iteration 69 current loss: 0.9521002173423767 current acc: 0.6\n",
      "iteration 70 current loss: 0.858685314655304 current acc: 0.6092\n",
      "iteration 71 current loss: 0.9575937390327454 current acc: 0.6166\n",
      "iteration 72 current loss: 0.8996583223342896 current acc: 0.625\n",
      "iteration 73 current loss: 1.0233803987503052 current acc: 0.6328\n",
      "iteration 74 current loss: 0.9182696342468262 current acc: 0.6416\n",
      "iteration 75 current loss: 1.0340361595153809 current acc: 0.6498\n",
      "iteration 76 current loss: 1.0755438804626465 current acc: 0.6574\n",
      "iteration 77 current loss: 0.9132105112075806 current acc: 0.6662\n",
      "iteration 78 current loss: 1.1404900550842285 current acc: 0.667\n",
      "\t\tTrain Epoch 62/100,Train Accuracy: 0.667, Train Loss: 0.9327588956567305.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 62/100, Validation Accuracy: 0.51275, Validation Loss: 1.3741058478355408\n",
      "iteration 0 current loss: 1.065434217453003 current acc: 0.0086\n",
      "iteration 1 current loss: 0.9161911010742188 current acc: 0.0168\n",
      "iteration 2 current loss: 1.0416980981826782 current acc: 0.0252\n",
      "iteration 3 current loss: 0.8873074650764465 current acc: 0.034\n",
      "iteration 4 current loss: 1.2991214990615845 current acc: 0.0412\n",
      "iteration 5 current loss: 1.0746262073516846 current acc: 0.05\n",
      "iteration 6 current loss: 0.8711068034172058 current acc: 0.0592\n",
      "iteration 7 current loss: 1.0279616117477417 current acc: 0.0672\n",
      "iteration 8 current loss: 1.0130198001861572 current acc: 0.0756\n",
      "iteration 9 current loss: 1.1097925901412964 current acc: 0.0834\n",
      "iteration 10 current loss: 0.8489974141120911 current acc: 0.092\n",
      "iteration 11 current loss: 1.0901837348937988 current acc: 0.1002\n",
      "iteration 12 current loss: 0.9851970672607422 current acc: 0.1076\n",
      "iteration 13 current loss: 0.9410626292228699 current acc: 0.1154\n",
      "iteration 14 current loss: 1.1227154731750488 current acc: 0.1228\n",
      "iteration 15 current loss: 1.0351223945617676 current acc: 0.1308\n",
      "iteration 16 current loss: 0.947654664516449 current acc: 0.14\n",
      "iteration 17 current loss: 1.2305794954299927 current acc: 0.1468\n",
      "iteration 18 current loss: 0.9965673089027405 current acc: 0.1552\n",
      "iteration 19 current loss: 1.0220447778701782 current acc: 0.1634\n",
      "iteration 20 current loss: 0.8148776888847351 current acc: 0.173\n",
      "iteration 21 current loss: 0.8643026947975159 current acc: 0.182\n",
      "iteration 22 current loss: 0.9814153909683228 current acc: 0.1902\n",
      "iteration 23 current loss: 1.0943825244903564 current acc: 0.1984\n",
      "iteration 24 current loss: 0.9761190414428711 current acc: 0.2064\n",
      "iteration 25 current loss: 0.9128966927528381 current acc: 0.2142\n",
      "iteration 26 current loss: 0.919242799282074 current acc: 0.2226\n",
      "iteration 27 current loss: 1.0315325260162354 current acc: 0.231\n",
      "iteration 28 current loss: 0.8415188193321228 current acc: 0.2408\n",
      "iteration 29 current loss: 0.8274115324020386 current acc: 0.2494\n",
      "iteration 30 current loss: 0.7536503076553345 current acc: 0.2598\n",
      "iteration 31 current loss: 0.8154062032699585 current acc: 0.2682\n",
      "iteration 32 current loss: 0.7522887587547302 current acc: 0.2772\n",
      "iteration 33 current loss: 0.8021587133407593 current acc: 0.2866\n",
      "iteration 34 current loss: 0.9079855680465698 current acc: 0.2958\n",
      "iteration 35 current loss: 1.1033692359924316 current acc: 0.3036\n",
      "iteration 36 current loss: 1.109559416770935 current acc: 0.3108\n",
      "iteration 37 current loss: 0.8788871765136719 current acc: 0.3192\n",
      "iteration 38 current loss: 0.8243736028671265 current acc: 0.3282\n",
      "iteration 39 current loss: 1.0083956718444824 current acc: 0.3356\n",
      "iteration 40 current loss: 0.9353679418563843 current acc: 0.3434\n",
      "iteration 41 current loss: 1.2455253601074219 current acc: 0.3508\n",
      "iteration 42 current loss: 1.2288856506347656 current acc: 0.358\n",
      "iteration 43 current loss: 1.1583950519561768 current acc: 0.3664\n",
      "iteration 44 current loss: 1.104727864265442 current acc: 0.3738\n",
      "iteration 45 current loss: 0.9443134069442749 current acc: 0.3826\n",
      "iteration 46 current loss: 0.7474838495254517 current acc: 0.392\n",
      "iteration 47 current loss: 1.0624785423278809 current acc: 0.4\n",
      "iteration 48 current loss: 0.860338032245636 current acc: 0.409\n",
      "iteration 49 current loss: 0.7755714058876038 current acc: 0.4184\n",
      "iteration 50 current loss: 0.8865722417831421 current acc: 0.4272\n",
      "iteration 51 current loss: 0.8989207148551941 current acc: 0.4356\n",
      "iteration 52 current loss: 1.0546300411224365 current acc: 0.4438\n",
      "iteration 53 current loss: 1.1357570886611938 current acc: 0.4516\n",
      "iteration 54 current loss: 0.9095237255096436 current acc: 0.4604\n",
      "iteration 55 current loss: 0.7572121620178223 current acc: 0.4702\n",
      "iteration 56 current loss: 1.000097632408142 current acc: 0.4778\n",
      "iteration 57 current loss: 1.0185954570770264 current acc: 0.4858\n",
      "iteration 58 current loss: 0.98512864112854 current acc: 0.4936\n",
      "iteration 59 current loss: 0.7913251519203186 current acc: 0.5028\n",
      "iteration 60 current loss: 0.9930172562599182 current acc: 0.5114\n",
      "iteration 61 current loss: 0.9714149832725525 current acc: 0.5194\n",
      "iteration 62 current loss: 0.9212275743484497 current acc: 0.5272\n",
      "iteration 63 current loss: 0.9296244382858276 current acc: 0.535\n",
      "iteration 64 current loss: 0.7857929468154907 current acc: 0.5446\n",
      "iteration 65 current loss: 0.93269944190979 current acc: 0.553\n",
      "iteration 66 current loss: 1.0248470306396484 current acc: 0.561\n",
      "iteration 67 current loss: 0.7732523083686829 current acc: 0.5712\n",
      "iteration 68 current loss: 1.1133413314819336 current acc: 0.5792\n",
      "iteration 69 current loss: 0.921896755695343 current acc: 0.5876\n",
      "iteration 70 current loss: 0.8404507637023926 current acc: 0.5974\n",
      "iteration 71 current loss: 0.7738450169563293 current acc: 0.607\n",
      "iteration 72 current loss: 1.101917028427124 current acc: 0.615\n",
      "iteration 73 current loss: 1.0199143886566162 current acc: 0.6232\n",
      "iteration 74 current loss: 1.0589796304702759 current acc: 0.6306\n",
      "iteration 75 current loss: 0.8485866785049438 current acc: 0.6392\n",
      "iteration 76 current loss: 1.0904444456100464 current acc: 0.6468\n",
      "iteration 77 current loss: 1.007264494895935 current acc: 0.6548\n",
      "iteration 78 current loss: 0.7909481525421143 current acc: 0.656\n",
      "\t\tTrain Epoch 63/100,Train Accuracy: 0.656, Train Loss: 0.9638794094701356.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 63/100, Validation Accuracy: 0.56325, Validation Loss: 1.2162529816627503\n",
      "iteration 0 current loss: 0.9607273936271667 current acc: 0.0092\n",
      "iteration 1 current loss: 0.835175633430481 current acc: 0.0182\n",
      "iteration 2 current loss: 0.8110020160675049 current acc: 0.0268\n",
      "iteration 3 current loss: 1.0816258192062378 current acc: 0.0346\n",
      "iteration 4 current loss: 1.0074585676193237 current acc: 0.0418\n",
      "iteration 5 current loss: 0.8928517699241638 current acc: 0.05\n",
      "iteration 6 current loss: 0.8629230260848999 current acc: 0.0588\n",
      "iteration 7 current loss: 0.9808504581451416 current acc: 0.0668\n",
      "iteration 8 current loss: 0.9477707147598267 current acc: 0.0758\n",
      "iteration 9 current loss: 0.8108879923820496 current acc: 0.0852\n",
      "iteration 10 current loss: 0.840157151222229 current acc: 0.094\n",
      "iteration 11 current loss: 0.9940920472145081 current acc: 0.1024\n",
      "iteration 12 current loss: 0.8472803235054016 current acc: 0.1114\n",
      "iteration 13 current loss: 0.8878189325332642 current acc: 0.12\n",
      "iteration 14 current loss: 0.9906837940216064 current acc: 0.1278\n",
      "iteration 15 current loss: 0.9399628043174744 current acc: 0.1358\n",
      "iteration 16 current loss: 0.8607631921768188 current acc: 0.1454\n",
      "iteration 17 current loss: 0.887533962726593 current acc: 0.1542\n",
      "iteration 18 current loss: 0.9757010340690613 current acc: 0.1626\n",
      "iteration 19 current loss: 0.6793248057365417 current acc: 0.1716\n",
      "iteration 20 current loss: 0.7282547950744629 current acc: 0.1818\n",
      "iteration 21 current loss: 0.7712544202804565 current acc: 0.191\n",
      "iteration 22 current loss: 1.0940918922424316 current acc: 0.1984\n",
      "iteration 23 current loss: 1.0102146863937378 current acc: 0.2072\n",
      "iteration 24 current loss: 0.8697127103805542 current acc: 0.2164\n",
      "iteration 25 current loss: 0.9224404692649841 current acc: 0.2248\n",
      "iteration 26 current loss: 1.095064640045166 current acc: 0.2322\n",
      "iteration 27 current loss: 0.775352418422699 current acc: 0.2414\n",
      "iteration 28 current loss: 0.9076951146125793 current acc: 0.2498\n",
      "iteration 29 current loss: 0.8081541061401367 current acc: 0.258\n",
      "iteration 30 current loss: 1.0507277250289917 current acc: 0.2662\n",
      "iteration 31 current loss: 0.9689537882804871 current acc: 0.275\n",
      "iteration 32 current loss: 0.9875870943069458 current acc: 0.2834\n",
      "iteration 33 current loss: 0.9986732602119446 current acc: 0.2912\n",
      "iteration 34 current loss: 0.8881964683532715 current acc: 0.3\n",
      "iteration 35 current loss: 0.9400643110275269 current acc: 0.3082\n",
      "iteration 36 current loss: 0.8628679513931274 current acc: 0.317\n",
      "iteration 37 current loss: 0.8110638856887817 current acc: 0.3264\n",
      "iteration 38 current loss: 0.8766757249832153 current acc: 0.3358\n",
      "iteration 39 current loss: 0.961609959602356 current acc: 0.3448\n",
      "iteration 40 current loss: 1.045557975769043 current acc: 0.3524\n",
      "iteration 41 current loss: 1.166828989982605 current acc: 0.3598\n",
      "iteration 42 current loss: 1.0302116870880127 current acc: 0.3684\n",
      "iteration 43 current loss: 1.06747305393219 current acc: 0.376\n",
      "iteration 44 current loss: 1.0158730745315552 current acc: 0.3842\n",
      "iteration 45 current loss: 1.0715336799621582 current acc: 0.3924\n",
      "iteration 46 current loss: 0.9071942567825317 current acc: 0.4006\n",
      "iteration 47 current loss: 1.04776132106781 current acc: 0.4082\n",
      "iteration 48 current loss: 0.7808775305747986 current acc: 0.4178\n",
      "iteration 49 current loss: 1.0117584466934204 current acc: 0.4256\n",
      "iteration 50 current loss: 1.030967354774475 current acc: 0.4334\n",
      "iteration 51 current loss: 1.0053303241729736 current acc: 0.4414\n",
      "iteration 52 current loss: 0.9734821319580078 current acc: 0.4498\n",
      "iteration 53 current loss: 0.8990853428840637 current acc: 0.4588\n",
      "iteration 54 current loss: 0.9366450309753418 current acc: 0.4668\n",
      "iteration 55 current loss: 0.9262295365333557 current acc: 0.4758\n",
      "iteration 56 current loss: 0.7587449550628662 current acc: 0.4858\n",
      "iteration 57 current loss: 1.0171256065368652 current acc: 0.4942\n",
      "iteration 58 current loss: 0.824322521686554 current acc: 0.504\n",
      "iteration 59 current loss: 0.8496531844139099 current acc: 0.5124\n",
      "iteration 60 current loss: 0.9725015163421631 current acc: 0.5208\n",
      "iteration 61 current loss: 0.8168638944625854 current acc: 0.53\n",
      "iteration 62 current loss: 0.9693014025688171 current acc: 0.539\n",
      "iteration 63 current loss: 1.1565595865249634 current acc: 0.5464\n",
      "iteration 64 current loss: 0.9548115730285645 current acc: 0.5552\n",
      "iteration 65 current loss: 0.8425029516220093 current acc: 0.5634\n",
      "iteration 66 current loss: 0.9613177180290222 current acc: 0.5716\n",
      "iteration 67 current loss: 0.9712982773780823 current acc: 0.58\n",
      "iteration 68 current loss: 0.8917410373687744 current acc: 0.589\n",
      "iteration 69 current loss: 0.9200124144554138 current acc: 0.5974\n",
      "iteration 70 current loss: 0.849168598651886 current acc: 0.6062\n",
      "iteration 71 current loss: 0.9121522903442383 current acc: 0.6142\n",
      "iteration 72 current loss: 0.6917830109596252 current acc: 0.6246\n",
      "iteration 73 current loss: 1.0257092714309692 current acc: 0.6318\n",
      "iteration 74 current loss: 0.9481756687164307 current acc: 0.6402\n",
      "iteration 75 current loss: 0.8835703134536743 current acc: 0.6488\n",
      "iteration 76 current loss: 0.9073753952980042 current acc: 0.6572\n",
      "iteration 77 current loss: 0.8926742076873779 current acc: 0.6666\n",
      "iteration 78 current loss: 0.6929616928100586 current acc: 0.6678\n",
      "\t\tTrain Epoch 64/100,Train Accuracy: 0.6678, Train Loss: 0.9246884011015107.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 64/100, Validation Accuracy: 0.5595, Validation Loss: 1.2356513652801513\n",
      "iteration 0 current loss: 1.0916389226913452 current acc: 0.0084\n",
      "iteration 1 current loss: 0.7493733763694763 current acc: 0.018\n",
      "iteration 2 current loss: 0.9393714070320129 current acc: 0.0274\n",
      "iteration 3 current loss: 0.9755980372428894 current acc: 0.0352\n",
      "iteration 4 current loss: 0.761028528213501 current acc: 0.0448\n",
      "iteration 5 current loss: 0.776435375213623 current acc: 0.055\n",
      "iteration 6 current loss: 0.7572615146636963 current acc: 0.0648\n",
      "iteration 7 current loss: 0.7426512241363525 current acc: 0.0742\n",
      "iteration 8 current loss: 0.8462758660316467 current acc: 0.0826\n",
      "iteration 9 current loss: 0.7928854823112488 current acc: 0.092\n",
      "iteration 10 current loss: 0.7358046770095825 current acc: 0.1026\n",
      "iteration 11 current loss: 0.7766273021697998 current acc: 0.1122\n",
      "iteration 12 current loss: 0.8632251024246216 current acc: 0.1216\n",
      "iteration 13 current loss: 1.0410144329071045 current acc: 0.13\n",
      "iteration 14 current loss: 0.89534991979599 current acc: 0.1388\n",
      "iteration 15 current loss: 0.9891666769981384 current acc: 0.1474\n",
      "iteration 16 current loss: 0.894477367401123 current acc: 0.1558\n",
      "iteration 17 current loss: 0.9682631492614746 current acc: 0.1644\n",
      "iteration 18 current loss: 1.0518070459365845 current acc: 0.1722\n",
      "iteration 19 current loss: 1.0725136995315552 current acc: 0.179\n",
      "iteration 20 current loss: 1.0007858276367188 current acc: 0.1864\n",
      "iteration 21 current loss: 0.8673151135444641 current acc: 0.1946\n",
      "iteration 22 current loss: 0.8283310532569885 current acc: 0.2034\n",
      "iteration 23 current loss: 0.9995557069778442 current acc: 0.211\n",
      "iteration 24 current loss: 0.9906253218650818 current acc: 0.2192\n",
      "iteration 25 current loss: 0.6585325598716736 current acc: 0.229\n",
      "iteration 26 current loss: 0.7916979789733887 current acc: 0.2384\n",
      "iteration 27 current loss: 0.8364382386207581 current acc: 0.2476\n",
      "iteration 28 current loss: 0.826419472694397 current acc: 0.2558\n",
      "iteration 29 current loss: 0.9515275955200195 current acc: 0.2644\n",
      "iteration 30 current loss: 0.7380192279815674 current acc: 0.274\n",
      "iteration 31 current loss: 1.0743666887283325 current acc: 0.281\n",
      "iteration 32 current loss: 1.1133201122283936 current acc: 0.2888\n",
      "iteration 33 current loss: 0.9340401887893677 current acc: 0.2982\n",
      "iteration 34 current loss: 0.964073121547699 current acc: 0.3062\n",
      "iteration 35 current loss: 0.8231487274169922 current acc: 0.315\n",
      "iteration 36 current loss: 0.8982020020484924 current acc: 0.3244\n",
      "iteration 37 current loss: 0.8104357719421387 current acc: 0.3336\n",
      "iteration 38 current loss: 0.8493833541870117 current acc: 0.3422\n",
      "iteration 39 current loss: 0.9371930956840515 current acc: 0.3516\n",
      "iteration 40 current loss: 0.8616503477096558 current acc: 0.3606\n",
      "iteration 41 current loss: 0.9219688177108765 current acc: 0.3694\n",
      "iteration 42 current loss: 1.0367367267608643 current acc: 0.377\n",
      "iteration 43 current loss: 1.2993794679641724 current acc: 0.3842\n",
      "iteration 44 current loss: 0.7768122553825378 current acc: 0.3934\n",
      "iteration 45 current loss: 0.9207713007926941 current acc: 0.402\n",
      "iteration 46 current loss: 0.7641143202781677 current acc: 0.4112\n",
      "iteration 47 current loss: 0.8626592755317688 current acc: 0.4202\n",
      "iteration 48 current loss: 0.7962228059768677 current acc: 0.4302\n",
      "iteration 49 current loss: 0.8401979207992554 current acc: 0.4398\n",
      "iteration 50 current loss: 0.9282971620559692 current acc: 0.4488\n",
      "iteration 51 current loss: 1.026940107345581 current acc: 0.4552\n",
      "iteration 52 current loss: 0.8851706981658936 current acc: 0.4644\n",
      "iteration 53 current loss: 1.011599063873291 current acc: 0.473\n",
      "iteration 54 current loss: 1.059686303138733 current acc: 0.4804\n",
      "iteration 55 current loss: 0.9357321858406067 current acc: 0.4892\n",
      "iteration 56 current loss: 0.9541878700256348 current acc: 0.4978\n",
      "iteration 57 current loss: 1.0383104085922241 current acc: 0.5058\n",
      "iteration 58 current loss: 0.9939318895339966 current acc: 0.5144\n",
      "iteration 59 current loss: 1.0351325273513794 current acc: 0.5222\n",
      "iteration 60 current loss: 0.7108525037765503 current acc: 0.5318\n",
      "iteration 61 current loss: 0.9045702815055847 current acc: 0.541\n",
      "iteration 62 current loss: 1.0629504919052124 current acc: 0.5484\n",
      "iteration 63 current loss: 1.0735729932785034 current acc: 0.5554\n",
      "iteration 64 current loss: 0.7308273911476135 current acc: 0.5654\n",
      "iteration 65 current loss: 0.9785898923873901 current acc: 0.5738\n",
      "iteration 66 current loss: 1.1987732648849487 current acc: 0.581\n",
      "iteration 67 current loss: 1.0011601448059082 current acc: 0.589\n",
      "iteration 68 current loss: 1.0224934816360474 current acc: 0.5968\n",
      "iteration 69 current loss: 0.9658363461494446 current acc: 0.6056\n",
      "iteration 70 current loss: 0.8585716485977173 current acc: 0.6146\n",
      "iteration 71 current loss: 0.9555577635765076 current acc: 0.6236\n",
      "iteration 72 current loss: 1.1094286441802979 current acc: 0.631\n",
      "iteration 73 current loss: 0.8482275009155273 current acc: 0.6398\n",
      "iteration 74 current loss: 1.058820366859436 current acc: 0.6482\n",
      "iteration 75 current loss: 0.9589347839355469 current acc: 0.6574\n",
      "iteration 76 current loss: 0.9000867605209351 current acc: 0.6666\n",
      "iteration 77 current loss: 1.0694886445999146 current acc: 0.6742\n",
      "iteration 78 current loss: 1.0009206533432007 current acc: 0.6752\n",
      "\t\tTrain Epoch 65/100,Train Accuracy: 0.6752, Train Loss: 0.9237132313885267.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 65/100, Validation Accuracy: 0.5475, Validation Loss: 1.2523624563217164\n",
      "iteration 0 current loss: 1.0117311477661133 current acc: 0.0084\n",
      "iteration 1 current loss: 0.9567471146583557 current acc: 0.0164\n",
      "iteration 2 current loss: 0.9099912047386169 current acc: 0.025\n",
      "iteration 3 current loss: 0.9192396998405457 current acc: 0.034\n",
      "iteration 4 current loss: 0.8571112751960754 current acc: 0.0428\n",
      "iteration 5 current loss: 1.0456711053848267 current acc: 0.05\n",
      "iteration 6 current loss: 0.7155068516731262 current acc: 0.0596\n",
      "iteration 7 current loss: 0.8768635988235474 current acc: 0.0686\n",
      "iteration 8 current loss: 0.7565945982933044 current acc: 0.0784\n",
      "iteration 9 current loss: 1.090382695198059 current acc: 0.0872\n",
      "iteration 10 current loss: 0.9451173543930054 current acc: 0.0956\n",
      "iteration 11 current loss: 0.8715950846672058 current acc: 0.1046\n",
      "iteration 12 current loss: 1.0580402612686157 current acc: 0.1116\n",
      "iteration 13 current loss: 0.9165471792221069 current acc: 0.1202\n",
      "iteration 14 current loss: 0.8304983377456665 current acc: 0.1296\n",
      "iteration 15 current loss: 0.8064278960227966 current acc: 0.1396\n",
      "iteration 16 current loss: 0.8207383751869202 current acc: 0.1498\n",
      "iteration 17 current loss: 1.0088424682617188 current acc: 0.1576\n",
      "iteration 18 current loss: 0.7929171323776245 current acc: 0.1664\n",
      "iteration 19 current loss: 0.8884377479553223 current acc: 0.1752\n",
      "iteration 20 current loss: 1.0791454315185547 current acc: 0.1828\n",
      "iteration 21 current loss: 1.070810079574585 current acc: 0.1912\n",
      "iteration 22 current loss: 0.9506888389587402 current acc: 0.1992\n",
      "iteration 23 current loss: 0.8681985139846802 current acc: 0.2088\n",
      "iteration 24 current loss: 0.8850933313369751 current acc: 0.2176\n",
      "iteration 25 current loss: 0.941010057926178 current acc: 0.225\n",
      "iteration 26 current loss: 0.7539883852005005 current acc: 0.2344\n",
      "iteration 27 current loss: 0.974734902381897 current acc: 0.2426\n",
      "iteration 28 current loss: 1.0630598068237305 current acc: 0.251\n",
      "iteration 29 current loss: 1.1560077667236328 current acc: 0.2586\n",
      "iteration 30 current loss: 0.918673038482666 current acc: 0.2666\n",
      "iteration 31 current loss: 0.9377533793449402 current acc: 0.2752\n",
      "iteration 32 current loss: 1.07112455368042 current acc: 0.2828\n",
      "iteration 33 current loss: 0.9868796467781067 current acc: 0.2906\n",
      "iteration 34 current loss: 0.7377562522888184 current acc: 0.2996\n",
      "iteration 35 current loss: 0.9029741287231445 current acc: 0.308\n",
      "iteration 36 current loss: 0.7444875836372375 current acc: 0.3172\n",
      "iteration 37 current loss: 0.9118587970733643 current acc: 0.326\n",
      "iteration 38 current loss: 0.9352251291275024 current acc: 0.3346\n",
      "iteration 39 current loss: 0.80379319190979 current acc: 0.3434\n",
      "iteration 40 current loss: 1.158829689025879 current acc: 0.35\n",
      "iteration 41 current loss: 0.9486164450645447 current acc: 0.359\n",
      "iteration 42 current loss: 0.7858434915542603 current acc: 0.3686\n",
      "iteration 43 current loss: 0.9303637742996216 current acc: 0.377\n",
      "iteration 44 current loss: 0.9813979864120483 current acc: 0.3852\n",
      "iteration 45 current loss: 0.8591089248657227 current acc: 0.3942\n",
      "iteration 46 current loss: 0.9487478733062744 current acc: 0.4024\n",
      "iteration 47 current loss: 0.8423712849617004 current acc: 0.4118\n",
      "iteration 48 current loss: 0.8827745914459229 current acc: 0.42\n",
      "iteration 49 current loss: 0.9631657004356384 current acc: 0.4292\n",
      "iteration 50 current loss: 0.9668046832084656 current acc: 0.437\n",
      "iteration 51 current loss: 0.8915246725082397 current acc: 0.4454\n",
      "iteration 52 current loss: 0.9254786372184753 current acc: 0.453\n",
      "iteration 53 current loss: 1.0315258502960205 current acc: 0.4614\n",
      "iteration 54 current loss: 0.9488086104393005 current acc: 0.47\n",
      "iteration 55 current loss: 1.1487030982971191 current acc: 0.477\n",
      "iteration 56 current loss: 0.8065515756607056 current acc: 0.4858\n",
      "iteration 57 current loss: 1.0028188228607178 current acc: 0.4944\n",
      "iteration 58 current loss: 1.049478530883789 current acc: 0.503\n",
      "iteration 59 current loss: 1.080888032913208 current acc: 0.511\n",
      "iteration 60 current loss: 1.0339566469192505 current acc: 0.5186\n",
      "iteration 61 current loss: 0.8616030812263489 current acc: 0.5278\n",
      "iteration 62 current loss: 0.865013837814331 current acc: 0.5362\n",
      "iteration 63 current loss: 1.123805284500122 current acc: 0.5432\n",
      "iteration 64 current loss: 0.9728192090988159 current acc: 0.5512\n",
      "iteration 65 current loss: 0.8349387049674988 current acc: 0.5596\n",
      "iteration 66 current loss: 0.9581016302108765 current acc: 0.5674\n",
      "iteration 67 current loss: 0.9024134278297424 current acc: 0.5756\n",
      "iteration 68 current loss: 0.842995822429657 current acc: 0.5846\n",
      "iteration 69 current loss: 0.9683055281639099 current acc: 0.5926\n",
      "iteration 70 current loss: 0.895145833492279 current acc: 0.601\n",
      "iteration 71 current loss: 0.9045316576957703 current acc: 0.6094\n",
      "iteration 72 current loss: 0.9430598616600037 current acc: 0.6182\n",
      "iteration 73 current loss: 0.7782558798789978 current acc: 0.627\n",
      "iteration 74 current loss: 0.8910447955131531 current acc: 0.6364\n",
      "iteration 75 current loss: 0.9670660495758057 current acc: 0.6444\n",
      "iteration 76 current loss: 1.0032204389572144 current acc: 0.6524\n",
      "iteration 77 current loss: 1.0118465423583984 current acc: 0.6612\n",
      "iteration 78 current loss: 1.5449159145355225 current acc: 0.6616\n",
      "\t\tTrain Epoch 66/100,Train Accuracy: 0.6616, Train Loss: 0.9396089413498021.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 66/100, Validation Accuracy: 0.56, Validation Loss: 1.2053322100639343\n",
      "iteration 0 current loss: 0.9565498232841492 current acc: 0.008\n",
      "iteration 1 current loss: 0.8167291283607483 current acc: 0.0172\n",
      "iteration 2 current loss: 1.0388095378875732 current acc: 0.0258\n",
      "iteration 3 current loss: 1.110314130783081 current acc: 0.0334\n",
      "iteration 4 current loss: 0.9676690101623535 current acc: 0.041\n",
      "iteration 5 current loss: 0.735083281993866 current acc: 0.0508\n",
      "iteration 6 current loss: 1.0898326635360718 current acc: 0.0574\n",
      "iteration 7 current loss: 0.8919557332992554 current acc: 0.0666\n",
      "iteration 8 current loss: 1.2296639680862427 current acc: 0.0738\n",
      "iteration 9 current loss: 0.9076109528541565 current acc: 0.0826\n",
      "iteration 10 current loss: 0.933760941028595 current acc: 0.0908\n",
      "iteration 11 current loss: 0.8375087976455688 current acc: 0.1\n",
      "iteration 12 current loss: 0.894133448600769 current acc: 0.1094\n",
      "iteration 13 current loss: 0.9836772084236145 current acc: 0.1174\n",
      "iteration 14 current loss: 0.8649771213531494 current acc: 0.1254\n",
      "iteration 15 current loss: 1.107216477394104 current acc: 0.1324\n",
      "iteration 16 current loss: 0.9761244654655457 current acc: 0.1402\n",
      "iteration 17 current loss: 1.0058178901672363 current acc: 0.148\n",
      "iteration 18 current loss: 0.9930028915405273 current acc: 0.1564\n",
      "iteration 19 current loss: 0.8663429021835327 current acc: 0.1648\n",
      "iteration 20 current loss: 0.8013290166854858 current acc: 0.174\n",
      "iteration 21 current loss: 0.8531854152679443 current acc: 0.1828\n",
      "iteration 22 current loss: 0.9243883490562439 current acc: 0.1912\n",
      "iteration 23 current loss: 0.9042270183563232 current acc: 0.199\n",
      "iteration 24 current loss: 0.9986679553985596 current acc: 0.207\n",
      "iteration 25 current loss: 0.6938178539276123 current acc: 0.2172\n",
      "iteration 26 current loss: 0.8239100575447083 current acc: 0.2258\n",
      "iteration 27 current loss: 0.8065240979194641 current acc: 0.2346\n",
      "iteration 28 current loss: 0.9418256878852844 current acc: 0.2434\n",
      "iteration 29 current loss: 0.7912856936454773 current acc: 0.2524\n",
      "iteration 30 current loss: 0.8315114974975586 current acc: 0.261\n",
      "iteration 31 current loss: 0.9165946245193481 current acc: 0.2696\n",
      "iteration 32 current loss: 1.088135004043579 current acc: 0.277\n",
      "iteration 33 current loss: 1.042568325996399 current acc: 0.2848\n",
      "iteration 34 current loss: 1.0868864059448242 current acc: 0.2926\n",
      "iteration 35 current loss: 0.9525051116943359 current acc: 0.3002\n",
      "iteration 36 current loss: 0.9930990934371948 current acc: 0.3084\n",
      "iteration 37 current loss: 1.0078251361846924 current acc: 0.3152\n",
      "iteration 38 current loss: 1.040963053703308 current acc: 0.3234\n",
      "iteration 39 current loss: 0.9934256076812744 current acc: 0.3322\n",
      "iteration 40 current loss: 0.9773731827735901 current acc: 0.3404\n",
      "iteration 41 current loss: 0.9573639035224915 current acc: 0.3488\n",
      "iteration 42 current loss: 0.8817741870880127 current acc: 0.3574\n",
      "iteration 43 current loss: 0.7872939109802246 current acc: 0.3668\n",
      "iteration 44 current loss: 1.053105354309082 current acc: 0.375\n",
      "iteration 45 current loss: 1.0095022916793823 current acc: 0.383\n",
      "iteration 46 current loss: 0.9002211689949036 current acc: 0.3918\n",
      "iteration 47 current loss: 1.1149885654449463 current acc: 0.399\n",
      "iteration 48 current loss: 0.9456088542938232 current acc: 0.4078\n",
      "iteration 49 current loss: 0.7513086199760437 current acc: 0.418\n",
      "iteration 50 current loss: 1.103075623512268 current acc: 0.425\n",
      "iteration 51 current loss: 0.9983974099159241 current acc: 0.4332\n",
      "iteration 52 current loss: 1.2189505100250244 current acc: 0.4394\n",
      "iteration 53 current loss: 0.8276781439781189 current acc: 0.4478\n",
      "iteration 54 current loss: 1.035838007926941 current acc: 0.4562\n",
      "iteration 55 current loss: 0.9434483647346497 current acc: 0.4644\n",
      "iteration 56 current loss: 1.091299295425415 current acc: 0.4724\n",
      "iteration 57 current loss: 0.96788489818573 current acc: 0.481\n",
      "iteration 58 current loss: 0.88861483335495 current acc: 0.4896\n",
      "iteration 59 current loss: 0.7540817856788635 current acc: 0.4994\n",
      "iteration 60 current loss: 1.0803062915802002 current acc: 0.5066\n",
      "iteration 61 current loss: 0.9276794195175171 current acc: 0.515\n",
      "iteration 62 current loss: 0.9424049854278564 current acc: 0.5238\n",
      "iteration 63 current loss: 1.1230616569519043 current acc: 0.5312\n",
      "iteration 64 current loss: 0.9000276327133179 current acc: 0.5402\n",
      "iteration 65 current loss: 0.8409473299980164 current acc: 0.5496\n",
      "iteration 66 current loss: 0.9066197276115417 current acc: 0.5586\n",
      "iteration 67 current loss: 0.8751711845397949 current acc: 0.5676\n",
      "iteration 68 current loss: 0.9563919305801392 current acc: 0.576\n",
      "iteration 69 current loss: 0.8095291256904602 current acc: 0.5848\n",
      "iteration 70 current loss: 0.8416121602058411 current acc: 0.594\n",
      "iteration 71 current loss: 1.1204439401626587 current acc: 0.602\n",
      "iteration 72 current loss: 0.9327386617660522 current acc: 0.6102\n",
      "iteration 73 current loss: 0.8548271059989929 current acc: 0.6184\n",
      "iteration 74 current loss: 0.9693301916122437 current acc: 0.627\n",
      "iteration 75 current loss: 1.0173704624176025 current acc: 0.6354\n",
      "iteration 76 current loss: 0.965275764465332 current acc: 0.643\n",
      "iteration 77 current loss: 0.8069264888763428 current acc: 0.6518\n",
      "iteration 78 current loss: 0.9413570165634155 current acc: 0.6532\n",
      "\t\tTrain Epoch 67/100,Train Accuracy: 0.6532, Train Loss: 0.9458137388470806.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 67/100, Validation Accuracy: 0.5395, Validation Loss: 1.2664292120933534\n",
      "iteration 0 current loss: 1.024418592453003 current acc: 0.0074\n",
      "iteration 1 current loss: 0.9077437520027161 current acc: 0.0162\n",
      "iteration 2 current loss: 0.9360607862472534 current acc: 0.0248\n",
      "iteration 3 current loss: 0.9075455665588379 current acc: 0.0336\n",
      "iteration 4 current loss: 1.0912432670593262 current acc: 0.0412\n",
      "iteration 5 current loss: 0.958315908908844 current acc: 0.05\n",
      "iteration 6 current loss: 1.0063217878341675 current acc: 0.058\n",
      "iteration 7 current loss: 0.9799812436103821 current acc: 0.0656\n",
      "iteration 8 current loss: 1.0043113231658936 current acc: 0.0738\n",
      "iteration 9 current loss: 0.8293494582176208 current acc: 0.083\n",
      "iteration 10 current loss: 0.904255747795105 current acc: 0.0916\n",
      "iteration 11 current loss: 0.8322182297706604 current acc: 0.101\n",
      "iteration 12 current loss: 0.8392259478569031 current acc: 0.1102\n",
      "iteration 13 current loss: 0.9544429183006287 current acc: 0.1186\n",
      "iteration 14 current loss: 0.8041464686393738 current acc: 0.1276\n",
      "iteration 15 current loss: 1.0053125619888306 current acc: 0.136\n",
      "iteration 16 current loss: 0.9358109831809998 current acc: 0.1452\n",
      "iteration 17 current loss: 0.8108734488487244 current acc: 0.1542\n",
      "iteration 18 current loss: 0.8918982148170471 current acc: 0.1626\n",
      "iteration 19 current loss: 1.038867712020874 current acc: 0.1702\n",
      "iteration 20 current loss: 0.9393969774246216 current acc: 0.1784\n",
      "iteration 21 current loss: 0.8914111256599426 current acc: 0.1878\n",
      "iteration 22 current loss: 0.8971683382987976 current acc: 0.1966\n",
      "iteration 23 current loss: 1.0101104974746704 current acc: 0.2048\n",
      "iteration 24 current loss: 1.0166611671447754 current acc: 0.2136\n",
      "iteration 25 current loss: 1.125018835067749 current acc: 0.2208\n",
      "iteration 26 current loss: 0.995468258857727 current acc: 0.2286\n",
      "iteration 27 current loss: 0.7520015239715576 current acc: 0.2382\n",
      "iteration 28 current loss: 0.7919618487358093 current acc: 0.2478\n",
      "iteration 29 current loss: 1.0421539545059204 current acc: 0.256\n",
      "iteration 30 current loss: 0.684684157371521 current acc: 0.2664\n",
      "iteration 31 current loss: 0.8686034083366394 current acc: 0.2752\n",
      "iteration 32 current loss: 1.041733980178833 current acc: 0.2842\n",
      "iteration 33 current loss: 0.8016788363456726 current acc: 0.2938\n",
      "iteration 34 current loss: 0.9239805340766907 current acc: 0.3024\n",
      "iteration 35 current loss: 0.7452357411384583 current acc: 0.3128\n",
      "iteration 36 current loss: 0.848996639251709 current acc: 0.3222\n",
      "iteration 37 current loss: 0.9108358025550842 current acc: 0.3314\n",
      "iteration 38 current loss: 0.903320848941803 current acc: 0.3396\n",
      "iteration 39 current loss: 0.8361350297927856 current acc: 0.3492\n",
      "iteration 40 current loss: 0.8787315487861633 current acc: 0.3584\n",
      "iteration 41 current loss: 0.8796801567077637 current acc: 0.367\n",
      "iteration 42 current loss: 1.1326696872711182 current acc: 0.3748\n",
      "iteration 43 current loss: 1.014159083366394 current acc: 0.3826\n",
      "iteration 44 current loss: 0.7787326574325562 current acc: 0.3918\n",
      "iteration 45 current loss: 0.8016085624694824 current acc: 0.4004\n",
      "iteration 46 current loss: 1.193682312965393 current acc: 0.4076\n",
      "iteration 47 current loss: 1.146144151687622 current acc: 0.415\n",
      "iteration 48 current loss: 1.059387445449829 current acc: 0.4232\n",
      "iteration 49 current loss: 0.7672534584999084 current acc: 0.4322\n",
      "iteration 50 current loss: 0.8888263702392578 current acc: 0.4406\n",
      "iteration 51 current loss: 0.9504464268684387 current acc: 0.4492\n",
      "iteration 52 current loss: 1.0698862075805664 current acc: 0.4564\n",
      "iteration 53 current loss: 0.8901624083518982 current acc: 0.4656\n",
      "iteration 54 current loss: 0.9108642339706421 current acc: 0.4734\n",
      "iteration 55 current loss: 0.7660064101219177 current acc: 0.4832\n",
      "iteration 56 current loss: 1.003049373626709 current acc: 0.491\n",
      "iteration 57 current loss: 0.7431875467300415 current acc: 0.501\n",
      "iteration 58 current loss: 0.798864483833313 current acc: 0.5102\n",
      "iteration 59 current loss: 0.9904119968414307 current acc: 0.5182\n",
      "iteration 60 current loss: 0.8383513689041138 current acc: 0.5268\n",
      "iteration 61 current loss: 0.828951358795166 current acc: 0.535\n",
      "iteration 62 current loss: 1.1365619897842407 current acc: 0.5424\n",
      "iteration 63 current loss: 0.8613280653953552 current acc: 0.5514\n",
      "iteration 64 current loss: 0.8537920713424683 current acc: 0.5606\n",
      "iteration 65 current loss: 1.086930274963379 current acc: 0.569\n",
      "iteration 66 current loss: 0.9878754615783691 current acc: 0.5776\n",
      "iteration 67 current loss: 0.9731922149658203 current acc: 0.5858\n",
      "iteration 68 current loss: 0.911034107208252 current acc: 0.5946\n",
      "iteration 69 current loss: 0.9402123689651489 current acc: 0.6034\n",
      "iteration 70 current loss: 0.8763535618782043 current acc: 0.6122\n",
      "iteration 71 current loss: 1.0519224405288696 current acc: 0.6206\n",
      "iteration 72 current loss: 1.0029655694961548 current acc: 0.6284\n",
      "iteration 73 current loss: 1.1152559518814087 current acc: 0.6362\n",
      "iteration 74 current loss: 1.0343327522277832 current acc: 0.644\n",
      "iteration 75 current loss: 0.8973756432533264 current acc: 0.6524\n",
      "iteration 76 current loss: 1.047825574874878 current acc: 0.6602\n",
      "iteration 77 current loss: 0.8564633727073669 current acc: 0.669\n",
      "iteration 78 current loss: 1.2744940519332886 current acc: 0.6696\n",
      "\t\tTrain Epoch 68/100,Train Accuracy: 0.6696, Train Loss: 0.9357958752897721.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 68/100, Validation Accuracy: 0.579125, Validation Loss: 1.1684447417259216\n",
      "iteration 0 current loss: 0.82465660572052 current acc: 0.0104\n",
      "iteration 1 current loss: 0.8258943557739258 current acc: 0.02\n",
      "iteration 2 current loss: 0.8060683012008667 current acc: 0.0298\n",
      "iteration 3 current loss: 0.9160417318344116 current acc: 0.0386\n",
      "iteration 4 current loss: 1.1123584508895874 current acc: 0.0466\n",
      "iteration 5 current loss: 1.1450613737106323 current acc: 0.0542\n",
      "iteration 6 current loss: 0.76813805103302 current acc: 0.0628\n",
      "iteration 7 current loss: 1.1019833087921143 current acc: 0.07\n",
      "iteration 8 current loss: 0.90230393409729 current acc: 0.0788\n",
      "iteration 9 current loss: 1.0089343786239624 current acc: 0.0872\n",
      "iteration 10 current loss: 0.9070107936859131 current acc: 0.0958\n",
      "iteration 11 current loss: 0.8888813257217407 current acc: 0.1044\n",
      "iteration 12 current loss: 0.9655358195304871 current acc: 0.1132\n",
      "iteration 13 current loss: 0.8784612417221069 current acc: 0.1218\n",
      "iteration 14 current loss: 0.7473336458206177 current acc: 0.1316\n",
      "iteration 15 current loss: 1.1726770401000977 current acc: 0.1398\n",
      "iteration 16 current loss: 0.9690631628036499 current acc: 0.1478\n",
      "iteration 17 current loss: 0.9294400811195374 current acc: 0.156\n",
      "iteration 18 current loss: 1.014449954032898 current acc: 0.164\n",
      "iteration 19 current loss: 0.9288970232009888 current acc: 0.173\n",
      "iteration 20 current loss: 0.8263466954231262 current acc: 0.1832\n",
      "iteration 21 current loss: 0.7842238545417786 current acc: 0.192\n",
      "iteration 22 current loss: 0.9861251711845398 current acc: 0.2004\n",
      "iteration 23 current loss: 0.8342739343643188 current acc: 0.2102\n",
      "iteration 24 current loss: 1.1737738847732544 current acc: 0.217\n",
      "iteration 25 current loss: 0.9229821562767029 current acc: 0.225\n",
      "iteration 26 current loss: 1.0272239446640015 current acc: 0.2332\n",
      "iteration 27 current loss: 0.9852965474128723 current acc: 0.2426\n",
      "iteration 28 current loss: 1.0757920742034912 current acc: 0.2494\n",
      "iteration 29 current loss: 1.062620997428894 current acc: 0.257\n",
      "iteration 30 current loss: 0.8685340881347656 current acc: 0.2656\n",
      "iteration 31 current loss: 0.912527859210968 current acc: 0.2742\n",
      "iteration 32 current loss: 0.9184881448745728 current acc: 0.2828\n",
      "iteration 33 current loss: 0.9375740885734558 current acc: 0.2918\n",
      "iteration 34 current loss: 0.94300377368927 current acc: 0.3006\n",
      "iteration 35 current loss: 0.9403308629989624 current acc: 0.3088\n",
      "iteration 36 current loss: 0.8757930994033813 current acc: 0.3182\n",
      "iteration 37 current loss: 1.0181605815887451 current acc: 0.3252\n",
      "iteration 38 current loss: 0.9213314652442932 current acc: 0.3334\n",
      "iteration 39 current loss: 0.8745602965354919 current acc: 0.342\n",
      "iteration 40 current loss: 0.9161303639411926 current acc: 0.3508\n",
      "iteration 41 current loss: 0.98387610912323 current acc: 0.3596\n",
      "iteration 42 current loss: 1.103865146636963 current acc: 0.3672\n",
      "iteration 43 current loss: 1.0203758478164673 current acc: 0.3756\n",
      "iteration 44 current loss: 1.1181889772415161 current acc: 0.3836\n",
      "iteration 45 current loss: 0.8766012191772461 current acc: 0.3926\n",
      "iteration 46 current loss: 0.9205067157745361 current acc: 0.4016\n",
      "iteration 47 current loss: 0.8944637775421143 current acc: 0.4102\n",
      "iteration 48 current loss: 0.9976368546485901 current acc: 0.4184\n",
      "iteration 49 current loss: 0.9428717494010925 current acc: 0.4268\n",
      "iteration 50 current loss: 0.9278892874717712 current acc: 0.4356\n",
      "iteration 51 current loss: 1.0344160795211792 current acc: 0.443\n",
      "iteration 52 current loss: 1.060698390007019 current acc: 0.4514\n",
      "iteration 53 current loss: 0.876800537109375 current acc: 0.4608\n",
      "iteration 54 current loss: 0.9524068832397461 current acc: 0.4688\n",
      "iteration 55 current loss: 0.836664080619812 current acc: 0.4792\n",
      "iteration 56 current loss: 0.9016282558441162 current acc: 0.4886\n",
      "iteration 57 current loss: 0.9385558366775513 current acc: 0.4972\n",
      "iteration 58 current loss: 0.9931269288063049 current acc: 0.5058\n",
      "iteration 59 current loss: 0.8425435423851013 current acc: 0.5146\n",
      "iteration 60 current loss: 0.8925616145133972 current acc: 0.5228\n",
      "iteration 61 current loss: 0.967484176158905 current acc: 0.532\n",
      "iteration 62 current loss: 0.9359886050224304 current acc: 0.5392\n",
      "iteration 63 current loss: 0.8911101222038269 current acc: 0.5486\n",
      "iteration 64 current loss: 0.7101744413375854 current acc: 0.5586\n",
      "iteration 65 current loss: 1.1760286092758179 current acc: 0.5656\n",
      "iteration 66 current loss: 1.005043387413025 current acc: 0.5728\n",
      "iteration 67 current loss: 1.0664262771606445 current acc: 0.5802\n",
      "iteration 68 current loss: 0.9325821399688721 current acc: 0.5892\n",
      "iteration 69 current loss: 0.8872396945953369 current acc: 0.5978\n",
      "iteration 70 current loss: 0.8132244348526001 current acc: 0.6066\n",
      "iteration 71 current loss: 0.9397955536842346 current acc: 0.615\n",
      "iteration 72 current loss: 1.018924355506897 current acc: 0.6232\n",
      "iteration 73 current loss: 0.8666638731956482 current acc: 0.6316\n",
      "iteration 74 current loss: 1.1353600025177002 current acc: 0.6394\n",
      "iteration 75 current loss: 0.8940381407737732 current acc: 0.6484\n",
      "iteration 76 current loss: 0.9305686354637146 current acc: 0.6566\n",
      "iteration 77 current loss: 0.7180837392807007 current acc: 0.6654\n",
      "iteration 78 current loss: 1.0336247682571411 current acc: 0.6664\n",
      "\t\tTrain Epoch 69/100,Train Accuracy: 0.6664, Train Loss: 0.9450167500520055.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 69/100, Validation Accuracy: 0.56675, Validation Loss: 1.1862826476097106\n",
      "iteration 0 current loss: 0.8872853517532349 current acc: 0.0086\n",
      "iteration 1 current loss: 0.6943075656890869 current acc: 0.0184\n",
      "iteration 2 current loss: 0.7406785488128662 current acc: 0.0282\n",
      "iteration 3 current loss: 1.1014047861099243 current acc: 0.0358\n",
      "iteration 4 current loss: 0.8884413242340088 current acc: 0.0446\n",
      "iteration 5 current loss: 0.9832217693328857 current acc: 0.0536\n",
      "iteration 6 current loss: 0.8809176087379456 current acc: 0.062\n",
      "iteration 7 current loss: 1.144801378250122 current acc: 0.0694\n",
      "iteration 8 current loss: 0.8208337426185608 current acc: 0.0776\n",
      "iteration 9 current loss: 0.8452510237693787 current acc: 0.0864\n",
      "iteration 10 current loss: 0.8686907291412354 current acc: 0.095\n",
      "iteration 11 current loss: 0.9323541522026062 current acc: 0.1042\n",
      "iteration 12 current loss: 0.6831674575805664 current acc: 0.1152\n",
      "iteration 13 current loss: 0.8422682881355286 current acc: 0.1242\n",
      "iteration 14 current loss: 1.1090497970581055 current acc: 0.1322\n",
      "iteration 15 current loss: 0.9936633706092834 current acc: 0.1402\n",
      "iteration 16 current loss: 0.794551432132721 current acc: 0.1492\n",
      "iteration 17 current loss: 0.8610919117927551 current acc: 0.1586\n",
      "iteration 18 current loss: 1.1330522298812866 current acc: 0.166\n",
      "iteration 19 current loss: 1.1513880491256714 current acc: 0.1736\n",
      "iteration 20 current loss: 1.0050504207611084 current acc: 0.1822\n",
      "iteration 21 current loss: 0.9521493911743164 current acc: 0.1908\n",
      "iteration 22 current loss: 0.9449087381362915 current acc: 0.1986\n",
      "iteration 23 current loss: 0.8399632573127747 current acc: 0.2078\n",
      "iteration 24 current loss: 0.8667421936988831 current acc: 0.2168\n",
      "iteration 25 current loss: 0.9251930117607117 current acc: 0.2254\n",
      "iteration 26 current loss: 1.0349550247192383 current acc: 0.233\n",
      "iteration 27 current loss: 0.9858250021934509 current acc: 0.2414\n",
      "iteration 28 current loss: 1.0152873992919922 current acc: 0.2492\n",
      "iteration 29 current loss: 1.0718635320663452 current acc: 0.2568\n",
      "iteration 30 current loss: 0.9093707799911499 current acc: 0.265\n",
      "iteration 31 current loss: 0.9122015237808228 current acc: 0.2736\n",
      "iteration 32 current loss: 0.9532403349876404 current acc: 0.2816\n",
      "iteration 33 current loss: 0.8596785068511963 current acc: 0.2912\n",
      "iteration 34 current loss: 1.0711643695831299 current acc: 0.2988\n",
      "iteration 35 current loss: 1.1496455669403076 current acc: 0.3058\n",
      "iteration 36 current loss: 0.7848678827285767 current acc: 0.3154\n",
      "iteration 37 current loss: 0.9787477850914001 current acc: 0.3242\n",
      "iteration 38 current loss: 1.0361976623535156 current acc: 0.3326\n",
      "iteration 39 current loss: 1.0784777402877808 current acc: 0.3414\n",
      "iteration 40 current loss: 1.046400785446167 current acc: 0.3486\n",
      "iteration 41 current loss: 0.8104389905929565 current acc: 0.3582\n",
      "iteration 42 current loss: 1.085565447807312 current acc: 0.3656\n",
      "iteration 43 current loss: 0.942970871925354 current acc: 0.374\n",
      "iteration 44 current loss: 1.0912754535675049 current acc: 0.3814\n",
      "iteration 45 current loss: 0.9703658223152161 current acc: 0.3906\n",
      "iteration 46 current loss: 0.7794215679168701 current acc: 0.4006\n",
      "iteration 47 current loss: 0.8074216246604919 current acc: 0.4104\n",
      "iteration 48 current loss: 1.1099084615707397 current acc: 0.4184\n",
      "iteration 49 current loss: 0.9244529604911804 current acc: 0.4282\n",
      "iteration 50 current loss: 0.9346031546592712 current acc: 0.437\n",
      "iteration 51 current loss: 0.9093908667564392 current acc: 0.4454\n",
      "iteration 52 current loss: 0.9587165117263794 current acc: 0.4538\n",
      "iteration 53 current loss: 0.9362302422523499 current acc: 0.4624\n",
      "iteration 54 current loss: 0.8727336525917053 current acc: 0.4712\n",
      "iteration 55 current loss: 0.8164758086204529 current acc: 0.4806\n",
      "iteration 56 current loss: 0.9470211267471313 current acc: 0.4892\n",
      "iteration 57 current loss: 1.0468326807022095 current acc: 0.4978\n",
      "iteration 58 current loss: 0.9345942735671997 current acc: 0.5068\n",
      "iteration 59 current loss: 0.7792659997940063 current acc: 0.5166\n",
      "iteration 60 current loss: 1.1519267559051514 current acc: 0.524\n",
      "iteration 61 current loss: 0.7853794693946838 current acc: 0.5332\n",
      "iteration 62 current loss: 1.0449024438858032 current acc: 0.5412\n",
      "iteration 63 current loss: 0.9697726368904114 current acc: 0.5498\n",
      "iteration 64 current loss: 0.8834461569786072 current acc: 0.5592\n",
      "iteration 65 current loss: 0.9979771971702576 current acc: 0.5674\n",
      "iteration 66 current loss: 1.1415542364120483 current acc: 0.5742\n",
      "iteration 67 current loss: 1.0112618207931519 current acc: 0.5818\n",
      "iteration 68 current loss: 1.06757652759552 current acc: 0.5896\n",
      "iteration 69 current loss: 0.8614686131477356 current acc: 0.5982\n",
      "iteration 70 current loss: 0.9164270758628845 current acc: 0.6072\n",
      "iteration 71 current loss: 0.9152511358261108 current acc: 0.6154\n",
      "iteration 72 current loss: 1.1260517835617065 current acc: 0.6228\n",
      "iteration 73 current loss: 0.9261681437492371 current acc: 0.6312\n",
      "iteration 74 current loss: 0.9726682305335999 current acc: 0.639\n",
      "iteration 75 current loss: 1.2605427503585815 current acc: 0.6464\n",
      "iteration 76 current loss: 0.8984312415122986 current acc: 0.6548\n",
      "iteration 77 current loss: 1.0139296054840088 current acc: 0.662\n",
      "iteration 78 current loss: 1.183764100074768 current acc: 0.6628\n",
      "\t\tTrain Epoch 70/100,Train Accuracy: 0.6628, Train Loss: 0.9565131249307077.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 70/100, Validation Accuracy: 0.561875, Validation Loss: 1.214177299976349\n",
      "iteration 0 current loss: 0.8710611462593079 current acc: 0.0088\n",
      "iteration 1 current loss: 0.8130480051040649 current acc: 0.0182\n",
      "iteration 2 current loss: 0.9540371298789978 current acc: 0.0276\n",
      "iteration 3 current loss: 1.039300799369812 current acc: 0.0358\n",
      "iteration 4 current loss: 1.0185871124267578 current acc: 0.0444\n",
      "iteration 5 current loss: 0.939732551574707 current acc: 0.053\n",
      "iteration 6 current loss: 1.1387118101119995 current acc: 0.0598\n",
      "iteration 7 current loss: 0.8543274402618408 current acc: 0.0686\n",
      "iteration 8 current loss: 0.9392388463020325 current acc: 0.0764\n",
      "iteration 9 current loss: 0.8752973675727844 current acc: 0.0848\n",
      "iteration 10 current loss: 0.9834001660346985 current acc: 0.0934\n",
      "iteration 11 current loss: 0.8981885313987732 current acc: 0.102\n",
      "iteration 12 current loss: 0.9011867046356201 current acc: 0.1116\n",
      "iteration 13 current loss: 0.8796188235282898 current acc: 0.1198\n",
      "iteration 14 current loss: 0.9237866401672363 current acc: 0.1276\n",
      "iteration 15 current loss: 0.9950453639030457 current acc: 0.1358\n",
      "iteration 16 current loss: 0.8390677571296692 current acc: 0.1448\n",
      "iteration 17 current loss: 1.0427136421203613 current acc: 0.1534\n",
      "iteration 18 current loss: 0.9964244961738586 current acc: 0.161\n",
      "iteration 19 current loss: 1.0028793811798096 current acc: 0.1692\n",
      "iteration 20 current loss: 0.9712903499603271 current acc: 0.1772\n",
      "iteration 21 current loss: 0.9651209115982056 current acc: 0.1856\n",
      "iteration 22 current loss: 1.2846307754516602 current acc: 0.192\n",
      "iteration 23 current loss: 0.971574068069458 current acc: 0.1996\n",
      "iteration 24 current loss: 0.8910816311836243 current acc: 0.2084\n",
      "iteration 25 current loss: 1.013278841972351 current acc: 0.2162\n",
      "iteration 26 current loss: 1.1051945686340332 current acc: 0.2242\n",
      "iteration 27 current loss: 1.2025290727615356 current acc: 0.2314\n",
      "iteration 28 current loss: 0.883652925491333 current acc: 0.2402\n",
      "iteration 29 current loss: 0.8867338299751282 current acc: 0.2488\n",
      "iteration 30 current loss: 0.8868640661239624 current acc: 0.2578\n",
      "iteration 31 current loss: 1.0619640350341797 current acc: 0.2654\n",
      "iteration 32 current loss: 0.9632335305213928 current acc: 0.2734\n",
      "iteration 33 current loss: 0.9920799732208252 current acc: 0.2812\n",
      "iteration 34 current loss: 1.0017645359039307 current acc: 0.2898\n",
      "iteration 35 current loss: 0.8937746286392212 current acc: 0.2984\n",
      "iteration 36 current loss: 0.9360016584396362 current acc: 0.3076\n",
      "iteration 37 current loss: 1.0113509893417358 current acc: 0.316\n",
      "iteration 38 current loss: 0.741058349609375 current acc: 0.3256\n",
      "iteration 39 current loss: 1.0202040672302246 current acc: 0.3338\n",
      "iteration 40 current loss: 1.0731725692749023 current acc: 0.341\n",
      "iteration 41 current loss: 1.0068819522857666 current acc: 0.3496\n",
      "iteration 42 current loss: 0.6967321634292603 current acc: 0.3596\n",
      "iteration 43 current loss: 0.8717086315155029 current acc: 0.3692\n",
      "iteration 44 current loss: 1.0812065601348877 current acc: 0.3766\n",
      "iteration 45 current loss: 1.1972122192382812 current acc: 0.3838\n",
      "iteration 46 current loss: 1.088989019393921 current acc: 0.3918\n",
      "iteration 47 current loss: 0.921760082244873 current acc: 0.4002\n",
      "iteration 48 current loss: 1.0284150838851929 current acc: 0.4074\n",
      "iteration 49 current loss: 0.8156107068061829 current acc: 0.4154\n",
      "iteration 50 current loss: 1.0053526163101196 current acc: 0.424\n",
      "iteration 51 current loss: 1.037389874458313 current acc: 0.4324\n",
      "iteration 52 current loss: 1.1172473430633545 current acc: 0.44\n",
      "iteration 53 current loss: 0.8862757086753845 current acc: 0.449\n",
      "iteration 54 current loss: 1.090476393699646 current acc: 0.4572\n",
      "iteration 55 current loss: 0.9886058568954468 current acc: 0.4652\n",
      "iteration 56 current loss: 1.0016251802444458 current acc: 0.473\n",
      "iteration 57 current loss: 1.019004225730896 current acc: 0.4804\n",
      "iteration 58 current loss: 1.0217968225479126 current acc: 0.489\n",
      "iteration 59 current loss: 0.971741795539856 current acc: 0.4976\n",
      "iteration 60 current loss: 0.9029343724250793 current acc: 0.5064\n",
      "iteration 61 current loss: 0.956771731376648 current acc: 0.514\n",
      "iteration 62 current loss: 1.126224160194397 current acc: 0.5218\n",
      "iteration 63 current loss: 0.9605292677879333 current acc: 0.5296\n",
      "iteration 64 current loss: 0.8018829822540283 current acc: 0.5384\n",
      "iteration 65 current loss: 1.0137847661972046 current acc: 0.5468\n",
      "iteration 66 current loss: 0.9564998149871826 current acc: 0.5548\n",
      "iteration 67 current loss: 0.7727952599525452 current acc: 0.5644\n",
      "iteration 68 current loss: 0.9556495547294617 current acc: 0.5736\n",
      "iteration 69 current loss: 0.9864292144775391 current acc: 0.582\n",
      "iteration 70 current loss: 0.9674592614173889 current acc: 0.5902\n",
      "iteration 71 current loss: 0.799218475818634 current acc: 0.5992\n",
      "iteration 72 current loss: 0.9039883613586426 current acc: 0.6082\n",
      "iteration 73 current loss: 0.9640352725982666 current acc: 0.6162\n",
      "iteration 74 current loss: 0.9438077807426453 current acc: 0.6252\n",
      "iteration 75 current loss: 1.0144858360290527 current acc: 0.633\n",
      "iteration 76 current loss: 0.8928931355476379 current acc: 0.6412\n",
      "iteration 77 current loss: 0.9193958044052124 current acc: 0.6502\n",
      "iteration 78 current loss: 0.662619948387146 current acc: 0.6514\n",
      "\t\tTrain Epoch 71/100,Train Accuracy: 0.6514, Train Loss: 0.9621727383589442.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 71/100, Validation Accuracy: 0.53225, Validation Loss: 1.3443242197036742\n",
      "iteration 0 current loss: 0.9905356764793396 current acc: 0.008\n",
      "iteration 1 current loss: 1.145761251449585 current acc: 0.0148\n",
      "iteration 2 current loss: 0.7254586815834045 current acc: 0.0252\n",
      "iteration 3 current loss: 1.0599554777145386 current acc: 0.0326\n",
      "iteration 4 current loss: 0.805578887462616 current acc: 0.0412\n",
      "iteration 5 current loss: 0.7664438486099243 current acc: 0.0506\n",
      "iteration 6 current loss: 0.8935083150863647 current acc: 0.0594\n",
      "iteration 7 current loss: 0.8495627045631409 current acc: 0.0692\n",
      "iteration 8 current loss: 0.8632056713104248 current acc: 0.0774\n",
      "iteration 9 current loss: 1.0646404027938843 current acc: 0.0856\n",
      "iteration 10 current loss: 0.8167181611061096 current acc: 0.0952\n",
      "iteration 11 current loss: 0.913212776184082 current acc: 0.1038\n",
      "iteration 12 current loss: 0.9864341616630554 current acc: 0.1122\n",
      "iteration 13 current loss: 1.0178765058517456 current acc: 0.1208\n",
      "iteration 14 current loss: 1.0149914026260376 current acc: 0.1288\n",
      "iteration 15 current loss: 0.8904600143432617 current acc: 0.1374\n",
      "iteration 16 current loss: 0.9518677592277527 current acc: 0.1456\n",
      "iteration 17 current loss: 1.1168097257614136 current acc: 0.1538\n",
      "iteration 18 current loss: 0.9438585042953491 current acc: 0.162\n",
      "iteration 19 current loss: 0.7919570803642273 current acc: 0.1716\n",
      "iteration 20 current loss: 1.0666512250900269 current acc: 0.1802\n",
      "iteration 21 current loss: 1.240931749343872 current acc: 0.1866\n",
      "iteration 22 current loss: 1.0027925968170166 current acc: 0.1948\n",
      "iteration 23 current loss: 0.843628466129303 current acc: 0.2042\n",
      "iteration 24 current loss: 0.8463798761367798 current acc: 0.2134\n",
      "iteration 25 current loss: 0.92106032371521 current acc: 0.2224\n",
      "iteration 26 current loss: 0.9821481108665466 current acc: 0.2306\n",
      "iteration 27 current loss: 0.8825893402099609 current acc: 0.2396\n",
      "iteration 28 current loss: 0.8879096508026123 current acc: 0.249\n",
      "iteration 29 current loss: 0.9700313806533813 current acc: 0.2576\n",
      "iteration 30 current loss: 0.7842187881469727 current acc: 0.2666\n",
      "iteration 31 current loss: 0.8963532447814941 current acc: 0.2752\n",
      "iteration 32 current loss: 0.9105242490768433 current acc: 0.2836\n",
      "iteration 33 current loss: 0.9688019156455994 current acc: 0.2928\n",
      "iteration 34 current loss: 0.862672746181488 current acc: 0.3016\n",
      "iteration 35 current loss: 0.7801558971405029 current acc: 0.311\n",
      "iteration 36 current loss: 0.7093090415000916 current acc: 0.321\n",
      "iteration 37 current loss: 0.9963561296463013 current acc: 0.3294\n",
      "iteration 38 current loss: 0.9055956602096558 current acc: 0.338\n",
      "iteration 39 current loss: 1.0071816444396973 current acc: 0.3456\n",
      "iteration 40 current loss: 1.1366519927978516 current acc: 0.353\n",
      "iteration 41 current loss: 0.9162920713424683 current acc: 0.3616\n",
      "iteration 42 current loss: 0.9444088339805603 current acc: 0.37\n",
      "iteration 43 current loss: 0.8632696270942688 current acc: 0.378\n",
      "iteration 44 current loss: 0.9400875568389893 current acc: 0.3862\n",
      "iteration 45 current loss: 0.9907647967338562 current acc: 0.394\n",
      "iteration 46 current loss: 1.0220128297805786 current acc: 0.4022\n",
      "iteration 47 current loss: 0.9247299432754517 current acc: 0.4106\n",
      "iteration 48 current loss: 0.9341551661491394 current acc: 0.419\n",
      "iteration 49 current loss: 0.9767972230911255 current acc: 0.4264\n",
      "iteration 50 current loss: 0.9191014170646667 current acc: 0.4358\n",
      "iteration 51 current loss: 0.8961026668548584 current acc: 0.4448\n",
      "iteration 52 current loss: 0.9735578298568726 current acc: 0.4528\n",
      "iteration 53 current loss: 0.9879947304725647 current acc: 0.4608\n",
      "iteration 54 current loss: 1.0212066173553467 current acc: 0.4688\n",
      "iteration 55 current loss: 1.021604299545288 current acc: 0.4774\n",
      "iteration 56 current loss: 0.8916813135147095 current acc: 0.4866\n",
      "iteration 57 current loss: 0.9689270853996277 current acc: 0.4958\n",
      "iteration 58 current loss: 1.0765893459320068 current acc: 0.504\n",
      "iteration 59 current loss: 0.794039249420166 current acc: 0.5134\n",
      "iteration 60 current loss: 0.6790404319763184 current acc: 0.523\n",
      "iteration 61 current loss: 0.9879921674728394 current acc: 0.532\n",
      "iteration 62 current loss: 0.9303130507469177 current acc: 0.5394\n",
      "iteration 63 current loss: 0.768457293510437 current acc: 0.5492\n",
      "iteration 64 current loss: 0.73755943775177 current acc: 0.5588\n",
      "iteration 65 current loss: 0.9797587394714355 current acc: 0.5676\n",
      "iteration 66 current loss: 0.7628113627433777 current acc: 0.5768\n",
      "iteration 67 current loss: 0.9118524789810181 current acc: 0.5858\n",
      "iteration 68 current loss: 0.7768723368644714 current acc: 0.5954\n",
      "iteration 69 current loss: 1.0282278060913086 current acc: 0.6038\n",
      "iteration 70 current loss: 1.06120765209198 current acc: 0.6112\n",
      "iteration 71 current loss: 0.9360261559486389 current acc: 0.6192\n",
      "iteration 72 current loss: 1.0205553770065308 current acc: 0.626\n",
      "iteration 73 current loss: 0.9641947746276855 current acc: 0.6342\n",
      "iteration 74 current loss: 0.839705228805542 current acc: 0.644\n",
      "iteration 75 current loss: 1.1168601512908936 current acc: 0.651\n",
      "iteration 76 current loss: 0.8430824875831604 current acc: 0.6602\n",
      "iteration 77 current loss: 0.9041978716850281 current acc: 0.669\n",
      "iteration 78 current loss: 0.7034184336662292 current acc: 0.6704\n",
      "\t\tTrain Epoch 72/100,Train Accuracy: 0.6704, Train Loss: 0.9269143908838683.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 72/100, Validation Accuracy: 0.590125, Validation Loss: 1.1320347657203675\n",
      "iteration 0 current loss: 0.9542109370231628 current acc: 0.0088\n",
      "iteration 1 current loss: 0.9495324492454529 current acc: 0.0176\n",
      "iteration 2 current loss: 0.6630155444145203 current acc: 0.0278\n",
      "iteration 3 current loss: 0.9352471232414246 current acc: 0.0364\n",
      "iteration 4 current loss: 0.8771371245384216 current acc: 0.046\n",
      "iteration 5 current loss: 0.753568708896637 current acc: 0.0548\n",
      "iteration 6 current loss: 0.9568554162979126 current acc: 0.0626\n",
      "iteration 7 current loss: 0.9321672916412354 current acc: 0.0714\n",
      "iteration 8 current loss: 0.7501842975616455 current acc: 0.0816\n",
      "iteration 9 current loss: 0.8749754428863525 current acc: 0.0904\n",
      "iteration 10 current loss: 0.7672420740127563 current acc: 0.1004\n",
      "iteration 11 current loss: 0.853371798992157 current acc: 0.1092\n",
      "iteration 12 current loss: 0.7891286015510559 current acc: 0.119\n",
      "iteration 13 current loss: 0.8902844190597534 current acc: 0.127\n",
      "iteration 14 current loss: 0.6960299015045166 current acc: 0.1368\n",
      "iteration 15 current loss: 0.9683494567871094 current acc: 0.1448\n",
      "iteration 16 current loss: 0.7992708086967468 current acc: 0.154\n",
      "iteration 17 current loss: 0.7686372399330139 current acc: 0.1636\n",
      "iteration 18 current loss: 0.8884623050689697 current acc: 0.1734\n",
      "iteration 19 current loss: 0.9119871854782104 current acc: 0.1822\n",
      "iteration 20 current loss: 1.0534733533859253 current acc: 0.1896\n",
      "iteration 21 current loss: 0.8143310546875 current acc: 0.1992\n",
      "iteration 22 current loss: 0.8852320313453674 current acc: 0.2084\n",
      "iteration 23 current loss: 0.9298795461654663 current acc: 0.2178\n",
      "iteration 24 current loss: 0.7394282221794128 current acc: 0.2276\n",
      "iteration 25 current loss: 0.9389328360557556 current acc: 0.2356\n",
      "iteration 26 current loss: 0.9170082211494446 current acc: 0.2444\n",
      "iteration 27 current loss: 1.181226372718811 current acc: 0.252\n",
      "iteration 28 current loss: 0.7902188301086426 current acc: 0.2622\n",
      "iteration 29 current loss: 0.7968176603317261 current acc: 0.2712\n",
      "iteration 30 current loss: 0.6890209317207336 current acc: 0.2812\n",
      "iteration 31 current loss: 0.8821920156478882 current acc: 0.2898\n",
      "iteration 32 current loss: 0.8347651958465576 current acc: 0.2992\n",
      "iteration 33 current loss: 0.8756658434867859 current acc: 0.308\n",
      "iteration 34 current loss: 1.0651766061782837 current acc: 0.316\n",
      "iteration 35 current loss: 0.8919494152069092 current acc: 0.3248\n",
      "iteration 36 current loss: 0.8511194586753845 current acc: 0.3344\n",
      "iteration 37 current loss: 0.8545029759407043 current acc: 0.3424\n",
      "iteration 38 current loss: 0.9980843663215637 current acc: 0.3522\n",
      "iteration 39 current loss: 0.898829460144043 current acc: 0.3612\n",
      "iteration 40 current loss: 1.083916425704956 current acc: 0.3688\n",
      "iteration 41 current loss: 0.9380583763122559 current acc: 0.3768\n",
      "iteration 42 current loss: 1.0237751007080078 current acc: 0.385\n",
      "iteration 43 current loss: 0.8379661440849304 current acc: 0.394\n",
      "iteration 44 current loss: 1.013053297996521 current acc: 0.402\n",
      "iteration 45 current loss: 0.9636312127113342 current acc: 0.4108\n",
      "iteration 46 current loss: 0.8240123987197876 current acc: 0.42\n",
      "iteration 47 current loss: 0.8446709513664246 current acc: 0.428\n",
      "iteration 48 current loss: 1.0208543539047241 current acc: 0.4354\n",
      "iteration 49 current loss: 0.83177649974823 current acc: 0.4442\n",
      "iteration 50 current loss: 0.8567584753036499 current acc: 0.4526\n",
      "iteration 51 current loss: 1.1344434022903442 current acc: 0.4598\n",
      "iteration 52 current loss: 0.9270079135894775 current acc: 0.4684\n",
      "iteration 53 current loss: 0.946108877658844 current acc: 0.4768\n",
      "iteration 54 current loss: 1.299479365348816 current acc: 0.4832\n",
      "iteration 55 current loss: 0.9697451591491699 current acc: 0.4912\n",
      "iteration 56 current loss: 1.1627341508865356 current acc: 0.499\n",
      "iteration 57 current loss: 0.9197306632995605 current acc: 0.5078\n",
      "iteration 58 current loss: 0.9245687127113342 current acc: 0.5162\n",
      "iteration 59 current loss: 0.8310182094573975 current acc: 0.5252\n",
      "iteration 60 current loss: 1.0022504329681396 current acc: 0.5334\n",
      "iteration 61 current loss: 0.9976124167442322 current acc: 0.5412\n",
      "iteration 62 current loss: 0.884600818157196 current acc: 0.5504\n",
      "iteration 63 current loss: 1.1358940601348877 current acc: 0.5588\n",
      "iteration 64 current loss: 0.9777222871780396 current acc: 0.5674\n",
      "iteration 65 current loss: 1.1716288328170776 current acc: 0.575\n",
      "iteration 66 current loss: 0.9499974250793457 current acc: 0.5836\n",
      "iteration 67 current loss: 0.8328439593315125 current acc: 0.5928\n",
      "iteration 68 current loss: 0.9086701273918152 current acc: 0.6012\n",
      "iteration 69 current loss: 1.1593255996704102 current acc: 0.6096\n",
      "iteration 70 current loss: 0.8909366726875305 current acc: 0.6192\n",
      "iteration 71 current loss: 0.9393042325973511 current acc: 0.628\n",
      "iteration 72 current loss: 1.0824004411697388 current acc: 0.6348\n",
      "iteration 73 current loss: 1.0780566930770874 current acc: 0.6422\n",
      "iteration 74 current loss: 1.0806176662445068 current acc: 0.6496\n",
      "iteration 75 current loss: 0.9406487345695496 current acc: 0.6586\n",
      "iteration 76 current loss: 0.8267644643783569 current acc: 0.6678\n",
      "iteration 77 current loss: 1.0253899097442627 current acc: 0.6766\n",
      "iteration 78 current loss: 0.7571542263031006 current acc: 0.678\n",
      "\t\tTrain Epoch 73/100,Train Accuracy: 0.678, Train Loss: 0.9223119141180304.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 73/100, Validation Accuracy: 0.56425, Validation Loss: 1.2153761339187623\n",
      "iteration 0 current loss: 0.9548137187957764 current acc: 0.0084\n",
      "iteration 1 current loss: 0.8865929245948792 current acc: 0.0176\n",
      "iteration 2 current loss: 0.9610416293144226 current acc: 0.0262\n",
      "iteration 3 current loss: 1.002763032913208 current acc: 0.0344\n",
      "iteration 4 current loss: 1.0942548513412476 current acc: 0.0426\n",
      "iteration 5 current loss: 0.9569981694221497 current acc: 0.0504\n",
      "iteration 6 current loss: 0.868779718875885 current acc: 0.0594\n",
      "iteration 7 current loss: 0.6545006036758423 current acc: 0.0696\n",
      "iteration 8 current loss: 1.087566614151001 current acc: 0.0772\n",
      "iteration 9 current loss: 0.8200790882110596 current acc: 0.0866\n",
      "iteration 10 current loss: 0.9882892966270447 current acc: 0.0944\n",
      "iteration 11 current loss: 1.0107864141464233 current acc: 0.1026\n",
      "iteration 12 current loss: 1.1667503118515015 current acc: 0.1102\n",
      "iteration 13 current loss: 1.132094383239746 current acc: 0.1178\n",
      "iteration 14 current loss: 0.9157573580741882 current acc: 0.126\n",
      "iteration 15 current loss: 0.8599692583084106 current acc: 0.135\n",
      "iteration 16 current loss: 0.9094364047050476 current acc: 0.1428\n",
      "iteration 17 current loss: 0.915030837059021 current acc: 0.152\n",
      "iteration 18 current loss: 1.005769968032837 current acc: 0.16\n",
      "iteration 19 current loss: 0.8881622552871704 current acc: 0.1696\n",
      "iteration 20 current loss: 0.8210875988006592 current acc: 0.1792\n",
      "iteration 21 current loss: 0.7952577471733093 current acc: 0.1884\n",
      "iteration 22 current loss: 0.9091551303863525 current acc: 0.1972\n",
      "iteration 23 current loss: 1.1025558710098267 current acc: 0.205\n",
      "iteration 24 current loss: 1.0358078479766846 current acc: 0.2122\n",
      "iteration 25 current loss: 0.939710259437561 current acc: 0.221\n",
      "iteration 26 current loss: 1.083682656288147 current acc: 0.2282\n",
      "iteration 27 current loss: 0.9480466246604919 current acc: 0.237\n",
      "iteration 28 current loss: 0.7631914615631104 current acc: 0.247\n",
      "iteration 29 current loss: 0.8423500061035156 current acc: 0.255\n",
      "iteration 30 current loss: 0.824029266834259 current acc: 0.2646\n",
      "iteration 31 current loss: 0.8558109998703003 current acc: 0.2734\n",
      "iteration 32 current loss: 0.9548685550689697 current acc: 0.2824\n",
      "iteration 33 current loss: 0.8841208815574646 current acc: 0.2912\n",
      "iteration 34 current loss: 1.0011652708053589 current acc: 0.3002\n",
      "iteration 35 current loss: 0.993844211101532 current acc: 0.3084\n",
      "iteration 36 current loss: 0.9424944519996643 current acc: 0.3172\n",
      "iteration 37 current loss: 0.8814820647239685 current acc: 0.326\n",
      "iteration 38 current loss: 0.890228807926178 current acc: 0.3342\n",
      "iteration 39 current loss: 1.0152812004089355 current acc: 0.3416\n",
      "iteration 40 current loss: 0.7506428360939026 current acc: 0.3516\n",
      "iteration 41 current loss: 0.8312745094299316 current acc: 0.3606\n",
      "iteration 42 current loss: 1.0539968013763428 current acc: 0.3686\n",
      "iteration 43 current loss: 1.0629693269729614 current acc: 0.3762\n",
      "iteration 44 current loss: 0.9344865083694458 current acc: 0.3848\n",
      "iteration 45 current loss: 0.8448207974433899 current acc: 0.3944\n",
      "iteration 46 current loss: 0.9489302039146423 current acc: 0.4022\n",
      "iteration 47 current loss: 0.9305025935173035 current acc: 0.4102\n",
      "iteration 48 current loss: 1.0096635818481445 current acc: 0.4186\n",
      "iteration 49 current loss: 0.8507826328277588 current acc: 0.4282\n",
      "iteration 50 current loss: 0.9607276916503906 current acc: 0.4368\n",
      "iteration 51 current loss: 0.9814321398735046 current acc: 0.445\n",
      "iteration 52 current loss: 1.0040225982666016 current acc: 0.453\n",
      "iteration 53 current loss: 0.9751396179199219 current acc: 0.4612\n",
      "iteration 54 current loss: 0.9633955955505371 current acc: 0.4704\n",
      "iteration 55 current loss: 1.0475335121154785 current acc: 0.4788\n",
      "iteration 56 current loss: 0.8344963192939758 current acc: 0.4878\n",
      "iteration 57 current loss: 0.9021226167678833 current acc: 0.4964\n",
      "iteration 58 current loss: 0.7727285623550415 current acc: 0.506\n",
      "iteration 59 current loss: 1.0161278247833252 current acc: 0.5144\n",
      "iteration 60 current loss: 0.8785439133644104 current acc: 0.5236\n",
      "iteration 61 current loss: 0.9838585257530212 current acc: 0.5322\n",
      "iteration 62 current loss: 0.9995471835136414 current acc: 0.5404\n",
      "iteration 63 current loss: 1.05355703830719 current acc: 0.5484\n",
      "iteration 64 current loss: 0.7535382509231567 current acc: 0.558\n",
      "iteration 65 current loss: 0.9802733659744263 current acc: 0.5668\n",
      "iteration 66 current loss: 0.8540166616439819 current acc: 0.5752\n",
      "iteration 67 current loss: 0.9400526285171509 current acc: 0.5834\n",
      "iteration 68 current loss: 0.9393640160560608 current acc: 0.5914\n",
      "iteration 69 current loss: 0.8223538994789124 current acc: 0.6004\n",
      "iteration 70 current loss: 0.9801256060600281 current acc: 0.6094\n",
      "iteration 71 current loss: 0.8523200154304504 current acc: 0.6178\n",
      "iteration 72 current loss: 0.9240008592605591 current acc: 0.6268\n",
      "iteration 73 current loss: 1.1207927465438843 current acc: 0.6348\n",
      "iteration 74 current loss: 1.0085341930389404 current acc: 0.6426\n",
      "iteration 75 current loss: 0.795444130897522 current acc: 0.652\n",
      "iteration 76 current loss: 0.9383904933929443 current acc: 0.66\n",
      "iteration 77 current loss: 1.2065659761428833 current acc: 0.667\n",
      "iteration 78 current loss: 1.1652055978775024 current acc: 0.6678\n",
      "\t\tTrain Epoch 74/100,Train Accuracy: 0.6678, Train Loss: 0.9422264446186114.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 74/100, Validation Accuracy: 0.555375, Validation Loss: 1.2180911021232605\n",
      "iteration 0 current loss: 0.9702840447425842 current acc: 0.0084\n",
      "iteration 1 current loss: 0.7886617183685303 current acc: 0.0176\n",
      "iteration 2 current loss: 0.9779238700866699 current acc: 0.0258\n",
      "iteration 3 current loss: 1.158834457397461 current acc: 0.0334\n",
      "iteration 4 current loss: 0.9958117008209229 current acc: 0.041\n",
      "iteration 5 current loss: 1.0833683013916016 current acc: 0.0486\n",
      "iteration 6 current loss: 1.0014644861221313 current acc: 0.0566\n",
      "iteration 7 current loss: 1.0555585622787476 current acc: 0.064\n",
      "iteration 8 current loss: 0.8704976439476013 current acc: 0.0728\n",
      "iteration 9 current loss: 0.8293485641479492 current acc: 0.082\n",
      "iteration 10 current loss: 0.7639878988265991 current acc: 0.0916\n",
      "iteration 11 current loss: 0.9215637445449829 current acc: 0.1004\n",
      "iteration 12 current loss: 0.9825736880302429 current acc: 0.109\n",
      "iteration 13 current loss: 0.9652588367462158 current acc: 0.1174\n",
      "iteration 14 current loss: 1.0777685642242432 current acc: 0.1254\n",
      "iteration 15 current loss: 0.8850724697113037 current acc: 0.134\n",
      "iteration 16 current loss: 1.0869629383087158 current acc: 0.1416\n",
      "iteration 17 current loss: 0.9051927924156189 current acc: 0.1504\n",
      "iteration 18 current loss: 1.0041582584381104 current acc: 0.1578\n",
      "iteration 19 current loss: 1.0763944387435913 current acc: 0.1652\n",
      "iteration 20 current loss: 0.9473070502281189 current acc: 0.1736\n",
      "iteration 21 current loss: 1.059266209602356 current acc: 0.1818\n",
      "iteration 22 current loss: 0.8327072262763977 current acc: 0.1912\n",
      "iteration 23 current loss: 0.8763856887817383 current acc: 0.2\n",
      "iteration 24 current loss: 0.9284392595291138 current acc: 0.2082\n",
      "iteration 25 current loss: 1.059064269065857 current acc: 0.216\n",
      "iteration 26 current loss: 0.9965612888336182 current acc: 0.2244\n",
      "iteration 27 current loss: 1.1888020038604736 current acc: 0.2324\n",
      "iteration 28 current loss: 1.1407362222671509 current acc: 0.2406\n",
      "iteration 29 current loss: 0.9814642667770386 current acc: 0.2488\n",
      "iteration 30 current loss: 0.9052526950836182 current acc: 0.2572\n",
      "iteration 31 current loss: 0.8668942451477051 current acc: 0.2666\n",
      "iteration 32 current loss: 0.8127250075340271 current acc: 0.2762\n",
      "iteration 33 current loss: 0.8796566128730774 current acc: 0.2848\n",
      "iteration 34 current loss: 1.1312824487686157 current acc: 0.2924\n",
      "iteration 35 current loss: 1.0078766345977783 current acc: 0.3006\n",
      "iteration 36 current loss: 1.057296633720398 current acc: 0.3086\n",
      "iteration 37 current loss: 1.076472282409668 current acc: 0.3168\n",
      "iteration 38 current loss: 0.8653364181518555 current acc: 0.3266\n",
      "iteration 39 current loss: 0.8598983883857727 current acc: 0.3348\n",
      "iteration 40 current loss: 1.2698636054992676 current acc: 0.3422\n",
      "iteration 41 current loss: 1.0268805027008057 current acc: 0.3502\n",
      "iteration 42 current loss: 0.9235691428184509 current acc: 0.3586\n",
      "iteration 43 current loss: 1.0454442501068115 current acc: 0.367\n",
      "iteration 44 current loss: 0.9600167274475098 current acc: 0.3756\n",
      "iteration 45 current loss: 0.9401987791061401 current acc: 0.3844\n",
      "iteration 46 current loss: 0.9738574028015137 current acc: 0.3924\n",
      "iteration 47 current loss: 1.0251752138137817 current acc: 0.4\n",
      "iteration 48 current loss: 0.9246679544448853 current acc: 0.4092\n",
      "iteration 49 current loss: 0.9281454086303711 current acc: 0.4178\n",
      "iteration 50 current loss: 0.792339026927948 current acc: 0.4276\n",
      "iteration 51 current loss: 0.9898784756660461 current acc: 0.436\n",
      "iteration 52 current loss: 0.9433594942092896 current acc: 0.4448\n",
      "iteration 53 current loss: 0.9421066641807556 current acc: 0.4534\n",
      "iteration 54 current loss: 0.913872241973877 current acc: 0.4622\n",
      "iteration 55 current loss: 1.1669890880584717 current acc: 0.4698\n",
      "iteration 56 current loss: 0.9773237705230713 current acc: 0.4776\n",
      "iteration 57 current loss: 1.0207453966140747 current acc: 0.485\n",
      "iteration 58 current loss: 0.93930983543396 current acc: 0.494\n",
      "iteration 59 current loss: 0.8758834600448608 current acc: 0.5018\n",
      "iteration 60 current loss: 1.001037359237671 current acc: 0.5098\n",
      "iteration 61 current loss: 1.1549735069274902 current acc: 0.5168\n",
      "iteration 62 current loss: 1.005828619003296 current acc: 0.526\n",
      "iteration 63 current loss: 1.0693824291229248 current acc: 0.5342\n",
      "iteration 64 current loss: 0.7576525807380676 current acc: 0.5442\n",
      "iteration 65 current loss: 0.9182285070419312 current acc: 0.553\n",
      "iteration 66 current loss: 1.1321322917938232 current acc: 0.5608\n",
      "iteration 67 current loss: 0.9138919711112976 current acc: 0.5698\n",
      "iteration 68 current loss: 0.7492296695709229 current acc: 0.578\n",
      "iteration 69 current loss: 1.098042607307434 current acc: 0.586\n",
      "iteration 70 current loss: 0.6899899244308472 current acc: 0.5964\n",
      "iteration 71 current loss: 0.8239259123802185 current acc: 0.6058\n",
      "iteration 72 current loss: 1.2455443143844604 current acc: 0.613\n",
      "iteration 73 current loss: 1.0016744136810303 current acc: 0.6216\n",
      "iteration 74 current loss: 0.9418763518333435 current acc: 0.6304\n",
      "iteration 75 current loss: 0.94266277551651 current acc: 0.6388\n",
      "iteration 76 current loss: 1.1336262226104736 current acc: 0.6462\n",
      "iteration 77 current loss: 0.9463818073272705 current acc: 0.6546\n",
      "iteration 78 current loss: 1.013574481010437 current acc: 0.6554\n",
      "\t\tTrain Epoch 75/100,Train Accuracy: 0.6554, Train Loss: 0.9745999242686019.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 75/100, Validation Accuracy: 0.58625, Validation Loss: 1.153307334423065\n",
      "iteration 0 current loss: 0.8676854968070984 current acc: 0.009\n",
      "iteration 1 current loss: 0.7700924277305603 current acc: 0.019\n",
      "iteration 2 current loss: 0.908556342124939 current acc: 0.0278\n",
      "iteration 3 current loss: 0.8442481756210327 current acc: 0.0368\n",
      "iteration 4 current loss: 0.846818745136261 current acc: 0.0462\n",
      "iteration 5 current loss: 1.0518265962600708 current acc: 0.0546\n",
      "iteration 6 current loss: 0.8335983157157898 current acc: 0.0636\n",
      "iteration 7 current loss: 0.9378567337989807 current acc: 0.0706\n",
      "iteration 8 current loss: 1.0043537616729736 current acc: 0.0786\n",
      "iteration 9 current loss: 0.8303806781768799 current acc: 0.0884\n",
      "iteration 10 current loss: 0.9476840496063232 current acc: 0.0966\n",
      "iteration 11 current loss: 0.927893340587616 current acc: 0.1042\n",
      "iteration 12 current loss: 0.7834123373031616 current acc: 0.1132\n",
      "iteration 13 current loss: 0.7872263789176941 current acc: 0.123\n",
      "iteration 14 current loss: 1.080032467842102 current acc: 0.1312\n",
      "iteration 15 current loss: 0.899038553237915 current acc: 0.1406\n",
      "iteration 16 current loss: 0.7720687389373779 current acc: 0.1504\n",
      "iteration 17 current loss: 0.8815122246742249 current acc: 0.1594\n",
      "iteration 18 current loss: 1.02642822265625 current acc: 0.1678\n",
      "iteration 19 current loss: 0.873051643371582 current acc: 0.1766\n",
      "iteration 20 current loss: 1.0382877588272095 current acc: 0.1838\n",
      "iteration 21 current loss: 0.8680564165115356 current acc: 0.1924\n",
      "iteration 22 current loss: 1.153631329536438 current acc: 0.1998\n",
      "iteration 23 current loss: 0.7593392729759216 current acc: 0.2082\n",
      "iteration 24 current loss: 0.7671605348587036 current acc: 0.2182\n",
      "iteration 25 current loss: 0.8575645685195923 current acc: 0.2274\n",
      "iteration 26 current loss: 0.7896657586097717 current acc: 0.2368\n",
      "iteration 27 current loss: 1.0118141174316406 current acc: 0.2448\n",
      "iteration 28 current loss: 0.9154388308525085 current acc: 0.2536\n",
      "iteration 29 current loss: 0.9508527517318726 current acc: 0.2622\n",
      "iteration 30 current loss: 1.0161229372024536 current acc: 0.2704\n",
      "iteration 31 current loss: 0.9751042127609253 current acc: 0.2786\n",
      "iteration 32 current loss: 0.8992171287536621 current acc: 0.2878\n",
      "iteration 33 current loss: 0.983057975769043 current acc: 0.2968\n",
      "iteration 34 current loss: 1.0703152418136597 current acc: 0.305\n",
      "iteration 35 current loss: 0.9955320358276367 current acc: 0.3136\n",
      "iteration 36 current loss: 1.093274712562561 current acc: 0.3218\n",
      "iteration 37 current loss: 0.9790108799934387 current acc: 0.33\n",
      "iteration 38 current loss: 1.2737101316452026 current acc: 0.3368\n",
      "iteration 39 current loss: 1.0809416770935059 current acc: 0.3456\n",
      "iteration 40 current loss: 0.9744279384613037 current acc: 0.3536\n",
      "iteration 41 current loss: 1.0667117834091187 current acc: 0.3608\n",
      "iteration 42 current loss: 0.9586197137832642 current acc: 0.369\n",
      "iteration 43 current loss: 0.762895405292511 current acc: 0.3784\n",
      "iteration 44 current loss: 0.8547523617744446 current acc: 0.387\n",
      "iteration 45 current loss: 0.8993370532989502 current acc: 0.3958\n",
      "iteration 46 current loss: 0.9036346077919006 current acc: 0.4048\n",
      "iteration 47 current loss: 1.0027393102645874 current acc: 0.4132\n",
      "iteration 48 current loss: 0.9778165221214294 current acc: 0.4218\n",
      "iteration 49 current loss: 0.9469894766807556 current acc: 0.4302\n",
      "iteration 50 current loss: 1.0541478395462036 current acc: 0.4382\n",
      "iteration 51 current loss: 1.1503361463546753 current acc: 0.4454\n",
      "iteration 52 current loss: 1.062821865081787 current acc: 0.453\n",
      "iteration 53 current loss: 0.9897617101669312 current acc: 0.4612\n",
      "iteration 54 current loss: 0.9393510222434998 current acc: 0.4698\n",
      "iteration 55 current loss: 0.8708316683769226 current acc: 0.4784\n",
      "iteration 56 current loss: 0.9941892623901367 current acc: 0.486\n",
      "iteration 57 current loss: 0.8588793277740479 current acc: 0.4952\n",
      "iteration 58 current loss: 0.6825443506240845 current acc: 0.5058\n",
      "iteration 59 current loss: 0.9904513359069824 current acc: 0.5138\n",
      "iteration 60 current loss: 0.919162392616272 current acc: 0.522\n",
      "iteration 61 current loss: 0.7821837067604065 current acc: 0.5322\n",
      "iteration 62 current loss: 0.9993852972984314 current acc: 0.5412\n",
      "iteration 63 current loss: 0.9749506711959839 current acc: 0.5492\n",
      "iteration 64 current loss: 0.7776402235031128 current acc: 0.5586\n",
      "iteration 65 current loss: 0.8705750703811646 current acc: 0.5678\n",
      "iteration 66 current loss: 1.0448702573776245 current acc: 0.5758\n",
      "iteration 67 current loss: 0.9126020669937134 current acc: 0.584\n",
      "iteration 68 current loss: 1.0830514430999756 current acc: 0.592\n",
      "iteration 69 current loss: 0.7134970426559448 current acc: 0.6022\n",
      "iteration 70 current loss: 0.8473202586174011 current acc: 0.611\n",
      "iteration 71 current loss: 0.7106236219406128 current acc: 0.6206\n",
      "iteration 72 current loss: 0.8636108636856079 current acc: 0.6298\n",
      "iteration 73 current loss: 0.8936015963554382 current acc: 0.6384\n",
      "iteration 74 current loss: 0.8436919450759888 current acc: 0.6474\n",
      "iteration 75 current loss: 0.9205107688903809 current acc: 0.656\n",
      "iteration 76 current loss: 0.9912365078926086 current acc: 0.6638\n",
      "iteration 77 current loss: 0.7467758655548096 current acc: 0.6738\n",
      "iteration 78 current loss: 0.6557543277740479 current acc: 0.6748\n",
      "\t\tTrain Epoch 76/100,Train Accuracy: 0.6748, Train Loss: 0.9220777991451795.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 76/100, Validation Accuracy: 0.568875, Validation Loss: 1.1840338559150696\n",
      "iteration 0 current loss: 0.9682661890983582 current acc: 0.0088\n",
      "iteration 1 current loss: 0.8200122117996216 current acc: 0.0184\n",
      "iteration 2 current loss: 0.9545945525169373 current acc: 0.0272\n",
      "iteration 3 current loss: 1.0237351655960083 current acc: 0.0344\n",
      "iteration 4 current loss: 0.9657371640205383 current acc: 0.0426\n",
      "iteration 5 current loss: 0.8935304284095764 current acc: 0.0504\n",
      "iteration 6 current loss: 0.9151938557624817 current acc: 0.0586\n",
      "iteration 7 current loss: 0.8829143643379211 current acc: 0.0674\n",
      "iteration 8 current loss: 0.869303822517395 current acc: 0.0764\n",
      "iteration 9 current loss: 0.9760273098945618 current acc: 0.0854\n",
      "iteration 10 current loss: 0.8634995818138123 current acc: 0.094\n",
      "iteration 11 current loss: 1.1195580959320068 current acc: 0.1014\n",
      "iteration 12 current loss: 0.9091954231262207 current acc: 0.1098\n",
      "iteration 13 current loss: 0.6988464593887329 current acc: 0.1198\n",
      "iteration 14 current loss: 0.7719667553901672 current acc: 0.129\n",
      "iteration 15 current loss: 0.9012442827224731 current acc: 0.1376\n",
      "iteration 16 current loss: 0.9112136960029602 current acc: 0.146\n",
      "iteration 17 current loss: 0.976645827293396 current acc: 0.1542\n",
      "iteration 18 current loss: 0.9613041281700134 current acc: 0.1628\n",
      "iteration 19 current loss: 0.7881486415863037 current acc: 0.1724\n",
      "iteration 20 current loss: 1.1525005102157593 current acc: 0.1798\n",
      "iteration 21 current loss: 1.1000101566314697 current acc: 0.1872\n",
      "iteration 22 current loss: 1.0749828815460205 current acc: 0.1946\n",
      "iteration 23 current loss: 0.9323951005935669 current acc: 0.2026\n",
      "iteration 24 current loss: 0.8956058621406555 current acc: 0.2122\n",
      "iteration 25 current loss: 0.788800835609436 current acc: 0.2214\n",
      "iteration 26 current loss: 0.9487133622169495 current acc: 0.2292\n",
      "iteration 27 current loss: 1.0053479671478271 current acc: 0.237\n",
      "iteration 28 current loss: 0.7597354054450989 current acc: 0.2462\n",
      "iteration 29 current loss: 0.9060341715812683 current acc: 0.2548\n",
      "iteration 30 current loss: 1.041117548942566 current acc: 0.2622\n",
      "iteration 31 current loss: 0.76995849609375 current acc: 0.2714\n",
      "iteration 32 current loss: 0.9370354413986206 current acc: 0.28\n",
      "iteration 33 current loss: 1.184898853302002 current acc: 0.2868\n",
      "iteration 34 current loss: 0.8810674548149109 current acc: 0.295\n",
      "iteration 35 current loss: 0.5085862278938293 current acc: 0.3058\n",
      "iteration 36 current loss: 0.8438525199890137 current acc: 0.3148\n",
      "iteration 37 current loss: 0.7927828431129456 current acc: 0.3238\n",
      "iteration 38 current loss: 0.8812340497970581 current acc: 0.3326\n",
      "iteration 39 current loss: 0.9273210167884827 current acc: 0.341\n",
      "iteration 40 current loss: 0.9251015782356262 current acc: 0.3502\n",
      "iteration 41 current loss: 0.9430997371673584 current acc: 0.3588\n",
      "iteration 42 current loss: 0.996704638004303 current acc: 0.367\n",
      "iteration 43 current loss: 0.7850882411003113 current acc: 0.3768\n",
      "iteration 44 current loss: 0.830893874168396 current acc: 0.3862\n",
      "iteration 45 current loss: 1.0006660223007202 current acc: 0.3944\n",
      "iteration 46 current loss: 0.6643189787864685 current acc: 0.404\n",
      "iteration 47 current loss: 0.9300604462623596 current acc: 0.413\n",
      "iteration 48 current loss: 1.0040814876556396 current acc: 0.4208\n",
      "iteration 49 current loss: 1.2610331773757935 current acc: 0.4284\n",
      "iteration 50 current loss: 1.0104531049728394 current acc: 0.4366\n",
      "iteration 51 current loss: 0.8957768678665161 current acc: 0.446\n",
      "iteration 52 current loss: 0.9085164070129395 current acc: 0.455\n",
      "iteration 53 current loss: 0.9805439114570618 current acc: 0.4626\n",
      "iteration 54 current loss: 0.9214315414428711 current acc: 0.471\n",
      "iteration 55 current loss: 0.9784886837005615 current acc: 0.4784\n",
      "iteration 56 current loss: 0.8490052819252014 current acc: 0.4872\n",
      "iteration 57 current loss: 1.0810496807098389 current acc: 0.4958\n",
      "iteration 58 current loss: 0.9873509407043457 current acc: 0.5034\n",
      "iteration 59 current loss: 0.7766223549842834 current acc: 0.5128\n",
      "iteration 60 current loss: 1.0607047080993652 current acc: 0.5208\n",
      "iteration 61 current loss: 0.838460385799408 current acc: 0.5298\n",
      "iteration 62 current loss: 0.9135956764221191 current acc: 0.5386\n",
      "iteration 63 current loss: 0.8658447861671448 current acc: 0.5474\n",
      "iteration 64 current loss: 0.9231833219528198 current acc: 0.5562\n",
      "iteration 65 current loss: 1.0194716453552246 current acc: 0.5646\n",
      "iteration 66 current loss: 0.9002569913864136 current acc: 0.573\n",
      "iteration 67 current loss: 0.9026711583137512 current acc: 0.5812\n",
      "iteration 68 current loss: 0.8073967099189758 current acc: 0.5902\n",
      "iteration 69 current loss: 0.9579454064369202 current acc: 0.598\n",
      "iteration 70 current loss: 1.0539705753326416 current acc: 0.6054\n",
      "iteration 71 current loss: 0.9266905784606934 current acc: 0.6136\n",
      "iteration 72 current loss: 0.7806912064552307 current acc: 0.623\n",
      "iteration 73 current loss: 0.9611626267433167 current acc: 0.632\n",
      "iteration 74 current loss: 1.1118309497833252 current acc: 0.6396\n",
      "iteration 75 current loss: 1.0433409214019775 current acc: 0.6476\n",
      "iteration 76 current loss: 1.1617494821548462 current acc: 0.6546\n",
      "iteration 77 current loss: 1.0138072967529297 current acc: 0.6628\n",
      "iteration 78 current loss: 0.35781195759773254 current acc: 0.6644\n",
      "\t\tTrain Epoch 77/100,Train Accuracy: 0.6644, Train Loss: 0.9219467341899872.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 77/100, Validation Accuracy: 0.569125, Validation Loss: 1.1932570209503175\n",
      "iteration 0 current loss: 0.8026818037033081 current acc: 0.0092\n",
      "iteration 1 current loss: 0.9116131067276001 current acc: 0.0174\n",
      "iteration 2 current loss: 0.8976202011108398 current acc: 0.0262\n",
      "iteration 3 current loss: 0.8760182857513428 current acc: 0.0346\n",
      "iteration 4 current loss: 1.128373146057129 current acc: 0.042\n",
      "iteration 5 current loss: 0.9534062743186951 current acc: 0.0504\n",
      "iteration 6 current loss: 0.9337018132209778 current acc: 0.0588\n",
      "iteration 7 current loss: 1.0271921157836914 current acc: 0.067\n",
      "iteration 8 current loss: 0.8367002606391907 current acc: 0.0764\n",
      "iteration 9 current loss: 0.9846766591072083 current acc: 0.0844\n",
      "iteration 10 current loss: 0.6735711693763733 current acc: 0.094\n",
      "iteration 11 current loss: 0.7267468571662903 current acc: 0.1038\n",
      "iteration 12 current loss: 0.8725492358207703 current acc: 0.1126\n",
      "iteration 13 current loss: 0.9594119787216187 current acc: 0.1212\n",
      "iteration 14 current loss: 0.9159173965454102 current acc: 0.1298\n",
      "iteration 15 current loss: 0.8610545992851257 current acc: 0.1386\n",
      "iteration 16 current loss: 0.7994741201400757 current acc: 0.1482\n",
      "iteration 17 current loss: 0.7886379361152649 current acc: 0.157\n",
      "iteration 18 current loss: 1.1082066297531128 current acc: 0.165\n",
      "iteration 19 current loss: 1.0327486991882324 current acc: 0.1736\n",
      "iteration 20 current loss: 0.9613780379295349 current acc: 0.1824\n",
      "iteration 21 current loss: 0.9694774746894836 current acc: 0.1906\n",
      "iteration 22 current loss: 0.9540285468101501 current acc: 0.199\n",
      "iteration 23 current loss: 0.9692543148994446 current acc: 0.2068\n",
      "iteration 24 current loss: 0.9629136323928833 current acc: 0.2156\n",
      "iteration 25 current loss: 0.947218120098114 current acc: 0.2246\n",
      "iteration 26 current loss: 0.9329091906547546 current acc: 0.2336\n",
      "iteration 27 current loss: 1.0921489000320435 current acc: 0.2406\n",
      "iteration 28 current loss: 0.9001107811927795 current acc: 0.2494\n",
      "iteration 29 current loss: 0.975165843963623 current acc: 0.2582\n",
      "iteration 30 current loss: 0.8410428762435913 current acc: 0.267\n",
      "iteration 31 current loss: 0.7422230243682861 current acc: 0.2764\n",
      "iteration 32 current loss: 0.8759915232658386 current acc: 0.2856\n",
      "iteration 33 current loss: 0.8945223093032837 current acc: 0.2946\n",
      "iteration 34 current loss: 0.9443554282188416 current acc: 0.3028\n",
      "iteration 35 current loss: 0.7654051184654236 current acc: 0.3126\n",
      "iteration 36 current loss: 0.8576552867889404 current acc: 0.321\n",
      "iteration 37 current loss: 0.7232142686843872 current acc: 0.3304\n",
      "iteration 38 current loss: 0.8925244808197021 current acc: 0.3388\n",
      "iteration 39 current loss: 0.8043438196182251 current acc: 0.3484\n",
      "iteration 40 current loss: 0.953527569770813 current acc: 0.3564\n",
      "iteration 41 current loss: 1.0749247074127197 current acc: 0.364\n",
      "iteration 42 current loss: 0.957960307598114 current acc: 0.3712\n",
      "iteration 43 current loss: 0.7947964668273926 current acc: 0.3808\n",
      "iteration 44 current loss: 0.9294101595878601 current acc: 0.3902\n",
      "iteration 45 current loss: 0.9222033023834229 current acc: 0.3984\n",
      "iteration 46 current loss: 0.9138562083244324 current acc: 0.4074\n",
      "iteration 47 current loss: 0.8727336525917053 current acc: 0.4164\n",
      "iteration 48 current loss: 0.990046501159668 current acc: 0.424\n",
      "iteration 49 current loss: 0.9727264642715454 current acc: 0.4336\n",
      "iteration 50 current loss: 0.9835899472236633 current acc: 0.4414\n",
      "iteration 51 current loss: 1.0724613666534424 current acc: 0.4492\n",
      "iteration 52 current loss: 0.9438468813896179 current acc: 0.4572\n",
      "iteration 53 current loss: 0.9836499094963074 current acc: 0.466\n",
      "iteration 54 current loss: 0.9289344549179077 current acc: 0.4742\n",
      "iteration 55 current loss: 1.0402082204818726 current acc: 0.4818\n",
      "iteration 56 current loss: 0.8631029725074768 current acc: 0.4906\n",
      "iteration 57 current loss: 0.9670029282569885 current acc: 0.4992\n",
      "iteration 58 current loss: 0.7104965448379517 current acc: 0.5096\n",
      "iteration 59 current loss: 1.097891926765442 current acc: 0.5174\n",
      "iteration 60 current loss: 1.187687873840332 current acc: 0.5234\n",
      "iteration 61 current loss: 1.0795832872390747 current acc: 0.5306\n",
      "iteration 62 current loss: 0.9454442262649536 current acc: 0.5384\n",
      "iteration 63 current loss: 0.8873618245124817 current acc: 0.5476\n",
      "iteration 64 current loss: 0.8604806661605835 current acc: 0.5566\n",
      "iteration 65 current loss: 0.9408900141716003 current acc: 0.5642\n",
      "iteration 66 current loss: 1.0807849168777466 current acc: 0.5716\n",
      "iteration 67 current loss: 0.9153026938438416 current acc: 0.5806\n",
      "iteration 68 current loss: 0.8964331150054932 current acc: 0.5892\n",
      "iteration 69 current loss: 0.7967647314071655 current acc: 0.5984\n",
      "iteration 70 current loss: 0.880828857421875 current acc: 0.6074\n",
      "iteration 71 current loss: 0.9883612990379333 current acc: 0.6146\n",
      "iteration 72 current loss: 1.356723427772522 current acc: 0.6204\n",
      "iteration 73 current loss: 0.9971221685409546 current acc: 0.6292\n",
      "iteration 74 current loss: 0.8634631037712097 current acc: 0.638\n",
      "iteration 75 current loss: 0.9973077774047852 current acc: 0.6468\n",
      "iteration 76 current loss: 1.0535744428634644 current acc: 0.6542\n",
      "iteration 77 current loss: 0.8147236108779907 current acc: 0.663\n",
      "iteration 78 current loss: 0.7456949353218079 current acc: 0.6644\n",
      "\t\tTrain Epoch 78/100,Train Accuracy: 0.6644, Train Loss: 0.9285834016679209.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 78/100, Validation Accuracy: 0.548875, Validation Loss: 1.2600604319572448\n",
      "iteration 0 current loss: 0.9533499479293823 current acc: 0.0082\n",
      "iteration 1 current loss: 0.8799352049827576 current acc: 0.017\n",
      "iteration 2 current loss: 0.977806031703949 current acc: 0.0252\n",
      "iteration 3 current loss: 1.0942879915237427 current acc: 0.0318\n",
      "iteration 4 current loss: 0.9771811962127686 current acc: 0.0404\n",
      "iteration 5 current loss: 1.0693317651748657 current acc: 0.048\n",
      "iteration 6 current loss: 1.0101873874664307 current acc: 0.0562\n",
      "iteration 7 current loss: 0.7596262693405151 current acc: 0.0656\n",
      "iteration 8 current loss: 0.9110116958618164 current acc: 0.0738\n",
      "iteration 9 current loss: 0.9344215393066406 current acc: 0.0822\n",
      "iteration 10 current loss: 0.8631336092948914 current acc: 0.091\n",
      "iteration 11 current loss: 0.9133028984069824 current acc: 0.0998\n",
      "iteration 12 current loss: 0.9971945285797119 current acc: 0.1078\n",
      "iteration 13 current loss: 0.7993473410606384 current acc: 0.117\n",
      "iteration 14 current loss: 0.7327622771263123 current acc: 0.127\n",
      "iteration 15 current loss: 0.7252429723739624 current acc: 0.1364\n",
      "iteration 16 current loss: 1.0697275400161743 current acc: 0.1442\n",
      "iteration 17 current loss: 0.8714748024940491 current acc: 0.1534\n",
      "iteration 18 current loss: 0.9002690315246582 current acc: 0.162\n",
      "iteration 19 current loss: 0.9294650554656982 current acc: 0.17\n",
      "iteration 20 current loss: 0.8995665907859802 current acc: 0.179\n",
      "iteration 21 current loss: 0.8504904508590698 current acc: 0.188\n",
      "iteration 22 current loss: 0.9121391773223877 current acc: 0.1958\n",
      "iteration 23 current loss: 0.843868613243103 current acc: 0.2048\n",
      "iteration 24 current loss: 0.9273512959480286 current acc: 0.213\n",
      "iteration 25 current loss: 0.8716229200363159 current acc: 0.222\n",
      "iteration 26 current loss: 0.8886440396308899 current acc: 0.2314\n",
      "iteration 27 current loss: 0.7309350967407227 current acc: 0.2398\n",
      "iteration 28 current loss: 0.7911261320114136 current acc: 0.2498\n",
      "iteration 29 current loss: 0.9296153783798218 current acc: 0.2586\n",
      "iteration 30 current loss: 0.9579412937164307 current acc: 0.2668\n",
      "iteration 31 current loss: 0.8767197728157043 current acc: 0.2756\n",
      "iteration 32 current loss: 0.8610066175460815 current acc: 0.2846\n",
      "iteration 33 current loss: 0.9231332540512085 current acc: 0.2942\n",
      "iteration 34 current loss: 1.000457525253296 current acc: 0.303\n",
      "iteration 35 current loss: 0.9095250964164734 current acc: 0.3118\n",
      "iteration 36 current loss: 0.7797182202339172 current acc: 0.3206\n",
      "iteration 37 current loss: 0.8147459030151367 current acc: 0.3304\n",
      "iteration 38 current loss: 0.8674452304840088 current acc: 0.3392\n",
      "iteration 39 current loss: 0.8595460653305054 current acc: 0.3478\n",
      "iteration 40 current loss: 0.7579602003097534 current acc: 0.357\n",
      "iteration 41 current loss: 1.082512617111206 current acc: 0.3646\n",
      "iteration 42 current loss: 1.1190649271011353 current acc: 0.372\n",
      "iteration 43 current loss: 0.9127742052078247 current acc: 0.3808\n",
      "iteration 44 current loss: 0.7177044749259949 current acc: 0.3908\n",
      "iteration 45 current loss: 0.8701237440109253 current acc: 0.3996\n",
      "iteration 46 current loss: 0.9145113229751587 current acc: 0.4082\n",
      "iteration 47 current loss: 1.0447707176208496 current acc: 0.4172\n",
      "iteration 48 current loss: 1.1321229934692383 current acc: 0.4246\n",
      "iteration 49 current loss: 1.0357075929641724 current acc: 0.432\n",
      "iteration 50 current loss: 1.0566822290420532 current acc: 0.4396\n",
      "iteration 51 current loss: 0.8582790493965149 current acc: 0.4484\n",
      "iteration 52 current loss: 0.9737780690193176 current acc: 0.4566\n",
      "iteration 53 current loss: 0.9062044024467468 current acc: 0.4648\n",
      "iteration 54 current loss: 0.7857438921928406 current acc: 0.4744\n",
      "iteration 55 current loss: 0.9648674130439758 current acc: 0.4826\n",
      "iteration 56 current loss: 1.1721735000610352 current acc: 0.491\n",
      "iteration 57 current loss: 0.9764106273651123 current acc: 0.4992\n",
      "iteration 58 current loss: 1.049230694770813 current acc: 0.5078\n",
      "iteration 59 current loss: 1.0228520631790161 current acc: 0.5158\n",
      "iteration 60 current loss: 1.0141445398330688 current acc: 0.523\n",
      "iteration 61 current loss: 0.8053325414657593 current acc: 0.5318\n",
      "iteration 62 current loss: 1.100205898284912 current acc: 0.5392\n",
      "iteration 63 current loss: 0.9481159448623657 current acc: 0.5478\n",
      "iteration 64 current loss: 0.8812445998191833 current acc: 0.5568\n",
      "iteration 65 current loss: 0.9394860863685608 current acc: 0.566\n",
      "iteration 66 current loss: 0.925166130065918 current acc: 0.575\n",
      "iteration 67 current loss: 0.9134017825126648 current acc: 0.5836\n",
      "iteration 68 current loss: 1.1995606422424316 current acc: 0.591\n",
      "iteration 69 current loss: 1.0744632482528687 current acc: 0.5988\n",
      "iteration 70 current loss: 0.8765981197357178 current acc: 0.6082\n",
      "iteration 71 current loss: 0.9555732011795044 current acc: 0.6164\n",
      "iteration 72 current loss: 0.9619900584220886 current acc: 0.6244\n",
      "iteration 73 current loss: 0.9624214768409729 current acc: 0.633\n",
      "iteration 74 current loss: 0.8412749767303467 current acc: 0.642\n",
      "iteration 75 current loss: 1.2094439268112183 current acc: 0.6496\n",
      "iteration 76 current loss: 0.8579381704330444 current acc: 0.6584\n",
      "iteration 77 current loss: 0.84282386302948 current acc: 0.6668\n",
      "iteration 78 current loss: 1.2327009439468384 current acc: 0.6676\n",
      "\t\tTrain Epoch 79/100,Train Accuracy: 0.6676, Train Loss: 0.9336711221103426.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 79/100, Validation Accuracy: 0.570875, Validation Loss: 1.1755394349098205\n",
      "iteration 0 current loss: 0.917751133441925 current acc: 0.0086\n",
      "iteration 1 current loss: 0.7513271570205688 current acc: 0.0182\n",
      "iteration 2 current loss: 0.8718465566635132 current acc: 0.0274\n",
      "iteration 3 current loss: 1.003097653388977 current acc: 0.036\n",
      "iteration 4 current loss: 0.8767469525337219 current acc: 0.045\n",
      "iteration 5 current loss: 0.8680881261825562 current acc: 0.0544\n",
      "iteration 6 current loss: 0.98655766248703 current acc: 0.0628\n",
      "iteration 7 current loss: 0.9592167735099792 current acc: 0.0716\n",
      "iteration 8 current loss: 0.8722988963127136 current acc: 0.0804\n",
      "iteration 9 current loss: 0.9055765271186829 current acc: 0.0894\n",
      "iteration 10 current loss: 0.7708380222320557 current acc: 0.0988\n",
      "iteration 11 current loss: 0.9849194884300232 current acc: 0.1068\n",
      "iteration 12 current loss: 0.9955925345420837 current acc: 0.115\n",
      "iteration 13 current loss: 1.2688544988632202 current acc: 0.1216\n",
      "iteration 14 current loss: 1.0609955787658691 current acc: 0.1294\n",
      "iteration 15 current loss: 1.0445506572723389 current acc: 0.1368\n",
      "iteration 16 current loss: 0.8333703279495239 current acc: 0.1462\n",
      "iteration 17 current loss: 0.9580934047698975 current acc: 0.1548\n",
      "iteration 18 current loss: 0.8510352969169617 current acc: 0.1634\n",
      "iteration 19 current loss: 0.9708061218261719 current acc: 0.1714\n",
      "iteration 20 current loss: 0.8151220083236694 current acc: 0.1802\n",
      "iteration 21 current loss: 0.9255931377410889 current acc: 0.1882\n",
      "iteration 22 current loss: 0.8849490284919739 current acc: 0.1976\n",
      "iteration 23 current loss: 0.8712088465690613 current acc: 0.207\n",
      "iteration 24 current loss: 1.0155044794082642 current acc: 0.2148\n",
      "iteration 25 current loss: 0.86646968126297 current acc: 0.224\n",
      "iteration 26 current loss: 0.7173616290092468 current acc: 0.2342\n",
      "iteration 27 current loss: 1.0181825160980225 current acc: 0.243\n",
      "iteration 28 current loss: 0.8384369015693665 current acc: 0.2524\n",
      "iteration 29 current loss: 1.0051019191741943 current acc: 0.2608\n",
      "iteration 30 current loss: 0.8556762933731079 current acc: 0.2692\n",
      "iteration 31 current loss: 0.8105502128601074 current acc: 0.2782\n",
      "iteration 32 current loss: 1.0061882734298706 current acc: 0.2862\n",
      "iteration 33 current loss: 0.9525207877159119 current acc: 0.2952\n",
      "iteration 34 current loss: 0.8735942244529724 current acc: 0.3034\n",
      "iteration 35 current loss: 0.8431569933891296 current acc: 0.3124\n",
      "iteration 36 current loss: 0.8831612467765808 current acc: 0.3214\n",
      "iteration 37 current loss: 1.0837669372558594 current acc: 0.3294\n",
      "iteration 38 current loss: 0.8204013109207153 current acc: 0.3382\n",
      "iteration 39 current loss: 0.9726031422615051 current acc: 0.347\n",
      "iteration 40 current loss: 0.9415572881698608 current acc: 0.356\n",
      "iteration 41 current loss: 1.0557091236114502 current acc: 0.3636\n",
      "iteration 42 current loss: 0.8810490369796753 current acc: 0.373\n",
      "iteration 43 current loss: 0.6956840753555298 current acc: 0.3832\n",
      "iteration 44 current loss: 0.9126951694488525 current acc: 0.3912\n",
      "iteration 45 current loss: 0.8518096208572388 current acc: 0.3996\n",
      "iteration 46 current loss: 0.9659258127212524 current acc: 0.4078\n",
      "iteration 47 current loss: 0.8967773914337158 current acc: 0.4172\n",
      "iteration 48 current loss: 1.013956904411316 current acc: 0.425\n",
      "iteration 49 current loss: 1.0614627599716187 current acc: 0.4336\n",
      "iteration 50 current loss: 0.8302481174468994 current acc: 0.443\n",
      "iteration 51 current loss: 0.8735985159873962 current acc: 0.4518\n",
      "iteration 52 current loss: 0.7866783142089844 current acc: 0.4616\n",
      "iteration 53 current loss: 0.7586768865585327 current acc: 0.4716\n",
      "iteration 54 current loss: 1.009507417678833 current acc: 0.4808\n",
      "iteration 55 current loss: 1.057389736175537 current acc: 0.4888\n",
      "iteration 56 current loss: 0.8474647998809814 current acc: 0.4978\n",
      "iteration 57 current loss: 0.8285729289054871 current acc: 0.5066\n",
      "iteration 58 current loss: 1.009555697441101 current acc: 0.514\n",
      "iteration 59 current loss: 0.9753403067588806 current acc: 0.5224\n",
      "iteration 60 current loss: 1.0474282503128052 current acc: 0.5314\n",
      "iteration 61 current loss: 0.8183345198631287 current acc: 0.5406\n",
      "iteration 62 current loss: 0.9180514216423035 current acc: 0.5502\n",
      "iteration 63 current loss: 0.8082001209259033 current acc: 0.5602\n",
      "iteration 64 current loss: 0.8923236727714539 current acc: 0.5684\n",
      "iteration 65 current loss: 1.0483348369598389 current acc: 0.5764\n",
      "iteration 66 current loss: 0.9728312492370605 current acc: 0.5842\n",
      "iteration 67 current loss: 0.9906673431396484 current acc: 0.5916\n",
      "iteration 68 current loss: 0.9296800494194031 current acc: 0.6004\n",
      "iteration 69 current loss: 0.8951907157897949 current acc: 0.6086\n",
      "iteration 70 current loss: 1.0360112190246582 current acc: 0.6158\n",
      "iteration 71 current loss: 1.0354204177856445 current acc: 0.624\n",
      "iteration 72 current loss: 0.7674686908721924 current acc: 0.6338\n",
      "iteration 73 current loss: 1.0263582468032837 current acc: 0.6426\n",
      "iteration 74 current loss: 0.7877197265625 current acc: 0.6522\n",
      "iteration 75 current loss: 0.8995555639266968 current acc: 0.6612\n",
      "iteration 76 current loss: 1.06043541431427 current acc: 0.6686\n",
      "iteration 77 current loss: 1.107277274131775 current acc: 0.6762\n",
      "iteration 78 current loss: 1.043832778930664 current acc: 0.6774\n",
      "\t\tTrain Epoch 80/100,Train Accuracy: 0.6774, Train Loss: 0.925948029831995.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 80/100, Validation Accuracy: 0.572, Validation Loss: 1.1779489393234253\n",
      "iteration 0 current loss: 0.9304765462875366 current acc: 0.0086\n",
      "iteration 1 current loss: 0.8633736371994019 current acc: 0.0176\n",
      "iteration 2 current loss: 0.9289908409118652 current acc: 0.0256\n",
      "iteration 3 current loss: 1.0837013721466064 current acc: 0.0328\n",
      "iteration 4 current loss: 0.9501110911369324 current acc: 0.0418\n",
      "iteration 5 current loss: 0.8948850035667419 current acc: 0.0514\n",
      "iteration 6 current loss: 0.9500279426574707 current acc: 0.0586\n",
      "iteration 7 current loss: 1.0743147134780884 current acc: 0.066\n",
      "iteration 8 current loss: 0.7059880495071411 current acc: 0.076\n",
      "iteration 9 current loss: 0.927844762802124 current acc: 0.0844\n",
      "iteration 10 current loss: 0.9096409678459167 current acc: 0.0934\n",
      "iteration 11 current loss: 0.9377659559249878 current acc: 0.101\n",
      "iteration 12 current loss: 0.7490209937095642 current acc: 0.1106\n",
      "iteration 13 current loss: 1.1057804822921753 current acc: 0.1186\n",
      "iteration 14 current loss: 0.972509503364563 current acc: 0.1268\n",
      "iteration 15 current loss: 1.2481669187545776 current acc: 0.1336\n",
      "iteration 16 current loss: 0.8429259657859802 current acc: 0.1426\n",
      "iteration 17 current loss: 0.9771279096603394 current acc: 0.1508\n",
      "iteration 18 current loss: 1.0611169338226318 current acc: 0.1594\n",
      "iteration 19 current loss: 0.8113457560539246 current acc: 0.1686\n",
      "iteration 20 current loss: 0.7419881820678711 current acc: 0.1786\n",
      "iteration 21 current loss: 0.7880004048347473 current acc: 0.1884\n",
      "iteration 22 current loss: 0.9832095503807068 current acc: 0.1964\n",
      "iteration 23 current loss: 0.8062468767166138 current acc: 0.2058\n",
      "iteration 24 current loss: 0.9832051992416382 current acc: 0.2144\n",
      "iteration 25 current loss: 0.834515392780304 current acc: 0.2238\n",
      "iteration 26 current loss: 0.8115827441215515 current acc: 0.233\n",
      "iteration 27 current loss: 0.76694655418396 current acc: 0.242\n",
      "iteration 28 current loss: 0.7506318092346191 current acc: 0.2512\n",
      "iteration 29 current loss: 0.8982832431793213 current acc: 0.2598\n",
      "iteration 30 current loss: 0.8652706742286682 current acc: 0.2696\n",
      "iteration 31 current loss: 0.9331496953964233 current acc: 0.2784\n",
      "iteration 32 current loss: 0.9018137454986572 current acc: 0.287\n",
      "iteration 33 current loss: 1.3481049537658691 current acc: 0.2934\n",
      "iteration 34 current loss: 1.0150864124298096 current acc: 0.3024\n",
      "iteration 35 current loss: 1.1365216970443726 current acc: 0.309\n",
      "iteration 36 current loss: 0.7284457683563232 current acc: 0.3186\n",
      "iteration 37 current loss: 0.9031044840812683 current acc: 0.3272\n",
      "iteration 38 current loss: 1.10097074508667 current acc: 0.3354\n",
      "iteration 39 current loss: 0.791878879070282 current acc: 0.3448\n",
      "iteration 40 current loss: 0.8001545071601868 current acc: 0.3542\n",
      "iteration 41 current loss: 0.9866037964820862 current acc: 0.3628\n",
      "iteration 42 current loss: 0.9431968927383423 current acc: 0.371\n",
      "iteration 43 current loss: 0.7991490364074707 current acc: 0.3808\n",
      "iteration 44 current loss: 1.044107437133789 current acc: 0.3888\n",
      "iteration 45 current loss: 1.0240920782089233 current acc: 0.3964\n",
      "iteration 46 current loss: 1.0703816413879395 current acc: 0.4042\n",
      "iteration 47 current loss: 1.062994122505188 current acc: 0.412\n",
      "iteration 48 current loss: 0.9699674844741821 current acc: 0.421\n",
      "iteration 49 current loss: 0.6640177965164185 current acc: 0.4314\n",
      "iteration 50 current loss: 0.998904287815094 current acc: 0.4398\n",
      "iteration 51 current loss: 1.00285804271698 current acc: 0.4478\n",
      "iteration 52 current loss: 0.9954061508178711 current acc: 0.4562\n",
      "iteration 53 current loss: 1.1408900022506714 current acc: 0.4632\n",
      "iteration 54 current loss: 0.9656865000724792 current acc: 0.4712\n",
      "iteration 55 current loss: 1.0482746362686157 current acc: 0.4796\n",
      "iteration 56 current loss: 0.9772759675979614 current acc: 0.4882\n",
      "iteration 57 current loss: 0.9825310111045837 current acc: 0.4968\n",
      "iteration 58 current loss: 1.3108092546463013 current acc: 0.5034\n",
      "iteration 59 current loss: 1.1180737018585205 current acc: 0.5114\n",
      "iteration 60 current loss: 0.9017962217330933 current acc: 0.5198\n",
      "iteration 61 current loss: 0.9938403367996216 current acc: 0.528\n",
      "iteration 62 current loss: 0.8810147047042847 current acc: 0.5368\n",
      "iteration 63 current loss: 0.97282475233078 current acc: 0.5456\n",
      "iteration 64 current loss: 0.9669780731201172 current acc: 0.5542\n",
      "iteration 65 current loss: 1.018918514251709 current acc: 0.562\n",
      "iteration 66 current loss: 0.9681122899055481 current acc: 0.57\n",
      "iteration 67 current loss: 0.7583907842636108 current acc: 0.579\n",
      "iteration 68 current loss: 1.0725555419921875 current acc: 0.586\n",
      "iteration 69 current loss: 0.8863401412963867 current acc: 0.5948\n",
      "iteration 70 current loss: 0.9724534749984741 current acc: 0.6026\n",
      "iteration 71 current loss: 0.9643824696540833 current acc: 0.6106\n",
      "iteration 72 current loss: 0.927436888217926 current acc: 0.6194\n",
      "iteration 73 current loss: 0.914440393447876 current acc: 0.6288\n",
      "iteration 74 current loss: 0.8449689149856567 current acc: 0.6382\n",
      "iteration 75 current loss: 0.982994019985199 current acc: 0.6464\n",
      "iteration 76 current loss: 0.8534070253372192 current acc: 0.6552\n",
      "iteration 77 current loss: 0.854766309261322 current acc: 0.6644\n",
      "iteration 78 current loss: 1.155038595199585 current acc: 0.665\n",
      "\t\tTrain Epoch 81/100,Train Accuracy: 0.665, Train Loss: 0.9469633690918549.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 81/100, Validation Accuracy: 0.553875, Validation Loss: 1.254349585056305\n",
      "iteration 0 current loss: 0.8598976731300354 current acc: 0.0084\n",
      "iteration 1 current loss: 1.1260114908218384 current acc: 0.0168\n",
      "iteration 2 current loss: 0.9013465642929077 current acc: 0.0256\n",
      "iteration 3 current loss: 1.0908269882202148 current acc: 0.0338\n",
      "iteration 4 current loss: 1.1453607082366943 current acc: 0.0416\n",
      "iteration 5 current loss: 1.1077778339385986 current acc: 0.0492\n",
      "iteration 6 current loss: 1.1551988124847412 current acc: 0.0568\n",
      "iteration 7 current loss: 1.0157989263534546 current acc: 0.065\n",
      "iteration 8 current loss: 1.2535345554351807 current acc: 0.071\n",
      "iteration 9 current loss: 0.9730067849159241 current acc: 0.08\n",
      "iteration 10 current loss: 1.0483287572860718 current acc: 0.087\n",
      "iteration 11 current loss: 1.1717212200164795 current acc: 0.0948\n",
      "iteration 12 current loss: 0.8844793438911438 current acc: 0.104\n",
      "iteration 13 current loss: 0.9373511672019958 current acc: 0.1122\n",
      "iteration 14 current loss: 1.0328154563903809 current acc: 0.12\n",
      "iteration 15 current loss: 0.9811246395111084 current acc: 0.128\n",
      "iteration 16 current loss: 0.761846125125885 current acc: 0.1374\n",
      "iteration 17 current loss: 1.0968718528747559 current acc: 0.1456\n",
      "iteration 18 current loss: 0.999862015247345 current acc: 0.153\n",
      "iteration 19 current loss: 0.7899761199951172 current acc: 0.1632\n",
      "iteration 20 current loss: 0.8999834656715393 current acc: 0.172\n",
      "iteration 21 current loss: 1.1766259670257568 current acc: 0.179\n",
      "iteration 22 current loss: 0.9301462769508362 current acc: 0.1872\n",
      "iteration 23 current loss: 0.9825221300125122 current acc: 0.1962\n",
      "iteration 24 current loss: 0.7918184995651245 current acc: 0.206\n",
      "iteration 25 current loss: 0.8495581746101379 current acc: 0.2158\n",
      "iteration 26 current loss: 0.7437964677810669 current acc: 0.2258\n",
      "iteration 27 current loss: 0.8085125088691711 current acc: 0.235\n",
      "iteration 28 current loss: 0.9411694407463074 current acc: 0.2442\n",
      "iteration 29 current loss: 1.090410590171814 current acc: 0.2518\n",
      "iteration 30 current loss: 0.7550112009048462 current acc: 0.2616\n",
      "iteration 31 current loss: 0.9120587706565857 current acc: 0.2704\n",
      "iteration 32 current loss: 0.790988564491272 current acc: 0.2798\n",
      "iteration 33 current loss: 0.9562835693359375 current acc: 0.2884\n",
      "iteration 34 current loss: 0.8952865600585938 current acc: 0.2964\n",
      "iteration 35 current loss: 0.8901798129081726 current acc: 0.3052\n",
      "iteration 36 current loss: 1.0732417106628418 current acc: 0.313\n",
      "iteration 37 current loss: 0.8637408018112183 current acc: 0.3228\n",
      "iteration 38 current loss: 1.007840633392334 current acc: 0.331\n",
      "iteration 39 current loss: 0.9293015599250793 current acc: 0.3402\n",
      "iteration 40 current loss: 1.0080742835998535 current acc: 0.3488\n",
      "iteration 41 current loss: 1.0330679416656494 current acc: 0.3562\n",
      "iteration 42 current loss: 0.861240565776825 current acc: 0.365\n",
      "iteration 43 current loss: 0.6698529720306396 current acc: 0.375\n",
      "iteration 44 current loss: 0.7974101901054382 current acc: 0.3844\n",
      "iteration 45 current loss: 1.09346342086792 current acc: 0.392\n",
      "iteration 46 current loss: 0.9367555975914001 current acc: 0.4014\n",
      "iteration 47 current loss: 1.0164449214935303 current acc: 0.4092\n",
      "iteration 48 current loss: 1.0158361196517944 current acc: 0.4184\n",
      "iteration 49 current loss: 1.0568499565124512 current acc: 0.4264\n",
      "iteration 50 current loss: 0.8695016503334045 current acc: 0.4358\n",
      "iteration 51 current loss: 1.1018768548965454 current acc: 0.4432\n",
      "iteration 52 current loss: 1.0581837892532349 current acc: 0.451\n",
      "iteration 53 current loss: 1.008363127708435 current acc: 0.4594\n",
      "iteration 54 current loss: 0.9995512962341309 current acc: 0.4676\n",
      "iteration 55 current loss: 0.9380921721458435 current acc: 0.476\n",
      "iteration 56 current loss: 0.8795688152313232 current acc: 0.4844\n",
      "iteration 57 current loss: 0.9512868523597717 current acc: 0.493\n",
      "iteration 58 current loss: 1.0753673315048218 current acc: 0.501\n",
      "iteration 59 current loss: 0.8812233209609985 current acc: 0.5092\n",
      "iteration 60 current loss: 0.782475471496582 current acc: 0.5182\n",
      "iteration 61 current loss: 0.7634187340736389 current acc: 0.5274\n",
      "iteration 62 current loss: 0.721628725528717 current acc: 0.5374\n",
      "iteration 63 current loss: 0.9839681386947632 current acc: 0.546\n",
      "iteration 64 current loss: 0.904323935508728 current acc: 0.5548\n",
      "iteration 65 current loss: 1.2090446949005127 current acc: 0.5622\n",
      "iteration 66 current loss: 1.0142016410827637 current acc: 0.571\n",
      "iteration 67 current loss: 0.8911678194999695 current acc: 0.5796\n",
      "iteration 68 current loss: 0.9895747900009155 current acc: 0.5888\n",
      "iteration 69 current loss: 0.9673179388046265 current acc: 0.5978\n",
      "iteration 70 current loss: 0.9298630952835083 current acc: 0.606\n",
      "iteration 71 current loss: 0.9496234059333801 current acc: 0.6138\n",
      "iteration 72 current loss: 1.0183701515197754 current acc: 0.6216\n",
      "iteration 73 current loss: 0.9665181636810303 current acc: 0.6292\n",
      "iteration 74 current loss: 0.899917483329773 current acc: 0.6382\n",
      "iteration 75 current loss: 0.9943478107452393 current acc: 0.6464\n",
      "iteration 76 current loss: 1.1962224245071411 current acc: 0.6534\n",
      "iteration 77 current loss: 0.9330868721008301 current acc: 0.6614\n",
      "iteration 78 current loss: 1.5583897829055786 current acc: 0.662\n",
      "\t\tTrain Epoch 82/100,Train Accuracy: 0.662, Train Loss: 0.9689504557018038.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 82/100, Validation Accuracy: 0.560375, Validation Loss: 1.2255963048934937\n",
      "iteration 0 current loss: 0.9791169762611389 current acc: 0.008\n",
      "iteration 1 current loss: 1.0027514696121216 current acc: 0.0162\n",
      "iteration 2 current loss: 0.8217698335647583 current acc: 0.0252\n",
      "iteration 3 current loss: 0.9178544282913208 current acc: 0.0332\n",
      "iteration 4 current loss: 0.9362077116966248 current acc: 0.042\n",
      "iteration 5 current loss: 1.094059944152832 current acc: 0.0486\n",
      "iteration 6 current loss: 0.8708349466323853 current acc: 0.057\n",
      "iteration 7 current loss: 1.0800902843475342 current acc: 0.0646\n",
      "iteration 8 current loss: 1.0041491985321045 current acc: 0.0722\n",
      "iteration 9 current loss: 1.1264607906341553 current acc: 0.0798\n",
      "iteration 10 current loss: 0.8813034296035767 current acc: 0.0884\n",
      "iteration 11 current loss: 0.9364394545555115 current acc: 0.0968\n",
      "iteration 12 current loss: 0.766075849533081 current acc: 0.1062\n",
      "iteration 13 current loss: 0.7908830642700195 current acc: 0.117\n",
      "iteration 14 current loss: 0.9129316806793213 current acc: 0.126\n",
      "iteration 15 current loss: 1.1551601886749268 current acc: 0.1338\n",
      "iteration 16 current loss: 0.8834183216094971 current acc: 0.1422\n",
      "iteration 17 current loss: 0.9323957562446594 current acc: 0.1502\n",
      "iteration 18 current loss: 0.954982578754425 current acc: 0.158\n",
      "iteration 19 current loss: 0.9137371778488159 current acc: 0.1672\n",
      "iteration 20 current loss: 0.8859882354736328 current acc: 0.1756\n",
      "iteration 21 current loss: 0.911345899105072 current acc: 0.1842\n",
      "iteration 22 current loss: 0.9067566990852356 current acc: 0.1932\n",
      "iteration 23 current loss: 0.8829509019851685 current acc: 0.2026\n",
      "iteration 24 current loss: 0.8401585817337036 current acc: 0.2114\n",
      "iteration 25 current loss: 1.0365020036697388 current acc: 0.2198\n",
      "iteration 26 current loss: 1.0358707904815674 current acc: 0.2282\n",
      "iteration 27 current loss: 0.8707711696624756 current acc: 0.2368\n",
      "iteration 28 current loss: 0.8944435715675354 current acc: 0.2462\n",
      "iteration 29 current loss: 1.1295480728149414 current acc: 0.2534\n",
      "iteration 30 current loss: 0.8722634315490723 current acc: 0.2618\n",
      "iteration 31 current loss: 0.8998086452484131 current acc: 0.2706\n",
      "iteration 32 current loss: 1.1714664697647095 current acc: 0.278\n",
      "iteration 33 current loss: 0.9055315256118774 current acc: 0.286\n",
      "iteration 34 current loss: 0.8590375781059265 current acc: 0.2956\n",
      "iteration 35 current loss: 0.9229437708854675 current acc: 0.3048\n",
      "iteration 36 current loss: 0.9671676754951477 current acc: 0.3138\n",
      "iteration 37 current loss: 0.8533809185028076 current acc: 0.3226\n",
      "iteration 38 current loss: 0.8997982740402222 current acc: 0.331\n",
      "iteration 39 current loss: 1.04610013961792 current acc: 0.3388\n",
      "iteration 40 current loss: 0.8709073662757874 current acc: 0.3478\n",
      "iteration 41 current loss: 0.7929632067680359 current acc: 0.357\n",
      "iteration 42 current loss: 1.0709468126296997 current acc: 0.3638\n",
      "iteration 43 current loss: 1.1666076183319092 current acc: 0.3714\n",
      "iteration 44 current loss: 0.8808903694152832 current acc: 0.3794\n",
      "iteration 45 current loss: 0.7606425881385803 current acc: 0.3892\n",
      "iteration 46 current loss: 1.1216082572937012 current acc: 0.3966\n",
      "iteration 47 current loss: 0.9932344555854797 current acc: 0.4042\n",
      "iteration 48 current loss: 0.7453383803367615 current acc: 0.414\n",
      "iteration 49 current loss: 0.833989679813385 current acc: 0.4238\n",
      "iteration 50 current loss: 1.092671275138855 current acc: 0.432\n",
      "iteration 51 current loss: 1.0189319849014282 current acc: 0.4406\n",
      "iteration 52 current loss: 0.8300023078918457 current acc: 0.45\n",
      "iteration 53 current loss: 1.0738407373428345 current acc: 0.458\n",
      "iteration 54 current loss: 1.015587568283081 current acc: 0.4658\n",
      "iteration 55 current loss: 0.9262221455574036 current acc: 0.4748\n",
      "iteration 56 current loss: 0.8879854679107666 current acc: 0.4832\n",
      "iteration 57 current loss: 1.023461937904358 current acc: 0.4916\n",
      "iteration 58 current loss: 1.1796053647994995 current acc: 0.499\n",
      "iteration 59 current loss: 0.9046905040740967 current acc: 0.5078\n",
      "iteration 60 current loss: 1.0417906045913696 current acc: 0.5162\n",
      "iteration 61 current loss: 0.7813977599143982 current acc: 0.5262\n",
      "iteration 62 current loss: 0.8220828771591187 current acc: 0.536\n",
      "iteration 63 current loss: 0.8668049573898315 current acc: 0.5446\n",
      "iteration 64 current loss: 1.3271468877792358 current acc: 0.5508\n",
      "iteration 65 current loss: 1.0145728588104248 current acc: 0.5588\n",
      "iteration 66 current loss: 1.0006253719329834 current acc: 0.5664\n",
      "iteration 67 current loss: 0.8876267671585083 current acc: 0.5758\n",
      "iteration 68 current loss: 1.0658599138259888 current acc: 0.5838\n",
      "iteration 69 current loss: 0.8353586196899414 current acc: 0.5924\n",
      "iteration 70 current loss: 0.8908511996269226 current acc: 0.6014\n",
      "iteration 71 current loss: 0.9534394145011902 current acc: 0.6088\n",
      "iteration 72 current loss: 1.0995551347732544 current acc: 0.6164\n",
      "iteration 73 current loss: 0.8208651542663574 current acc: 0.6252\n",
      "iteration 74 current loss: 0.8310375809669495 current acc: 0.635\n",
      "iteration 75 current loss: 0.95167475938797 current acc: 0.6432\n",
      "iteration 76 current loss: 0.9143904447555542 current acc: 0.6518\n",
      "iteration 77 current loss: 0.9200436472892761 current acc: 0.6596\n",
      "iteration 78 current loss: 0.8361563682556152 current acc: 0.6608\n",
      "\t\tTrain Epoch 83/100,Train Accuracy: 0.6608, Train Loss: 0.9468843770932548.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 83/100, Validation Accuracy: 0.568125, Validation Loss: 1.1958798623085023\n",
      "iteration 0 current loss: 0.8066582679748535 current acc: 0.0092\n",
      "iteration 1 current loss: 0.9985923171043396 current acc: 0.0172\n",
      "iteration 2 current loss: 1.0318727493286133 current acc: 0.025\n",
      "iteration 3 current loss: 0.8885664939880371 current acc: 0.0338\n",
      "iteration 4 current loss: 0.823931097984314 current acc: 0.043\n",
      "iteration 5 current loss: 0.7647904753684998 current acc: 0.0528\n",
      "iteration 6 current loss: 1.059165120124817 current acc: 0.0602\n",
      "iteration 7 current loss: 0.9869175553321838 current acc: 0.0678\n",
      "iteration 8 current loss: 0.8722233772277832 current acc: 0.0772\n",
      "iteration 9 current loss: 0.834116518497467 current acc: 0.0868\n",
      "iteration 10 current loss: 0.8928444981575012 current acc: 0.0964\n",
      "iteration 11 current loss: 0.997532844543457 current acc: 0.1044\n",
      "iteration 12 current loss: 0.7336284518241882 current acc: 0.114\n",
      "iteration 13 current loss: 0.8747563362121582 current acc: 0.1226\n",
      "iteration 14 current loss: 0.9189092516899109 current acc: 0.1312\n",
      "iteration 15 current loss: 0.9413079619407654 current acc: 0.1402\n",
      "iteration 16 current loss: 0.9028176665306091 current acc: 0.149\n",
      "iteration 17 current loss: 0.7792959809303284 current acc: 0.158\n",
      "iteration 18 current loss: 0.7051839232444763 current acc: 0.1674\n",
      "iteration 19 current loss: 0.850332498550415 current acc: 0.1768\n",
      "iteration 20 current loss: 0.9912732839584351 current acc: 0.1858\n",
      "iteration 21 current loss: 0.9316480159759521 current acc: 0.1946\n",
      "iteration 22 current loss: 0.9139843583106995 current acc: 0.2032\n",
      "iteration 23 current loss: 1.0828510522842407 current acc: 0.2112\n",
      "iteration 24 current loss: 0.9230288863182068 current acc: 0.22\n",
      "iteration 25 current loss: 0.7714241147041321 current acc: 0.2298\n",
      "iteration 26 current loss: 1.0678653717041016 current acc: 0.238\n",
      "iteration 27 current loss: 0.9338598251342773 current acc: 0.2466\n",
      "iteration 28 current loss: 0.991762638092041 current acc: 0.2556\n",
      "iteration 29 current loss: 0.8451338410377502 current acc: 0.2646\n",
      "iteration 30 current loss: 0.8694337606430054 current acc: 0.2738\n",
      "iteration 31 current loss: 0.9979937672615051 current acc: 0.282\n",
      "iteration 32 current loss: 1.0333129167556763 current acc: 0.2904\n",
      "iteration 33 current loss: 0.9164457321166992 current acc: 0.2998\n",
      "iteration 34 current loss: 1.070673942565918 current acc: 0.3074\n",
      "iteration 35 current loss: 0.774339497089386 current acc: 0.3168\n",
      "iteration 36 current loss: 0.9003618359565735 current acc: 0.325\n",
      "iteration 37 current loss: 0.9683700799942017 current acc: 0.3338\n",
      "iteration 38 current loss: 0.9584705233573914 current acc: 0.3428\n",
      "iteration 39 current loss: 0.9232107400894165 current acc: 0.3512\n",
      "iteration 40 current loss: 0.9237733483314514 current acc: 0.3602\n",
      "iteration 41 current loss: 0.8446700572967529 current acc: 0.37\n",
      "iteration 42 current loss: 0.9446945190429688 current acc: 0.3786\n",
      "iteration 43 current loss: 0.9841014742851257 current acc: 0.3876\n",
      "iteration 44 current loss: 1.029618740081787 current acc: 0.395\n",
      "iteration 45 current loss: 0.9697131514549255 current acc: 0.4038\n",
      "iteration 46 current loss: 0.9978743195533752 current acc: 0.4124\n",
      "iteration 47 current loss: 1.159427523612976 current acc: 0.4198\n",
      "iteration 48 current loss: 1.1378681659698486 current acc: 0.4276\n",
      "iteration 49 current loss: 0.9046263694763184 current acc: 0.4358\n",
      "iteration 50 current loss: 0.9566996097564697 current acc: 0.4444\n",
      "iteration 51 current loss: 0.8787623643875122 current acc: 0.4536\n",
      "iteration 52 current loss: 1.0712872743606567 current acc: 0.4606\n",
      "iteration 53 current loss: 1.1206910610198975 current acc: 0.4684\n",
      "iteration 54 current loss: 0.9597885012626648 current acc: 0.4772\n",
      "iteration 55 current loss: 0.8042705655097961 current acc: 0.4862\n",
      "iteration 56 current loss: 0.9299548268318176 current acc: 0.4942\n",
      "iteration 57 current loss: 0.942798912525177 current acc: 0.5026\n",
      "iteration 58 current loss: 0.952818751335144 current acc: 0.5114\n",
      "iteration 59 current loss: 0.764870285987854 current acc: 0.5212\n",
      "iteration 60 current loss: 0.8931517004966736 current acc: 0.5298\n",
      "iteration 61 current loss: 1.0120790004730225 current acc: 0.5382\n",
      "iteration 62 current loss: 0.89688640832901 current acc: 0.547\n",
      "iteration 63 current loss: 1.0769026279449463 current acc: 0.5542\n",
      "iteration 64 current loss: 0.9757992029190063 current acc: 0.5618\n",
      "iteration 65 current loss: 1.0169471502304077 current acc: 0.5694\n",
      "iteration 66 current loss: 1.1796941757202148 current acc: 0.5756\n",
      "iteration 67 current loss: 0.7214492559432983 current acc: 0.5854\n",
      "iteration 68 current loss: 0.9064671397209167 current acc: 0.5942\n",
      "iteration 69 current loss: 0.9531166553497314 current acc: 0.6018\n",
      "iteration 70 current loss: 1.1315338611602783 current acc: 0.6086\n",
      "iteration 71 current loss: 1.1497230529785156 current acc: 0.6158\n",
      "iteration 72 current loss: 1.0323028564453125 current acc: 0.624\n",
      "iteration 73 current loss: 0.8287054896354675 current acc: 0.633\n",
      "iteration 74 current loss: 1.066765308380127 current acc: 0.6408\n",
      "iteration 75 current loss: 0.9790195822715759 current acc: 0.6492\n",
      "iteration 76 current loss: 1.0407767295837402 current acc: 0.6572\n",
      "iteration 77 current loss: 0.8301429748535156 current acc: 0.6658\n",
      "iteration 78 current loss: 1.2612842321395874 current acc: 0.6664\n",
      "\t\tTrain Epoch 84/100,Train Accuracy: 0.6664, Train Loss: 0.9463284151463569.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 84/100, Validation Accuracy: 0.583375, Validation Loss: 1.166219567298889\n",
      "iteration 0 current loss: 0.9220685362815857 current acc: 0.0082\n",
      "iteration 1 current loss: 1.0584229230880737 current acc: 0.0162\n",
      "iteration 2 current loss: 0.9143961071968079 current acc: 0.0248\n",
      "iteration 3 current loss: 0.948254406452179 current acc: 0.0336\n",
      "iteration 4 current loss: 1.005207896232605 current acc: 0.0422\n",
      "iteration 5 current loss: 0.9660338759422302 current acc: 0.0502\n",
      "iteration 6 current loss: 0.80129474401474 current acc: 0.0592\n",
      "iteration 7 current loss: 0.7798502445220947 current acc: 0.0682\n",
      "iteration 8 current loss: 0.7884620428085327 current acc: 0.0776\n",
      "iteration 9 current loss: 0.9009913206100464 current acc: 0.0864\n",
      "iteration 10 current loss: 0.950641930103302 current acc: 0.0946\n",
      "iteration 11 current loss: 0.7971468567848206 current acc: 0.104\n",
      "iteration 12 current loss: 1.0493292808532715 current acc: 0.1116\n",
      "iteration 13 current loss: 0.9948303699493408 current acc: 0.1198\n",
      "iteration 14 current loss: 0.8776042461395264 current acc: 0.1286\n",
      "iteration 15 current loss: 1.0748356580734253 current acc: 0.1364\n",
      "iteration 16 current loss: 0.9097394347190857 current acc: 0.145\n",
      "iteration 17 current loss: 0.9817463159561157 current acc: 0.153\n",
      "iteration 18 current loss: 0.9991862773895264 current acc: 0.1612\n",
      "iteration 19 current loss: 1.0263748168945312 current acc: 0.1688\n",
      "iteration 20 current loss: 1.105217456817627 current acc: 0.1764\n",
      "iteration 21 current loss: 0.93486487865448 current acc: 0.1848\n",
      "iteration 22 current loss: 0.9518261551856995 current acc: 0.193\n",
      "iteration 23 current loss: 0.9348709583282471 current acc: 0.2014\n",
      "iteration 24 current loss: 0.8587623834609985 current acc: 0.2102\n",
      "iteration 25 current loss: 0.8378127813339233 current acc: 0.2192\n",
      "iteration 26 current loss: 0.8651868104934692 current acc: 0.2286\n",
      "iteration 27 current loss: 1.0135071277618408 current acc: 0.2366\n",
      "iteration 28 current loss: 0.9823582768440247 current acc: 0.2448\n",
      "iteration 29 current loss: 0.9334756135940552 current acc: 0.2538\n",
      "iteration 30 current loss: 0.7542802095413208 current acc: 0.2632\n",
      "iteration 31 current loss: 1.0500110387802124 current acc: 0.2716\n",
      "iteration 32 current loss: 0.8299175500869751 current acc: 0.2806\n",
      "iteration 33 current loss: 1.3982925415039062 current acc: 0.2864\n",
      "iteration 34 current loss: 0.8841878175735474 current acc: 0.2948\n",
      "iteration 35 current loss: 0.9260610938072205 current acc: 0.3036\n",
      "iteration 36 current loss: 0.9685099720954895 current acc: 0.3122\n",
      "iteration 37 current loss: 0.7303673028945923 current acc: 0.3212\n",
      "iteration 38 current loss: 0.9866769313812256 current acc: 0.3294\n",
      "iteration 39 current loss: 0.9672195911407471 current acc: 0.338\n",
      "iteration 40 current loss: 1.0867700576782227 current acc: 0.3462\n",
      "iteration 41 current loss: 0.9031614661216736 current acc: 0.3544\n",
      "iteration 42 current loss: 0.9840018153190613 current acc: 0.3626\n",
      "iteration 43 current loss: 1.0035640001296997 current acc: 0.37\n",
      "iteration 44 current loss: 1.046558141708374 current acc: 0.377\n",
      "iteration 45 current loss: 1.1666078567504883 current acc: 0.3846\n",
      "iteration 46 current loss: 0.7845671772956848 current acc: 0.3938\n",
      "iteration 47 current loss: 0.7222124934196472 current acc: 0.404\n",
      "iteration 48 current loss: 0.9908178448677063 current acc: 0.4128\n",
      "iteration 49 current loss: 0.7594119310379028 current acc: 0.4224\n",
      "iteration 50 current loss: 1.0330678224563599 current acc: 0.4304\n",
      "iteration 51 current loss: 1.0509134531021118 current acc: 0.439\n",
      "iteration 52 current loss: 0.8962426781654358 current acc: 0.448\n",
      "iteration 53 current loss: 0.9578449130058289 current acc: 0.4564\n",
      "iteration 54 current loss: 0.7727970480918884 current acc: 0.4656\n",
      "iteration 55 current loss: 0.8469768166542053 current acc: 0.475\n",
      "iteration 56 current loss: 0.8796612024307251 current acc: 0.484\n",
      "iteration 57 current loss: 0.9177311062812805 current acc: 0.4924\n",
      "iteration 58 current loss: 0.8903622627258301 current acc: 0.5006\n",
      "iteration 59 current loss: 1.1065515279769897 current acc: 0.5082\n",
      "iteration 60 current loss: 1.036088466644287 current acc: 0.5164\n",
      "iteration 61 current loss: 0.9065314531326294 current acc: 0.5248\n",
      "iteration 62 current loss: 0.8334774374961853 current acc: 0.5342\n",
      "iteration 63 current loss: 0.8637662529945374 current acc: 0.5434\n",
      "iteration 64 current loss: 1.0642986297607422 current acc: 0.5512\n",
      "iteration 65 current loss: 0.8685144782066345 current acc: 0.5604\n",
      "iteration 66 current loss: 0.9527499675750732 current acc: 0.5694\n",
      "iteration 67 current loss: 0.8237519264221191 current acc: 0.5796\n",
      "iteration 68 current loss: 1.0552040338516235 current acc: 0.5874\n",
      "iteration 69 current loss: 0.9665964841842651 current acc: 0.5956\n",
      "iteration 70 current loss: 0.9290627241134644 current acc: 0.6046\n",
      "iteration 71 current loss: 1.1003292798995972 current acc: 0.6128\n",
      "iteration 72 current loss: 1.1635007858276367 current acc: 0.6206\n",
      "iteration 73 current loss: 0.9822296500205994 current acc: 0.6296\n",
      "iteration 74 current loss: 1.1645028591156006 current acc: 0.6368\n",
      "iteration 75 current loss: 1.0690433979034424 current acc: 0.6446\n",
      "iteration 76 current loss: 0.9194953441619873 current acc: 0.6532\n",
      "iteration 77 current loss: 0.9820128083229065 current acc: 0.6624\n",
      "iteration 78 current loss: 0.7601836323738098 current acc: 0.6636\n",
      "\t\tTrain Epoch 85/100,Train Accuracy: 0.6636, Train Loss: 0.9481137342090848.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 85/100, Validation Accuracy: 0.592625, Validation Loss: 1.1459228925704956\n",
      "iteration 0 current loss: 0.7064908146858215 current acc: 0.0096\n",
      "iteration 1 current loss: 0.7498968839645386 current acc: 0.0194\n",
      "iteration 2 current loss: 1.030552625656128 current acc: 0.027\n",
      "iteration 3 current loss: 0.9733420610427856 current acc: 0.036\n",
      "iteration 4 current loss: 0.828175961971283 current acc: 0.0454\n",
      "iteration 5 current loss: 0.9558981657028198 current acc: 0.054\n",
      "iteration 6 current loss: 1.060476303100586 current acc: 0.0626\n",
      "iteration 7 current loss: 0.9946154952049255 current acc: 0.071\n",
      "iteration 8 current loss: 0.8839127421379089 current acc: 0.0808\n",
      "iteration 9 current loss: 0.8341056704521179 current acc: 0.0902\n",
      "iteration 10 current loss: 1.032719612121582 current acc: 0.0994\n",
      "iteration 11 current loss: 1.079024314880371 current acc: 0.1076\n",
      "iteration 12 current loss: 0.8386356234550476 current acc: 0.117\n",
      "iteration 13 current loss: 0.9430401921272278 current acc: 0.125\n",
      "iteration 14 current loss: 0.8627012968063354 current acc: 0.1344\n",
      "iteration 15 current loss: 0.8441813588142395 current acc: 0.1432\n",
      "iteration 16 current loss: 0.8803447484970093 current acc: 0.1516\n",
      "iteration 17 current loss: 0.8860595226287842 current acc: 0.1604\n",
      "iteration 18 current loss: 0.916346549987793 current acc: 0.1692\n",
      "iteration 19 current loss: 0.8020356893539429 current acc: 0.1788\n",
      "iteration 20 current loss: 0.9463877081871033 current acc: 0.1866\n",
      "iteration 21 current loss: 0.995429515838623 current acc: 0.1948\n",
      "iteration 22 current loss: 0.8831568956375122 current acc: 0.2034\n",
      "iteration 23 current loss: 1.0088247060775757 current acc: 0.2114\n",
      "iteration 24 current loss: 0.8353984355926514 current acc: 0.2204\n",
      "iteration 25 current loss: 1.105002760887146 current acc: 0.229\n",
      "iteration 26 current loss: 0.929081916809082 current acc: 0.2376\n",
      "iteration 27 current loss: 0.9924903512001038 current acc: 0.2456\n",
      "iteration 28 current loss: 0.8685083389282227 current acc: 0.255\n",
      "iteration 29 current loss: 0.8071261644363403 current acc: 0.2638\n",
      "iteration 30 current loss: 0.8450168967247009 current acc: 0.2726\n",
      "iteration 31 current loss: 0.8962984681129456 current acc: 0.2814\n",
      "iteration 32 current loss: 1.03783118724823 current acc: 0.2894\n",
      "iteration 33 current loss: 0.9250296354293823 current acc: 0.2978\n",
      "iteration 34 current loss: 0.9495124220848083 current acc: 0.3064\n",
      "iteration 35 current loss: 0.8219038248062134 current acc: 0.3156\n",
      "iteration 36 current loss: 0.9264211058616638 current acc: 0.3238\n",
      "iteration 37 current loss: 1.0701212882995605 current acc: 0.3318\n",
      "iteration 38 current loss: 1.0483582019805908 current acc: 0.3396\n",
      "iteration 39 current loss: 0.946722149848938 current acc: 0.348\n",
      "iteration 40 current loss: 0.8602156043052673 current acc: 0.3566\n",
      "iteration 41 current loss: 0.9126238226890564 current acc: 0.365\n",
      "iteration 42 current loss: 0.8684994578361511 current acc: 0.374\n",
      "iteration 43 current loss: 0.9265778064727783 current acc: 0.3826\n",
      "iteration 44 current loss: 0.8705801367759705 current acc: 0.3916\n",
      "iteration 45 current loss: 0.9390679597854614 current acc: 0.4016\n",
      "iteration 46 current loss: 1.147055983543396 current acc: 0.4088\n",
      "iteration 47 current loss: 1.0612926483154297 current acc: 0.4166\n",
      "iteration 48 current loss: 0.9946557879447937 current acc: 0.4256\n",
      "iteration 49 current loss: 0.8823986649513245 current acc: 0.4348\n",
      "iteration 50 current loss: 0.8159547448158264 current acc: 0.4444\n",
      "iteration 51 current loss: 0.8450567722320557 current acc: 0.4534\n",
      "iteration 52 current loss: 0.9825880527496338 current acc: 0.4608\n",
      "iteration 53 current loss: 1.2115484476089478 current acc: 0.468\n",
      "iteration 54 current loss: 0.9593316912651062 current acc: 0.4764\n",
      "iteration 55 current loss: 0.9876595735549927 current acc: 0.484\n",
      "iteration 56 current loss: 1.1188597679138184 current acc: 0.492\n",
      "iteration 57 current loss: 1.1300849914550781 current acc: 0.4992\n",
      "iteration 58 current loss: 1.0105153322219849 current acc: 0.5072\n",
      "iteration 59 current loss: 0.8790318965911865 current acc: 0.5156\n",
      "iteration 60 current loss: 0.9332695603370667 current acc: 0.5244\n",
      "iteration 61 current loss: 0.9658554792404175 current acc: 0.5328\n",
      "iteration 62 current loss: 0.9808045625686646 current acc: 0.5402\n",
      "iteration 63 current loss: 0.8184674382209778 current acc: 0.5504\n",
      "iteration 64 current loss: 0.9198103547096252 current acc: 0.56\n",
      "iteration 65 current loss: 1.0134913921356201 current acc: 0.5684\n",
      "iteration 66 current loss: 0.9877492785453796 current acc: 0.5762\n",
      "iteration 67 current loss: 0.7636023759841919 current acc: 0.5866\n",
      "iteration 68 current loss: 1.0680632591247559 current acc: 0.5944\n",
      "iteration 69 current loss: 0.9261458516120911 current acc: 0.603\n",
      "iteration 70 current loss: 0.8185620903968811 current acc: 0.6128\n",
      "iteration 71 current loss: 0.9461506605148315 current acc: 0.6218\n",
      "iteration 72 current loss: 1.0287303924560547 current acc: 0.6298\n",
      "iteration 73 current loss: 1.1102614402770996 current acc: 0.6376\n",
      "iteration 74 current loss: 0.9111136198043823 current acc: 0.6466\n",
      "iteration 75 current loss: 0.7704047560691833 current acc: 0.6562\n",
      "iteration 76 current loss: 0.946565568447113 current acc: 0.6644\n",
      "iteration 77 current loss: 1.055978775024414 current acc: 0.6726\n",
      "iteration 78 current loss: 1.5367367267608643 current acc: 0.6732\n",
      "\t\tTrain Epoch 86/100,Train Accuracy: 0.6732, Train Loss: 0.9474245611625381.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 86/100, Validation Accuracy: 0.55275, Validation Loss: 1.2724240880012512\n",
      "iteration 0 current loss: 0.635116696357727 current acc: 0.0104\n",
      "iteration 1 current loss: 0.9580467939376831 current acc: 0.0192\n",
      "iteration 2 current loss: 1.0496091842651367 current acc: 0.0274\n",
      "iteration 3 current loss: 1.2020090818405151 current acc: 0.0342\n",
      "iteration 4 current loss: 0.9447322487831116 current acc: 0.0424\n",
      "iteration 5 current loss: 1.2092560529708862 current acc: 0.0502\n",
      "iteration 6 current loss: 0.9044480919837952 current acc: 0.0594\n",
      "iteration 7 current loss: 1.1083323955535889 current acc: 0.0668\n",
      "iteration 8 current loss: 1.0113123655319214 current acc: 0.075\n",
      "iteration 9 current loss: 1.1134005784988403 current acc: 0.083\n",
      "iteration 10 current loss: 0.9377304911613464 current acc: 0.0912\n",
      "iteration 11 current loss: 1.1169989109039307 current acc: 0.0982\n",
      "iteration 12 current loss: 1.138295292854309 current acc: 0.1054\n",
      "iteration 13 current loss: 1.1871405839920044 current acc: 0.1126\n",
      "iteration 14 current loss: 1.0307387113571167 current acc: 0.1208\n",
      "iteration 15 current loss: 0.8850586414337158 current acc: 0.1298\n",
      "iteration 16 current loss: 0.9297922253608704 current acc: 0.138\n",
      "iteration 17 current loss: 0.9997223615646362 current acc: 0.1462\n",
      "iteration 18 current loss: 0.8887187242507935 current acc: 0.1554\n",
      "iteration 19 current loss: 0.9637759327888489 current acc: 0.1638\n",
      "iteration 20 current loss: 0.8573864102363586 current acc: 0.173\n",
      "iteration 21 current loss: 1.0465633869171143 current acc: 0.1806\n",
      "iteration 22 current loss: 0.9735139608383179 current acc: 0.1892\n",
      "iteration 23 current loss: 0.871614933013916 current acc: 0.1982\n",
      "iteration 24 current loss: 0.8104958534240723 current acc: 0.2074\n",
      "iteration 25 current loss: 0.721311092376709 current acc: 0.2178\n",
      "iteration 26 current loss: 0.995530903339386 current acc: 0.226\n",
      "iteration 27 current loss: 0.9094110131263733 current acc: 0.2346\n",
      "iteration 28 current loss: 0.9931719899177551 current acc: 0.2422\n",
      "iteration 29 current loss: 0.7659214735031128 current acc: 0.2522\n",
      "iteration 30 current loss: 1.0502705574035645 current acc: 0.2596\n",
      "iteration 31 current loss: 1.014371633529663 current acc: 0.2666\n",
      "iteration 32 current loss: 0.7090789079666138 current acc: 0.2758\n",
      "iteration 33 current loss: 1.002644419670105 current acc: 0.2842\n",
      "iteration 34 current loss: 0.9431290030479431 current acc: 0.2926\n",
      "iteration 35 current loss: 1.10306715965271 current acc: 0.2998\n",
      "iteration 36 current loss: 1.02457594871521 current acc: 0.3082\n",
      "iteration 37 current loss: 0.8800371885299683 current acc: 0.3168\n",
      "iteration 38 current loss: 1.0317835807800293 current acc: 0.3252\n",
      "iteration 39 current loss: 1.1535083055496216 current acc: 0.3322\n",
      "iteration 40 current loss: 1.050205111503601 current acc: 0.341\n",
      "iteration 41 current loss: 0.9289226531982422 current acc: 0.3498\n",
      "iteration 42 current loss: 0.7796424627304077 current acc: 0.3588\n",
      "iteration 43 current loss: 1.173421025276184 current acc: 0.3664\n",
      "iteration 44 current loss: 1.0020641088485718 current acc: 0.3742\n",
      "iteration 45 current loss: 1.0006741285324097 current acc: 0.3828\n",
      "iteration 46 current loss: 0.9960035085678101 current acc: 0.3912\n",
      "iteration 47 current loss: 1.1298459768295288 current acc: 0.3986\n",
      "iteration 48 current loss: 1.0931164026260376 current acc: 0.4062\n",
      "iteration 49 current loss: 1.0024458169937134 current acc: 0.415\n",
      "iteration 50 current loss: 0.8700509667396545 current acc: 0.4234\n",
      "iteration 51 current loss: 1.0171988010406494 current acc: 0.4312\n",
      "iteration 52 current loss: 0.9290369749069214 current acc: 0.4402\n",
      "iteration 53 current loss: 0.933521032333374 current acc: 0.449\n",
      "iteration 54 current loss: 1.1143866777420044 current acc: 0.456\n",
      "iteration 55 current loss: 0.8870948553085327 current acc: 0.4652\n",
      "iteration 56 current loss: 1.126024842262268 current acc: 0.473\n",
      "iteration 57 current loss: 0.9989453554153442 current acc: 0.4806\n",
      "iteration 58 current loss: 1.0221608877182007 current acc: 0.4896\n",
      "iteration 59 current loss: 0.9230088591575623 current acc: 0.4972\n",
      "iteration 60 current loss: 0.9216412901878357 current acc: 0.5058\n",
      "iteration 61 current loss: 0.8858972787857056 current acc: 0.5156\n",
      "iteration 62 current loss: 0.9875187277793884 current acc: 0.5238\n",
      "iteration 63 current loss: 0.8774051070213318 current acc: 0.5326\n",
      "iteration 64 current loss: 1.0204284191131592 current acc: 0.5404\n",
      "iteration 65 current loss: 0.9110867977142334 current acc: 0.5496\n",
      "iteration 66 current loss: 1.057418942451477 current acc: 0.5574\n",
      "iteration 67 current loss: 1.0074149370193481 current acc: 0.5656\n",
      "iteration 68 current loss: 1.0232594013214111 current acc: 0.5744\n",
      "iteration 69 current loss: 1.0099631547927856 current acc: 0.5826\n",
      "iteration 70 current loss: 1.0512921810150146 current acc: 0.5912\n",
      "iteration 71 current loss: 1.1658798456192017 current acc: 0.598\n",
      "iteration 72 current loss: 1.1210899353027344 current acc: 0.6068\n",
      "iteration 73 current loss: 0.927225649356842 current acc: 0.6156\n",
      "iteration 74 current loss: 0.9950286746025085 current acc: 0.6236\n",
      "iteration 75 current loss: 0.9203912019729614 current acc: 0.633\n",
      "iteration 76 current loss: 0.9374526739120483 current acc: 0.642\n",
      "iteration 77 current loss: 0.8364884257316589 current acc: 0.6514\n",
      "iteration 78 current loss: 1.4658223390579224 current acc: 0.6518\n",
      "\t\tTrain Epoch 87/100,Train Accuracy: 0.6518, Train Loss: 0.989520234397695.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 87/100, Validation Accuracy: 0.5705, Validation Loss: 1.19726687335968\n",
      "iteration 0 current loss: 0.7555567622184753 current acc: 0.01\n",
      "iteration 1 current loss: 0.8988549113273621 current acc: 0.019\n",
      "iteration 2 current loss: 0.9044762849807739 current acc: 0.028\n",
      "iteration 3 current loss: 1.0115187168121338 current acc: 0.0358\n",
      "iteration 4 current loss: 1.2213443517684937 current acc: 0.0432\n",
      "iteration 5 current loss: 0.9692075252532959 current acc: 0.052\n",
      "iteration 6 current loss: 0.8799718022346497 current acc: 0.0604\n",
      "iteration 7 current loss: 0.9940659999847412 current acc: 0.0692\n",
      "iteration 8 current loss: 0.9790133237838745 current acc: 0.0766\n",
      "iteration 9 current loss: 1.0374263525009155 current acc: 0.0848\n",
      "iteration 10 current loss: 0.9666445851325989 current acc: 0.093\n",
      "iteration 11 current loss: 1.2417871952056885 current acc: 0.1\n",
      "iteration 12 current loss: 0.8818725347518921 current acc: 0.1086\n",
      "iteration 13 current loss: 1.1483973264694214 current acc: 0.1172\n",
      "iteration 14 current loss: 1.0523004531860352 current acc: 0.1246\n",
      "iteration 15 current loss: 0.9704543948173523 current acc: 0.1328\n",
      "iteration 16 current loss: 1.1775380373001099 current acc: 0.1406\n",
      "iteration 17 current loss: 0.5961803197860718 current acc: 0.1512\n",
      "iteration 18 current loss: 0.8720051050186157 current acc: 0.1596\n",
      "iteration 19 current loss: 0.876501739025116 current acc: 0.1688\n",
      "iteration 20 current loss: 0.9078705906867981 current acc: 0.1772\n",
      "iteration 21 current loss: 0.6764942407608032 current acc: 0.1876\n",
      "iteration 22 current loss: 0.7375117540359497 current acc: 0.1972\n",
      "iteration 23 current loss: 0.9302107095718384 current acc: 0.2066\n",
      "iteration 24 current loss: 1.0937176942825317 current acc: 0.2138\n",
      "iteration 25 current loss: 0.885334849357605 current acc: 0.2232\n",
      "iteration 26 current loss: 1.0322380065917969 current acc: 0.2318\n",
      "iteration 27 current loss: 0.9184682965278625 current acc: 0.2406\n",
      "iteration 28 current loss: 0.748050332069397 current acc: 0.2496\n",
      "iteration 29 current loss: 1.0870718955993652 current acc: 0.2572\n",
      "iteration 30 current loss: 1.0097657442092896 current acc: 0.2656\n",
      "iteration 31 current loss: 0.967283308506012 current acc: 0.2748\n",
      "iteration 32 current loss: 0.9525352120399475 current acc: 0.2836\n",
      "iteration 33 current loss: 0.8668290972709656 current acc: 0.2928\n",
      "iteration 34 current loss: 0.9903513193130493 current acc: 0.3022\n",
      "iteration 35 current loss: 0.922571063041687 current acc: 0.311\n",
      "iteration 36 current loss: 1.1739237308502197 current acc: 0.3182\n",
      "iteration 37 current loss: 0.9453713893890381 current acc: 0.3268\n",
      "iteration 38 current loss: 0.8843604326248169 current acc: 0.3356\n",
      "iteration 39 current loss: 0.9156834483146667 current acc: 0.3446\n",
      "iteration 40 current loss: 1.0503649711608887 current acc: 0.3528\n",
      "iteration 41 current loss: 0.9091387391090393 current acc: 0.3622\n",
      "iteration 42 current loss: 0.8768584132194519 current acc: 0.3712\n",
      "iteration 43 current loss: 0.9666988849639893 current acc: 0.38\n",
      "iteration 44 current loss: 0.9917107820510864 current acc: 0.3892\n",
      "iteration 45 current loss: 1.1040693521499634 current acc: 0.396\n",
      "iteration 46 current loss: 0.8769432306289673 current acc: 0.4052\n",
      "iteration 47 current loss: 0.9203442931175232 current acc: 0.4134\n",
      "iteration 48 current loss: 1.1506510972976685 current acc: 0.4214\n",
      "iteration 49 current loss: 0.9919431805610657 current acc: 0.4296\n",
      "iteration 50 current loss: 0.6648766994476318 current acc: 0.4404\n",
      "iteration 51 current loss: 0.8505025506019592 current acc: 0.4492\n",
      "iteration 52 current loss: 1.0195196866989136 current acc: 0.458\n",
      "iteration 53 current loss: 0.9195875525474548 current acc: 0.4672\n",
      "iteration 54 current loss: 0.9534890055656433 current acc: 0.4758\n",
      "iteration 55 current loss: 0.9681873321533203 current acc: 0.4848\n",
      "iteration 56 current loss: 0.9767491817474365 current acc: 0.4932\n",
      "iteration 57 current loss: 0.9252799153327942 current acc: 0.502\n",
      "iteration 58 current loss: 1.106493353843689 current acc: 0.509\n",
      "iteration 59 current loss: 0.7792285084724426 current acc: 0.5186\n",
      "iteration 60 current loss: 0.9574798345565796 current acc: 0.5268\n",
      "iteration 61 current loss: 1.09872567653656 current acc: 0.535\n",
      "iteration 62 current loss: 1.1627358198165894 current acc: 0.5416\n",
      "iteration 63 current loss: 1.03385591506958 current acc: 0.5492\n",
      "iteration 64 current loss: 0.8150344491004944 current acc: 0.5586\n",
      "iteration 65 current loss: 0.8800978064537048 current acc: 0.5672\n",
      "iteration 66 current loss: 0.7619137763977051 current acc: 0.577\n",
      "iteration 67 current loss: 1.1162559986114502 current acc: 0.5852\n",
      "iteration 68 current loss: 1.1213196516036987 current acc: 0.5932\n",
      "iteration 69 current loss: 1.2863527536392212 current acc: 0.5998\n",
      "iteration 70 current loss: 0.9265720844268799 current acc: 0.6086\n",
      "iteration 71 current loss: 0.7744887471199036 current acc: 0.6186\n",
      "iteration 72 current loss: 0.931125283241272 current acc: 0.6266\n",
      "iteration 73 current loss: 1.131142020225525 current acc: 0.6336\n",
      "iteration 74 current loss: 1.2805157899856567 current acc: 0.6398\n",
      "iteration 75 current loss: 0.9998912811279297 current acc: 0.648\n",
      "iteration 76 current loss: 0.863040030002594 current acc: 0.657\n",
      "iteration 77 current loss: 0.8937565088272095 current acc: 0.6654\n",
      "iteration 78 current loss: 0.9825353026390076 current acc: 0.6664\n",
      "\t\tTrain Epoch 88/100,Train Accuracy: 0.6664, Train Loss: 0.9632941303373892.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 88/100, Validation Accuracy: 0.583625, Validation Loss: 1.161243055343628\n",
      "iteration 0 current loss: 0.9333541393280029 current acc: 0.0082\n",
      "iteration 1 current loss: 0.8069913387298584 current acc: 0.018\n",
      "iteration 2 current loss: 0.7112992405891418 current acc: 0.0278\n",
      "iteration 3 current loss: 1.0118215084075928 current acc: 0.0364\n",
      "iteration 4 current loss: 1.0217853784561157 current acc: 0.0444\n",
      "iteration 5 current loss: 0.9075707197189331 current acc: 0.053\n",
      "iteration 6 current loss: 1.1269021034240723 current acc: 0.0602\n",
      "iteration 7 current loss: 1.0424069166183472 current acc: 0.0688\n",
      "iteration 8 current loss: 0.9676117897033691 current acc: 0.0768\n",
      "iteration 9 current loss: 0.7859618663787842 current acc: 0.0858\n",
      "iteration 10 current loss: 0.8144870400428772 current acc: 0.0956\n",
      "iteration 11 current loss: 1.0321481227874756 current acc: 0.1032\n",
      "iteration 12 current loss: 0.9107137322425842 current acc: 0.112\n",
      "iteration 13 current loss: 0.8992524147033691 current acc: 0.12\n",
      "iteration 14 current loss: 1.0898431539535522 current acc: 0.1264\n",
      "iteration 15 current loss: 0.975018322467804 current acc: 0.1348\n",
      "iteration 16 current loss: 0.8686339855194092 current acc: 0.143\n",
      "iteration 17 current loss: 0.9585987329483032 current acc: 0.1514\n",
      "iteration 18 current loss: 0.9514124393463135 current acc: 0.1598\n",
      "iteration 19 current loss: 0.8879109025001526 current acc: 0.1684\n",
      "iteration 20 current loss: 0.9646607637405396 current acc: 0.1758\n",
      "iteration 21 current loss: 1.0170912742614746 current acc: 0.1838\n",
      "iteration 22 current loss: 0.7564281821250916 current acc: 0.1936\n",
      "iteration 23 current loss: 0.7408549189567566 current acc: 0.2032\n",
      "iteration 24 current loss: 0.760347843170166 current acc: 0.2128\n",
      "iteration 25 current loss: 0.8204360604286194 current acc: 0.2222\n",
      "iteration 26 current loss: 0.7871396541595459 current acc: 0.231\n",
      "iteration 27 current loss: 0.8333938717842102 current acc: 0.2402\n",
      "iteration 28 current loss: 0.8668473958969116 current acc: 0.2494\n",
      "iteration 29 current loss: 1.0417908430099487 current acc: 0.2574\n",
      "iteration 30 current loss: 0.8955147862434387 current acc: 0.265\n",
      "iteration 31 current loss: 0.818321168422699 current acc: 0.2744\n",
      "iteration 32 current loss: 1.0445321798324585 current acc: 0.2824\n",
      "iteration 33 current loss: 0.978894054889679 current acc: 0.2912\n",
      "iteration 34 current loss: 0.9162624478340149 current acc: 0.3002\n",
      "iteration 35 current loss: 1.012307047843933 current acc: 0.3094\n",
      "iteration 36 current loss: 1.3614451885223389 current acc: 0.3156\n",
      "iteration 37 current loss: 0.9434175491333008 current acc: 0.3246\n",
      "iteration 38 current loss: 0.8491056561470032 current acc: 0.3336\n",
      "iteration 39 current loss: 0.9584994912147522 current acc: 0.3414\n",
      "iteration 40 current loss: 0.9219148755073547 current acc: 0.3498\n",
      "iteration 41 current loss: 1.1589566469192505 current acc: 0.3576\n",
      "iteration 42 current loss: 0.7865479588508606 current acc: 0.3674\n",
      "iteration 43 current loss: 0.837812066078186 current acc: 0.3768\n",
      "iteration 44 current loss: 0.9933599233627319 current acc: 0.3852\n",
      "iteration 45 current loss: 1.0324469804763794 current acc: 0.3932\n",
      "iteration 46 current loss: 0.8379449844360352 current acc: 0.4026\n",
      "iteration 47 current loss: 0.9977608323097229 current acc: 0.4108\n",
      "iteration 48 current loss: 0.7780624628067017 current acc: 0.4206\n",
      "iteration 49 current loss: 1.0278503894805908 current acc: 0.4286\n",
      "iteration 50 current loss: 0.9660138487815857 current acc: 0.4366\n",
      "iteration 51 current loss: 0.8560310006141663 current acc: 0.446\n",
      "iteration 52 current loss: 1.0056538581848145 current acc: 0.4536\n",
      "iteration 53 current loss: 0.7980796694755554 current acc: 0.4634\n",
      "iteration 54 current loss: 1.0328987836837769 current acc: 0.4718\n",
      "iteration 55 current loss: 0.7783020734786987 current acc: 0.4812\n",
      "iteration 56 current loss: 0.9786736369132996 current acc: 0.4892\n",
      "iteration 57 current loss: 1.0342015027999878 current acc: 0.4972\n",
      "iteration 58 current loss: 0.7573615312576294 current acc: 0.5068\n",
      "iteration 59 current loss: 1.1329174041748047 current acc: 0.514\n",
      "iteration 60 current loss: 1.0786691904067993 current acc: 0.5224\n",
      "iteration 61 current loss: 1.2046387195587158 current acc: 0.53\n",
      "iteration 62 current loss: 1.1420552730560303 current acc: 0.5368\n",
      "iteration 63 current loss: 0.985730767250061 current acc: 0.5456\n",
      "iteration 64 current loss: 0.8206787109375 current acc: 0.5552\n",
      "iteration 65 current loss: 0.9519113302230835 current acc: 0.5636\n",
      "iteration 66 current loss: 1.0849469900131226 current acc: 0.5716\n",
      "iteration 67 current loss: 0.9964460730552673 current acc: 0.5792\n",
      "iteration 68 current loss: 0.8143714666366577 current acc: 0.5878\n",
      "iteration 69 current loss: 0.9655400514602661 current acc: 0.5966\n",
      "iteration 70 current loss: 0.9375524520874023 current acc: 0.6054\n",
      "iteration 71 current loss: 1.0787490606307983 current acc: 0.6134\n",
      "iteration 72 current loss: 0.9665111899375916 current acc: 0.6212\n",
      "iteration 73 current loss: 0.9262306690216064 current acc: 0.6304\n",
      "iteration 74 current loss: 1.0177799463272095 current acc: 0.6386\n",
      "iteration 75 current loss: 1.070737600326538 current acc: 0.6466\n",
      "iteration 76 current loss: 0.8209538459777832 current acc: 0.6552\n",
      "iteration 77 current loss: 1.0059343576431274 current acc: 0.6632\n",
      "iteration 78 current loss: 1.1295135021209717 current acc: 0.6642\n",
      "\t\tTrain Epoch 89/100,Train Accuracy: 0.6642, Train Loss: 0.9466171626803241.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 89/100, Validation Accuracy: 0.549125, Validation Loss: 1.2527766098976136\n",
      "iteration 0 current loss: 0.8589076399803162 current acc: 0.009\n",
      "iteration 1 current loss: 0.9628427624702454 current acc: 0.0182\n",
      "iteration 2 current loss: 0.8782495856285095 current acc: 0.027\n",
      "iteration 3 current loss: 1.1042982339859009 current acc: 0.0344\n",
      "iteration 4 current loss: 0.7572677731513977 current acc: 0.0438\n",
      "iteration 5 current loss: 0.8825258612632751 current acc: 0.0522\n",
      "iteration 6 current loss: 0.986324667930603 current acc: 0.0606\n",
      "iteration 7 current loss: 1.0264908075332642 current acc: 0.069\n",
      "iteration 8 current loss: 1.0024553537368774 current acc: 0.0774\n",
      "iteration 9 current loss: 1.1210908889770508 current acc: 0.084\n",
      "iteration 10 current loss: 1.0782012939453125 current acc: 0.0916\n",
      "iteration 11 current loss: 1.036974310874939 current acc: 0.0998\n",
      "iteration 12 current loss: 1.0792570114135742 current acc: 0.1074\n",
      "iteration 13 current loss: 0.886236310005188 current acc: 0.1174\n",
      "iteration 14 current loss: 1.0361669063568115 current acc: 0.1254\n",
      "iteration 15 current loss: 0.8738716840744019 current acc: 0.1346\n",
      "iteration 16 current loss: 0.9398924708366394 current acc: 0.1432\n",
      "iteration 17 current loss: 0.7312312126159668 current acc: 0.1536\n",
      "iteration 18 current loss: 0.9825091361999512 current acc: 0.1624\n",
      "iteration 19 current loss: 1.088315725326538 current acc: 0.1708\n",
      "iteration 20 current loss: 0.9213921427726746 current acc: 0.1798\n",
      "iteration 21 current loss: 0.9305704236030579 current acc: 0.189\n",
      "iteration 22 current loss: 0.9007322192192078 current acc: 0.1978\n",
      "iteration 23 current loss: 1.0371860265731812 current acc: 0.205\n",
      "iteration 24 current loss: 0.8032140731811523 current acc: 0.214\n",
      "iteration 25 current loss: 0.8820345997810364 current acc: 0.2234\n",
      "iteration 26 current loss: 0.9819004535675049 current acc: 0.2316\n",
      "iteration 27 current loss: 0.8137807846069336 current acc: 0.2414\n",
      "iteration 28 current loss: 0.9250783324241638 current acc: 0.2516\n",
      "iteration 29 current loss: 1.1824650764465332 current acc: 0.2592\n",
      "iteration 30 current loss: 0.731859564781189 current acc: 0.2684\n",
      "iteration 31 current loss: 0.7473101615905762 current acc: 0.2778\n",
      "iteration 32 current loss: 0.9243623614311218 current acc: 0.2868\n",
      "iteration 33 current loss: 0.8718603849411011 current acc: 0.296\n",
      "iteration 34 current loss: 0.852464497089386 current acc: 0.3058\n",
      "iteration 35 current loss: 0.9921492338180542 current acc: 0.3144\n",
      "iteration 36 current loss: 0.8163883090019226 current acc: 0.3236\n",
      "iteration 37 current loss: 0.8068799376487732 current acc: 0.333\n",
      "iteration 38 current loss: 1.161155104637146 current acc: 0.3404\n",
      "iteration 39 current loss: 0.8485440015792847 current acc: 0.3494\n",
      "iteration 40 current loss: 0.9527018070220947 current acc: 0.3578\n",
      "iteration 41 current loss: 0.9129475951194763 current acc: 0.3658\n",
      "iteration 42 current loss: 0.9971774816513062 current acc: 0.373\n",
      "iteration 43 current loss: 0.9765087366104126 current acc: 0.381\n",
      "iteration 44 current loss: 1.4426922798156738 current acc: 0.3872\n",
      "iteration 45 current loss: 0.960121214389801 current acc: 0.3952\n",
      "iteration 46 current loss: 0.886107861995697 current acc: 0.4038\n",
      "iteration 47 current loss: 1.0637058019638062 current acc: 0.4118\n",
      "iteration 48 current loss: 0.9543705582618713 current acc: 0.4206\n",
      "iteration 49 current loss: 0.9902483224868774 current acc: 0.4284\n",
      "iteration 50 current loss: 0.9870216846466064 current acc: 0.4372\n",
      "iteration 51 current loss: 1.03180730342865 current acc: 0.4458\n",
      "iteration 52 current loss: 0.9543765783309937 current acc: 0.455\n",
      "iteration 53 current loss: 1.1029139757156372 current acc: 0.4622\n",
      "iteration 54 current loss: 0.8579436540603638 current acc: 0.4716\n",
      "iteration 55 current loss: 0.8836010694503784 current acc: 0.4802\n",
      "iteration 56 current loss: 0.9375515580177307 current acc: 0.4888\n",
      "iteration 57 current loss: 0.8616850972175598 current acc: 0.4982\n",
      "iteration 58 current loss: 1.0173511505126953 current acc: 0.5064\n",
      "iteration 59 current loss: 1.0193499326705933 current acc: 0.5144\n",
      "iteration 60 current loss: 1.0886393785476685 current acc: 0.5214\n",
      "iteration 61 current loss: 1.1380919218063354 current acc: 0.5288\n",
      "iteration 62 current loss: 1.179492712020874 current acc: 0.5364\n",
      "iteration 63 current loss: 0.8614099025726318 current acc: 0.5456\n",
      "iteration 64 current loss: 0.8708108067512512 current acc: 0.5542\n",
      "iteration 65 current loss: 0.9008580446243286 current acc: 0.5626\n",
      "iteration 66 current loss: 1.0908634662628174 current acc: 0.5696\n",
      "iteration 67 current loss: 1.0009719133377075 current acc: 0.5776\n",
      "iteration 68 current loss: 1.0399421453475952 current acc: 0.5858\n",
      "iteration 69 current loss: 1.247056484222412 current acc: 0.5916\n",
      "iteration 70 current loss: 1.052542805671692 current acc: 0.5994\n",
      "iteration 71 current loss: 1.0596704483032227 current acc: 0.6076\n",
      "iteration 72 current loss: 1.0598511695861816 current acc: 0.6164\n",
      "iteration 73 current loss: 0.8554560542106628 current acc: 0.6252\n",
      "iteration 74 current loss: 1.0332202911376953 current acc: 0.6336\n",
      "iteration 75 current loss: 0.9508958458900452 current acc: 0.6412\n",
      "iteration 76 current loss: 0.95937180519104 current acc: 0.6492\n",
      "iteration 77 current loss: 0.8808929920196533 current acc: 0.6578\n",
      "iteration 78 current loss: 1.3543578386306763 current acc: 0.6584\n",
      "\t\tTrain Epoch 90/100,Train Accuracy: 0.6584, Train Loss: 0.9728989495506769.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 90/100, Validation Accuracy: 0.553875, Validation Loss: 1.2847500123977662\n",
      "iteration 0 current loss: 1.0531058311462402 current acc: 0.0072\n",
      "iteration 1 current loss: 0.8464622497558594 current acc: 0.0156\n",
      "iteration 2 current loss: 0.9588836431503296 current acc: 0.0238\n",
      "iteration 3 current loss: 0.8539328575134277 current acc: 0.0324\n",
      "iteration 4 current loss: 0.9449269771575928 current acc: 0.0414\n",
      "iteration 5 current loss: 1.0153878927230835 current acc: 0.0502\n",
      "iteration 6 current loss: 1.1067750453948975 current acc: 0.0584\n",
      "iteration 7 current loss: 0.879568874835968 current acc: 0.0682\n",
      "iteration 8 current loss: 1.0575320720672607 current acc: 0.0756\n",
      "iteration 9 current loss: 1.0893505811691284 current acc: 0.0836\n",
      "iteration 10 current loss: 1.0294355154037476 current acc: 0.0922\n",
      "iteration 11 current loss: 0.7157519459724426 current acc: 0.102\n",
      "iteration 12 current loss: 0.9834104776382446 current acc: 0.1098\n",
      "iteration 13 current loss: 0.8590344786643982 current acc: 0.1186\n",
      "iteration 14 current loss: 0.9327074885368347 current acc: 0.127\n",
      "iteration 15 current loss: 1.0305854082107544 current acc: 0.1352\n",
      "iteration 16 current loss: 1.1270511150360107 current acc: 0.1432\n",
      "iteration 17 current loss: 0.9279706478118896 current acc: 0.1522\n",
      "iteration 18 current loss: 1.0844393968582153 current acc: 0.1602\n",
      "iteration 19 current loss: 1.0054244995117188 current acc: 0.1682\n",
      "iteration 20 current loss: 1.0395069122314453 current acc: 0.1758\n",
      "iteration 21 current loss: 1.1024178266525269 current acc: 0.1826\n",
      "iteration 22 current loss: 0.8908777832984924 current acc: 0.1906\n",
      "iteration 23 current loss: 0.9729364514350891 current acc: 0.1982\n",
      "iteration 24 current loss: 1.0267577171325684 current acc: 0.2068\n",
      "iteration 25 current loss: 0.9273947477340698 current acc: 0.2152\n",
      "iteration 26 current loss: 0.7929766178131104 current acc: 0.2236\n",
      "iteration 27 current loss: 0.9513590335845947 current acc: 0.2322\n",
      "iteration 28 current loss: 0.935605525970459 current acc: 0.2404\n",
      "iteration 29 current loss: 1.128131628036499 current acc: 0.2472\n",
      "iteration 30 current loss: 0.9877630472183228 current acc: 0.2558\n",
      "iteration 31 current loss: 1.1824827194213867 current acc: 0.2628\n",
      "iteration 32 current loss: 0.8384326696395874 current acc: 0.2724\n",
      "iteration 33 current loss: 1.0156564712524414 current acc: 0.2808\n",
      "iteration 34 current loss: 0.7717755436897278 current acc: 0.2906\n",
      "iteration 35 current loss: 0.9366839528083801 current acc: 0.3004\n",
      "iteration 36 current loss: 1.0230270624160767 current acc: 0.3084\n",
      "iteration 37 current loss: 0.9667294025421143 current acc: 0.3166\n",
      "iteration 38 current loss: 0.8779501914978027 current acc: 0.326\n",
      "iteration 39 current loss: 0.9241397380828857 current acc: 0.3348\n",
      "iteration 40 current loss: 1.0145230293273926 current acc: 0.3434\n",
      "iteration 41 current loss: 1.06489896774292 current acc: 0.352\n",
      "iteration 42 current loss: 0.8457663655281067 current acc: 0.3606\n",
      "iteration 43 current loss: 1.042672872543335 current acc: 0.3692\n",
      "iteration 44 current loss: 0.9889785647392273 current acc: 0.3782\n",
      "iteration 45 current loss: 0.7949725985527039 current acc: 0.3874\n",
      "iteration 46 current loss: 1.1378772258758545 current acc: 0.395\n",
      "iteration 47 current loss: 1.0709720849990845 current acc: 0.4028\n",
      "iteration 48 current loss: 1.020941138267517 current acc: 0.4108\n",
      "iteration 49 current loss: 0.9133050441741943 current acc: 0.4186\n",
      "iteration 50 current loss: 0.9019518494606018 current acc: 0.427\n",
      "iteration 51 current loss: 0.8921196460723877 current acc: 0.4362\n",
      "iteration 52 current loss: 1.0714390277862549 current acc: 0.444\n",
      "iteration 53 current loss: 1.1269495487213135 current acc: 0.451\n",
      "iteration 54 current loss: 0.8886314034461975 current acc: 0.4602\n",
      "iteration 55 current loss: 0.844545304775238 current acc: 0.4696\n",
      "iteration 56 current loss: 0.8782445192337036 current acc: 0.4792\n",
      "iteration 57 current loss: 0.8488444685935974 current acc: 0.4888\n",
      "iteration 58 current loss: 0.9238702058792114 current acc: 0.4968\n",
      "iteration 59 current loss: 0.8425039052963257 current acc: 0.5062\n",
      "iteration 60 current loss: 0.9269518852233887 current acc: 0.5144\n",
      "iteration 61 current loss: 0.889737606048584 current acc: 0.524\n",
      "iteration 62 current loss: 0.8666165471076965 current acc: 0.533\n",
      "iteration 63 current loss: 0.9890713691711426 current acc: 0.5414\n",
      "iteration 64 current loss: 0.9092545509338379 current acc: 0.5504\n",
      "iteration 65 current loss: 1.0581247806549072 current acc: 0.5586\n",
      "iteration 66 current loss: 1.0448211431503296 current acc: 0.5668\n",
      "iteration 67 current loss: 0.8205468654632568 current acc: 0.5748\n",
      "iteration 68 current loss: 1.011267066001892 current acc: 0.5832\n",
      "iteration 69 current loss: 1.0037800073623657 current acc: 0.591\n",
      "iteration 70 current loss: 0.8295340538024902 current acc: 0.6006\n",
      "iteration 71 current loss: 1.0078963041305542 current acc: 0.609\n",
      "iteration 72 current loss: 0.8659924268722534 current acc: 0.6174\n",
      "iteration 73 current loss: 1.0596740245819092 current acc: 0.6256\n",
      "iteration 74 current loss: 1.0657812356948853 current acc: 0.6332\n",
      "iteration 75 current loss: 1.151248574256897 current acc: 0.6408\n",
      "iteration 76 current loss: 1.157078504562378 current acc: 0.6478\n",
      "iteration 77 current loss: 1.1274945735931396 current acc: 0.6546\n",
      "iteration 78 current loss: 0.8187596201896667 current acc: 0.656\n",
      "\t\tTrain Epoch 91/100,Train Accuracy: 0.656, Train Loss: 0.9685060879852199.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 91/100, Validation Accuracy: 0.547375, Validation Loss: 1.3106561126708984\n",
      "iteration 0 current loss: 1.1641230583190918 current acc: 0.0072\n",
      "iteration 1 current loss: 1.1207717657089233 current acc: 0.0152\n",
      "iteration 2 current loss: 0.8655370473861694 current acc: 0.0236\n",
      "iteration 3 current loss: 0.9782968163490295 current acc: 0.0318\n",
      "iteration 4 current loss: 1.0251686573028564 current acc: 0.0398\n",
      "iteration 5 current loss: 0.9116548895835876 current acc: 0.0482\n",
      "iteration 6 current loss: 0.8953410387039185 current acc: 0.0572\n",
      "iteration 7 current loss: 0.7918626070022583 current acc: 0.0664\n",
      "iteration 8 current loss: 0.7780544757843018 current acc: 0.076\n",
      "iteration 9 current loss: 0.8939066529273987 current acc: 0.0848\n",
      "iteration 10 current loss: 0.8163315653800964 current acc: 0.0944\n",
      "iteration 11 current loss: 0.9313592314720154 current acc: 0.1028\n",
      "iteration 12 current loss: 0.7736296653747559 current acc: 0.1122\n",
      "iteration 13 current loss: 1.009384036064148 current acc: 0.1198\n",
      "iteration 14 current loss: 0.9563863277435303 current acc: 0.1278\n",
      "iteration 15 current loss: 0.8502824306488037 current acc: 0.1374\n",
      "iteration 16 current loss: 0.7958222031593323 current acc: 0.1458\n",
      "iteration 17 current loss: 0.9686753749847412 current acc: 0.1544\n",
      "iteration 18 current loss: 0.6909462213516235 current acc: 0.1644\n",
      "iteration 19 current loss: 0.7688952684402466 current acc: 0.1742\n",
      "iteration 20 current loss: 0.8002602458000183 current acc: 0.1844\n",
      "iteration 21 current loss: 0.8524330258369446 current acc: 0.194\n",
      "iteration 22 current loss: 0.9060204029083252 current acc: 0.2022\n",
      "iteration 23 current loss: 0.8609569668769836 current acc: 0.2122\n",
      "iteration 24 current loss: 1.0392730236053467 current acc: 0.2206\n",
      "iteration 25 current loss: 1.0351587533950806 current acc: 0.2284\n",
      "iteration 26 current loss: 1.2101836204528809 current acc: 0.2358\n",
      "iteration 27 current loss: 1.0866512060165405 current acc: 0.243\n",
      "iteration 28 current loss: 0.8686408996582031 current acc: 0.2516\n",
      "iteration 29 current loss: 1.0690076351165771 current acc: 0.2584\n",
      "iteration 30 current loss: 0.8987504243850708 current acc: 0.2674\n",
      "iteration 31 current loss: 0.8560228943824768 current acc: 0.276\n",
      "iteration 32 current loss: 0.8696205615997314 current acc: 0.285\n",
      "iteration 33 current loss: 1.009427785873413 current acc: 0.293\n",
      "iteration 34 current loss: 0.9564631581306458 current acc: 0.3014\n",
      "iteration 35 current loss: 0.8809242844581604 current acc: 0.3098\n",
      "iteration 36 current loss: 0.8457929491996765 current acc: 0.3188\n",
      "iteration 37 current loss: 1.0403811931610107 current acc: 0.3256\n",
      "iteration 38 current loss: 0.9742345213890076 current acc: 0.3336\n",
      "iteration 39 current loss: 1.054579257965088 current acc: 0.3418\n",
      "iteration 40 current loss: 0.8810257911682129 current acc: 0.3504\n",
      "iteration 41 current loss: 0.8144615888595581 current acc: 0.3602\n",
      "iteration 42 current loss: 0.9343594908714294 current acc: 0.3694\n",
      "iteration 43 current loss: 0.8037662506103516 current acc: 0.3784\n",
      "iteration 44 current loss: 0.9082185626029968 current acc: 0.387\n",
      "iteration 45 current loss: 0.7943697571754456 current acc: 0.396\n",
      "iteration 46 current loss: 0.8614937663078308 current acc: 0.4054\n",
      "iteration 47 current loss: 0.9451748728752136 current acc: 0.414\n",
      "iteration 48 current loss: 0.9775180816650391 current acc: 0.4218\n",
      "iteration 49 current loss: 1.3036353588104248 current acc: 0.4296\n",
      "iteration 50 current loss: 0.99679034948349 current acc: 0.4376\n",
      "iteration 51 current loss: 0.7788147926330566 current acc: 0.4466\n",
      "iteration 52 current loss: 1.0111277103424072 current acc: 0.4544\n",
      "iteration 53 current loss: 0.9558126330375671 current acc: 0.4632\n",
      "iteration 54 current loss: 1.1437376737594604 current acc: 0.4712\n",
      "iteration 55 current loss: 1.0222008228302002 current acc: 0.4792\n",
      "iteration 56 current loss: 0.9602890610694885 current acc: 0.4878\n",
      "iteration 57 current loss: 1.0068109035491943 current acc: 0.496\n",
      "iteration 58 current loss: 0.8694853186607361 current acc: 0.5046\n",
      "iteration 59 current loss: 0.8744649291038513 current acc: 0.514\n",
      "iteration 60 current loss: 1.0316710472106934 current acc: 0.5216\n",
      "iteration 61 current loss: 1.0708787441253662 current acc: 0.5286\n",
      "iteration 62 current loss: 1.1546279191970825 current acc: 0.5362\n",
      "iteration 63 current loss: 1.0706067085266113 current acc: 0.5438\n",
      "iteration 64 current loss: 0.8787859082221985 current acc: 0.5528\n",
      "iteration 65 current loss: 0.9510164856910706 current acc: 0.5614\n",
      "iteration 66 current loss: 0.9417520761489868 current acc: 0.5698\n",
      "iteration 67 current loss: 1.1553047895431519 current acc: 0.5778\n",
      "iteration 68 current loss: 1.0351346731185913 current acc: 0.5856\n",
      "iteration 69 current loss: 1.0961112976074219 current acc: 0.594\n",
      "iteration 70 current loss: 0.9031704068183899 current acc: 0.602\n",
      "iteration 71 current loss: 1.1481972932815552 current acc: 0.609\n",
      "iteration 72 current loss: 0.9674494862556458 current acc: 0.618\n",
      "iteration 73 current loss: 1.0896180868148804 current acc: 0.6258\n",
      "iteration 74 current loss: 1.0413345098495483 current acc: 0.6334\n",
      "iteration 75 current loss: 1.1720181703567505 current acc: 0.6414\n",
      "iteration 76 current loss: 1.1174904108047485 current acc: 0.6494\n",
      "iteration 77 current loss: 0.9708342552185059 current acc: 0.6572\n",
      "iteration 78 current loss: 0.88093101978302 current acc: 0.6586\n",
      "\t\tTrain Epoch 92/100,Train Accuracy: 0.6586, Train Loss: 0.9572367487074454.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 92/100, Validation Accuracy: 0.584375, Validation Loss: 1.1629041271209717\n",
      "iteration 0 current loss: 0.838992178440094 current acc: 0.0086\n",
      "iteration 1 current loss: 0.8103232979774475 current acc: 0.0186\n",
      "iteration 2 current loss: 0.8597939610481262 current acc: 0.0272\n",
      "iteration 3 current loss: 0.9808162450790405 current acc: 0.036\n",
      "iteration 4 current loss: 1.0854589939117432 current acc: 0.0432\n",
      "iteration 5 current loss: 1.0436726808547974 current acc: 0.0508\n",
      "iteration 6 current loss: 1.0387152433395386 current acc: 0.0582\n",
      "iteration 7 current loss: 0.9970123171806335 current acc: 0.0664\n",
      "iteration 8 current loss: 0.8896781206130981 current acc: 0.075\n",
      "iteration 9 current loss: 0.8952332735061646 current acc: 0.0846\n",
      "iteration 10 current loss: 0.853162407875061 current acc: 0.0938\n",
      "iteration 11 current loss: 1.0018305778503418 current acc: 0.1014\n",
      "iteration 12 current loss: 0.989127516746521 current acc: 0.109\n",
      "iteration 13 current loss: 1.0536352396011353 current acc: 0.1166\n",
      "iteration 14 current loss: 1.2004375457763672 current acc: 0.124\n",
      "iteration 15 current loss: 0.9345226883888245 current acc: 0.1322\n",
      "iteration 16 current loss: 0.7850613594055176 current acc: 0.142\n",
      "iteration 17 current loss: 0.9384617805480957 current acc: 0.1506\n",
      "iteration 18 current loss: 0.8806111216545105 current acc: 0.1596\n",
      "iteration 19 current loss: 1.0289522409439087 current acc: 0.168\n",
      "iteration 20 current loss: 0.9775555729866028 current acc: 0.177\n",
      "iteration 21 current loss: 0.9594767689704895 current acc: 0.1852\n",
      "iteration 22 current loss: 0.8996526598930359 current acc: 0.1952\n",
      "iteration 23 current loss: 0.9509223699569702 current acc: 0.2034\n",
      "iteration 24 current loss: 0.8857983350753784 current acc: 0.2122\n",
      "iteration 25 current loss: 1.0358316898345947 current acc: 0.22\n",
      "iteration 26 current loss: 0.8446556925773621 current acc: 0.2288\n",
      "iteration 27 current loss: 1.1045997142791748 current acc: 0.2368\n",
      "iteration 28 current loss: 0.8251245617866516 current acc: 0.2462\n",
      "iteration 29 current loss: 1.028232455253601 current acc: 0.254\n",
      "iteration 30 current loss: 0.9832311272621155 current acc: 0.2622\n",
      "iteration 31 current loss: 0.9518638849258423 current acc: 0.2716\n",
      "iteration 32 current loss: 0.8825902938842773 current acc: 0.2808\n",
      "iteration 33 current loss: 0.8339224457740784 current acc: 0.2902\n",
      "iteration 34 current loss: 0.9725975394248962 current acc: 0.2984\n",
      "iteration 35 current loss: 0.9183865785598755 current acc: 0.3066\n",
      "iteration 36 current loss: 1.0224974155426025 current acc: 0.3146\n",
      "iteration 37 current loss: 0.9344301819801331 current acc: 0.3238\n",
      "iteration 38 current loss: 1.185727596282959 current acc: 0.3312\n",
      "iteration 39 current loss: 0.8975443840026855 current acc: 0.34\n",
      "iteration 40 current loss: 0.9967849850654602 current acc: 0.3486\n",
      "iteration 41 current loss: 1.0704684257507324 current acc: 0.3572\n",
      "iteration 42 current loss: 0.9723817706108093 current acc: 0.3658\n",
      "iteration 43 current loss: 1.1949738264083862 current acc: 0.3726\n",
      "iteration 44 current loss: 0.9756529927253723 current acc: 0.3818\n",
      "iteration 45 current loss: 1.0573816299438477 current acc: 0.3896\n",
      "iteration 46 current loss: 0.7785963416099548 current acc: 0.3986\n",
      "iteration 47 current loss: 0.8375691175460815 current acc: 0.4082\n",
      "iteration 48 current loss: 0.8373469114303589 current acc: 0.4182\n",
      "iteration 49 current loss: 1.0223875045776367 current acc: 0.426\n",
      "iteration 50 current loss: 0.7405902147293091 current acc: 0.4356\n",
      "iteration 51 current loss: 0.8474807143211365 current acc: 0.4444\n",
      "iteration 52 current loss: 0.8261268734931946 current acc: 0.4534\n",
      "iteration 53 current loss: 0.981781005859375 current acc: 0.4614\n",
      "iteration 54 current loss: 0.8529688715934753 current acc: 0.47\n",
      "iteration 55 current loss: 1.0596977472305298 current acc: 0.479\n",
      "iteration 56 current loss: 0.9453470706939697 current acc: 0.4882\n",
      "iteration 57 current loss: 0.9211593270301819 current acc: 0.497\n",
      "iteration 58 current loss: 1.0346934795379639 current acc: 0.5056\n",
      "iteration 59 current loss: 1.092441439628601 current acc: 0.5138\n",
      "iteration 60 current loss: 0.866216242313385 current acc: 0.5228\n",
      "iteration 61 current loss: 0.8141541481018066 current acc: 0.532\n",
      "iteration 62 current loss: 0.9821963906288147 current acc: 0.5402\n",
      "iteration 63 current loss: 1.050064206123352 current acc: 0.5482\n",
      "iteration 64 current loss: 1.2613722085952759 current acc: 0.5556\n",
      "iteration 65 current loss: 0.8480191230773926 current acc: 0.5644\n",
      "iteration 66 current loss: 0.8810254335403442 current acc: 0.5738\n",
      "iteration 67 current loss: 1.0716934204101562 current acc: 0.5818\n",
      "iteration 68 current loss: 0.9539000391960144 current acc: 0.591\n",
      "iteration 69 current loss: 1.2188276052474976 current acc: 0.5984\n",
      "iteration 70 current loss: 1.0177946090698242 current acc: 0.6064\n",
      "iteration 71 current loss: 0.8577394485473633 current acc: 0.6152\n",
      "iteration 72 current loss: 0.917773425579071 current acc: 0.623\n",
      "iteration 73 current loss: 0.8036843538284302 current acc: 0.6326\n",
      "iteration 74 current loss: 0.9613833427429199 current acc: 0.641\n",
      "iteration 75 current loss: 0.9826121926307678 current acc: 0.6492\n",
      "iteration 76 current loss: 1.2307674884796143 current acc: 0.6558\n",
      "iteration 77 current loss: 1.1932555437088013 current acc: 0.6622\n",
      "iteration 78 current loss: 0.7887267470359802 current acc: 0.6634\n",
      "\t\tTrain Epoch 93/100,Train Accuracy: 0.6634, Train Loss: 0.9613566617422467.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 93/100, Validation Accuracy: 0.5605, Validation Loss: 1.2198534107208252\n",
      "iteration 0 current loss: 0.9030579328536987 current acc: 0.0086\n",
      "iteration 1 current loss: 1.239033818244934 current acc: 0.016\n",
      "iteration 2 current loss: 1.044671893119812 current acc: 0.0242\n",
      "iteration 3 current loss: 0.8295149207115173 current acc: 0.0338\n",
      "iteration 4 current loss: 1.0981264114379883 current acc: 0.0416\n",
      "iteration 5 current loss: 0.8098281621932983 current acc: 0.0518\n",
      "iteration 6 current loss: 0.9978015422821045 current acc: 0.0604\n",
      "iteration 7 current loss: 0.9873487949371338 current acc: 0.069\n",
      "iteration 8 current loss: 0.9118714928627014 current acc: 0.077\n",
      "iteration 9 current loss: 0.9226024150848389 current acc: 0.0856\n",
      "iteration 10 current loss: 0.8734889030456543 current acc: 0.094\n",
      "iteration 11 current loss: 0.9035074710845947 current acc: 0.1032\n",
      "iteration 12 current loss: 0.8062780499458313 current acc: 0.1134\n",
      "iteration 13 current loss: 0.8967097401618958 current acc: 0.1222\n",
      "iteration 14 current loss: 1.0644354820251465 current acc: 0.1296\n",
      "iteration 15 current loss: 0.9017165899276733 current acc: 0.1386\n",
      "iteration 16 current loss: 0.9432514905929565 current acc: 0.1476\n",
      "iteration 17 current loss: 0.9719378352165222 current acc: 0.156\n",
      "iteration 18 current loss: 0.9883605241775513 current acc: 0.164\n",
      "iteration 19 current loss: 0.9829092621803284 current acc: 0.1712\n",
      "iteration 20 current loss: 0.8807728886604309 current acc: 0.1802\n",
      "iteration 21 current loss: 0.9701737761497498 current acc: 0.1882\n",
      "iteration 22 current loss: 0.9133092164993286 current acc: 0.1968\n",
      "iteration 23 current loss: 0.8248407244682312 current acc: 0.2058\n",
      "iteration 24 current loss: 0.9560074806213379 current acc: 0.215\n",
      "iteration 25 current loss: 1.1697965860366821 current acc: 0.2228\n",
      "iteration 26 current loss: 1.0345898866653442 current acc: 0.2308\n",
      "iteration 27 current loss: 0.9886565804481506 current acc: 0.2392\n",
      "iteration 28 current loss: 0.7990447282791138 current acc: 0.2482\n",
      "iteration 29 current loss: 0.8746943473815918 current acc: 0.2574\n",
      "iteration 30 current loss: 0.7826065421104431 current acc: 0.2662\n",
      "iteration 31 current loss: 0.9412789344787598 current acc: 0.2752\n",
      "iteration 32 current loss: 1.0650899410247803 current acc: 0.283\n",
      "iteration 33 current loss: 0.9983218908309937 current acc: 0.2916\n",
      "iteration 34 current loss: 0.9056785702705383 current acc: 0.2998\n",
      "iteration 35 current loss: 0.9566551446914673 current acc: 0.308\n",
      "iteration 36 current loss: 0.921363353729248 current acc: 0.317\n",
      "iteration 37 current loss: 1.2278980016708374 current acc: 0.3232\n",
      "iteration 38 current loss: 1.0024840831756592 current acc: 0.3318\n",
      "iteration 39 current loss: 1.0499740839004517 current acc: 0.3398\n",
      "iteration 40 current loss: 1.0151684284210205 current acc: 0.3484\n",
      "iteration 41 current loss: 1.2709486484527588 current acc: 0.3546\n",
      "iteration 42 current loss: 1.0350518226623535 current acc: 0.3628\n",
      "iteration 43 current loss: 0.9302259087562561 current acc: 0.3712\n",
      "iteration 44 current loss: 0.9596460461616516 current acc: 0.3792\n",
      "iteration 45 current loss: 0.9010916352272034 current acc: 0.388\n",
      "iteration 46 current loss: 0.8903337121009827 current acc: 0.3968\n",
      "iteration 47 current loss: 0.8084819316864014 current acc: 0.4064\n",
      "iteration 48 current loss: 0.9733142256736755 current acc: 0.4148\n",
      "iteration 49 current loss: 0.9940574169158936 current acc: 0.4228\n",
      "iteration 50 current loss: 0.8475844264030457 current acc: 0.4318\n",
      "iteration 51 current loss: 0.8942976593971252 current acc: 0.4404\n",
      "iteration 52 current loss: 0.8387830257415771 current acc: 0.4496\n",
      "iteration 53 current loss: 0.8811245560646057 current acc: 0.4586\n",
      "iteration 54 current loss: 0.8899714946746826 current acc: 0.4682\n",
      "iteration 55 current loss: 0.9068652987480164 current acc: 0.4768\n",
      "iteration 56 current loss: 0.8784207105636597 current acc: 0.4846\n",
      "iteration 57 current loss: 0.8840516209602356 current acc: 0.4934\n",
      "iteration 58 current loss: 1.0430383682250977 current acc: 0.501\n",
      "iteration 59 current loss: 1.0260175466537476 current acc: 0.5084\n",
      "iteration 60 current loss: 0.9818688631057739 current acc: 0.517\n",
      "iteration 61 current loss: 0.9976799488067627 current acc: 0.5248\n",
      "iteration 62 current loss: 0.8862940669059753 current acc: 0.5334\n",
      "iteration 63 current loss: 0.8431279063224792 current acc: 0.5436\n",
      "iteration 64 current loss: 0.8250107765197754 current acc: 0.553\n",
      "iteration 65 current loss: 0.9643611907958984 current acc: 0.5618\n",
      "iteration 66 current loss: 1.11262845993042 current acc: 0.5698\n",
      "iteration 67 current loss: 1.007922887802124 current acc: 0.5778\n",
      "iteration 68 current loss: 1.06846284866333 current acc: 0.5864\n",
      "iteration 69 current loss: 0.7449519634246826 current acc: 0.5962\n",
      "iteration 70 current loss: 1.0719248056411743 current acc: 0.6042\n",
      "iteration 71 current loss: 0.9404768347740173 current acc: 0.6126\n",
      "iteration 72 current loss: 1.065730094909668 current acc: 0.6206\n",
      "iteration 73 current loss: 0.7768868207931519 current acc: 0.6314\n",
      "iteration 74 current loss: 1.0552656650543213 current acc: 0.6398\n",
      "iteration 75 current loss: 1.027712345123291 current acc: 0.6476\n",
      "iteration 76 current loss: 0.8073060512542725 current acc: 0.657\n",
      "iteration 77 current loss: 0.8842035531997681 current acc: 0.666\n",
      "iteration 78 current loss: 1.6942154169082642 current acc: 0.6666\n",
      "\t\tTrain Epoch 94/100,Train Accuracy: 0.6666, Train Loss: 0.9615720819823349.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 94/100, Validation Accuracy: 0.575875, Validation Loss: 1.2159117817878724\n",
      "iteration 0 current loss: 0.8210579752922058 current acc: 0.0098\n",
      "iteration 1 current loss: 1.0248690843582153 current acc: 0.0178\n",
      "iteration 2 current loss: 1.1818045377731323 current acc: 0.0256\n",
      "iteration 3 current loss: 1.1319379806518555 current acc: 0.0328\n",
      "iteration 4 current loss: 1.1458714008331299 current acc: 0.0408\n",
      "iteration 5 current loss: 0.8213745355606079 current acc: 0.051\n",
      "iteration 6 current loss: 1.062516212463379 current acc: 0.0586\n",
      "iteration 7 current loss: 1.0414119958877563 current acc: 0.0664\n",
      "iteration 8 current loss: 0.8772850632667542 current acc: 0.0754\n",
      "iteration 9 current loss: 0.7583483457565308 current acc: 0.0848\n",
      "iteration 10 current loss: 1.049513816833496 current acc: 0.0932\n",
      "iteration 11 current loss: 1.0922226905822754 current acc: 0.1016\n",
      "iteration 12 current loss: 1.0440150499343872 current acc: 0.1098\n",
      "iteration 13 current loss: 1.0714662075042725 current acc: 0.117\n",
      "iteration 14 current loss: 1.0444939136505127 current acc: 0.1248\n",
      "iteration 15 current loss: 1.0572397708892822 current acc: 0.1326\n",
      "iteration 16 current loss: 0.9209050536155701 current acc: 0.1416\n",
      "iteration 17 current loss: 0.930386483669281 current acc: 0.1502\n",
      "iteration 18 current loss: 0.9476649165153503 current acc: 0.159\n",
      "iteration 19 current loss: 1.0798686742782593 current acc: 0.1658\n",
      "iteration 20 current loss: 0.9198324680328369 current acc: 0.1746\n",
      "iteration 21 current loss: 0.9772253632545471 current acc: 0.1836\n",
      "iteration 22 current loss: 0.8729512095451355 current acc: 0.1926\n",
      "iteration 23 current loss: 0.9884328246116638 current acc: 0.2006\n",
      "iteration 24 current loss: 0.8860194087028503 current acc: 0.2102\n",
      "iteration 25 current loss: 0.9834654927253723 current acc: 0.2184\n",
      "iteration 26 current loss: 1.1628308296203613 current acc: 0.2256\n",
      "iteration 27 current loss: 0.9970602989196777 current acc: 0.2342\n",
      "iteration 28 current loss: 1.1281477212905884 current acc: 0.2424\n",
      "iteration 29 current loss: 0.7463529706001282 current acc: 0.2522\n",
      "iteration 30 current loss: 0.8625838756561279 current acc: 0.2612\n",
      "iteration 31 current loss: 0.8788583874702454 current acc: 0.2696\n",
      "iteration 32 current loss: 1.0598535537719727 current acc: 0.277\n",
      "iteration 33 current loss: 1.0898730754852295 current acc: 0.2842\n",
      "iteration 34 current loss: 0.8524665832519531 current acc: 0.2938\n",
      "iteration 35 current loss: 0.7903943657875061 current acc: 0.3036\n",
      "iteration 36 current loss: 0.7998514771461487 current acc: 0.3124\n",
      "iteration 37 current loss: 0.9133172631263733 current acc: 0.3218\n",
      "iteration 38 current loss: 0.7841620445251465 current acc: 0.3314\n",
      "iteration 39 current loss: 0.9368720650672913 current acc: 0.3412\n",
      "iteration 40 current loss: 1.031873106956482 current acc: 0.3486\n",
      "iteration 41 current loss: 1.0742862224578857 current acc: 0.3572\n",
      "iteration 42 current loss: 1.0374171733856201 current acc: 0.3642\n",
      "iteration 43 current loss: 0.9354925155639648 current acc: 0.3728\n",
      "iteration 44 current loss: 0.8917643427848816 current acc: 0.382\n",
      "iteration 45 current loss: 0.8314869403839111 current acc: 0.392\n",
      "iteration 46 current loss: 1.1636357307434082 current acc: 0.3988\n",
      "iteration 47 current loss: 1.0457813739776611 current acc: 0.407\n",
      "iteration 48 current loss: 0.9514263272285461 current acc: 0.416\n",
      "iteration 49 current loss: 0.8993520140647888 current acc: 0.4242\n",
      "iteration 50 current loss: 0.7193924784660339 current acc: 0.4344\n",
      "iteration 51 current loss: 0.681585431098938 current acc: 0.4448\n",
      "iteration 52 current loss: 1.0618358850479126 current acc: 0.4532\n",
      "iteration 53 current loss: 0.8280624151229858 current acc: 0.4632\n",
      "iteration 54 current loss: 0.9173796772956848 current acc: 0.4712\n",
      "iteration 55 current loss: 0.9703564643859863 current acc: 0.4796\n",
      "iteration 56 current loss: 1.06745445728302 current acc: 0.4872\n",
      "iteration 57 current loss: 1.0922167301177979 current acc: 0.4946\n",
      "iteration 58 current loss: 0.8311625719070435 current acc: 0.5036\n",
      "iteration 59 current loss: 0.9939996004104614 current acc: 0.5124\n",
      "iteration 60 current loss: 0.9676914811134338 current acc: 0.5204\n",
      "iteration 61 current loss: 0.9888700842857361 current acc: 0.528\n",
      "iteration 62 current loss: 1.090561866760254 current acc: 0.536\n",
      "iteration 63 current loss: 1.2252117395401 current acc: 0.543\n",
      "iteration 64 current loss: 0.8496350646018982 current acc: 0.5524\n",
      "iteration 65 current loss: 1.139422059059143 current acc: 0.5606\n",
      "iteration 66 current loss: 1.0192739963531494 current acc: 0.5698\n",
      "iteration 67 current loss: 0.9973819255828857 current acc: 0.5778\n",
      "iteration 68 current loss: 1.131937026977539 current acc: 0.5856\n",
      "iteration 69 current loss: 1.0182019472122192 current acc: 0.5938\n",
      "iteration 70 current loss: 1.1176190376281738 current acc: 0.602\n",
      "iteration 71 current loss: 1.0890538692474365 current acc: 0.6092\n",
      "iteration 72 current loss: 0.9330683350563049 current acc: 0.618\n",
      "iteration 73 current loss: 1.028698205947876 current acc: 0.6256\n",
      "iteration 74 current loss: 0.9209152460098267 current acc: 0.6336\n",
      "iteration 75 current loss: 0.8122243881225586 current acc: 0.6428\n",
      "iteration 76 current loss: 0.8716880083084106 current acc: 0.652\n",
      "iteration 77 current loss: 0.8962511420249939 current acc: 0.6606\n",
      "iteration 78 current loss: 1.5885539054870605 current acc: 0.6614\n",
      "\t\tTrain Epoch 95/100,Train Accuracy: 0.6614, Train Loss: 0.9803420984292333.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 95/100, Validation Accuracy: 0.570375, Validation Loss: 1.190103006362915\n",
      "iteration 0 current loss: 1.1940206289291382 current acc: 0.008\n",
      "iteration 1 current loss: 1.0022821426391602 current acc: 0.0162\n",
      "iteration 2 current loss: 1.0953816175460815 current acc: 0.0236\n",
      "iteration 3 current loss: 0.966681957244873 current acc: 0.032\n",
      "iteration 4 current loss: 1.1232008934020996 current acc: 0.0396\n",
      "iteration 5 current loss: 0.9966819286346436 current acc: 0.0474\n",
      "iteration 6 current loss: 0.9306089282035828 current acc: 0.0558\n",
      "iteration 7 current loss: 1.105307698249817 current acc: 0.0636\n",
      "iteration 8 current loss: 0.9946275949478149 current acc: 0.072\n",
      "iteration 9 current loss: 0.9808963537216187 current acc: 0.0808\n",
      "iteration 10 current loss: 1.0529500246047974 current acc: 0.0888\n",
      "iteration 11 current loss: 0.8574376106262207 current acc: 0.0978\n",
      "iteration 12 current loss: 0.8144559264183044 current acc: 0.1074\n",
      "iteration 13 current loss: 0.7840084433555603 current acc: 0.1168\n",
      "iteration 14 current loss: 1.0046992301940918 current acc: 0.1244\n",
      "iteration 15 current loss: 1.0325928926467896 current acc: 0.132\n",
      "iteration 16 current loss: 0.994941234588623 current acc: 0.1402\n",
      "iteration 17 current loss: 1.1475931406021118 current acc: 0.1478\n",
      "iteration 18 current loss: 0.8463118076324463 current acc: 0.1574\n",
      "iteration 19 current loss: 0.9007255434989929 current acc: 0.1656\n",
      "iteration 20 current loss: 0.959425151348114 current acc: 0.1736\n",
      "iteration 21 current loss: 0.8796966671943665 current acc: 0.1832\n",
      "iteration 22 current loss: 0.9537073373794556 current acc: 0.1918\n",
      "iteration 23 current loss: 1.1522209644317627 current acc: 0.199\n",
      "iteration 24 current loss: 1.1253538131713867 current acc: 0.2064\n",
      "iteration 25 current loss: 0.9727158546447754 current acc: 0.2152\n",
      "iteration 26 current loss: 0.9443625807762146 current acc: 0.2234\n",
      "iteration 27 current loss: 0.8857022523880005 current acc: 0.2328\n",
      "iteration 28 current loss: 1.0421725511550903 current acc: 0.2404\n",
      "iteration 29 current loss: 0.8547330498695374 current acc: 0.2494\n",
      "iteration 30 current loss: 0.9499109983444214 current acc: 0.2578\n",
      "iteration 31 current loss: 0.8449745774269104 current acc: 0.2676\n",
      "iteration 32 current loss: 0.7541072368621826 current acc: 0.277\n",
      "iteration 33 current loss: 0.8725377917289734 current acc: 0.2858\n",
      "iteration 34 current loss: 0.8308353424072266 current acc: 0.2948\n",
      "iteration 35 current loss: 0.9995129704475403 current acc: 0.302\n",
      "iteration 36 current loss: 1.0595574378967285 current acc: 0.3096\n",
      "iteration 37 current loss: 1.013991355895996 current acc: 0.317\n",
      "iteration 38 current loss: 0.826772153377533 current acc: 0.3258\n",
      "iteration 39 current loss: 0.892366886138916 current acc: 0.3348\n",
      "iteration 40 current loss: 1.146986961364746 current acc: 0.3426\n",
      "iteration 41 current loss: 1.023423433303833 current acc: 0.3512\n",
      "iteration 42 current loss: 0.8768826723098755 current acc: 0.36\n",
      "iteration 43 current loss: 1.1251721382141113 current acc: 0.367\n",
      "iteration 44 current loss: 0.8817644119262695 current acc: 0.3756\n",
      "iteration 45 current loss: 0.9331623911857605 current acc: 0.3836\n",
      "iteration 46 current loss: 1.1318978071212769 current acc: 0.3912\n",
      "iteration 47 current loss: 0.9553375840187073 current acc: 0.3994\n",
      "iteration 48 current loss: 0.8907142281532288 current acc: 0.4086\n",
      "iteration 49 current loss: 1.0477848052978516 current acc: 0.4166\n",
      "iteration 50 current loss: 0.8494147658348083 current acc: 0.4262\n",
      "iteration 51 current loss: 0.9087818264961243 current acc: 0.435\n",
      "iteration 52 current loss: 0.9341821074485779 current acc: 0.4436\n",
      "iteration 53 current loss: 0.7574078440666199 current acc: 0.4532\n",
      "iteration 54 current loss: 1.0569267272949219 current acc: 0.461\n",
      "iteration 55 current loss: 0.8079479932785034 current acc: 0.4712\n",
      "iteration 56 current loss: 1.0060508251190186 current acc: 0.4794\n",
      "iteration 57 current loss: 0.9732918739318848 current acc: 0.4884\n",
      "iteration 58 current loss: 1.0368494987487793 current acc: 0.4966\n",
      "iteration 59 current loss: 0.9380095601081848 current acc: 0.506\n",
      "iteration 60 current loss: 1.0246355533599854 current acc: 0.5134\n",
      "iteration 61 current loss: 1.1297364234924316 current acc: 0.5212\n",
      "iteration 62 current loss: 1.075284719467163 current acc: 0.5288\n",
      "iteration 63 current loss: 0.8505423069000244 current acc: 0.5384\n",
      "iteration 64 current loss: 1.0397497415542603 current acc: 0.5476\n",
      "iteration 65 current loss: 1.1912623643875122 current acc: 0.5554\n",
      "iteration 66 current loss: 1.2347272634506226 current acc: 0.5622\n",
      "iteration 67 current loss: 1.275805115699768 current acc: 0.569\n",
      "iteration 68 current loss: 1.0605432987213135 current acc: 0.5766\n",
      "iteration 69 current loss: 0.9171838164329529 current acc: 0.5846\n",
      "iteration 70 current loss: 1.070151925086975 current acc: 0.5924\n",
      "iteration 71 current loss: 0.9453580379486084 current acc: 0.6008\n",
      "iteration 72 current loss: 1.042868733406067 current acc: 0.6086\n",
      "iteration 73 current loss: 0.8637402057647705 current acc: 0.6176\n",
      "iteration 74 current loss: 0.9168131947517395 current acc: 0.6266\n",
      "iteration 75 current loss: 0.8570157289505005 current acc: 0.6362\n",
      "iteration 76 current loss: 0.9276951551437378 current acc: 0.6458\n",
      "iteration 77 current loss: 1.1081124544143677 current acc: 0.6542\n",
      "iteration 78 current loss: 0.9410967826843262 current acc: 0.6552\n",
      "\t\tTrain Epoch 96/100,Train Accuracy: 0.6552, Train Loss: 0.9796756309799001.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 96/100, Validation Accuracy: 0.593125, Validation Loss: 1.1268819885253907\n",
      "iteration 0 current loss: 0.8065915107727051 current acc: 0.0098\n",
      "iteration 1 current loss: 0.8982065320014954 current acc: 0.018\n",
      "iteration 2 current loss: 0.9239627718925476 current acc: 0.0268\n",
      "iteration 3 current loss: 0.9822271466255188 current acc: 0.0348\n",
      "iteration 4 current loss: 0.8348158001899719 current acc: 0.0438\n",
      "iteration 5 current loss: 1.0368822813034058 current acc: 0.051\n",
      "iteration 6 current loss: 1.0431475639343262 current acc: 0.0586\n",
      "iteration 7 current loss: 0.874146580696106 current acc: 0.0676\n",
      "iteration 8 current loss: 0.8537397384643555 current acc: 0.0772\n",
      "iteration 9 current loss: 1.0010936260223389 current acc: 0.0858\n",
      "iteration 10 current loss: 0.9441590309143066 current acc: 0.0942\n",
      "iteration 11 current loss: 0.9642484188079834 current acc: 0.103\n",
      "iteration 12 current loss: 0.8599767088890076 current acc: 0.1116\n",
      "iteration 13 current loss: 0.8066607117652893 current acc: 0.1208\n",
      "iteration 14 current loss: 0.9609242677688599 current acc: 0.13\n",
      "iteration 15 current loss: 0.9494733810424805 current acc: 0.1376\n",
      "iteration 16 current loss: 0.9290502667427063 current acc: 0.1468\n",
      "iteration 17 current loss: 0.9100703001022339 current acc: 0.1556\n",
      "iteration 18 current loss: 0.8646056652069092 current acc: 0.1648\n",
      "iteration 19 current loss: 0.6934366226196289 current acc: 0.1744\n",
      "iteration 20 current loss: 1.0283845663070679 current acc: 0.1824\n",
      "iteration 21 current loss: 1.0487788915634155 current acc: 0.191\n",
      "iteration 22 current loss: 0.876051664352417 current acc: 0.1994\n",
      "iteration 23 current loss: 0.9617419838905334 current acc: 0.208\n",
      "iteration 24 current loss: 0.7800917029380798 current acc: 0.218\n",
      "iteration 25 current loss: 0.7393826246261597 current acc: 0.228\n",
      "iteration 26 current loss: 0.8686280846595764 current acc: 0.2372\n",
      "iteration 27 current loss: 1.053470492362976 current acc: 0.2454\n",
      "iteration 28 current loss: 0.9522894620895386 current acc: 0.2544\n",
      "iteration 29 current loss: 1.016530990600586 current acc: 0.2624\n",
      "iteration 30 current loss: 0.9188212156295776 current acc: 0.2716\n",
      "iteration 31 current loss: 1.1967157125473022 current acc: 0.2788\n",
      "iteration 32 current loss: 0.7993066906929016 current acc: 0.2878\n",
      "iteration 33 current loss: 0.8041027188301086 current acc: 0.2968\n",
      "iteration 34 current loss: 1.2241599559783936 current acc: 0.304\n",
      "iteration 35 current loss: 1.0990036725997925 current acc: 0.3122\n",
      "iteration 36 current loss: 0.8733201026916504 current acc: 0.3208\n",
      "iteration 37 current loss: 1.1809791326522827 current acc: 0.3282\n",
      "iteration 38 current loss: 0.7774249315261841 current acc: 0.3368\n",
      "iteration 39 current loss: 0.9109131693840027 current acc: 0.3456\n",
      "iteration 40 current loss: 0.9229502081871033 current acc: 0.3542\n",
      "iteration 41 current loss: 0.9493008255958557 current acc: 0.3624\n",
      "iteration 42 current loss: 1.001684546470642 current acc: 0.3706\n",
      "iteration 43 current loss: 1.0662020444869995 current acc: 0.3788\n",
      "iteration 44 current loss: 1.1298075914382935 current acc: 0.3862\n",
      "iteration 45 current loss: 1.1107351779937744 current acc: 0.3948\n",
      "iteration 46 current loss: 0.8617587089538574 current acc: 0.4046\n",
      "iteration 47 current loss: 0.9542544484138489 current acc: 0.4126\n",
      "iteration 48 current loss: 1.0314345359802246 current acc: 0.4214\n",
      "iteration 49 current loss: 0.9157780408859253 current acc: 0.4304\n",
      "iteration 50 current loss: 1.0576261281967163 current acc: 0.4382\n",
      "iteration 51 current loss: 0.9373118877410889 current acc: 0.4466\n",
      "iteration 52 current loss: 0.9426121711730957 current acc: 0.4546\n",
      "iteration 53 current loss: 1.0266245603561401 current acc: 0.4628\n",
      "iteration 54 current loss: 1.0482995510101318 current acc: 0.4706\n",
      "iteration 55 current loss: 0.9045459628105164 current acc: 0.4798\n",
      "iteration 56 current loss: 1.1230121850967407 current acc: 0.4878\n",
      "iteration 57 current loss: 1.1110783815383911 current acc: 0.4952\n",
      "iteration 58 current loss: 1.0223723649978638 current acc: 0.5034\n",
      "iteration 59 current loss: 1.0549062490463257 current acc: 0.5112\n",
      "iteration 60 current loss: 0.8781284689903259 current acc: 0.52\n",
      "iteration 61 current loss: 0.9776663780212402 current acc: 0.5282\n",
      "iteration 62 current loss: 0.9135797619819641 current acc: 0.537\n",
      "iteration 63 current loss: 0.8947199583053589 current acc: 0.5458\n",
      "iteration 64 current loss: 0.8510991334915161 current acc: 0.5544\n",
      "iteration 65 current loss: 1.0994746685028076 current acc: 0.5614\n",
      "iteration 66 current loss: 1.0268669128417969 current acc: 0.57\n",
      "iteration 67 current loss: 0.9662635326385498 current acc: 0.5786\n",
      "iteration 68 current loss: 1.002445101737976 current acc: 0.5868\n",
      "iteration 69 current loss: 0.9676611423492432 current acc: 0.595\n",
      "iteration 70 current loss: 0.8951819539070129 current acc: 0.6036\n",
      "iteration 71 current loss: 1.0321239233016968 current acc: 0.6112\n",
      "iteration 72 current loss: 0.9830551743507385 current acc: 0.6192\n",
      "iteration 73 current loss: 0.937140166759491 current acc: 0.6268\n",
      "iteration 74 current loss: 0.7144140005111694 current acc: 0.6362\n",
      "iteration 75 current loss: 1.0183302164077759 current acc: 0.6438\n",
      "iteration 76 current loss: 1.1304872035980225 current acc: 0.6518\n",
      "iteration 77 current loss: 1.0150315761566162 current acc: 0.6602\n",
      "iteration 78 current loss: 0.5052056908607483 current acc: 0.6616\n",
      "\t\tTrain Epoch 97/100,Train Accuracy: 0.6616, Train Loss: 0.9522618636300292.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 97/100, Validation Accuracy: 0.593375, Validation Loss: 1.1279502964019776\n",
      "iteration 0 current loss: 1.0101717710494995 current acc: 0.008\n",
      "iteration 1 current loss: 0.998211145401001 current acc: 0.016\n",
      "iteration 2 current loss: 0.8014349341392517 current acc: 0.0258\n",
      "iteration 3 current loss: 0.8699415922164917 current acc: 0.034\n",
      "iteration 4 current loss: 0.843669056892395 current acc: 0.0432\n",
      "iteration 5 current loss: 0.8359842300415039 current acc: 0.0524\n",
      "iteration 6 current loss: 1.0434578657150269 current acc: 0.0604\n",
      "iteration 7 current loss: 0.802639365196228 current acc: 0.0696\n",
      "iteration 8 current loss: 0.948413074016571 current acc: 0.0784\n",
      "iteration 9 current loss: 0.8649553656578064 current acc: 0.0878\n",
      "iteration 10 current loss: 1.0147674083709717 current acc: 0.0964\n",
      "iteration 11 current loss: 1.0555721521377563 current acc: 0.1048\n",
      "iteration 12 current loss: 0.8672237992286682 current acc: 0.1132\n",
      "iteration 13 current loss: 0.9439100623130798 current acc: 0.122\n",
      "iteration 14 current loss: 0.9527255892753601 current acc: 0.131\n",
      "iteration 15 current loss: 1.1679800748825073 current acc: 0.1384\n",
      "iteration 16 current loss: 0.996564507484436 current acc: 0.1464\n",
      "iteration 17 current loss: 0.8325626254081726 current acc: 0.1556\n",
      "iteration 18 current loss: 0.8391795754432678 current acc: 0.1646\n",
      "iteration 19 current loss: 0.803069531917572 current acc: 0.1736\n",
      "iteration 20 current loss: 0.8464663028717041 current acc: 0.183\n",
      "iteration 21 current loss: 0.6659265160560608 current acc: 0.1934\n",
      "iteration 22 current loss: 1.1447478532791138 current acc: 0.2012\n",
      "iteration 23 current loss: 0.9368389844894409 current acc: 0.2096\n",
      "iteration 24 current loss: 0.8889394402503967 current acc: 0.2182\n",
      "iteration 25 current loss: 0.9744645357131958 current acc: 0.227\n",
      "iteration 26 current loss: 1.008714199066162 current acc: 0.2346\n",
      "iteration 27 current loss: 1.0192664861679077 current acc: 0.2428\n",
      "iteration 28 current loss: 0.9161306619644165 current acc: 0.2514\n",
      "iteration 29 current loss: 0.9641245603561401 current acc: 0.2608\n",
      "iteration 30 current loss: 1.1043943166732788 current acc: 0.2688\n",
      "iteration 31 current loss: 1.1019896268844604 current acc: 0.2768\n",
      "iteration 32 current loss: 0.9259722828865051 current acc: 0.2854\n",
      "iteration 33 current loss: 0.8565393090248108 current acc: 0.2946\n",
      "iteration 34 current loss: 0.8564528822898865 current acc: 0.3046\n",
      "iteration 35 current loss: 1.18032705783844 current acc: 0.312\n",
      "iteration 36 current loss: 1.0599141120910645 current acc: 0.3192\n",
      "iteration 37 current loss: 0.9003125429153442 current acc: 0.3286\n",
      "iteration 38 current loss: 0.9831985831260681 current acc: 0.3366\n",
      "iteration 39 current loss: 1.2825795412063599 current acc: 0.3438\n",
      "iteration 40 current loss: 1.1106919050216675 current acc: 0.351\n",
      "iteration 41 current loss: 1.0937517881393433 current acc: 0.3586\n",
      "iteration 42 current loss: 0.9343444108963013 current acc: 0.3666\n",
      "iteration 43 current loss: 1.0397089719772339 current acc: 0.3742\n",
      "iteration 44 current loss: 0.9264191389083862 current acc: 0.383\n",
      "iteration 45 current loss: 0.84787917137146 current acc: 0.3914\n",
      "iteration 46 current loss: 0.7782503962516785 current acc: 0.4008\n",
      "iteration 47 current loss: 1.0067373514175415 current acc: 0.4096\n",
      "iteration 48 current loss: 0.9204950928688049 current acc: 0.418\n",
      "iteration 49 current loss: 1.1634725332260132 current acc: 0.4252\n",
      "iteration 50 current loss: 0.8818047046661377 current acc: 0.4346\n",
      "iteration 51 current loss: 0.9245452284812927 current acc: 0.4436\n",
      "iteration 52 current loss: 1.093658208847046 current acc: 0.4508\n",
      "iteration 53 current loss: 0.9703624248504639 current acc: 0.4584\n",
      "iteration 54 current loss: 0.8041170835494995 current acc: 0.4678\n",
      "iteration 55 current loss: 0.7649802565574646 current acc: 0.4772\n",
      "iteration 56 current loss: 1.0206562280654907 current acc: 0.4848\n",
      "iteration 57 current loss: 0.9936708807945251 current acc: 0.493\n",
      "iteration 58 current loss: 1.057549238204956 current acc: 0.5004\n",
      "iteration 59 current loss: 0.8816217184066772 current acc: 0.5088\n",
      "iteration 60 current loss: 0.8909153342247009 current acc: 0.5184\n",
      "iteration 61 current loss: 0.9684531092643738 current acc: 0.526\n",
      "iteration 62 current loss: 0.9285779595375061 current acc: 0.534\n",
      "iteration 63 current loss: 1.0492416620254517 current acc: 0.5412\n",
      "iteration 64 current loss: 0.8593076467514038 current acc: 0.5508\n",
      "iteration 65 current loss: 0.896798312664032 current acc: 0.56\n",
      "iteration 66 current loss: 0.986355185508728 current acc: 0.5682\n",
      "iteration 67 current loss: 0.7909413576126099 current acc: 0.5776\n",
      "iteration 68 current loss: 0.9759349822998047 current acc: 0.5868\n",
      "iteration 69 current loss: 0.780829906463623 current acc: 0.5962\n",
      "iteration 70 current loss: 1.122878909111023 current acc: 0.6042\n",
      "iteration 71 current loss: 1.032361626625061 current acc: 0.6118\n",
      "iteration 72 current loss: 0.7024705410003662 current acc: 0.6222\n",
      "iteration 73 current loss: 0.8800751566886902 current acc: 0.6316\n",
      "iteration 74 current loss: 0.8961509466171265 current acc: 0.641\n",
      "iteration 75 current loss: 0.9099264740943909 current acc: 0.6504\n",
      "iteration 76 current loss: 0.9489822387695312 current acc: 0.6586\n",
      "iteration 77 current loss: 0.9582098126411438 current acc: 0.6674\n",
      "iteration 78 current loss: 1.3794612884521484 current acc: 0.6682\n",
      "\t\tTrain Epoch 98/100,Train Accuracy: 0.6682, Train Loss: 0.9529407937315446.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 98/100, Validation Accuracy: 0.59775, Validation Loss: 1.1279334750175476\n",
      "iteration 0 current loss: 0.8942790627479553 current acc: 0.0086\n",
      "iteration 1 current loss: 0.92145836353302 current acc: 0.017\n",
      "iteration 2 current loss: 0.8873510360717773 current acc: 0.0256\n",
      "iteration 3 current loss: 1.0103329420089722 current acc: 0.0342\n",
      "iteration 4 current loss: 1.0248782634735107 current acc: 0.0424\n",
      "iteration 5 current loss: 0.969424843788147 current acc: 0.0516\n",
      "iteration 6 current loss: 1.0015720129013062 current acc: 0.0602\n",
      "iteration 7 current loss: 1.0163698196411133 current acc: 0.0688\n",
      "iteration 8 current loss: 0.9277145862579346 current acc: 0.078\n",
      "iteration 9 current loss: 1.0840628147125244 current acc: 0.0862\n",
      "iteration 10 current loss: 0.9278475046157837 current acc: 0.0942\n",
      "iteration 11 current loss: 0.9578137993812561 current acc: 0.1032\n",
      "iteration 12 current loss: 1.112874150276184 current acc: 0.1104\n",
      "iteration 13 current loss: 1.1119990348815918 current acc: 0.1188\n",
      "iteration 14 current loss: 1.014250636100769 current acc: 0.127\n",
      "iteration 15 current loss: 0.7587167024612427 current acc: 0.1368\n",
      "iteration 16 current loss: 0.9346621036529541 current acc: 0.1454\n",
      "iteration 17 current loss: 0.9672119617462158 current acc: 0.1538\n",
      "iteration 18 current loss: 0.882289469242096 current acc: 0.1624\n",
      "iteration 19 current loss: 1.0568641424179077 current acc: 0.171\n",
      "iteration 20 current loss: 1.1133337020874023 current acc: 0.179\n",
      "iteration 21 current loss: 1.1222221851348877 current acc: 0.1864\n",
      "iteration 22 current loss: 0.7783746123313904 current acc: 0.1964\n",
      "iteration 23 current loss: 1.0942741632461548 current acc: 0.2036\n",
      "iteration 24 current loss: 0.9118873476982117 current acc: 0.213\n",
      "iteration 25 current loss: 1.0217214822769165 current acc: 0.2216\n",
      "iteration 26 current loss: 0.9883503317832947 current acc: 0.2286\n",
      "iteration 27 current loss: 0.9580569267272949 current acc: 0.2372\n",
      "iteration 28 current loss: 1.163596272468567 current acc: 0.2452\n",
      "iteration 29 current loss: 0.9005154371261597 current acc: 0.2538\n",
      "iteration 30 current loss: 1.109620451927185 current acc: 0.2606\n",
      "iteration 31 current loss: 1.037519931793213 current acc: 0.2686\n",
      "iteration 32 current loss: 0.7788165211677551 current acc: 0.2778\n",
      "iteration 33 current loss: 0.8386262655258179 current acc: 0.2872\n",
      "iteration 34 current loss: 1.0619498491287231 current acc: 0.2946\n",
      "iteration 35 current loss: 1.0352333784103394 current acc: 0.3034\n",
      "iteration 36 current loss: 0.9933114647865295 current acc: 0.3114\n",
      "iteration 37 current loss: 1.0040409564971924 current acc: 0.3194\n",
      "iteration 38 current loss: 0.8637474775314331 current acc: 0.3286\n",
      "iteration 39 current loss: 0.854158341884613 current acc: 0.3362\n",
      "iteration 40 current loss: 1.0458215475082397 current acc: 0.3446\n",
      "iteration 41 current loss: 1.0323090553283691 current acc: 0.3522\n",
      "iteration 42 current loss: 0.992760956287384 current acc: 0.3602\n",
      "iteration 43 current loss: 1.2057844400405884 current acc: 0.3668\n",
      "iteration 44 current loss: 0.9837133288383484 current acc: 0.3746\n",
      "iteration 45 current loss: 1.0594693422317505 current acc: 0.3826\n",
      "iteration 46 current loss: 0.940045952796936 current acc: 0.3922\n",
      "iteration 47 current loss: 0.9197226166725159 current acc: 0.4008\n",
      "iteration 48 current loss: 1.1070476770401 current acc: 0.4084\n",
      "iteration 49 current loss: 0.8691511750221252 current acc: 0.4172\n",
      "iteration 50 current loss: 1.0682470798492432 current acc: 0.425\n",
      "iteration 51 current loss: 0.809721827507019 current acc: 0.4346\n",
      "iteration 52 current loss: 0.9786686301231384 current acc: 0.4436\n",
      "iteration 53 current loss: 0.9849209189414978 current acc: 0.4514\n",
      "iteration 54 current loss: 1.0744367837905884 current acc: 0.459\n",
      "iteration 55 current loss: 0.8499298095703125 current acc: 0.4682\n",
      "iteration 56 current loss: 1.1112687587738037 current acc: 0.4762\n",
      "iteration 57 current loss: 1.3203034400939941 current acc: 0.4824\n",
      "iteration 58 current loss: 0.9763858318328857 current acc: 0.491\n",
      "iteration 59 current loss: 0.9627603888511658 current acc: 0.4988\n",
      "iteration 60 current loss: 0.9546828866004944 current acc: 0.5066\n",
      "iteration 61 current loss: 0.9559422135353088 current acc: 0.5146\n",
      "iteration 62 current loss: 0.8863617777824402 current acc: 0.5238\n",
      "iteration 63 current loss: 0.7708442211151123 current acc: 0.534\n",
      "iteration 64 current loss: 0.9229661822319031 current acc: 0.5424\n",
      "iteration 65 current loss: 1.008094310760498 current acc: 0.5508\n",
      "iteration 66 current loss: 0.7127798199653625 current acc: 0.5606\n",
      "iteration 67 current loss: 1.0949290990829468 current acc: 0.5686\n",
      "iteration 68 current loss: 0.836033821105957 current acc: 0.5774\n",
      "iteration 69 current loss: 1.0426539182662964 current acc: 0.585\n",
      "iteration 70 current loss: 0.8043577671051025 current acc: 0.595\n",
      "iteration 71 current loss: 0.9140902161598206 current acc: 0.604\n",
      "iteration 72 current loss: 1.1913868188858032 current acc: 0.6108\n",
      "iteration 73 current loss: 0.9641600251197815 current acc: 0.6192\n",
      "iteration 74 current loss: 0.9300435185432434 current acc: 0.6276\n",
      "iteration 75 current loss: 0.9390211701393127 current acc: 0.6364\n",
      "iteration 76 current loss: 1.0735740661621094 current acc: 0.645\n",
      "iteration 77 current loss: 0.9177111983299255 current acc: 0.6536\n",
      "iteration 78 current loss: 0.9656626582145691 current acc: 0.6546\n",
      "\t\tTrain Epoch 99/100,Train Accuracy: 0.6546, Train Loss: 0.9780898177171056.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 99/100, Validation Accuracy: 0.584875, Validation Loss: 1.1497526454925537\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import STL10\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "learning_rate = 0.0001\n",
    "batch_size = 256\n",
    "weight_decay = 0.04\n",
    "\n",
    "\n",
    "# Create a new transformation that resizes the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    # Convert the PIL Image to Torch Tensor before RandomErasing\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomErasing(p=0.5, scale=(0.2, 0.33), ratio=(0.3, 0.3), value='random'),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Load STL-10 dataset\n",
    "train_dataset = STL10(root='./data', split='train', transform=transform, download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "print(len(train_dataset))\n",
    "print(len(train_loader))\n",
    "\n",
    "test_dataset = STL10(root='./data', split='test', transform=transform, download=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "print(len(test_dataset))\n",
    "print(len(test_loader))\n",
    "\n",
    "\n",
    "# Number of channels in the training images. For color images this is 3\n",
    "nc = 3\n",
    "\n",
    "# Size of z latent vector (i.e. size of generator input)\n",
    "nz = 64\n",
    "\n",
    "# Size of feature maps in discriminator\n",
    "ndf = 96\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 100\n",
    "\n",
    "\n",
    "# Learning rate for optimizers\n",
    "#lr=0.001\n",
    "lr=learning_rate\n",
    "\n",
    "# Beta1 hyperparameter for Adam optimizers\n",
    "beta1 = 0.5\n",
    "\n",
    "# Number of GPUs available. Use 0 for CPU mode.\n",
    "ngpu = 1\n",
    "\n",
    "\n",
    "# Decide which device we want to run on\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu, dim_z, num_classes):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        nc = 3  # Number of input channels for the 96x96x3 image\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 96 x 96\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size: (ndf) x 48 x 48\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size: (ndf*2) x 24 x 24\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size: (ndf*4) x 12 x 12\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size: (ndf*8) x 6 x 6\n",
    "            nn.Conv2d(ndf * 8, dim_z, 6, 1, 0, bias=False)\n",
    "        )\n",
    "        self.fc = nn.Linear(dim_z, num_classes)\n",
    "\n",
    "    def forward(self, input):\n",
    "        z = self.main(input)\n",
    "        z = z.view(input.size(0), -1)  # Flatten z to (batch_size, dim_z)\n",
    "        c = self.fc(z)\n",
    "        return c\n",
    "\n",
    "# Instantiate the model\n",
    "encoder = Discriminator(ngpu=0, dim_z=64, num_classes=10).to(device)\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "encoder.apply(weights_init)\n",
    "\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(encoder.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "# set up wandb\n",
    "wandb.login()\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"dcgan-project\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": lr,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"weight_decay\": weight_decay,\n",
    "    \"epochs\": num_epochs,\n",
    "    \"architecture\": \"SBL-DA\",\n",
    "    \"dataset\": \"STL-10\",\n",
    "    }\n",
    ")\n",
    "    \n",
    "# Training loop\n",
    "best_loss = float('inf')\n",
    "best_model_state = None\n",
    "\n",
    "\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "# For each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss_value = 0\n",
    "    train_correct = 0\n",
    "    \n",
    "    # Set Network to Train Mode\n",
    "    encoder.train()\n",
    "\n",
    "    # For each batch in the dataloader\n",
    "    for i, (data, labels) in enumerate(train_loader, 0): \n",
    "        data_real = data.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = encoder(data_real)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        predicted = torch.argmax(output.data, dim=1).to(device)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        \n",
    "        train_correct += correct\n",
    "        train_loss_value += loss.item()\n",
    "        \n",
    "        train_accuracy = train_correct / len(train_dataset)\n",
    "        print(f'iteration {i} current loss: {loss.item()} current acc: {train_accuracy}')\n",
    "\n",
    "\n",
    "    wandb.log({\"TrainAccuracy\": train_accuracy})\n",
    "\n",
    "    train_loss = train_loss_value / len(train_loader)\n",
    "    wandb.log({\"TrainLoss\": loss.item()})\n",
    "\n",
    "\n",
    "    print(f'\\t\\tTrain Epoch {epoch}/{num_epochs},Train Accuracy: {train_accuracy}, Train Loss: {train_loss}.')\n",
    "\n",
    "\n",
    "\n",
    "    # Validation Step\n",
    "    print('Starting Validation Loop...')\n",
    "    val_correct = 0\n",
    "    val_loss_value = 0\n",
    "    val_running_total = 0\n",
    "\n",
    "    # Set the model to valuation mode\n",
    "    encoder.eval()\n",
    "\n",
    "\n",
    "    # Iterate over the validation dataset in batches\n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_loader:\n",
    "\n",
    "            # Put val data to device (CPU, GPU, or TPU)\n",
    "            data_real = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "\n",
    "            # Forward pass batch through D\n",
    "            output = encoder(data_real)\n",
    "\n",
    "            # Calculate loss on validation batch\n",
    "            v_loss = criterion(output, labels)\n",
    "\n",
    "            # Compute Predicted Labels for a Batch in Validation Dataset\n",
    "            predicted = torch.argmax(output.data, dim=1).to(device)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Update Val Data\n",
    "            val_loss_value += v_loss.item()\n",
    "\n",
    "\n",
    "    val_accuracy = val_correct / len(test_dataset)\n",
    "    wandb.log({\"ValidationAccuracy\": val_accuracy})\n",
    "\n",
    "    \n",
    "    val_loss = val_loss_value / len(test_loader)\n",
    "    wandb.log({\"ValidationLoss\": val_loss})\n",
    "\n",
    "    print(f\"\\t\\tValidation Epoch {epoch}/{num_epochs}, Validation Accuracy: {val_accuracy}, Validation Loss: {val_loss}\")\n",
    "\n",
    "    # Update best model if this epoch had the higest accuracy so far\n",
    "    if train_loss < best_loss:\n",
    "        best_loss = train_loss\n",
    "        print(f'best loss {best_loss}')\n",
    "        best_model_state = encoder.main.state_dict()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Save the best model\n",
    "if best_model_state is not None:\n",
    "    PATH = '../models/sbl_da_{}_{}_{}.pth'.format(learning_rate, batch_size, weight_decay)\n",
    "    torch.save(best_model_state, PATH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47165784-ed4d-47fc-9957-65d3002ba4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fffb447e-ce61-4e0e-baa2-b680b964a5e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c128c672-1c60-414c-b05f-393922823d04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
