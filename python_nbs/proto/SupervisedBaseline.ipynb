{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee692aa2-4962-4051-b67c-d5f906766a50",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f246f1-5c8d-40c1-956c-c92a091d7cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import STL10\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5cb75a-5df2-418b-bdd1-fed3190f3a5b",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6d114a-b43a-48bc-98a5-f020814a2591",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "image_size = 96\n",
    "\n",
    "# Create a new transformation that resizes the images\n",
    "transform=transforms.Compose([\n",
    "                               transforms.Resize(image_size),\n",
    "                               transforms.CenterCrop(image_size),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "\n",
    "# Load STL-10 dataset\n",
    "train_dataset = STL10(root='./data', split='train', transform=transform, download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "print(len(train_dataset))\n",
    "print(len(train_loader))\n",
    "\n",
    "test_dataset = STL10(root='./data', split='test', transform=transform, download=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "print(len(test_dataset))\n",
    "print(len(test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fbd265-736f-43c1-a85b-9a2a746cabe2",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c10e5a0-09e3-4370-8523-b3fda30de789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of channels in the training images. For color images this is 3\n",
    "nc = 3\n",
    "\n",
    "# Size of z latent vector (i.e. size of generator input)\n",
    "nz = 64\n",
    "\n",
    "# Size of feature maps in discriminator\n",
    "ndf = 96\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 50\n",
    "\n",
    "\n",
    "# Learning rate for optimizers\n",
    "lr=0.0001\n",
    "\n",
    "# Beta1 hyperparameter for Adam optimizers\n",
    "beta1 = 0.5\n",
    "\n",
    "# Number of GPUs available. Use 0 for CPU mode.\n",
    "ngpu = 1\n",
    "\n",
    "weight_decay=0.0004\n",
    "\n",
    "# Decide which device we want to run on\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5ad761-83ac-4767-9c35-677cc7634a66",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c4e9ac-1326-4175-a410-ad415cdc6699",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu, dim_z, num_classes):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        nc = 3  # Number of input channels for the 96x96x3 image\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 96 x 96\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size: (ndf) x 48 x 48\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size: (ndf*2) x 24 x 24\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size: (ndf*4) x 12 x 12\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size: (ndf*8) x 6 x 6\n",
    "            nn.Conv2d(ndf * 8, dim_z, 6, 1, 0, bias=False)\n",
    "        )\n",
    "        self.fc = nn.Linear(dim_z, num_classes)\n",
    "\n",
    "    def forward(self, input):\n",
    "        z = self.main(input)\n",
    "        z = z.view(input.size(0), -1)  # Flatten z to (batch_size, dim_z)\n",
    "        c = self.fc(z)\n",
    "        return c\n",
    "\n",
    "# Instantiate the model\n",
    "netD = Discriminator(ngpu=0, dim_z=64, num_classes=10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f9e85e-e7e7-4543-8a93-66e696935de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom weights initialization called on ``netG`` and ``netD``\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "netD.apply(weights_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da68eb03-0d8e-4819-99f5-4faebb27c891",
   "metadata": {},
   "source": [
    "# Criterion / Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab81ecbf-94f4-4f29-85ae-b96810bb91df",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08e7f6c-dea5-4992-94ae-f32a28d7ff5f",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c010e881-8e23-4488-b09c-cba0e8698fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(netD.parameters(), lr=lr, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60379802-c268-4c15-a81c-df9568fe2f4a",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cecb43-30cb-4f28-841d-d7b0bbe3ef2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up wandb\n",
    "wandb.login()\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"dcgan-project\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": lr,\n",
    "    \"weight_decay\": weight_decay,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"architecture\": \"Supervised-Baseline\",\n",
    "    \"dataset\": \"STL-10\",\n",
    "    \"epochs\": num_epochs,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b951135c-73d9-40f9-90fc-7f08c21d1555",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "best_loss = float('inf')\n",
    "best_model_state = None\n",
    "num_train = len(train_dataset)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Set up total loss/acc trackers\n",
    "all_loss = []\n",
    "all_acc = []\n",
    "all_correct = 0\n",
    "train_running_total = 0\n",
    "\n",
    "\n",
    "\n",
    "# Set up epochal loss/acc trackers\n",
    "epoch_loss = []\n",
    "epoch_acc = []\n",
    "\n",
    "\n",
    "# Set up validation loss/acc trackers\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "val_running_total = 0\n",
    "\n",
    "# option 1 - List of Lists\n",
    "metrics = []\n",
    "metrics.append(['train', epoch, accuracy, loss]) # [str, int, float, float]\n",
    "metrics = pd.DataFrame(metrics, columns=['Set Name', 'Epoch', 'ACC', 'Loss'])\n",
    "\n",
    "# option 2 - Dict of Lists\n",
    "metrics = dict()\n",
    "metrics['Train/ACC'] = [] # List of floats\n",
    "metrics['Train/Loss'] = [] # List of floasts\n",
    "metrics = pd.DataFrame.from_dict(metrics)\n",
    "\n",
    "metrics.to_csv(f'{OUTPUT_PATH}/metrics.csv', index=False)\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "\n",
    "# For each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    # Refresh Epoch Statistics\n",
    "    print('reset epoch statistics')\n",
    "    epoch_correct = 0\n",
    "    epoch_loss_val = 0\n",
    "\n",
    "\n",
    "    # Set Network to Train Mode\n",
    "    netD.train()\n",
    "\n",
    "\n",
    "    # For each batch in the dataloader\n",
    "    for i, (data, labels) in enumerate(train_loader, 0):\n",
    "        # Put train data to device (CPU, GPU, or TPU)\n",
    "        data_real = data.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        #  what does this do? why is this needed here?\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass batch through D\n",
    "        output = netD(data_real)\n",
    "\n",
    "\n",
    "        # Calculate loss on batch\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "        # Compute Predicted Labels for a Batch in Training Dataset\n",
    "        predicted = torch.argmax(output.data, dim=1).to(device)\n",
    "\n",
    "\n",
    "\n",
    "        correct = (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Update All Data\n",
    "        all_loss.append(loss.item())\n",
    "\n",
    "        all_correct += correct\n",
    "        train_running_total += labels.size(0)\n",
    "\n",
    "\n",
    "        # Compute All Loss/Acc at each datapoint\n",
    "        all_accuracy = all_correct / train_running_total\n",
    "        all_acc.append(all_accuracy)\n",
    "\n",
    "        print(f'iteration {i} current loss: {loss.item()} current acc: {all_accuracy}')\n",
    "\n",
    "        # Log All metrics to wandb\n",
    "        wandb.log({\"All Loss\": loss.item(), \"All Accuracy\": all_accuracy})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Update Epoch Data\n",
    "        epoch_correct += correct\n",
    "        epoch_loss_val += loss.item()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Compute Epoch Loss/Acc at end of Epoch\n",
    "    epoch_accuracy = epoch_correct / num_train\n",
    "    epoch_acc.append(epoch_accuracy)\n",
    "\n",
    "    avg_epoch_loss = epoch_loss_val / len(train_loader)\n",
    "    epoch_loss.append(avg_epoch_loss)\n",
    "\n",
    "    print(f'\\t\\tEpoch {epoch}/{num_epochs} complete. Epoch loss {avg_epoch_loss} Epoch accuracy {epoch_accuracy}')\n",
    "\n",
    "    # Log Epoch metrics to wandb\n",
    "    wandb.log({\"Epoch Loss\": avg_epoch_loss, \"Epoch Accuracy\": epoch_accuracy})\n",
    "\n",
    "\n",
    "\n",
    "    # Validation Step\n",
    "    print('Starting Validation Loop...')\n",
    "\n",
    "\n",
    "\n",
    "    # Refresh Validation Statistics\n",
    "    print('reset Validation statistics')\n",
    "    val_correct = 0\n",
    "    val_loss_value = 0\n",
    "\n",
    "\n",
    "    # Set the model to valuation mode\n",
    "    netD.eval()\n",
    "\n",
    "\n",
    "    # Iterate over the validation dataset in batches\n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_loader:\n",
    "\n",
    "            # Put val data to device (CPU, GPU, or TPU)\n",
    "            data_real = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "\n",
    "            # Forward pass batch through D\n",
    "            output = netD(data_real)\n",
    "\n",
    "            # Calculate loss on validation batch\n",
    "            v_loss = criterion(output, labels)\n",
    "            wandb.log({\"Epoch val_loss\": v_loss.item()})\n",
    "\n",
    "\n",
    "            # Compute Predicted Labels for a Batch in Validation Dataset\n",
    "            predicted = torch.argmax(output.data, dim=1).to(device)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Update Val Data\n",
    "            val_loss_value += v_loss.item()\n",
    "\n",
    "\n",
    "\n",
    "    val_accuracy = val_correct / len(test_dataset)\n",
    "    val_acc.append(val_accuracy)\n",
    "\n",
    "    avg_val_loss = val_loss_value / len(test_loader)\n",
    "    val_loss.append(avg_val_loss)\n",
    "\n",
    "    print(f\"\\t\\tValidation Epoch {epoch}, Validation Accuracy: {val_accuracy}, Validation Loss: {avg_val_loss}\")\n",
    "\n",
    "\n",
    "    # Log metrics to wandb\n",
    "    wandb.log({\"Validation/Loss\": avg_val_loss})\n",
    "    wandb.log({\"Validation/Accuracy\": val_accuracy})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Update best model if this epoch had the higest accuracy so far\n",
    "    if avg_epoch_loss < best_loss:\n",
    "        best_loss = avg_epoch_loss\n",
    "        print(f'best loss {best_loss}')\n",
    "        best_model_state = netD.main.state_dict()\n",
    "\n",
    "\n",
    "\n",
    "# Save the best model\n",
    "if best_model_state is not None:\n",
    "    PATH = '../models/supervised_baseline_{}_{}_{}.pth'.format(learning_rate, batch_size, weight_decay)\n",
    "    torch.save(best_model_state, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53af41b4-250b-49d4-8a8c-6d73e28d2c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e65a84-31a5-43f5-bb7b-7872ffc7c641",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(all_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047ec00a-1924-4615-a96e-f331cc9ba654",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(all_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d18abc-adeb-4432-b50e-6c7ec1c3a921",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c63daf-c207-415c-977a-c6b2aa5d49e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df509a26-e2c6-4113-aa5d-33c651a6810b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490f4bda-2de3-42ea-a3ae-416c1768473e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
