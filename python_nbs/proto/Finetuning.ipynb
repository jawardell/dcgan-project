{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee692aa2-4962-4051-b67c-d5f906766a50",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39f246f1-5c8d-40c1-956c-c92a091d7cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import STL10\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5cb75a-5df2-418b-bdd1-fed3190f3a5b",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c6d114a-b43a-48bc-98a5-f020814a2591",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "image_size = 96\n",
    "\n",
    "# Create a new transformation that resizes the images\n",
    "transform=transforms.Compose([\n",
    "                               transforms.Resize(image_size),\n",
    "                               transforms.CenterCrop(image_size),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "\n",
    "# Load STL-10 dataset\n",
    "train_dataset = STL10(root='./data', split='train', transform=transform, download=False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "test_dataset = STL10(root='./data', split='test', transform=transform, download=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fbd265-736f-43c1-a85b-9a2a746cabe2",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c10e5a0-09e3-4370-8523-b3fda30de789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of channels in the training images. For color images this is 3\n",
    "nc = 3\n",
    "\n",
    "# Size of z latent vector (i.e. size of generator input)\n",
    "nz = 64\n",
    "\n",
    "# Size of feature maps in encoder\n",
    "ndf = 96\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 100\n",
    "\n",
    "\n",
    "# Learning rate for optimizers\n",
    "lr=0.0001\n",
    "\n",
    "# Beta1 hyperparameter for Adam optimizers\n",
    "beta1 = 0.5\n",
    "\n",
    "# Number of GPUs available. Use 0 for CPU mode.\n",
    "ngpu = 1\n",
    "\n",
    "# Weight  Decay\n",
    "weight_decay = 0.0004\n",
    "\n",
    "\n",
    "# Decide which device we want to run on\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5ad761-83ac-4767-9c35-677cc7634a66",
   "metadata": {},
   "source": [
    "# Supervised Encoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3c4e9ac-1326-4175-a410-ad415cdc6699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (main): Sequential(\n",
       "    (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (2): Conv2d(96, 192, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (3): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (5): Conv2d(192, 384, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (6): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (8): Conv2d(384, 768, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (9): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (11): Conv2d(768, 64, kernel_size=(6, 6), stride=(1, 1), bias=False)\n",
       "  )\n",
       "  (fc): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, ngpu, dim_z, num_classes):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        nc = 3  # Number of input channels for the 96x96x3 image\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 96 x 96\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size: (ndf) x 48 x 48\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size: (ndf*2) x 24 x 24\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size: (ndf*4) x 12 x 12\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size: (ndf*8) x 6 x 6\n",
    "            nn.Conv2d(ndf * 8, dim_z, 6, 1, 0, bias=False)\n",
    "        )\n",
    "        self.fc = nn.Linear(dim_z, num_classes)\n",
    "\n",
    "    def forward(self, input):\n",
    "        z = self.main(input)\n",
    "        z = z.view(input.size(0), -1)  # Flatten z to (batch_size, dim_z)\n",
    "        c = self.fc(z)\n",
    "        return c\n",
    "\n",
    "# Load Pretrained Weights\n",
    "encoder = Encoder(ngpu=0, dim_z=64, num_classes=10).to(device)\n",
    "PATH='/data/users2/jwardell1/dcgan-project/models/ae_pretraining_0.0001_256_0.0004.pth'\n",
    "encoder.main.load_state_dict(torch.load(PATH))\n",
    "encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da68eb03-0d8e-4819-99f5-4faebb27c891",
   "metadata": {},
   "source": [
    "# Criterion / Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab81ecbf-94f4-4f29-85ae-b96810bb91df",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08e7f6c-dea5-4992-94ae-f32a28d7ff5f",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c010e881-8e23-4488-b09c-cba0e8698fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(encoder.parameters(), lr=lr, weight_decay=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60379802-c268-4c15-a81c-df9568fe2f4a",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1d2d7de-bb51-4bd1-a102-cfd6242f4c7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/users2/jwardell1/dcgan-project/python_nbs/wandb/run-20231126_224643-1z9k16h8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jawardell/dcgan-project/runs/1z9k16h8' target=\"_blank\">copper-star-79</a></strong> to <a href='https://wandb.ai/jawardell/dcgan-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jawardell/dcgan-project' target=\"_blank\">https://wandb.ai/jawardell/dcgan-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jawardell/dcgan-project/runs/1z9k16h8' target=\"_blank\">https://wandb.ai/jawardell/dcgan-project/runs/1z9k16h8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/jawardell/dcgan-project/runs/1z9k16h8?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7ff2c4a6b690>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set up wandb\n",
    "wandb.login()\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"dcgan-project\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": lr,\n",
    "    \"weight_decay\": weight_decay,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"architecture\": \"Encoder Finetuning\",\n",
    "    \"dataset\": \"STL-10\",\n",
    "    \"epochs\": num_epochs,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b951135c-73d9-40f9-90fc-7f08c21d1555",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Loop...\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 2.2734405994415283 current acc: 0.18\n",
      "iteration 1 current loss: 2.3048720359802246 current acc: 0.14\n",
      "iteration 2 current loss: 2.2935402393341064 current acc: 0.12666666666666668\n",
      "iteration 3 current loss: 2.28391170501709 current acc: 0.13\n",
      "iteration 4 current loss: 2.2932605743408203 current acc: 0.136\n",
      "iteration 5 current loss: 2.2997336387634277 current acc: 0.12333333333333334\n",
      "iteration 6 current loss: 2.3006174564361572 current acc: 0.10857142857142857\n",
      "iteration 7 current loss: 2.296865463256836 current acc: 0.1125\n",
      "iteration 8 current loss: 2.265316963195801 current acc: 0.12666666666666668\n",
      "iteration 9 current loss: 2.285517692565918 current acc: 0.124\n",
      "iteration 10 current loss: 2.269239664077759 current acc: 0.13090909090909092\n",
      "iteration 11 current loss: 2.289463758468628 current acc: 0.12666666666666668\n",
      "iteration 12 current loss: 2.2634096145629883 current acc: 0.13230769230769232\n",
      "iteration 13 current loss: 2.2684779167175293 current acc: 0.13714285714285715\n",
      "iteration 14 current loss: 2.263294219970703 current acc: 0.14\n",
      "iteration 15 current loss: 2.269684076309204 current acc: 0.14375\n",
      "iteration 16 current loss: 2.2408154010772705 current acc: 0.14705882352941177\n",
      "iteration 17 current loss: 2.2562029361724854 current acc: 0.15222222222222223\n",
      "iteration 18 current loss: 2.2513582706451416 current acc: 0.15157894736842106\n",
      "iteration 19 current loss: 2.2584118843078613 current acc: 0.155\n",
      "iteration 20 current loss: 2.221940279006958 current acc: 0.15904761904761905\n",
      "iteration 21 current loss: 2.215003728866577 current acc: 0.1618181818181818\n",
      "iteration 22 current loss: 2.2150237560272217 current acc: 0.16608695652173913\n",
      "iteration 23 current loss: 2.213135004043579 current acc: 0.17416666666666666\n",
      "iteration 24 current loss: 2.232370376586914 current acc: 0.1752\n",
      "iteration 25 current loss: 2.2261383533477783 current acc: 0.17384615384615384\n",
      "iteration 26 current loss: 2.1916141510009766 current acc: 0.17851851851851852\n",
      "iteration 27 current loss: 2.2028236389160156 current acc: 0.18\n",
      "iteration 28 current loss: 2.1855900287628174 current acc: 0.1820689655172414\n",
      "iteration 29 current loss: 2.1563403606414795 current acc: 0.186\n",
      "iteration 30 current loss: 2.164977788925171 current acc: 0.18838709677419355\n",
      "iteration 31 current loss: 2.131971597671509 current acc: 0.19375\n",
      "iteration 32 current loss: 2.136674404144287 current acc: 0.19393939393939394\n",
      "iteration 33 current loss: 2.1388437747955322 current acc: 0.19823529411764707\n",
      "iteration 34 current loss: 2.115407943725586 current acc: 0.2022857142857143\n",
      "iteration 35 current loss: 2.1585090160369873 current acc: 0.20166666666666666\n",
      "iteration 36 current loss: 2.13864803314209 current acc: 0.20270270270270271\n",
      "iteration 37 current loss: 2.1320271492004395 current acc: 0.20157894736842105\n",
      "iteration 38 current loss: 2.0826117992401123 current acc: 0.20461538461538462\n",
      "iteration 39 current loss: 2.0629520416259766 current acc: 0.208\n",
      "iteration 40 current loss: 2.0158019065856934 current acc: 0.21317073170731707\n",
      "iteration 41 current loss: 2.093588352203369 current acc: 0.21285714285714286\n",
      "iteration 42 current loss: 2.105891704559326 current acc: 0.21348837209302327\n",
      "iteration 43 current loss: 2.060065507888794 current acc: 0.21545454545454545\n",
      "iteration 44 current loss: 2.057762384414673 current acc: 0.21688888888888888\n",
      "iteration 45 current loss: 2.074920415878296 current acc: 0.21608695652173912\n",
      "iteration 46 current loss: 2.0420756340026855 current acc: 0.2170212765957447\n",
      "iteration 47 current loss: 2.017305374145508 current acc: 0.21875\n",
      "iteration 48 current loss: 2.0193376541137695 current acc: 0.22163265306122448\n",
      "iteration 49 current loss: 1.982418417930603 current acc: 0.2224\n",
      "iteration 50 current loss: 1.948927640914917 current acc: 0.22509803921568627\n",
      "iteration 51 current loss: 1.9655532836914062 current acc: 0.22653846153846155\n",
      "iteration 52 current loss: 1.9466526508331299 current acc: 0.2309433962264151\n",
      "iteration 53 current loss: 2.0374181270599365 current acc: 0.2311111111111111\n",
      "iteration 54 current loss: 2.019033432006836 current acc: 0.22945454545454547\n",
      "iteration 55 current loss: 1.9582287073135376 current acc: 0.23107142857142857\n",
      "iteration 56 current loss: 1.976351022720337 current acc: 0.23157894736842105\n",
      "iteration 57 current loss: 1.960800290107727 current acc: 0.23310344827586207\n",
      "iteration 58 current loss: 1.939394235610962 current acc: 0.23559322033898306\n",
      "iteration 59 current loss: 1.9217064380645752 current acc: 0.23933333333333334\n",
      "iteration 60 current loss: 2.0249130725860596 current acc: 0.239672131147541\n",
      "iteration 61 current loss: 1.9124395847320557 current acc: 0.24161290322580645\n",
      "iteration 62 current loss: 1.9697155952453613 current acc: 0.2415873015873016\n",
      "iteration 63 current loss: 1.9476487636566162 current acc: 0.2425\n",
      "iteration 64 current loss: 1.8478726148605347 current acc: 0.2449230769230769\n",
      "iteration 65 current loss: 1.881723165512085 current acc: 0.24696969696969698\n",
      "iteration 66 current loss: 1.8597357273101807 current acc: 0.2480597014925373\n",
      "iteration 67 current loss: 1.8935141563415527 current acc: 0.25029411764705883\n",
      "iteration 68 current loss: 1.8793816566467285 current acc: 0.25130434782608696\n",
      "iteration 69 current loss: 1.9354678392410278 current acc: 0.2522857142857143\n",
      "iteration 70 current loss: 1.807073950767517 current acc: 0.2540845070422535\n",
      "iteration 71 current loss: 1.7965840101242065 current acc: 0.2561111111111111\n",
      "iteration 72 current loss: 1.8752697706222534 current acc: 0.25643835616438354\n",
      "iteration 73 current loss: 1.79970383644104 current acc: 0.2578378378378378\n",
      "iteration 74 current loss: 1.8084841966629028 current acc: 0.25866666666666666\n",
      "iteration 75 current loss: 1.8528013229370117 current acc: 0.25842105263157894\n",
      "iteration 76 current loss: 1.8836662769317627 current acc: 0.25766233766233765\n",
      "iteration 77 current loss: 1.9346907138824463 current acc: 0.25743589743589745\n",
      "iteration 78 current loss: 1.712114691734314 current acc: 0.25848101265822787\n",
      "iteration 79 current loss: 1.789947509765625 current acc: 0.258\n",
      "iteration 80 current loss: 1.986069679260254 current acc: 0.2565432098765432\n",
      "iteration 81 current loss: 1.9687007665634155 current acc: 0.25585365853658537\n",
      "iteration 82 current loss: 1.8791404962539673 current acc: 0.256144578313253\n",
      "iteration 83 current loss: 1.7541357278823853 current acc: 0.2573809523809524\n",
      "iteration 84 current loss: 1.8405075073242188 current acc: 0.25694117647058823\n",
      "iteration 85 current loss: 1.7331123352050781 current acc: 0.25720930232558137\n",
      "iteration 86 current loss: 1.8658889532089233 current acc: 0.25701149425287356\n",
      "iteration 87 current loss: 1.812959909439087 current acc: 0.25772727272727275\n",
      "iteration 88 current loss: 1.7819842100143433 current acc: 0.25820224719101126\n",
      "iteration 89 current loss: 1.7947211265563965 current acc: 0.2584444444444444\n",
      "iteration 90 current loss: 1.8293747901916504 current acc: 0.258021978021978\n",
      "iteration 91 current loss: 1.6587539911270142 current acc: 0.2597826086956522\n",
      "iteration 92 current loss: 1.806688666343689 current acc: 0.26\n",
      "iteration 93 current loss: 1.7176247835159302 current acc: 0.26170212765957446\n",
      "iteration 94 current loss: 1.8231812715530396 current acc: 0.2616842105263158\n",
      "iteration 95 current loss: 1.804388403892517 current acc: 0.2629166666666667\n",
      "iteration 96 current loss: 1.7872675657272339 current acc: 0.2622680412371134\n",
      "iteration 97 current loss: 1.8877983093261719 current acc: 0.2620408163265306\n",
      "iteration 98 current loss: 1.841079831123352 current acc: 0.26222222222222225\n",
      "iteration 99 current loss: 1.7267913818359375 current acc: 0.2624\n",
      "\t\tEpoch 0/100 complete. Epoch loss 2.0270618867874144 Epoch accuracy 0.2624\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 0, Validation Accuracy: 0.316375, Validation Loss: 1.7510449096560479\n",
      "best loss 2.0270618867874144\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 1.7119594812393188 current acc: 0.2637623762376238\n",
      "iteration 1 current loss: 1.6965267658233643 current acc: 0.2643137254901961\n",
      "iteration 2 current loss: 1.7728371620178223 current acc: 0.26485436893203884\n",
      "iteration 3 current loss: 1.7371444702148438 current acc: 0.265\n",
      "iteration 4 current loss: 1.7152477502822876 current acc: 0.26666666666666666\n",
      "iteration 5 current loss: 1.6258840560913086 current acc: 0.26867924528301884\n",
      "iteration 6 current loss: 1.7087210416793823 current acc: 0.26897196261682244\n",
      "iteration 7 current loss: 1.6746524572372437 current acc: 0.26981481481481484\n",
      "iteration 8 current loss: 1.818581223487854 current acc: 0.26972477064220185\n",
      "iteration 9 current loss: 1.6833491325378418 current acc: 0.27090909090909093\n",
      "iteration 10 current loss: 1.7956303358078003 current acc: 0.2708108108108108\n",
      "iteration 11 current loss: 1.6033341884613037 current acc: 0.2725\n",
      "iteration 12 current loss: 1.711818814277649 current acc: 0.2732743362831858\n",
      "iteration 13 current loss: 1.7459993362426758 current acc: 0.27473684210526317\n",
      "iteration 14 current loss: 1.7518486976623535 current acc: 0.2751304347826087\n",
      "iteration 15 current loss: 1.6302562952041626 current acc: 0.2760344827586207\n",
      "iteration 16 current loss: 1.6200953722000122 current acc: 0.2764102564102564\n",
      "iteration 17 current loss: 1.6333848237991333 current acc: 0.27728813559322035\n",
      "iteration 18 current loss: 1.732111930847168 current acc: 0.27747899159663864\n",
      "iteration 19 current loss: 1.667836308479309 current acc: 0.2776666666666667\n",
      "iteration 20 current loss: 1.668623447418213 current acc: 0.2781818181818182\n",
      "iteration 21 current loss: 1.6953469514846802 current acc: 0.27852459016393444\n",
      "iteration 22 current loss: 1.6567801237106323 current acc: 0.27902439024390246\n",
      "iteration 23 current loss: 1.6389330625534058 current acc: 0.27983870967741936\n",
      "iteration 24 current loss: 1.7000311613082886 current acc: 0.28032\n",
      "iteration 25 current loss: 1.8238393068313599 current acc: 0.2804761904761905\n",
      "iteration 26 current loss: 1.7327601909637451 current acc: 0.28062992125984254\n",
      "iteration 27 current loss: 1.6698299646377563 current acc: 0.28125\n",
      "iteration 28 current loss: 1.6462582349777222 current acc: 0.28232558139534886\n",
      "iteration 29 current loss: 1.705276370048523 current acc: 0.28307692307692306\n",
      "iteration 30 current loss: 1.6349018812179565 current acc: 0.2830534351145038\n",
      "iteration 31 current loss: 1.6308552026748657 current acc: 0.2837878787878788\n",
      "iteration 32 current loss: 1.6403177976608276 current acc: 0.28466165413533834\n",
      "iteration 33 current loss: 1.537418246269226 current acc: 0.2852238805970149\n",
      "iteration 34 current loss: 1.657652735710144 current acc: 0.2848888888888889\n",
      "iteration 35 current loss: 1.655889868736267 current acc: 0.28544117647058825\n",
      "iteration 36 current loss: 1.7106738090515137 current acc: 0.2854014598540146\n",
      "iteration 37 current loss: 1.5534775257110596 current acc: 0.28594202898550725\n",
      "iteration 38 current loss: 1.529813289642334 current acc: 0.28633093525179854\n",
      "iteration 39 current loss: 1.485784888267517 current acc: 0.28714285714285714\n",
      "iteration 40 current loss: 1.692897081375122 current acc: 0.28709219858156027\n",
      "iteration 41 current loss: 1.5437415838241577 current acc: 0.28774647887323945\n",
      "iteration 42 current loss: 1.7904363870620728 current acc: 0.2871328671328671\n",
      "iteration 43 current loss: 1.616713047027588 current acc: 0.2872222222222222\n",
      "iteration 44 current loss: 1.536136507987976 current acc: 0.2891034482758621\n",
      "iteration 45 current loss: 1.5957114696502686 current acc: 0.29027397260273974\n",
      "iteration 46 current loss: 1.6644552946090698 current acc: 0.29061224489795917\n",
      "iteration 47 current loss: 1.804785966873169 current acc: 0.29\n",
      "iteration 48 current loss: 1.5960863828659058 current acc: 0.29114093959731546\n",
      "iteration 49 current loss: 1.5401419401168823 current acc: 0.29146666666666665\n",
      "iteration 50 current loss: 1.5267153978347778 current acc: 0.2928476821192053\n",
      "iteration 51 current loss: 1.5314065217971802 current acc: 0.2930263157894737\n",
      "iteration 52 current loss: 1.5996017456054688 current acc: 0.29333333333333333\n",
      "iteration 53 current loss: 1.66045343875885 current acc: 0.2937662337662338\n",
      "iteration 54 current loss: 1.6340030431747437 current acc: 0.29496774193548386\n",
      "iteration 55 current loss: 1.4476230144500732 current acc: 0.2958974358974359\n",
      "iteration 56 current loss: 1.6411452293395996 current acc: 0.2960509554140127\n",
      "iteration 57 current loss: 1.8700828552246094 current acc: 0.2956962025316456\n",
      "iteration 58 current loss: 1.5360019207000732 current acc: 0.29660377358490564\n",
      "iteration 59 current loss: 1.7718493938446045 current acc: 0.296375\n",
      "iteration 60 current loss: 1.6445938348770142 current acc: 0.2965217391304348\n",
      "iteration 61 current loss: 1.6734347343444824 current acc: 0.2971604938271605\n",
      "iteration 62 current loss: 1.6163935661315918 current acc: 0.29680981595092026\n",
      "iteration 63 current loss: 1.5900355577468872 current acc: 0.2973170731707317\n",
      "iteration 64 current loss: 1.5805518627166748 current acc: 0.29793939393939395\n",
      "iteration 65 current loss: 1.5202354192733765 current acc: 0.29843373493975905\n",
      "iteration 66 current loss: 1.5611540079116821 current acc: 0.29952095808383233\n",
      "iteration 67 current loss: 1.4318976402282715 current acc: 0.3005952380952381\n",
      "iteration 68 current loss: 1.6625721454620361 current acc: 0.30118343195266273\n",
      "iteration 69 current loss: 1.691989541053772 current acc: 0.30164705882352943\n",
      "iteration 70 current loss: 1.6471757888793945 current acc: 0.3018713450292398\n",
      "iteration 71 current loss: 1.9533681869506836 current acc: 0.3016279069767442\n",
      "iteration 72 current loss: 1.6763087511062622 current acc: 0.30184971098265895\n",
      "iteration 73 current loss: 1.559821605682373 current acc: 0.3026436781609195\n",
      "iteration 74 current loss: 1.6856133937835693 current acc: 0.3032\n",
      "iteration 75 current loss: 1.6582653522491455 current acc: 0.3038636363636364\n",
      "iteration 76 current loss: 1.480606198310852 current acc: 0.30542372881355934\n",
      "iteration 77 current loss: 1.527649998664856 current acc: 0.30674157303370786\n",
      "iteration 78 current loss: 1.549086093902588 current acc: 0.30726256983240224\n",
      "iteration 79 current loss: 1.5376455783843994 current acc: 0.3081111111111111\n",
      "iteration 80 current loss: 1.672045111656189 current acc: 0.3079558011049724\n",
      "iteration 81 current loss: 1.5746287107467651 current acc: 0.308021978021978\n",
      "iteration 82 current loss: 1.475493311882019 current acc: 0.3085245901639344\n",
      "iteration 83 current loss: 1.4090144634246826 current acc: 0.3095652173913043\n",
      "iteration 84 current loss: 1.6540205478668213 current acc: 0.3095135135135135\n",
      "iteration 85 current loss: 1.4696059226989746 current acc: 0.30989247311827955\n",
      "iteration 86 current loss: 1.5463699102401733 current acc: 0.3104812834224599\n",
      "iteration 87 current loss: 1.4740043878555298 current acc: 0.31127659574468086\n",
      "iteration 88 current loss: 1.376839280128479 current acc: 0.31174603174603177\n",
      "iteration 89 current loss: 1.5324403047561646 current acc: 0.31242105263157893\n",
      "iteration 90 current loss: 1.4249650239944458 current acc: 0.3134031413612565\n",
      "iteration 91 current loss: 1.3765264749526978 current acc: 0.3140625\n",
      "iteration 92 current loss: 1.424221396446228 current acc: 0.31523316062176165\n",
      "iteration 93 current loss: 1.3695249557495117 current acc: 0.3163917525773196\n",
      "iteration 94 current loss: 1.5726690292358398 current acc: 0.3169230769230769\n",
      "iteration 95 current loss: 1.550939679145813 current acc: 0.3174489795918367\n",
      "iteration 96 current loss: 1.4721312522888184 current acc: 0.3182741116751269\n",
      "iteration 97 current loss: 1.6335357427597046 current acc: 0.3188888888888889\n",
      "iteration 98 current loss: 1.4387729167938232 current acc: 0.31959798994974875\n",
      "iteration 99 current loss: 1.7912379503250122 current acc: 0.3195\n",
      "\t\tEpoch 1/100 complete. Epoch loss 1.6209905755519867 Epoch accuracy 0.3766\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 1, Validation Accuracy: 0.39075, Validation Loss: 1.641449087113142\n",
      "best loss 1.6209905755519867\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 1.6449220180511475 current acc: 0.3195024875621891\n",
      "iteration 1 current loss: 1.4274388551712036 current acc: 0.3201980198019802\n",
      "iteration 2 current loss: 1.3642463684082031 current acc: 0.3211822660098522\n",
      "iteration 3 current loss: 1.5989794731140137 current acc: 0.32166666666666666\n",
      "iteration 4 current loss: 1.6712020635604858 current acc: 0.32195121951219513\n",
      "iteration 5 current loss: 1.430910348892212 current acc: 0.322621359223301\n",
      "iteration 6 current loss: 1.5388522148132324 current acc: 0.32280193236714977\n",
      "iteration 7 current loss: 1.5505690574645996 current acc: 0.3227884615384615\n",
      "iteration 8 current loss: 1.304802417755127 current acc: 0.3241148325358852\n",
      "iteration 9 current loss: 1.5484024286270142 current acc: 0.3242857142857143\n",
      "iteration 10 current loss: 1.5177127122879028 current acc: 0.3248341232227488\n",
      "iteration 11 current loss: 1.4651991128921509 current acc: 0.325188679245283\n",
      "iteration 12 current loss: 1.4653760194778442 current acc: 0.32572769953051645\n",
      "iteration 13 current loss: 1.6452864408493042 current acc: 0.3257943925233645\n",
      "iteration 14 current loss: 1.5239254236221313 current acc: 0.32595348837209304\n",
      "iteration 15 current loss: 1.5474773645401 current acc: 0.32592592592592595\n",
      "iteration 16 current loss: 1.4942984580993652 current acc: 0.32645161290322583\n",
      "iteration 17 current loss: 1.4883100986480713 current acc: 0.32697247706422017\n",
      "iteration 18 current loss: 1.4500259160995483 current acc: 0.3275799086757991\n",
      "iteration 19 current loss: 1.5738641023635864 current acc: 0.32745454545454544\n",
      "iteration 20 current loss: 1.493719458580017 current acc: 0.32823529411764707\n",
      "iteration 21 current loss: 1.4584825038909912 current acc: 0.3286486486486486\n",
      "iteration 22 current loss: 1.7316497564315796 current acc: 0.3283408071748879\n",
      "iteration 23 current loss: 1.6639971733093262 current acc: 0.3283035714285714\n",
      "iteration 24 current loss: 1.479770302772522 current acc: 0.32853333333333334\n",
      "iteration 25 current loss: 1.587284803390503 current acc: 0.3292035398230089\n",
      "iteration 26 current loss: 1.6458019018173218 current acc: 0.3290748898678414\n",
      "iteration 27 current loss: 1.3910703659057617 current acc: 0.32956140350877194\n",
      "iteration 28 current loss: 1.6985608339309692 current acc: 0.32995633187772927\n",
      "iteration 29 current loss: 1.418020486831665 current acc: 0.3307826086956522\n",
      "iteration 30 current loss: 1.4512706995010376 current acc: 0.3313419913419913\n",
      "iteration 31 current loss: 1.4695664644241333 current acc: 0.33198275862068966\n",
      "iteration 32 current loss: 1.394873857498169 current acc: 0.332274678111588\n",
      "iteration 33 current loss: 1.3581080436706543 current acc: 0.3330769230769231\n",
      "iteration 34 current loss: 1.462736964225769 current acc: 0.3334468085106383\n",
      "iteration 35 current loss: 1.6590893268585205 current acc: 0.33347457627118643\n",
      "iteration 36 current loss: 1.7659777402877808 current acc: 0.33350210970464134\n",
      "iteration 37 current loss: 1.4047691822052002 current acc: 0.3335294117647059\n",
      "iteration 38 current loss: 1.580310344696045 current acc: 0.3337238493723849\n",
      "iteration 39 current loss: 1.3517581224441528 current acc: 0.33408333333333334\n",
      "iteration 40 current loss: 1.597426176071167 current acc: 0.33427385892116185\n",
      "iteration 41 current loss: 1.5594825744628906 current acc: 0.33429752066115703\n",
      "iteration 42 current loss: 1.4508023262023926 current acc: 0.3346502057613169\n",
      "iteration 43 current loss: 1.5784677267074585 current acc: 0.3348360655737705\n",
      "iteration 44 current loss: 1.6250745058059692 current acc: 0.335265306122449\n",
      "iteration 45 current loss: 1.468544602394104 current acc: 0.335609756097561\n",
      "iteration 46 current loss: 1.462782382965088 current acc: 0.3362753036437247\n",
      "iteration 47 current loss: 1.5122065544128418 current acc: 0.3367741935483871\n",
      "iteration 48 current loss: 1.4430720806121826 current acc: 0.33759036144578314\n",
      "iteration 49 current loss: 1.5029562711715698 current acc: 0.33784\n",
      "iteration 50 current loss: 1.2932822704315186 current acc: 0.33896414342629483\n",
      "iteration 51 current loss: 1.2960089445114136 current acc: 0.3395238095238095\n",
      "iteration 52 current loss: 1.441311240196228 current acc: 0.33992094861660077\n",
      "iteration 53 current loss: 1.4662485122680664 current acc: 0.3403149606299213\n",
      "iteration 54 current loss: 1.6137487888336182 current acc: 0.3405490196078431\n",
      "iteration 55 current loss: 1.4199295043945312 current acc: 0.341171875\n",
      "iteration 56 current loss: 1.5301264524459839 current acc: 0.34155642023346305\n",
      "iteration 57 current loss: 1.3838679790496826 current acc: 0.341937984496124\n",
      "iteration 58 current loss: 1.5017876625061035 current acc: 0.34254826254826254\n",
      "iteration 59 current loss: 1.3762524127960205 current acc: 0.34323076923076923\n",
      "iteration 60 current loss: 1.3968243598937988 current acc: 0.3435249042145594\n",
      "iteration 61 current loss: 1.4005016088485718 current acc: 0.34389312977099235\n",
      "iteration 62 current loss: 1.499355673789978 current acc: 0.34403041825095054\n",
      "iteration 63 current loss: 1.4286195039749146 current acc: 0.34424242424242424\n",
      "iteration 64 current loss: 1.526047945022583 current acc: 0.3442264150943396\n",
      "iteration 65 current loss: 1.5835744142532349 current acc: 0.3445112781954887\n",
      "iteration 66 current loss: 1.4612029790878296 current acc: 0.3449438202247191\n",
      "iteration 67 current loss: 1.3051966428756714 current acc: 0.34567164179104476\n",
      "iteration 68 current loss: 1.3618885278701782 current acc: 0.3463197026022305\n",
      "iteration 69 current loss: 1.485589861869812 current acc: 0.3466666666666667\n",
      "iteration 70 current loss: 1.5152826309204102 current acc: 0.3465682656826568\n",
      "iteration 71 current loss: 1.4840753078460693 current acc: 0.3472794117647059\n",
      "iteration 72 current loss: 1.4488590955734253 current acc: 0.3473992673992674\n",
      "iteration 73 current loss: 1.522576093673706 current acc: 0.3475912408759124\n",
      "iteration 74 current loss: 1.3103731870651245 current acc: 0.3482181818181818\n",
      "iteration 75 current loss: 1.3304153680801392 current acc: 0.3488405797101449\n",
      "iteration 76 current loss: 1.4542698860168457 current acc: 0.34924187725631767\n",
      "iteration 77 current loss: 1.3818467855453491 current acc: 0.34992805755395684\n",
      "iteration 78 current loss: 1.506333351135254 current acc: 0.35003584229390683\n",
      "iteration 79 current loss: 1.4554507732391357 current acc: 0.35014285714285714\n",
      "iteration 80 current loss: 1.4749990701675415 current acc: 0.3504626334519573\n",
      "iteration 81 current loss: 1.4696992635726929 current acc: 0.3506382978723404\n",
      "iteration 82 current loss: 1.4178401231765747 current acc: 0.3511660777385159\n",
      "iteration 83 current loss: 1.4678877592086792 current acc: 0.3515492957746479\n",
      "iteration 84 current loss: 1.4475829601287842 current acc: 0.352\n",
      "iteration 85 current loss: 1.3265488147735596 current acc: 0.3523776223776224\n",
      "iteration 86 current loss: 1.500778079032898 current acc: 0.3521951219512195\n",
      "iteration 87 current loss: 1.5745353698730469 current acc: 0.35215277777777776\n",
      "iteration 88 current loss: 1.3461635112762451 current acc: 0.35252595155709343\n",
      "iteration 89 current loss: 1.6151018142700195 current acc: 0.3526896551724138\n",
      "iteration 90 current loss: 1.5322632789611816 current acc: 0.3527835051546392\n",
      "iteration 91 current loss: 1.3315860033035278 current acc: 0.35342465753424657\n",
      "iteration 92 current loss: 1.3509433269500732 current acc: 0.35378839590443684\n",
      "iteration 93 current loss: 1.3733428716659546 current acc: 0.35428571428571426\n",
      "iteration 94 current loss: 1.4473681449890137 current acc: 0.3544406779661017\n",
      "iteration 95 current loss: 1.2998242378234863 current acc: 0.35452702702702704\n",
      "iteration 96 current loss: 1.4554226398468018 current acc: 0.3548148148148148\n",
      "iteration 97 current loss: 1.2973403930664062 current acc: 0.3553691275167785\n",
      "iteration 98 current loss: 1.3548588752746582 current acc: 0.3559866220735786\n",
      "iteration 99 current loss: 1.3773183822631836 current acc: 0.35673333333333335\n",
      "\t\tEpoch 2/100 complete. Epoch loss 1.4756168711185456 Epoch accuracy 0.4312\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 2, Validation Accuracy: 0.4385, Validation Loss: 1.4447669081389904\n",
      "best loss 1.4756168711185456\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 1.404208779335022 current acc: 0.35714285714285715\n",
      "iteration 1 current loss: 1.4575976133346558 current acc: 0.3574834437086093\n",
      "iteration 2 current loss: 1.2624146938323975 current acc: 0.3582178217821782\n",
      "iteration 3 current loss: 1.3145369291305542 current acc: 0.358421052631579\n",
      "iteration 4 current loss: 1.416406512260437 current acc: 0.3586229508196721\n",
      "iteration 5 current loss: 1.2396565675735474 current acc: 0.359281045751634\n",
      "iteration 6 current loss: 1.5161441564559937 current acc: 0.35960912052117266\n",
      "iteration 7 current loss: 1.273073673248291 current acc: 0.36\n",
      "iteration 8 current loss: 1.204369068145752 current acc: 0.36077669902912624\n",
      "iteration 9 current loss: 1.474577784538269 current acc: 0.3610322580645161\n",
      "iteration 10 current loss: 1.4226146936416626 current acc: 0.3612861736334405\n",
      "iteration 11 current loss: 1.3226019144058228 current acc: 0.3619871794871795\n",
      "iteration 12 current loss: 1.4770008325576782 current acc: 0.3620447284345048\n",
      "iteration 13 current loss: 1.2631845474243164 current acc: 0.36235668789808917\n",
      "iteration 14 current loss: 1.2870705127716064 current acc: 0.3627936507936508\n",
      "iteration 15 current loss: 1.3486924171447754 current acc: 0.36322784810126585\n",
      "iteration 16 current loss: 1.2042092084884644 current acc: 0.36384858044164037\n",
      "iteration 17 current loss: 1.5501192808151245 current acc: 0.36383647798742136\n",
      "iteration 18 current loss: 1.6722731590270996 current acc: 0.3638871473354232\n",
      "iteration 19 current loss: 1.2864378690719604 current acc: 0.3641875\n",
      "iteration 20 current loss: 1.2130180597305298 current acc: 0.36467289719626167\n",
      "iteration 21 current loss: 1.2627559900283813 current acc: 0.36527950310559004\n",
      "iteration 22 current loss: 1.4170868396759033 current acc: 0.3656346749226006\n",
      "iteration 23 current loss: 1.2075270414352417 current acc: 0.36617283950617285\n",
      "iteration 24 current loss: 1.4570101499557495 current acc: 0.36646153846153845\n",
      "iteration 25 current loss: 1.3441792726516724 current acc: 0.36687116564417177\n",
      "iteration 26 current loss: 1.3568261861801147 current acc: 0.3674617737003058\n",
      "iteration 27 current loss: 1.3257315158843994 current acc: 0.3678658536585366\n",
      "iteration 28 current loss: 1.5209215879440308 current acc: 0.3677203647416413\n",
      "iteration 29 current loss: 1.3209741115570068 current acc: 0.368\n",
      "iteration 30 current loss: 1.248740792274475 current acc: 0.3686404833836858\n",
      "iteration 31 current loss: 1.3360307216644287 current acc: 0.368855421686747\n",
      "iteration 32 current loss: 1.2925922870635986 current acc: 0.36912912912912915\n",
      "iteration 33 current loss: 1.2583215236663818 current acc: 0.3696407185628742\n",
      "iteration 34 current loss: 1.0882716178894043 current acc: 0.3703880597014925\n",
      "iteration 35 current loss: 1.5449005365371704 current acc: 0.3705357142857143\n",
      "iteration 36 current loss: 1.3450138568878174 current acc: 0.3709792284866469\n",
      "iteration 37 current loss: 1.1813437938690186 current acc: 0.3714792899408284\n",
      "iteration 38 current loss: 1.3458590507507324 current acc: 0.371740412979351\n",
      "iteration 39 current loss: 1.4325495958328247 current acc: 0.37176470588235294\n",
      "iteration 40 current loss: 1.175533652305603 current acc: 0.3721407624633431\n",
      "iteration 41 current loss: 1.2158998250961304 current acc: 0.37257309941520467\n",
      "iteration 42 current loss: 1.0973089933395386 current acc: 0.3731195335276968\n",
      "iteration 43 current loss: 1.451360821723938 current acc: 0.3733139534883721\n",
      "iteration 44 current loss: 1.4103702306747437 current acc: 0.37344927536231887\n",
      "iteration 45 current loss: 1.455085039138794 current acc: 0.37358381502890176\n",
      "iteration 46 current loss: 1.5964422225952148 current acc: 0.3736023054755043\n",
      "iteration 47 current loss: 1.4611331224441528 current acc: 0.3737931034482759\n",
      "iteration 48 current loss: 1.499168872833252 current acc: 0.37386819484240685\n",
      "iteration 49 current loss: 1.6029400825500488 current acc: 0.3737714285714286\n",
      "iteration 50 current loss: 1.2721073627471924 current acc: 0.37413105413105413\n",
      "iteration 51 current loss: 1.3384017944335938 current acc: 0.3743181818181818\n",
      "iteration 52 current loss: 1.4783748388290405 current acc: 0.3742209631728045\n",
      "iteration 53 current loss: 1.3611390590667725 current acc: 0.3744632768361582\n",
      "iteration 54 current loss: 1.3490458726882935 current acc: 0.3748169014084507\n",
      "iteration 55 current loss: 1.490038275718689 current acc: 0.3748876404494382\n",
      "iteration 56 current loss: 1.3920660018920898 current acc: 0.37523809523809526\n",
      "iteration 57 current loss: 1.471664547920227 current acc: 0.3754748603351955\n",
      "iteration 58 current loss: 1.4515447616577148 current acc: 0.37571030640668523\n",
      "iteration 59 current loss: 1.6119275093078613 current acc: 0.3756111111111111\n",
      "iteration 60 current loss: 1.3626947402954102 current acc: 0.37601108033241\n",
      "iteration 61 current loss: 1.4429255723953247 current acc: 0.3761878453038674\n",
      "iteration 62 current loss: 1.2782871723175049 current acc: 0.3765840220385675\n",
      "iteration 63 current loss: 1.4228793382644653 current acc: 0.3769230769230769\n",
      "iteration 64 current loss: 1.3805159330368042 current acc: 0.3774246575342466\n",
      "iteration 65 current loss: 1.2582712173461914 current acc: 0.37775956284153006\n",
      "iteration 66 current loss: 1.382416844367981 current acc: 0.3779291553133515\n",
      "iteration 67 current loss: 1.4816977977752686 current acc: 0.3782608695652174\n",
      "iteration 68 current loss: 1.4382729530334473 current acc: 0.378319783197832\n",
      "iteration 69 current loss: 1.1817734241485596 current acc: 0.37864864864864867\n",
      "iteration 70 current loss: 1.3790339231491089 current acc: 0.37908355795148246\n",
      "iteration 71 current loss: 1.44850754737854 current acc: 0.3793010752688172\n",
      "iteration 72 current loss: 1.2506945133209229 current acc: 0.37994638069705094\n",
      "iteration 73 current loss: 1.4503289461135864 current acc: 0.38037433155080214\n",
      "iteration 74 current loss: 1.3206839561462402 current acc: 0.3808533333333333\n",
      "iteration 75 current loss: 1.4600746631622314 current acc: 0.38079787234042556\n",
      "iteration 76 current loss: 1.5537627935409546 current acc: 0.3810610079575597\n",
      "iteration 77 current loss: 1.5090380907058716 current acc: 0.38126984126984126\n",
      "iteration 78 current loss: 1.305416226387024 current acc: 0.38184696569920845\n",
      "iteration 79 current loss: 1.2977198362350464 current acc: 0.38226315789473686\n",
      "iteration 80 current loss: 1.2898379564285278 current acc: 0.3826246719160105\n",
      "iteration 81 current loss: 1.3471190929412842 current acc: 0.38287958115183246\n",
      "iteration 82 current loss: 1.3162357807159424 current acc: 0.3833420365535248\n",
      "iteration 83 current loss: 1.3743175268173218 current acc: 0.38375\n",
      "iteration 84 current loss: 1.2974966764450073 current acc: 0.38405194805194803\n",
      "iteration 85 current loss: 1.207459568977356 current acc: 0.3844041450777202\n",
      "iteration 86 current loss: 1.3897595405578613 current acc: 0.3847545219638243\n",
      "iteration 87 current loss: 1.600854754447937 current acc: 0.38489690721649483\n",
      "iteration 88 current loss: 1.2103042602539062 current acc: 0.3856041131105398\n",
      "iteration 89 current loss: 1.5682529211044312 current acc: 0.3854871794871795\n",
      "iteration 90 current loss: 1.3098258972167969 current acc: 0.3858312020460358\n",
      "iteration 91 current loss: 1.403335690498352 current acc: 0.38612244897959186\n",
      "iteration 92 current loss: 1.359960675239563 current acc: 0.38646310432569975\n",
      "iteration 93 current loss: 1.4313679933547974 current acc: 0.3865989847715736\n",
      "iteration 94 current loss: 1.2878719568252563 current acc: 0.3869367088607595\n",
      "iteration 95 current loss: 1.3852512836456299 current acc: 0.38732323232323235\n",
      "iteration 96 current loss: 1.4428825378417969 current acc: 0.3876070528967254\n",
      "iteration 97 current loss: 1.3124051094055176 current acc: 0.3877889447236181\n",
      "iteration 98 current loss: 1.3275264501571655 current acc: 0.3879699248120301\n",
      "iteration 99 current loss: 1.3967132568359375 current acc: 0.38835\n",
      "\t\tEpoch 3/100 complete. Epoch loss 1.3694217205047607 Epoch accuracy 0.4832\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 3, Validation Accuracy: 0.465375, Validation Loss: 1.407236485183239\n",
      "best loss 1.3694217205047607\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 1.2895302772521973 current acc: 0.3886783042394015\n",
      "iteration 1 current loss: 1.3382253646850586 current acc: 0.3891044776119403\n",
      "iteration 2 current loss: 1.175465703010559 current acc: 0.38957816377171217\n",
      "iteration 3 current loss: 1.1962902545928955 current acc: 0.3900990099009901\n",
      "iteration 4 current loss: 1.0865179300308228 current acc: 0.3907654320987654\n",
      "iteration 5 current loss: 1.4170069694519043 current acc: 0.3908374384236453\n",
      "iteration 6 current loss: 1.253252387046814 current acc: 0.39095823095823096\n",
      "iteration 7 current loss: 1.6904619932174683 current acc: 0.3909313725490196\n",
      "iteration 8 current loss: 1.1757607460021973 current acc: 0.3913447432762836\n",
      "iteration 9 current loss: 1.327752947807312 current acc: 0.39170731707317075\n",
      "iteration 10 current loss: 1.1680994033813477 current acc: 0.39211678832116786\n",
      "iteration 11 current loss: 1.3806456327438354 current acc: 0.39223300970873787\n",
      "iteration 12 current loss: 1.3840320110321045 current acc: 0.392590799031477\n",
      "iteration 13 current loss: 1.321936011314392 current acc: 0.3929951690821256\n",
      "iteration 14 current loss: 1.2189452648162842 current acc: 0.3935421686746988\n",
      "iteration 15 current loss: 1.0878236293792725 current acc: 0.39389423076923075\n",
      "iteration 16 current loss: 1.2521716356277466 current acc: 0.3942925659472422\n",
      "iteration 17 current loss: 1.243478536605835 current acc: 0.39473684210526316\n",
      "iteration 18 current loss: 1.4634385108947754 current acc: 0.3948448687350835\n",
      "iteration 19 current loss: 1.287550449371338 current acc: 0.39514285714285713\n",
      "iteration 20 current loss: 1.236229419708252 current acc: 0.3955344418052257\n",
      "iteration 21 current loss: 1.2104002237319946 current acc: 0.39606635071090046\n",
      "iteration 22 current loss: 1.1982883214950562 current acc: 0.39654846335697397\n",
      "iteration 23 current loss: 1.2180190086364746 current acc: 0.3968867924528302\n",
      "iteration 24 current loss: 1.0693947076797485 current acc: 0.39736470588235295\n",
      "iteration 25 current loss: 1.4198764562606812 current acc: 0.3976525821596244\n",
      "iteration 26 current loss: 1.2285833358764648 current acc: 0.3981264637002342\n",
      "iteration 27 current loss: 1.489283561706543 current acc: 0.39822429906542056\n",
      "iteration 28 current loss: 1.3428311347961426 current acc: 0.39836829836829835\n",
      "iteration 29 current loss: 1.1635278463363647 current acc: 0.39893023255813953\n",
      "iteration 30 current loss: 1.3016287088394165 current acc: 0.3990719257540603\n",
      "iteration 31 current loss: 1.4860849380493164 current acc: 0.3990740740740741\n",
      "iteration 32 current loss: 0.9979038238525391 current acc: 0.39976905311778294\n",
      "iteration 33 current loss: 1.3949637413024902 current acc: 0.3999078341013825\n",
      "iteration 34 current loss: 1.425971508026123 current acc: 0.40027586206896554\n",
      "iteration 35 current loss: 1.334524393081665 current acc: 0.40073394495412845\n",
      "iteration 36 current loss: 1.3014642000198364 current acc: 0.4010526315789474\n",
      "iteration 37 current loss: 1.2603777647018433 current acc: 0.4014611872146119\n",
      "iteration 38 current loss: 1.2600728273391724 current acc: 0.4016856492027335\n",
      "iteration 39 current loss: 1.3788576126098633 current acc: 0.4018181818181818\n",
      "iteration 40 current loss: 1.3937687873840332 current acc: 0.40199546485260773\n",
      "iteration 41 current loss: 1.4077632427215576 current acc: 0.4020814479638009\n",
      "iteration 42 current loss: 1.2504173517227173 current acc: 0.40234762979683975\n",
      "iteration 43 current loss: 1.3254907131195068 current acc: 0.4025225225225225\n",
      "iteration 44 current loss: 1.3138949871063232 current acc: 0.4027415730337079\n",
      "iteration 45 current loss: 1.3481320142745972 current acc: 0.4030493273542601\n",
      "iteration 46 current loss: 1.347191572189331 current acc: 0.4032662192393736\n",
      "iteration 47 current loss: 1.11417555809021 current acc: 0.4036160714285714\n",
      "iteration 48 current loss: 1.3095042705535889 current acc: 0.4039198218262806\n",
      "iteration 49 current loss: 1.3713749647140503 current acc: 0.40404444444444443\n",
      "iteration 50 current loss: 1.1437768936157227 current acc: 0.40452328159645234\n",
      "iteration 51 current loss: 1.0803812742233276 current acc: 0.40486725663716816\n",
      "iteration 52 current loss: 1.3947087526321411 current acc: 0.40503311258278146\n",
      "iteration 53 current loss: 1.1115174293518066 current acc: 0.40550660792951543\n",
      "iteration 54 current loss: 1.2162280082702637 current acc: 0.40584615384615386\n",
      "iteration 55 current loss: 1.3937158584594727 current acc: 0.40614035087719297\n",
      "iteration 56 current loss: 1.002740740776062 current acc: 0.40656455142231945\n",
      "iteration 57 current loss: 1.2031738758087158 current acc: 0.40703056768558954\n",
      "iteration 58 current loss: 1.2204657793045044 current acc: 0.40736383442265794\n",
      "iteration 59 current loss: 1.2186402082443237 current acc: 0.4077826086956522\n",
      "iteration 60 current loss: 1.4382483959197998 current acc: 0.4078091106290672\n",
      "iteration 61 current loss: 1.3566545248031616 current acc: 0.4080952380952381\n",
      "iteration 62 current loss: 1.2661556005477905 current acc: 0.4085961123110151\n",
      "iteration 63 current loss: 1.2827479839324951 current acc: 0.4088793103448276\n",
      "iteration 64 current loss: 0.9519370794296265 current acc: 0.40959139784946236\n",
      "iteration 65 current loss: 1.3912371397018433 current acc: 0.40969957081545066\n",
      "iteration 66 current loss: 1.3413865566253662 current acc: 0.4097644539614561\n",
      "iteration 67 current loss: 1.1493630409240723 current acc: 0.41012820512820514\n",
      "iteration 68 current loss: 1.3722970485687256 current acc: 0.41031982942430706\n",
      "iteration 69 current loss: 1.0706697702407837 current acc: 0.4106808510638298\n",
      "iteration 70 current loss: 1.243268370628357 current acc: 0.4110828025477707\n",
      "iteration 71 current loss: 1.0479109287261963 current acc: 0.4115677966101695\n",
      "iteration 72 current loss: 1.135945439338684 current acc: 0.41183932346723046\n",
      "iteration 73 current loss: 1.4089558124542236 current acc: 0.4120253164556962\n",
      "iteration 74 current loss: 1.2424860000610352 current acc: 0.41237894736842107\n",
      "iteration 75 current loss: 1.3155052661895752 current acc: 0.41247899159663864\n",
      "iteration 76 current loss: 1.300683856010437 current acc: 0.4127882599580713\n",
      "iteration 77 current loss: 1.3839622735977173 current acc: 0.4125941422594142\n",
      "iteration 78 current loss: 1.2977583408355713 current acc: 0.412776617954071\n",
      "iteration 79 current loss: 1.1931204795837402 current acc: 0.4131666666666667\n",
      "iteration 80 current loss: 1.2678872346878052 current acc: 0.4133887733887734\n",
      "iteration 81 current loss: 1.203351378440857 current acc: 0.41356846473029046\n",
      "iteration 82 current loss: 1.2617385387420654 current acc: 0.41378881987577637\n",
      "iteration 83 current loss: 1.1249041557312012 current acc: 0.4141322314049587\n",
      "iteration 84 current loss: 1.220627784729004 current acc: 0.41430927835051545\n",
      "iteration 85 current loss: 1.2536638975143433 current acc: 0.4144855967078189\n",
      "iteration 86 current loss: 1.5946966409683228 current acc: 0.4146611909650924\n",
      "iteration 87 current loss: 1.3825000524520874 current acc: 0.41471311475409833\n",
      "iteration 88 current loss: 1.2830040454864502 current acc: 0.41480572597137016\n",
      "iteration 89 current loss: 1.2466520071029663 current acc: 0.41489795918367345\n",
      "iteration 90 current loss: 1.2214980125427246 current acc: 0.415193482688391\n",
      "iteration 91 current loss: 1.4001905918121338 current acc: 0.41528455284552845\n",
      "iteration 92 current loss: 1.1453964710235596 current acc: 0.41557809330628803\n",
      "iteration 93 current loss: 1.3044873476028442 current acc: 0.41578947368421054\n",
      "iteration 94 current loss: 1.1376523971557617 current acc: 0.416\n",
      "iteration 95 current loss: 1.0568885803222656 current acc: 0.4164516129032258\n",
      "iteration 96 current loss: 1.2127900123596191 current acc: 0.41653923541247484\n",
      "iteration 97 current loss: 1.2773561477661133 current acc: 0.4168273092369478\n",
      "iteration 98 current loss: 1.2522718906402588 current acc: 0.4171142284569138\n",
      "iteration 99 current loss: 1.1918461322784424 current acc: 0.41744\n",
      "\t\tEpoch 4/100 complete. Epoch loss 1.2687342870235443 Epoch accuracy 0.5338\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 4, Validation Accuracy: 0.4965, Validation Loss: 1.3400535799562932\n",
      "best loss 1.2687342870235443\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 1.2389405965805054 current acc: 0.4177245508982036\n",
      "iteration 1 current loss: 1.1071538925170898 current acc: 0.41816733067729084\n",
      "iteration 2 current loss: 1.2110803127288818 current acc: 0.4183300198807157\n",
      "iteration 3 current loss: 1.174758791923523 current acc: 0.4185714285714286\n",
      "iteration 4 current loss: 1.1239955425262451 current acc: 0.41885148514851483\n",
      "iteration 5 current loss: 1.0390970706939697 current acc: 0.4192094861660079\n",
      "iteration 6 current loss: 1.254407525062561 current acc: 0.4193293885601578\n",
      "iteration 7 current loss: 1.3989384174346924 current acc: 0.4194488188976378\n",
      "iteration 8 current loss: 1.1497406959533691 current acc: 0.4199607072691552\n",
      "iteration 9 current loss: 0.9013415575027466 current acc: 0.4204705882352941\n",
      "iteration 10 current loss: 1.1907544136047363 current acc: 0.4207436399217221\n",
      "iteration 11 current loss: 1.0544474124908447 current acc: 0.4211328125\n",
      "iteration 12 current loss: 1.19657564163208 current acc: 0.421364522417154\n",
      "iteration 13 current loss: 1.1052151918411255 current acc: 0.42167315175097275\n",
      "iteration 14 current loss: 1.256907343864441 current acc: 0.42198058252427184\n",
      "iteration 15 current loss: 1.0258976221084595 current acc: 0.42248062015503873\n",
      "iteration 16 current loss: 1.2674612998962402 current acc: 0.42274661508704064\n",
      "iteration 17 current loss: 1.0442177057266235 current acc: 0.42312741312741314\n",
      "iteration 18 current loss: 0.9986132979393005 current acc: 0.4235452793834297\n",
      "iteration 19 current loss: 1.1237794160842896 current acc: 0.4237692307692308\n",
      "iteration 20 current loss: 1.0636004209518433 current acc: 0.42429942418426103\n",
      "iteration 21 current loss: 1.286260962486267 current acc: 0.4244827586206897\n",
      "iteration 22 current loss: 1.1471338272094727 current acc: 0.4247418738049713\n",
      "iteration 23 current loss: 1.3269124031066895 current acc: 0.4248473282442748\n",
      "iteration 24 current loss: 1.0086970329284668 current acc: 0.42537142857142857\n",
      "iteration 25 current loss: 1.2730257511138916 current acc: 0.4253992395437262\n",
      "iteration 26 current loss: 0.9786624908447266 current acc: 0.42584440227703985\n",
      "iteration 27 current loss: 1.0904291868209839 current acc: 0.4263636363636364\n",
      "iteration 28 current loss: 1.165785789489746 current acc: 0.4266918714555766\n",
      "iteration 29 current loss: 1.2488075494766235 current acc: 0.4267547169811321\n",
      "iteration 30 current loss: 1.1216713190078735 current acc: 0.4271939736346516\n",
      "iteration 31 current loss: 1.343711256980896 current acc: 0.4272932330827068\n",
      "iteration 32 current loss: 1.180647850036621 current acc: 0.4276923076923077\n",
      "iteration 33 current loss: 0.9841905236244202 current acc: 0.4280898876404494\n",
      "iteration 34 current loss: 1.0797308683395386 current acc: 0.4285607476635514\n",
      "iteration 35 current loss: 1.395693302154541 current acc: 0.4286567164179104\n",
      "iteration 36 current loss: 1.1149312257766724 current acc: 0.4289385474860335\n",
      "iteration 37 current loss: 1.1367560625076294 current acc: 0.4292565055762082\n",
      "iteration 38 current loss: 1.2751537561416626 current acc: 0.4294990723562152\n",
      "iteration 39 current loss: 1.001466989517212 current acc: 0.42977777777777776\n",
      "iteration 40 current loss: 1.2085611820220947 current acc: 0.42990757855822553\n",
      "iteration 41 current loss: 1.1608734130859375 current acc: 0.4301476014760148\n",
      "iteration 42 current loss: 1.2599999904632568 current acc: 0.4304972375690608\n",
      "iteration 43 current loss: 1.2280659675598145 current acc: 0.4305514705882353\n",
      "iteration 44 current loss: 0.976570725440979 current acc: 0.4308256880733945\n",
      "iteration 45 current loss: 1.1387567520141602 current acc: 0.431025641025641\n",
      "iteration 46 current loss: 1.2049192190170288 current acc: 0.4312248628884826\n",
      "iteration 47 current loss: 1.2157915830612183 current acc: 0.43135036496350365\n",
      "iteration 48 current loss: 1.2312530279159546 current acc: 0.4317304189435337\n",
      "iteration 49 current loss: 1.1622010469436646 current acc: 0.432\n",
      "iteration 50 current loss: 1.2082966566085815 current acc: 0.4321234119782214\n",
      "iteration 51 current loss: 1.324415922164917 current acc: 0.4322463768115942\n",
      "iteration 52 current loss: 1.1212503910064697 current acc: 0.4325497287522604\n",
      "iteration 53 current loss: 1.1726199388504028 current acc: 0.4328158844765343\n",
      "iteration 54 current loss: 1.1066007614135742 current acc: 0.4331891891891892\n",
      "iteration 55 current loss: 1.0739409923553467 current acc: 0.4335611510791367\n",
      "iteration 56 current loss: 1.0060505867004395 current acc: 0.433967684021544\n",
      "iteration 57 current loss: 0.9376838803291321 current acc: 0.4343727598566308\n",
      "iteration 58 current loss: 1.2421932220458984 current acc: 0.4344901610017889\n",
      "iteration 59 current loss: 1.0510950088500977 current acc: 0.43475\n",
      "iteration 60 current loss: 1.0828349590301514 current acc: 0.4351515151515152\n",
      "iteration 61 current loss: 1.067142128944397 current acc: 0.43555160142348753\n",
      "iteration 62 current loss: 1.216821312904358 current acc: 0.43577264653641207\n",
      "iteration 63 current loss: 1.1220002174377441 current acc: 0.43609929078014187\n",
      "iteration 64 current loss: 1.0623098611831665 current acc: 0.43631858407079643\n",
      "iteration 65 current loss: 1.2347652912139893 current acc: 0.4363250883392226\n",
      "iteration 66 current loss: 0.9908632040023804 current acc: 0.4366137566137566\n",
      "iteration 67 current loss: 1.3038246631622314 current acc: 0.43672535211267605\n",
      "iteration 68 current loss: 1.306941270828247 current acc: 0.4368365553602812\n",
      "iteration 69 current loss: 1.268913984298706 current acc: 0.43708771929824564\n",
      "iteration 70 current loss: 0.9188585877418518 current acc: 0.43751313485113835\n",
      "iteration 71 current loss: 1.4567526578903198 current acc: 0.4376223776223776\n",
      "iteration 72 current loss: 1.088045358657837 current acc: 0.43776614310645723\n",
      "iteration 73 current loss: 1.2318403720855713 current acc: 0.43794425087108013\n",
      "iteration 74 current loss: 1.0150823593139648 current acc: 0.4383304347826087\n",
      "iteration 75 current loss: 1.3102649450302124 current acc: 0.43854166666666666\n",
      "iteration 76 current loss: 1.240072250366211 current acc: 0.4387521663778163\n",
      "iteration 77 current loss: 1.3365713357925415 current acc: 0.43899653979238756\n",
      "iteration 78 current loss: 1.3784080743789673 current acc: 0.4390673575129534\n",
      "iteration 79 current loss: 1.4989320039749146 current acc: 0.43913793103448273\n",
      "iteration 80 current loss: 1.2204879522323608 current acc: 0.43941480206540445\n",
      "iteration 81 current loss: 1.381089448928833 current acc: 0.439553264604811\n",
      "iteration 82 current loss: 1.130893349647522 current acc: 0.4395883361921098\n",
      "iteration 83 current loss: 1.3574665784835815 current acc: 0.43965753424657533\n",
      "iteration 84 current loss: 1.2174252271652222 current acc: 0.4398632478632479\n",
      "iteration 85 current loss: 1.3734490871429443 current acc: 0.4398976109215017\n",
      "iteration 86 current loss: 1.2261719703674316 current acc: 0.4402044293015332\n",
      "iteration 87 current loss: 1.5139992237091064 current acc: 0.44017006802721087\n",
      "iteration 88 current loss: 1.1280641555786133 current acc: 0.4404414261460102\n",
      "iteration 89 current loss: 1.0528488159179688 current acc: 0.4407457627118644\n",
      "iteration 90 current loss: 1.455878734588623 current acc: 0.4408121827411168\n",
      "iteration 91 current loss: 1.0773425102233887 current acc: 0.4410472972972973\n",
      "iteration 92 current loss: 1.2361032962799072 current acc: 0.4411129848229342\n",
      "iteration 93 current loss: 1.2359508275985718 current acc: 0.4412121212121212\n",
      "iteration 94 current loss: 1.1682170629501343 current acc: 0.44134453781512606\n",
      "iteration 95 current loss: 1.240134358406067 current acc: 0.44157718120805367\n",
      "iteration 96 current loss: 1.19010591506958 current acc: 0.44187604690117255\n",
      "iteration 97 current loss: 1.2852023839950562 current acc: 0.4420066889632107\n",
      "iteration 98 current loss: 1.1874358654022217 current acc: 0.44237061769616026\n",
      "iteration 99 current loss: 1.1531939506530762 current acc: 0.4424666666666667\n",
      "\t\tEpoch 5/100 complete. Epoch loss 1.1808043813705444 Epoch accuracy 0.5676\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 5, Validation Accuracy: 0.503, Validation Loss: 1.326203668117523\n",
      "best loss 1.1808043813705444\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 1.301154375076294 current acc: 0.44262895174708816\n",
      "iteration 1 current loss: 1.1410610675811768 current acc: 0.44275747508305646\n",
      "iteration 2 current loss: 1.0441350936889648 current acc: 0.4431177446102819\n",
      "iteration 3 current loss: 1.0604474544525146 current acc: 0.4434437086092715\n",
      "iteration 4 current loss: 1.204284429550171 current acc: 0.4437685950413223\n",
      "iteration 5 current loss: 1.1952584981918335 current acc: 0.44396039603960397\n",
      "iteration 6 current loss: 1.2646061182022095 current acc: 0.4441845140032949\n",
      "iteration 7 current loss: 1.028294324874878 current acc: 0.44450657894736845\n",
      "iteration 8 current loss: 0.979044497013092 current acc: 0.44489326765188836\n",
      "iteration 9 current loss: 1.1616765260696411 current acc: 0.44511475409836065\n",
      "iteration 10 current loss: 1.1078572273254395 current acc: 0.44543371522094927\n",
      "iteration 11 current loss: 1.2979588508605957 current acc: 0.4454901960784314\n",
      "iteration 12 current loss: 1.0094287395477295 current acc: 0.44580750407830344\n",
      "iteration 13 current loss: 0.9322417974472046 current acc: 0.44628664495114007\n",
      "iteration 14 current loss: 0.9503402709960938 current acc: 0.4466341463414634\n",
      "iteration 15 current loss: 1.1366463899612427 current acc: 0.4469480519480519\n",
      "iteration 16 current loss: 1.1763108968734741 current acc: 0.4471636952998379\n",
      "iteration 17 current loss: 1.1204925775527954 current acc: 0.4473462783171521\n",
      "iteration 18 current loss: 0.9349489808082581 current acc: 0.44765751211631666\n",
      "iteration 19 current loss: 1.0157734155654907 current acc: 0.448\n",
      "iteration 20 current loss: 1.2686591148376465 current acc: 0.4480193236714976\n",
      "iteration 21 current loss: 0.9928573369979858 current acc: 0.4483279742765273\n",
      "iteration 22 current loss: 1.1466575860977173 current acc: 0.44847512038523274\n",
      "iteration 23 current loss: 1.137468695640564 current acc: 0.44868589743589743\n",
      "iteration 24 current loss: 1.1258569955825806 current acc: 0.448864\n",
      "iteration 25 current loss: 0.94277024269104 current acc: 0.4492332268370607\n",
      "iteration 26 current loss: 1.233713150024414 current acc: 0.4494736842105263\n",
      "iteration 27 current loss: 1.1653659343719482 current acc: 0.44961783439490444\n",
      "iteration 28 current loss: 0.9489724040031433 current acc: 0.44988871224165344\n",
      "iteration 29 current loss: 1.070534348487854 current acc: 0.4500952380952381\n",
      "iteration 30 current loss: 1.0066173076629639 current acc: 0.4504278922345483\n",
      "iteration 31 current loss: 1.245890736579895 current acc: 0.4504746835443038\n",
      "iteration 32 current loss: 1.0876160860061646 current acc: 0.4507740916271722\n",
      "iteration 33 current loss: 1.048231840133667 current acc: 0.4510094637223975\n",
      "iteration 34 current loss: 1.166293740272522 current acc: 0.4511811023622047\n",
      "iteration 35 current loss: 1.065425157546997 current acc: 0.4514150943396226\n",
      "iteration 36 current loss: 1.242002010345459 current acc: 0.4515541601255887\n",
      "iteration 37 current loss: 1.204280972480774 current acc: 0.45163009404388715\n",
      "iteration 38 current loss: 1.0248990058898926 current acc: 0.45198748043818465\n",
      "iteration 39 current loss: 1.0454200506210327 current acc: 0.45215625\n",
      "iteration 40 current loss: 1.1011085510253906 current acc: 0.45244929797191885\n",
      "iteration 41 current loss: 1.1194316148757935 current acc: 0.45274143302180686\n",
      "iteration 42 current loss: 1.1094790697097778 current acc: 0.4528771384136858\n",
      "iteration 43 current loss: 1.1698552370071411 current acc: 0.45301242236024847\n",
      "iteration 44 current loss: 0.8742552995681763 current acc: 0.4533333333333333\n",
      "iteration 45 current loss: 1.0905150175094604 current acc: 0.4535294117647059\n",
      "iteration 46 current loss: 1.0586020946502686 current acc: 0.4537557959814529\n",
      "iteration 47 current loss: 1.0578092336654663 current acc: 0.45401234567901233\n",
      "iteration 48 current loss: 1.0431435108184814 current acc: 0.45442218798151\n",
      "iteration 49 current loss: 1.1905161142349243 current acc: 0.45455384615384614\n",
      "iteration 50 current loss: 1.224360466003418 current acc: 0.45474654377880186\n",
      "iteration 51 current loss: 1.3672279119491577 current acc: 0.45475460122699385\n",
      "iteration 52 current loss: 1.103020429611206 current acc: 0.454854517611026\n",
      "iteration 53 current loss: 1.1094059944152832 current acc: 0.4550764525993884\n",
      "iteration 54 current loss: 0.9789257645606995 current acc: 0.45545038167938934\n",
      "iteration 55 current loss: 1.1191020011901855 current acc: 0.4557012195121951\n",
      "iteration 56 current loss: 1.121229887008667 current acc: 0.4560121765601218\n",
      "iteration 57 current loss: 1.168078899383545 current acc: 0.4560790273556231\n",
      "iteration 58 current loss: 1.072532296180725 current acc: 0.4563277693474962\n",
      "iteration 59 current loss: 1.0120025873184204 current acc: 0.4566363636363636\n",
      "iteration 60 current loss: 0.7979235649108887 current acc: 0.4571558245083207\n",
      "iteration 61 current loss: 1.1116982698440552 current acc: 0.4573413897280967\n",
      "iteration 62 current loss: 1.1374680995941162 current acc: 0.4574358974358974\n",
      "iteration 63 current loss: 1.190751075744629 current acc: 0.4575301204819277\n",
      "iteration 64 current loss: 0.9955251216888428 current acc: 0.4578045112781955\n",
      "iteration 65 current loss: 1.0730929374694824 current acc: 0.458018018018018\n",
      "iteration 66 current loss: 1.2332507371902466 current acc: 0.45811094452773615\n",
      "iteration 67 current loss: 0.8639416694641113 current acc: 0.4584431137724551\n",
      "iteration 68 current loss: 1.0002834796905518 current acc: 0.4586846038863976\n",
      "iteration 69 current loss: 1.0557836294174194 current acc: 0.458955223880597\n",
      "iteration 70 current loss: 0.9578658938407898 current acc: 0.45934426229508196\n",
      "iteration 71 current loss: 1.5227789878845215 current acc: 0.4594047619047619\n",
      "iteration 72 current loss: 1.0295276641845703 current acc: 0.45973254086181276\n",
      "iteration 73 current loss: 1.1244654655456543 current acc: 0.4599406528189911\n",
      "iteration 74 current loss: 1.0145173072814941 current acc: 0.46026666666666666\n",
      "iteration 75 current loss: 1.1251704692840576 current acc: 0.46059171597633136\n",
      "iteration 76 current loss: 1.1988356113433838 current acc: 0.4608567208271787\n",
      "iteration 77 current loss: 1.0261914730072021 current acc: 0.46123893805309735\n",
      "iteration 78 current loss: 1.0094579458236694 current acc: 0.4615611192930781\n",
      "iteration 79 current loss: 1.075394868850708 current acc: 0.4617352941176471\n",
      "iteration 80 current loss: 0.9550967216491699 current acc: 0.4620264317180617\n",
      "iteration 81 current loss: 0.9669296741485596 current acc: 0.46234604105571847\n",
      "iteration 82 current loss: 0.905559778213501 current acc: 0.4627232796486091\n",
      "iteration 83 current loss: 1.3935253620147705 current acc: 0.46283625730994155\n",
      "iteration 84 current loss: 1.2030608654022217 current acc: 0.463007299270073\n",
      "iteration 85 current loss: 1.1698451042175293 current acc: 0.4630612244897959\n",
      "iteration 86 current loss: 1.1149715185165405 current acc: 0.46334788937409027\n",
      "iteration 87 current loss: 1.3159445524215698 current acc: 0.46343023255813953\n",
      "iteration 88 current loss: 1.1556211709976196 current acc: 0.4636865021770682\n",
      "iteration 89 current loss: 1.349747896194458 current acc: 0.4638260869565217\n",
      "iteration 90 current loss: 0.967506468296051 current acc: 0.4640810419681621\n",
      "iteration 91 current loss: 0.8276235461235046 current acc: 0.4644797687861272\n",
      "iteration 92 current loss: 1.0378353595733643 current acc: 0.4647330447330447\n",
      "iteration 93 current loss: 0.7900420427322388 current acc: 0.46504322766570605\n",
      "iteration 94 current loss: 1.2605897188186646 current acc: 0.4651798561151079\n",
      "iteration 95 current loss: 0.8268669247627258 current acc: 0.4656034482758621\n",
      "iteration 96 current loss: 0.9456724524497986 current acc: 0.46591104734576755\n",
      "iteration 97 current loss: 1.109747290611267 current acc: 0.4660458452722063\n",
      "iteration 98 current loss: 1.0151091814041138 current acc: 0.4663519313304721\n",
      "iteration 99 current loss: 1.2492167949676514 current acc: 0.46662857142857145\n",
      "\t\tEpoch 6/100 complete. Epoch loss 1.0942653292417526 Epoch accuracy 0.6116\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 6, Validation Accuracy: 0.545375, Validation Loss: 1.240063229203224\n",
      "best loss 1.0942653292417526\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 1.0071598291397095 current acc: 0.4668758915834522\n",
      "iteration 1 current loss: 0.9000473022460938 current acc: 0.46717948717948715\n",
      "iteration 2 current loss: 0.9390029907226562 current acc: 0.46745376955903273\n",
      "iteration 3 current loss: 0.7950149774551392 current acc: 0.4677840909090909\n",
      "iteration 4 current loss: 0.8230753540992737 current acc: 0.46811347517730495\n",
      "iteration 5 current loss: 1.0017197132110596 current acc: 0.468356940509915\n",
      "iteration 6 current loss: 0.9809929728507996 current acc: 0.4685997171145686\n",
      "iteration 7 current loss: 0.9368656873703003 current acc: 0.4688418079096045\n",
      "iteration 8 current loss: 1.0679961442947388 current acc: 0.46899858956276447\n",
      "iteration 9 current loss: 0.8554641008377075 current acc: 0.4692676056338028\n",
      "iteration 10 current loss: 0.9127164483070374 current acc: 0.46950773558368497\n",
      "iteration 11 current loss: 0.8991421461105347 current acc: 0.46974719101123596\n",
      "iteration 12 current loss: 0.8914138078689575 current acc: 0.46990182328190744\n",
      "iteration 13 current loss: 1.0885783433914185 current acc: 0.47019607843137257\n",
      "iteration 14 current loss: 1.2330312728881836 current acc: 0.47026573426573426\n",
      "iteration 15 current loss: 0.8021009564399719 current acc: 0.47064245810055866\n",
      "iteration 16 current loss: 0.9843964576721191 current acc: 0.4709065550906555\n",
      "iteration 17 current loss: 0.9470261931419373 current acc: 0.47114206128133707\n",
      "iteration 18 current loss: 1.011389970779419 current acc: 0.4713769123783032\n",
      "iteration 19 current loss: 1.2352179288864136 current acc: 0.4715\n",
      "iteration 20 current loss: 1.1780726909637451 current acc: 0.47159500693481277\n",
      "iteration 21 current loss: 1.240256667137146 current acc: 0.47166204986149585\n",
      "iteration 22 current loss: 0.8989707827568054 current acc: 0.4718948824343015\n",
      "iteration 23 current loss: 1.0903174877166748 current acc: 0.4719613259668508\n",
      "iteration 24 current loss: 0.9712583422660828 current acc: 0.47224827586206897\n",
      "iteration 25 current loss: 0.9913629293441772 current acc: 0.4725068870523416\n",
      "iteration 26 current loss: 1.0942448377609253 current acc: 0.4726547455295736\n",
      "iteration 27 current loss: 1.1673108339309692 current acc: 0.47285714285714286\n",
      "iteration 28 current loss: 0.9311283826828003 current acc: 0.4730315500685871\n",
      "iteration 29 current loss: 0.8762190937995911 current acc: 0.47326027397260273\n",
      "iteration 30 current loss: 0.7570372223854065 current acc: 0.4736525307797538\n",
      "iteration 31 current loss: 0.999494194984436 current acc: 0.4739344262295082\n",
      "iteration 32 current loss: 0.9182437658309937 current acc: 0.4742155525238745\n",
      "iteration 33 current loss: 0.8741984367370605 current acc: 0.47455040871934606\n",
      "iteration 34 current loss: 0.8823978900909424 current acc: 0.47477551020408165\n",
      "iteration 35 current loss: 1.1367676258087158 current acc: 0.47489130434782606\n",
      "iteration 36 current loss: 0.9515470266342163 current acc: 0.4750881953867028\n",
      "iteration 37 current loss: 0.9112725853919983 current acc: 0.47536585365853656\n",
      "iteration 38 current loss: 1.1031368970870972 current acc: 0.47553450608930986\n",
      "iteration 39 current loss: 1.1718419790267944 current acc: 0.4755405405405405\n",
      "iteration 40 current loss: 0.8680371642112732 current acc: 0.4758164642375169\n",
      "iteration 41 current loss: 1.002906084060669 current acc: 0.4759299191374663\n",
      "iteration 42 current loss: 0.9430615901947021 current acc: 0.4762314939434724\n",
      "iteration 43 current loss: 1.137609839439392 current acc: 0.47634408602150535\n",
      "iteration 44 current loss: 0.8428601026535034 current acc: 0.4767248322147651\n",
      "iteration 45 current loss: 1.310569405555725 current acc: 0.47672922252010724\n",
      "iteration 46 current loss: 0.9744856953620911 current acc: 0.47686746987951806\n",
      "iteration 47 current loss: 1.0256133079528809 current acc: 0.47697860962566846\n",
      "iteration 48 current loss: 1.316010594367981 current acc: 0.4770894526034713\n",
      "iteration 49 current loss: 0.9154515266418457 current acc: 0.47736\n",
      "iteration 50 current loss: 0.8759675025939941 current acc: 0.47765645805592544\n",
      "iteration 51 current loss: 1.0200526714324951 current acc: 0.4777659574468085\n",
      "iteration 52 current loss: 1.0606242418289185 current acc: 0.47787516600265606\n",
      "iteration 53 current loss: 1.3636666536331177 current acc: 0.47782493368700263\n",
      "iteration 54 current loss: 0.9556588530540466 current acc: 0.4781192052980133\n",
      "iteration 55 current loss: 0.9755597710609436 current acc: 0.4783597883597884\n",
      "iteration 56 current loss: 1.0939677953720093 current acc: 0.4785997357992074\n",
      "iteration 57 current loss: 0.9417986869812012 current acc: 0.47897097625329815\n",
      "iteration 58 current loss: 1.1665031909942627 current acc: 0.4790513833992095\n",
      "iteration 59 current loss: 1.0819505453109741 current acc: 0.4793421052631579\n",
      "iteration 60 current loss: 1.0518385171890259 current acc: 0.4796057818659658\n",
      "iteration 61 current loss: 1.0608580112457275 current acc: 0.47986876640419945\n",
      "iteration 62 current loss: 0.9671236276626587 current acc: 0.4800786369593709\n",
      "iteration 63 current loss: 0.9026320576667786 current acc: 0.4803664921465969\n",
      "iteration 64 current loss: 1.085264801979065 current acc: 0.48062745098039217\n",
      "iteration 65 current loss: 0.9970414638519287 current acc: 0.48088772845953004\n",
      "iteration 66 current loss: 0.8444074392318726 current acc: 0.4811734028683181\n",
      "iteration 67 current loss: 0.9450750946998596 current acc: 0.48130208333333335\n",
      "iteration 68 current loss: 0.8645265698432922 current acc: 0.48161248374512355\n",
      "iteration 69 current loss: 0.7681832909584045 current acc: 0.482\n",
      "iteration 70 current loss: 1.1552408933639526 current acc: 0.48217898832684825\n",
      "iteration 71 current loss: 1.0668748617172241 current acc: 0.48233160621761656\n",
      "iteration 72 current loss: 1.165050983428955 current acc: 0.48248382923674\n",
      "iteration 73 current loss: 0.8864269256591797 current acc: 0.4827906976744186\n",
      "iteration 74 current loss: 1.0789899826049805 current acc: 0.4829935483870968\n",
      "iteration 75 current loss: 1.0205347537994385 current acc: 0.4832216494845361\n",
      "iteration 76 current loss: 1.2032455205917358 current acc: 0.4833204633204633\n",
      "iteration 77 current loss: 0.9070494771003723 current acc: 0.48362467866323905\n",
      "iteration 78 current loss: 1.0825484991073608 current acc: 0.48377406931964057\n",
      "iteration 79 current loss: 0.781307578086853 current acc: 0.48407692307692307\n",
      "iteration 80 current loss: 1.1615228652954102 current acc: 0.4841741357234315\n",
      "iteration 81 current loss: 1.0766568183898926 current acc: 0.48434782608695653\n",
      "iteration 82 current loss: 1.0518027544021606 current acc: 0.48452107279693485\n",
      "iteration 83 current loss: 1.339991569519043 current acc: 0.48471938775510204\n",
      "iteration 84 current loss: 1.0335187911987305 current acc: 0.48494267515923567\n",
      "iteration 85 current loss: 1.083977222442627 current acc: 0.4851145038167939\n",
      "iteration 86 current loss: 1.0893405675888062 current acc: 0.4853113087674714\n",
      "iteration 87 current loss: 0.8955668807029724 current acc: 0.4855329949238579\n",
      "iteration 88 current loss: 0.9050421118736267 current acc: 0.48567807351077313\n",
      "iteration 89 current loss: 1.0081559419631958 current acc: 0.485873417721519\n",
      "iteration 90 current loss: 1.248010516166687 current acc: 0.4860176991150442\n",
      "iteration 91 current loss: 1.0301475524902344 current acc: 0.48618686868686867\n",
      "iteration 92 current loss: 1.110450029373169 current acc: 0.48645649432534677\n",
      "iteration 93 current loss: 0.8974209427833557 current acc: 0.4866750629722922\n",
      "iteration 94 current loss: 1.1027731895446777 current acc: 0.4867924528301887\n",
      "iteration 95 current loss: 1.0354663133621216 current acc: 0.4869597989949749\n",
      "iteration 96 current loss: 0.8928614258766174 current acc: 0.487176913425345\n",
      "iteration 97 current loss: 1.3132224082946777 current acc: 0.48724310776942353\n",
      "iteration 98 current loss: 1.1398638486862183 current acc: 0.48733416770963706\n",
      "iteration 99 current loss: 0.932138204574585 current acc: 0.48755\n",
      "\t\tEpoch 7/100 complete. Epoch loss 1.0151193779706955 Epoch accuracy 0.634\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 7, Validation Accuracy: 0.554, Validation Loss: 1.2204648450016975\n",
      "best loss 1.0151193779706955\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.9809055328369141 current acc: 0.4876903870162297\n",
      "iteration 1 current loss: 0.8683189153671265 current acc: 0.487930174563591\n",
      "iteration 2 current loss: 0.8289386034011841 current acc: 0.4881942714819427\n",
      "iteration 3 current loss: 0.7705368995666504 current acc: 0.4884825870646766\n",
      "iteration 4 current loss: 1.241468906402588 current acc: 0.48864596273291927\n",
      "iteration 5 current loss: 1.1018668413162231 current acc: 0.4887593052109181\n",
      "iteration 6 current loss: 0.7340744137763977 current acc: 0.4891449814126394\n",
      "iteration 7 current loss: 0.778346836566925 current acc: 0.48948019801980197\n",
      "iteration 8 current loss: 1.1214741468429565 current acc: 0.4896415327564895\n",
      "iteration 9 current loss: 1.0416080951690674 current acc: 0.48977777777777776\n",
      "iteration 10 current loss: 0.8431601524353027 current acc: 0.4900616522811344\n",
      "iteration 11 current loss: 0.787933349609375 current acc: 0.4903448275862069\n",
      "iteration 12 current loss: 1.0677467584609985 current acc: 0.4906027060270603\n",
      "iteration 13 current loss: 1.0856051445007324 current acc: 0.49073710073710075\n",
      "iteration 14 current loss: 0.946472704410553 current acc: 0.4909693251533742\n",
      "iteration 15 current loss: 0.9213496446609497 current acc: 0.4911519607843137\n",
      "iteration 16 current loss: 1.0826375484466553 current acc: 0.4912607099143207\n",
      "iteration 17 current loss: 1.017129898071289 current acc: 0.4914669926650367\n",
      "iteration 18 current loss: 1.0343990325927734 current acc: 0.49169719169719167\n",
      "iteration 19 current loss: 1.0245692729949951 current acc: 0.49185365853658536\n",
      "iteration 20 current loss: 1.0400643348693848 current acc: 0.49193666260657737\n",
      "iteration 21 current loss: 0.8605787754058838 current acc: 0.49223844282238444\n",
      "iteration 22 current loss: 0.7364022135734558 current acc: 0.49256379100850545\n",
      "iteration 23 current loss: 1.0576679706573486 current acc: 0.49264563106796116\n",
      "iteration 24 current loss: 1.210313081741333 current acc: 0.49275151515151516\n",
      "iteration 25 current loss: 0.8805896639823914 current acc: 0.49300242130750604\n",
      "iteration 26 current loss: 1.0582417249679565 current acc: 0.493180169286578\n",
      "iteration 27 current loss: 1.255348563194275 current acc: 0.49318840579710144\n",
      "iteration 28 current loss: 0.8257898092269897 current acc: 0.49346200241254523\n",
      "iteration 29 current loss: 0.9193215370178223 current acc: 0.49366265060240966\n",
      "iteration 30 current loss: 1.120385766029358 current acc: 0.49386281588447656\n",
      "iteration 31 current loss: 0.8092851042747498 current acc: 0.4942067307692308\n",
      "iteration 32 current loss: 0.8909919261932373 current acc: 0.49440576230492195\n",
      "iteration 33 current loss: 1.158299446105957 current acc: 0.49455635491606714\n",
      "iteration 34 current loss: 0.9494771361351013 current acc: 0.4947065868263473\n",
      "iteration 35 current loss: 0.9076493978500366 current acc: 0.495\n",
      "iteration 36 current loss: 0.9051504731178284 current acc: 0.49524492234169654\n",
      "iteration 37 current loss: 1.0498692989349365 current acc: 0.4953937947494033\n",
      "iteration 38 current loss: 0.8745496273040771 current acc: 0.49558998808104887\n",
      "iteration 39 current loss: 0.8148906230926514 current acc: 0.4957857142857143\n",
      "iteration 40 current loss: 0.8727296590805054 current acc: 0.4959809750297265\n",
      "iteration 41 current loss: 1.1680017709732056 current acc: 0.4960807600950119\n",
      "iteration 42 current loss: 1.1003395318984985 current acc: 0.4962514827995255\n",
      "iteration 43 current loss: 0.8558562994003296 current acc: 0.49646919431279624\n",
      "iteration 44 current loss: 0.7380322217941284 current acc: 0.4967573964497041\n",
      "iteration 45 current loss: 0.9259131550788879 current acc: 0.49699763593380614\n",
      "iteration 46 current loss: 0.981680691242218 current acc: 0.4972136953955136\n",
      "iteration 47 current loss: 1.0796453952789307 current acc: 0.4973820754716981\n",
      "iteration 48 current loss: 0.681862473487854 current acc: 0.49773851590106005\n",
      "iteration 49 current loss: 1.0874452590942383 current acc: 0.4978823529411765\n",
      "iteration 50 current loss: 1.0070494413375854 current acc: 0.49797884841363105\n",
      "iteration 51 current loss: 0.8554276823997498 current acc: 0.49826291079812207\n",
      "iteration 52 current loss: 1.1020134687423706 current acc: 0.4983118405627198\n",
      "iteration 53 current loss: 0.9648233652114868 current acc: 0.49847775175644027\n",
      "iteration 54 current loss: 0.7164005041122437 current acc: 0.49873684210526315\n",
      "iteration 55 current loss: 1.0220073461532593 current acc: 0.4989252336448598\n",
      "iteration 56 current loss: 0.9880836009979248 current acc: 0.4991598599766628\n",
      "iteration 57 current loss: 0.8152522444725037 current acc: 0.4993939393939394\n",
      "iteration 58 current loss: 0.7353863716125488 current acc: 0.49965075669383\n",
      "iteration 59 current loss: 1.0241488218307495 current acc: 0.4997441860465116\n",
      "iteration 60 current loss: 0.8722347021102905 current acc: 0.4999535423925668\n",
      "iteration 61 current loss: 1.0057764053344727 current acc: 0.5000928074245939\n",
      "iteration 62 current loss: 1.0781145095825195 current acc: 0.5003012746234067\n",
      "iteration 63 current loss: 1.1213316917419434 current acc: 0.5003240740740741\n",
      "iteration 64 current loss: 1.1267625093460083 current acc: 0.5003930635838151\n",
      "iteration 65 current loss: 0.8179068565368652 current acc: 0.5006235565819861\n",
      "iteration 66 current loss: 0.8357077240943909 current acc: 0.5008535178777394\n",
      "iteration 67 current loss: 0.9867711663246155 current acc: 0.501036866359447\n",
      "iteration 68 current loss: 0.794160783290863 current acc: 0.5013808975834292\n",
      "iteration 69 current loss: 0.8288633823394775 current acc: 0.501632183908046\n",
      "iteration 70 current loss: 0.7899404764175415 current acc: 0.5019058553386911\n",
      "iteration 71 current loss: 1.1230370998382568 current acc: 0.5020183486238532\n",
      "iteration 72 current loss: 0.960934579372406 current acc: 0.5021076746849943\n",
      "iteration 73 current loss: 1.1736736297607422 current acc: 0.5022196796338673\n",
      "iteration 74 current loss: 1.0662341117858887 current acc: 0.5024228571428572\n",
      "iteration 75 current loss: 0.8871866464614868 current acc: 0.5025570776255708\n",
      "iteration 76 current loss: 0.819742739200592 current acc: 0.5028050171037628\n",
      "iteration 77 current loss: 0.8203174471855164 current acc: 0.5030296127562642\n",
      "iteration 78 current loss: 0.9058412909507751 current acc: 0.5031626848691695\n",
      "iteration 79 current loss: 0.9515001773834229 current acc: 0.5033636363636363\n",
      "iteration 80 current loss: 0.9263284802436829 current acc: 0.5036095346197503\n",
      "iteration 81 current loss: 1.0496642589569092 current acc: 0.5036961451247165\n",
      "iteration 82 current loss: 0.7166252136230469 current acc: 0.50400906002265\n",
      "iteration 83 current loss: 0.9178330302238464 current acc: 0.5042307692307693\n",
      "iteration 84 current loss: 0.9342494010925293 current acc: 0.5044519774011299\n",
      "iteration 85 current loss: 0.7491800785064697 current acc: 0.5046501128668172\n",
      "iteration 86 current loss: 0.952684223651886 current acc: 0.5048252536640361\n",
      "iteration 87 current loss: 1.0970653295516968 current acc: 0.5049099099099099\n",
      "iteration 88 current loss: 0.941784143447876 current acc: 0.5050843644544432\n",
      "iteration 89 current loss: 0.9610700011253357 current acc: 0.5052359550561798\n",
      "iteration 90 current loss: 0.957173764705658 current acc: 0.5054994388327722\n",
      "iteration 91 current loss: 1.0556731224060059 current acc: 0.5057174887892377\n",
      "iteration 92 current loss: 0.8671295642852783 current acc: 0.5059126539753639\n",
      "iteration 93 current loss: 0.901832640171051 current acc: 0.5061297539149888\n",
      "iteration 94 current loss: 0.8109309673309326 current acc: 0.5063016759776536\n",
      "iteration 95 current loss: 0.9304599761962891 current acc: 0.50640625\n",
      "iteration 96 current loss: 0.9582452178001404 current acc: 0.5065997770345596\n",
      "iteration 97 current loss: 0.8803589344024658 current acc: 0.5068819599109131\n",
      "iteration 98 current loss: 1.0256646871566772 current acc: 0.5070522803114572\n",
      "iteration 99 current loss: 0.8288173079490662 current acc: 0.5072888888888889\n",
      "\t\tEpoch 8/100 complete. Epoch loss 0.9475431674718857 Epoch accuracy 0.6652\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 8, Validation Accuracy: 0.563125, Validation Loss: 1.2110392760485411\n",
      "best loss 0.9475431674718857\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.7329892516136169 current acc: 0.507591564927858\n",
      "iteration 1 current loss: 0.7639982104301453 current acc: 0.5077605321507761\n",
      "iteration 2 current loss: 0.9190752506256104 current acc: 0.5079734219269103\n",
      "iteration 3 current loss: 0.8284293413162231 current acc: 0.5081858407079646\n",
      "iteration 4 current loss: 0.8494483828544617 current acc: 0.5083314917127072\n",
      "iteration 5 current loss: 1.225267767906189 current acc: 0.5084105960264901\n",
      "iteration 6 current loss: 0.8907797932624817 current acc: 0.5085997794928335\n",
      "iteration 7 current loss: 1.1566174030303955 current acc: 0.508590308370044\n",
      "iteration 8 current loss: 0.9249817728996277 current acc: 0.5087568756875688\n",
      "iteration 9 current loss: 0.9523069262504578 current acc: 0.5088791208791209\n",
      "iteration 10 current loss: 0.765882670879364 current acc: 0.5091108671789243\n",
      "iteration 11 current loss: 0.8085438013076782 current acc: 0.5093859649122807\n",
      "iteration 12 current loss: 0.9383992552757263 current acc: 0.5094852135815992\n",
      "iteration 13 current loss: 0.8793897032737732 current acc: 0.5096936542669585\n",
      "iteration 14 current loss: 0.9695785641670227 current acc: 0.5098360655737705\n",
      "iteration 15 current loss: 0.9455291628837585 current acc: 0.5099344978165938\n",
      "iteration 16 current loss: 0.8568630218505859 current acc: 0.5102071973827699\n",
      "iteration 17 current loss: 0.8715044260025024 current acc: 0.5103703703703704\n",
      "iteration 18 current loss: 1.0361180305480957 current acc: 0.510467899891186\n",
      "iteration 19 current loss: 0.9773715138435364 current acc: 0.5106521739130435\n",
      "iteration 20 current loss: 0.8961176872253418 current acc: 0.5107926167209554\n",
      "iteration 21 current loss: 0.8758019208908081 current acc: 0.5109761388286334\n",
      "iteration 22 current loss: 0.7712740898132324 current acc: 0.5111375947995667\n",
      "iteration 23 current loss: 0.9047889709472656 current acc: 0.5113419913419913\n",
      "iteration 24 current loss: 0.9666994214057922 current acc: 0.5115027027027027\n",
      "iteration 25 current loss: 0.9132958054542542 current acc: 0.5116414686825054\n",
      "iteration 26 current loss: 1.0516667366027832 current acc: 0.5117367853290183\n",
      "iteration 27 current loss: 0.7662662267684937 current acc: 0.5120043103448276\n",
      "iteration 28 current loss: 0.9197483658790588 current acc: 0.5122282023681378\n",
      "iteration 29 current loss: 0.8454191088676453 current acc: 0.5123870967741936\n",
      "iteration 30 current loss: 0.783887505531311 current acc: 0.512610096670247\n",
      "iteration 31 current loss: 0.860757052898407 current acc: 0.5128326180257511\n",
      "iteration 32 current loss: 0.6524814367294312 current acc: 0.5130975348338692\n",
      "iteration 33 current loss: 0.8918027281761169 current acc: 0.5132976445396146\n",
      "iteration 34 current loss: 0.7838218808174133 current acc: 0.5135401069518717\n",
      "iteration 35 current loss: 0.8759209513664246 current acc: 0.5137179487179487\n",
      "iteration 36 current loss: 0.8427562117576599 current acc: 0.5139167556029882\n",
      "iteration 37 current loss: 0.7801706194877625 current acc: 0.5141151385927505\n",
      "iteration 38 current loss: 0.8759725093841553 current acc: 0.5143130990415336\n",
      "iteration 39 current loss: 0.8652350902557373 current acc: 0.5144893617021277\n",
      "iteration 40 current loss: 0.9296557903289795 current acc: 0.5146227417640807\n",
      "iteration 41 current loss: 0.7055345773696899 current acc: 0.5149256900212315\n",
      "iteration 42 current loss: 0.8571231365203857 current acc: 0.5151007423117709\n",
      "iteration 43 current loss: 0.9915796518325806 current acc: 0.5151906779661017\n",
      "iteration 44 current loss: 1.0265618562698364 current acc: 0.5152592592592593\n",
      "iteration 45 current loss: 1.0277173519134521 current acc: 0.5153911205073995\n",
      "iteration 46 current loss: 1.1159952878952026 current acc: 0.5155227032734953\n",
      "iteration 47 current loss: 1.099269151687622 current acc: 0.515506329113924\n",
      "iteration 48 current loss: 0.7145836353302002 current acc: 0.5157850368809273\n",
      "iteration 49 current loss: 0.9258849620819092 current acc: 0.5159157894736842\n",
      "iteration 50 current loss: 0.7733609676361084 current acc: 0.5161514195583596\n",
      "iteration 51 current loss: 0.9157534241676331 current acc: 0.5163025210084033\n",
      "iteration 52 current loss: 0.9541606903076172 current acc: 0.5164533053515216\n",
      "iteration 53 current loss: 0.85969078540802 current acc: 0.5166457023060796\n",
      "iteration 54 current loss: 0.889324963092804 current acc: 0.5168586387434555\n",
      "iteration 55 current loss: 0.9232679605484009 current acc: 0.5170292887029289\n",
      "iteration 56 current loss: 0.888031005859375 current acc: 0.5171786833855799\n",
      "iteration 57 current loss: 0.8853127360343933 current acc: 0.5172860125260961\n",
      "iteration 58 current loss: 1.0301730632781982 current acc: 0.517393117831074\n",
      "iteration 59 current loss: 0.7732146978378296 current acc: 0.5176458333333334\n",
      "iteration 60 current loss: 0.784030556678772 current acc: 0.5178147762747138\n",
      "iteration 61 current loss: 0.9417356848716736 current acc: 0.518004158004158\n",
      "iteration 62 current loss: 0.7875550985336304 current acc: 0.5182969885773624\n",
      "iteration 63 current loss: 0.8075389266014099 current acc: 0.5184854771784232\n",
      "iteration 64 current loss: 0.9561242461204529 current acc: 0.5185699481865285\n",
      "iteration 65 current loss: 1.031358003616333 current acc: 0.5187577639751553\n",
      "iteration 66 current loss: 0.9703747034072876 current acc: 0.5189038262668045\n",
      "iteration 67 current loss: 0.8889988660812378 current acc: 0.519090909090909\n",
      "iteration 68 current loss: 0.8502965569496155 current acc: 0.519298245614035\n",
      "iteration 69 current loss: 0.8937413096427917 current acc: 0.5194432989690722\n",
      "iteration 70 current loss: 0.9090021252632141 current acc: 0.5196086508753862\n",
      "iteration 71 current loss: 0.9482974410057068 current acc: 0.5197736625514403\n",
      "iteration 72 current loss: 1.0138819217681885 current acc: 0.5198561151079136\n",
      "iteration 73 current loss: 0.8110981583595276 current acc: 0.520041067761807\n",
      "iteration 74 current loss: 1.0498486757278442 current acc: 0.5200820512820513\n",
      "iteration 75 current loss: 0.853535532951355 current acc: 0.5202868852459016\n",
      "iteration 76 current loss: 0.7903417944908142 current acc: 0.5205117707267144\n",
      "iteration 77 current loss: 0.8106124401092529 current acc: 0.5206952965235174\n",
      "iteration 78 current loss: 0.798790693283081 current acc: 0.5208784473953013\n",
      "iteration 79 current loss: 0.7532636523246765 current acc: 0.5211224489795918\n",
      "iteration 80 current loss: 1.0168317556381226 current acc: 0.5212028542303772\n",
      "iteration 81 current loss: 0.9536212086677551 current acc: 0.5213441955193483\n",
      "iteration 82 current loss: 0.7711129784584045 current acc: 0.5215259409969482\n",
      "iteration 83 current loss: 0.7492572069168091 current acc: 0.5217479674796748\n",
      "iteration 84 current loss: 0.575079083442688 current acc: 0.5219898477157361\n",
      "iteration 85 current loss: 0.7150464653968811 current acc: 0.5222920892494929\n",
      "iteration 86 current loss: 0.9133240580558777 current acc: 0.5224316109422492\n",
      "iteration 87 current loss: 0.8898788690567017 current acc: 0.5225910931174089\n",
      "iteration 88 current loss: 0.9047546982765198 current acc: 0.5228311425682508\n",
      "iteration 89 current loss: 0.8864985108375549 current acc: 0.5230505050505051\n",
      "iteration 90 current loss: 0.8291416168212891 current acc: 0.5232492431886983\n",
      "iteration 91 current loss: 1.0002976655960083 current acc: 0.5233870967741936\n",
      "iteration 92 current loss: 0.9845216870307922 current acc: 0.5235045317220544\n",
      "iteration 93 current loss: 0.8300547003746033 current acc: 0.523702213279678\n",
      "iteration 94 current loss: 1.1238328218460083 current acc: 0.5237788944723618\n",
      "iteration 95 current loss: 0.7918022274971008 current acc: 0.5239759036144578\n",
      "iteration 96 current loss: 0.8045990467071533 current acc: 0.5241123370110331\n",
      "iteration 97 current loss: 0.6729459166526794 current acc: 0.5243486973947896\n",
      "iteration 98 current loss: 0.8101010918617249 current acc: 0.5245445445445446\n",
      "iteration 99 current loss: 1.0688036680221558 current acc: 0.52458\n",
      "\t\tEpoch 9/100 complete. Epoch loss 0.8864508593082427 Epoch accuracy 0.6802\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 9, Validation Accuracy: 0.590375, Validation Loss: 1.1308758731931448\n",
      "best loss 0.8864508593082427\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.8424075245857239 current acc: 0.5247752247752248\n",
      "iteration 1 current loss: 0.5861409902572632 current acc: 0.5250299401197605\n",
      "iteration 2 current loss: 0.9550933837890625 current acc: 0.5251046859421735\n",
      "iteration 3 current loss: 0.8751261830329895 current acc: 0.5252191235059761\n",
      "iteration 4 current loss: 0.8809201121330261 current acc: 0.5253333333333333\n",
      "iteration 5 current loss: 0.9143601059913635 current acc: 0.5254671968190855\n",
      "iteration 6 current loss: 0.8858588337898254 current acc: 0.5256802383316782\n",
      "iteration 7 current loss: 0.7714885473251343 current acc: 0.5258730158730158\n",
      "iteration 8 current loss: 0.7326197624206543 current acc: 0.5261248761149653\n",
      "iteration 9 current loss: 0.8218089938163757 current acc: 0.5262574257425743\n",
      "iteration 10 current loss: 0.7061643004417419 current acc: 0.5264688427299703\n",
      "iteration 11 current loss: 0.9485203623771667 current acc: 0.5265415019762846\n",
      "iteration 12 current loss: 0.5872884392738342 current acc: 0.5268311944718658\n",
      "iteration 13 current loss: 0.8936904668807983 current acc: 0.5269428007889546\n",
      "iteration 14 current loss: 0.8754401206970215 current acc: 0.5270344827586206\n",
      "iteration 15 current loss: 0.9065055251121521 current acc: 0.5271259842519685\n",
      "iteration 16 current loss: 0.8921347260475159 current acc: 0.5271976401179941\n",
      "iteration 17 current loss: 0.7215322256088257 current acc: 0.5274066797642436\n",
      "iteration 18 current loss: 0.7847535014152527 current acc: 0.527576054955839\n",
      "iteration 19 current loss: 0.7698175311088562 current acc: 0.5278039215686274\n",
      "iteration 20 current loss: 0.7368438839912415 current acc: 0.5280117531831537\n",
      "iteration 21 current loss: 0.6835567355155945 current acc: 0.5282387475538161\n",
      "iteration 22 current loss: 0.9847092628479004 current acc: 0.5284261974584555\n",
      "iteration 23 current loss: 0.677649736404419 current acc: 0.52865234375\n",
      "iteration 24 current loss: 0.877206027507782 current acc: 0.5287414634146341\n",
      "iteration 25 current loss: 0.9392566680908203 current acc: 0.5289083820662768\n",
      "iteration 26 current loss: 0.7731848955154419 current acc: 0.5290944498539435\n",
      "iteration 27 current loss: 0.7050708532333374 current acc: 0.52931906614786\n",
      "iteration 28 current loss: 0.7583949565887451 current acc: 0.5295238095238095\n",
      "iteration 29 current loss: 0.8451162576675415 current acc: 0.529747572815534\n",
      "iteration 30 current loss: 0.9356977939605713 current acc: 0.5299127061105723\n",
      "iteration 31 current loss: 0.7808279991149902 current acc: 0.5300581395348837\n",
      "iteration 32 current loss: 0.9424446821212769 current acc: 0.5301645692158761\n",
      "iteration 33 current loss: 0.8907163739204407 current acc: 0.5303094777562862\n",
      "iteration 34 current loss: 0.8700246214866638 current acc: 0.5304734299516908\n",
      "iteration 35 current loss: 1.0141431093215942 current acc: 0.5305791505791506\n",
      "iteration 36 current loss: 0.6869944930076599 current acc: 0.5307232401157184\n",
      "iteration 37 current loss: 0.8487715721130371 current acc: 0.5308670520231213\n",
      "iteration 38 current loss: 1.002845048904419 current acc: 0.5310105871029837\n",
      "iteration 39 current loss: 0.7079625129699707 current acc: 0.5311730769230769\n",
      "iteration 40 current loss: 0.5975354909896851 current acc: 0.5314121037463977\n",
      "iteration 41 current loss: 1.056765079498291 current acc: 0.531573896353167\n",
      "iteration 42 current loss: 1.1269161701202393 current acc: 0.531581975071908\n",
      "iteration 43 current loss: 0.8321564197540283 current acc: 0.5317432950191571\n",
      "iteration 44 current loss: 0.9479422569274902 current acc: 0.5318851674641148\n",
      "iteration 45 current loss: 1.0502909421920776 current acc: 0.5319502868068834\n",
      "iteration 46 current loss: 0.886320948600769 current acc: 0.5321107927411652\n",
      "iteration 47 current loss: 0.8428909778594971 current acc: 0.5322519083969466\n",
      "iteration 48 current loss: 0.7170878648757935 current acc: 0.5324118207816969\n",
      "iteration 49 current loss: 0.9292725324630737 current acc: 0.5325333333333333\n",
      "iteration 50 current loss: 0.7810537219047546 current acc: 0.5327307326355851\n",
      "iteration 51 current loss: 1.1267900466918945 current acc: 0.5328136882129277\n",
      "iteration 52 current loss: 1.0906283855438232 current acc: 0.5329344729344729\n",
      "iteration 53 current loss: 0.7180752754211426 current acc: 0.5330929791271347\n",
      "iteration 54 current loss: 0.7558430433273315 current acc: 0.533345971563981\n",
      "iteration 55 current loss: 0.6390213966369629 current acc: 0.5336174242424242\n",
      "iteration 56 current loss: 0.8254581689834595 current acc: 0.5337748344370861\n",
      "iteration 57 current loss: 0.8185721039772034 current acc: 0.5339130434782609\n",
      "iteration 58 current loss: 0.9131805300712585 current acc: 0.5340509915014164\n",
      "iteration 59 current loss: 0.9770362973213196 current acc: 0.5342075471698113\n",
      "iteration 60 current loss: 1.122176170349121 current acc: 0.5342130065975494\n",
      "iteration 61 current loss: 0.88667893409729 current acc: 0.5343126177024482\n",
      "iteration 62 current loss: 0.7741449475288391 current acc: 0.5344496707431797\n",
      "iteration 63 current loss: 0.8823715448379517 current acc: 0.5345300751879699\n",
      "iteration 64 current loss: 0.709075927734375 current acc: 0.5347793427230046\n",
      "iteration 65 current loss: 0.6638058423995972 current acc: 0.5350093808630394\n",
      "iteration 66 current loss: 0.5966992378234863 current acc: 0.5352764761012184\n",
      "iteration 67 current loss: 0.8618782162666321 current acc: 0.5354307116104869\n",
      "iteration 68 current loss: 0.803631603717804 current acc: 0.5355285313376987\n",
      "iteration 69 current loss: 0.7268674969673157 current acc: 0.5357570093457944\n",
      "iteration 70 current loss: 0.7147838473320007 current acc: 0.5359663865546218\n",
      "iteration 71 current loss: 0.882683277130127 current acc: 0.5361194029850747\n",
      "iteration 72 current loss: 0.7743955850601196 current acc: 0.5363280521901211\n",
      "iteration 73 current loss: 0.9144384264945984 current acc: 0.5364245810055865\n",
      "iteration 74 current loss: 0.8199107646942139 current acc: 0.536613953488372\n",
      "iteration 75 current loss: 0.9340285658836365 current acc: 0.5367472118959108\n",
      "iteration 76 current loss: 0.7382998466491699 current acc: 0.5369730733519035\n",
      "iteration 77 current loss: 0.6701621413230896 current acc: 0.5372170686456401\n",
      "iteration 78 current loss: 0.9002396464347839 current acc: 0.537386468952734\n",
      "iteration 79 current loss: 0.9000096917152405 current acc: 0.5375\n",
      "iteration 80 current loss: 0.7709510326385498 current acc: 0.5377243293246994\n",
      "iteration 81 current loss: 0.6606997847557068 current acc: 0.5378927911275416\n",
      "iteration 82 current loss: 0.7588893175125122 current acc: 0.5381532779316713\n",
      "iteration 83 current loss: 0.7727290987968445 current acc: 0.5383210332103321\n",
      "iteration 84 current loss: 0.7031170725822449 current acc: 0.5385437788018433\n",
      "iteration 85 current loss: 0.8873996734619141 current acc: 0.5386556169429098\n",
      "iteration 86 current loss: 0.635396420955658 current acc: 0.5388592456301748\n",
      "iteration 87 current loss: 0.6231212615966797 current acc: 0.5391176470588235\n",
      "iteration 88 current loss: 0.7713121175765991 current acc: 0.5392837465564738\n",
      "iteration 89 current loss: 1.017989158630371 current acc: 0.539394495412844\n",
      "iteration 90 current loss: 0.8774534463882446 current acc: 0.5395600366636114\n",
      "iteration 91 current loss: 0.6255844235420227 current acc: 0.5397985347985348\n",
      "iteration 92 current loss: 0.8568655252456665 current acc: 0.5399634034766697\n",
      "iteration 93 current loss: 0.7552509307861328 current acc: 0.540127970749543\n",
      "iteration 94 current loss: 0.9417669773101807 current acc: 0.5402191780821918\n",
      "iteration 95 current loss: 0.8245131969451904 current acc: 0.5404014598540146\n",
      "iteration 96 current loss: 0.7117486596107483 current acc: 0.5406016408386509\n",
      "iteration 97 current loss: 0.7022907733917236 current acc: 0.5408196721311476\n",
      "iteration 98 current loss: 1.0217822790145874 current acc: 0.5408553230209281\n",
      "iteration 99 current loss: 0.917600154876709 current acc: 0.541\n",
      "\t\tEpoch 10/100 complete. Epoch loss 0.827586697936058 Epoch accuracy 0.7052\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 10, Validation Accuracy: 0.606, Validation Loss: 1.0978550992906093\n",
      "best loss 0.827586697936058\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.706501841545105 current acc: 0.5411989100817439\n",
      "iteration 1 current loss: 0.5395594239234924 current acc: 0.5415063520871143\n",
      "iteration 2 current loss: 0.680499792098999 current acc: 0.5417407071622847\n",
      "iteration 3 current loss: 0.6442825198173523 current acc: 0.5419565217391304\n",
      "iteration 4 current loss: 0.7438051104545593 current acc: 0.5421176470588235\n",
      "iteration 5 current loss: 0.636070966720581 current acc: 0.5423869801084991\n",
      "iteration 6 current loss: 0.8273705840110779 current acc: 0.5425474254742547\n",
      "iteration 7 current loss: 0.8567177057266235 current acc: 0.5426895306859206\n",
      "iteration 8 current loss: 0.8819286227226257 current acc: 0.5428494138863841\n",
      "iteration 9 current loss: 0.6671150326728821 current acc: 0.5430090090090091\n",
      "iteration 10 current loss: 0.634306013584137 current acc: 0.5432043204320433\n",
      "iteration 11 current loss: 0.9655211567878723 current acc: 0.5432913669064748\n",
      "iteration 12 current loss: 0.7097668647766113 current acc: 0.5435040431266847\n",
      "iteration 13 current loss: 0.8403388857841492 current acc: 0.5435906642728905\n",
      "iteration 14 current loss: 0.9420773386955261 current acc: 0.5437488789237668\n",
      "iteration 15 current loss: 0.6332433819770813 current acc: 0.5439605734767025\n",
      "iteration 16 current loss: 0.8288748264312744 current acc: 0.544136078782453\n",
      "iteration 17 current loss: 0.7829177975654602 current acc: 0.5442933810375671\n",
      "iteration 18 current loss: 0.6960626840591431 current acc: 0.5444504021447721\n",
      "iteration 19 current loss: 0.8354811072349548 current acc: 0.5445535714285714\n",
      "iteration 20 current loss: 0.7889268398284912 current acc: 0.5447100802854594\n",
      "iteration 21 current loss: 0.803180456161499 current acc: 0.5448306595365419\n",
      "iteration 22 current loss: 0.6039819121360779 current acc: 0.5450756901157614\n",
      "iteration 23 current loss: 0.7725623250007629 current acc: 0.5452491103202847\n",
      "iteration 24 current loss: 0.8280476331710815 current acc: 0.5453866666666667\n",
      "iteration 25 current loss: 0.7076029777526855 current acc: 0.5455417406749556\n",
      "iteration 26 current loss: 0.8772171139717102 current acc: 0.5456433007985803\n",
      "iteration 27 current loss: 0.7931221127510071 current acc: 0.5457624113475177\n",
      "iteration 28 current loss: 0.6992846727371216 current acc: 0.5459875996457042\n",
      "iteration 29 current loss: 0.6334024667739868 current acc: 0.5461946902654867\n",
      "iteration 30 current loss: 0.8813019394874573 current acc: 0.5463129973474801\n",
      "iteration 31 current loss: 1.0356019735336304 current acc: 0.5463604240282686\n",
      "iteration 32 current loss: 0.7063508629798889 current acc: 0.5465666372462489\n",
      "iteration 33 current loss: 0.6678903102874756 current acc: 0.5467548500881835\n",
      "iteration 34 current loss: 0.6781607270240784 current acc: 0.546942731277533\n",
      "iteration 35 current loss: 0.7129085063934326 current acc: 0.5471126760563381\n",
      "iteration 36 current loss: 0.8805714249610901 current acc: 0.5472999120492524\n",
      "iteration 37 current loss: 0.6476451754570007 current acc: 0.5474516695957821\n",
      "iteration 38 current loss: 0.7810121774673462 current acc: 0.5476207199297629\n",
      "iteration 39 current loss: 0.7817671895027161 current acc: 0.547719298245614\n",
      "iteration 40 current loss: 0.7390261292457581 current acc: 0.5478878177037686\n",
      "iteration 41 current loss: 0.8647693395614624 current acc: 0.5480385288966725\n",
      "iteration 42 current loss: 0.7868227362632751 current acc: 0.5482239720034996\n",
      "iteration 43 current loss: 0.877132773399353 current acc: 0.5482692307692307\n",
      "iteration 44 current loss: 0.8597084283828735 current acc: 0.5483668122270743\n",
      "iteration 45 current loss: 0.7637596130371094 current acc: 0.548586387434555\n",
      "iteration 46 current loss: 0.5256447196006775 current acc: 0.5488055797733217\n",
      "iteration 47 current loss: 0.7261474132537842 current acc: 0.5489721254355401\n",
      "iteration 48 current loss: 0.8981109857559204 current acc: 0.5490165361183638\n",
      "iteration 49 current loss: 0.9288065433502197 current acc: 0.5491130434782608\n",
      "iteration 50 current loss: 0.7031645178794861 current acc: 0.5492788879235447\n",
      "iteration 51 current loss: 0.8315670490264893 current acc: 0.5494270833333333\n",
      "iteration 52 current loss: 0.7314629554748535 current acc: 0.5496444058976583\n",
      "iteration 53 current loss: 1.002393364906311 current acc: 0.5497573656845753\n",
      "iteration 54 current loss: 0.749013364315033 current acc: 0.5499047619047619\n",
      "iteration 55 current loss: 0.6283623576164246 current acc: 0.5501384083044982\n",
      "iteration 56 current loss: 0.8061190247535706 current acc: 0.55028522039758\n",
      "iteration 57 current loss: 0.9974675178527832 current acc: 0.5503972366148532\n",
      "iteration 58 current loss: 0.73688143491745 current acc: 0.5505263157894736\n",
      "iteration 59 current loss: 0.6461082696914673 current acc: 0.5507241379310345\n",
      "iteration 60 current loss: 0.8009158968925476 current acc: 0.550869939707149\n",
      "iteration 61 current loss: 0.8585968017578125 current acc: 0.5510154905335628\n",
      "iteration 62 current loss: 0.6601593494415283 current acc: 0.5512639724849527\n",
      "iteration 63 current loss: 0.5993929505348206 current acc: 0.5514776632302405\n",
      "iteration 64 current loss: 0.8410418629646301 current acc: 0.5516051502145922\n",
      "iteration 65 current loss: 0.625877320766449 current acc: 0.5518010291595197\n",
      "iteration 66 current loss: 0.6378746032714844 current acc: 0.5519965724078835\n",
      "iteration 67 current loss: 0.5943950414657593 current acc: 0.552277397260274\n",
      "iteration 68 current loss: 0.9727694988250732 current acc: 0.5523866552609068\n",
      "iteration 69 current loss: 0.578590989112854 current acc: 0.5525811965811965\n",
      "iteration 70 current loss: 0.7449798583984375 current acc: 0.5527754056362084\n",
      "iteration 71 current loss: 0.8044112324714661 current acc: 0.552901023890785\n",
      "iteration 72 current loss: 0.7291314005851746 current acc: 0.5530946291560103\n",
      "iteration 73 current loss: 0.7206241488456726 current acc: 0.5532879045996593\n",
      "iteration 74 current loss: 0.8646466135978699 current acc: 0.5533617021276596\n",
      "iteration 75 current loss: 0.7986762523651123 current acc: 0.5535034013605442\n",
      "iteration 76 current loss: 0.8381344676017761 current acc: 0.5536108751062022\n",
      "iteration 77 current loss: 0.7813454270362854 current acc: 0.5537521222410866\n",
      "iteration 78 current loss: 0.7535802721977234 current acc: 0.5539440203562341\n",
      "iteration 79 current loss: 0.501995325088501 current acc: 0.5542372881355933\n",
      "iteration 80 current loss: 0.6335085034370422 current acc: 0.5544453852667232\n",
      "iteration 81 current loss: 0.7185348272323608 current acc: 0.5546700507614213\n",
      "iteration 82 current loss: 0.5799606442451477 current acc: 0.5549112426035503\n",
      "iteration 83 current loss: 0.9144200086593628 current acc: 0.555\n",
      "iteration 84 current loss: 0.5687339305877686 current acc: 0.5551898734177215\n",
      "iteration 85 current loss: 0.9291623830795288 current acc: 0.5553119730185497\n",
      "iteration 86 current loss: 0.7330043315887451 current acc: 0.5554675652906487\n",
      "iteration 87 current loss: 1.0134360790252686 current acc: 0.5555555555555556\n",
      "iteration 88 current loss: 0.7364132404327393 current acc: 0.5557106812447434\n",
      "iteration 89 current loss: 0.7362653613090515 current acc: 0.5558319327731093\n",
      "iteration 90 current loss: 0.7056899070739746 current acc: 0.5560201511335012\n",
      "iteration 91 current loss: 0.697584331035614 current acc: 0.5561912751677852\n",
      "iteration 92 current loss: 0.7220341563224792 current acc: 0.5562950544844929\n",
      "iteration 93 current loss: 0.7823837399482727 current acc: 0.5564154103852597\n",
      "iteration 94 current loss: 0.6830196976661682 current acc: 0.5565690376569038\n",
      "iteration 95 current loss: 0.6612334251403809 current acc: 0.5568060200668896\n",
      "iteration 96 current loss: 0.7249085307121277 current acc: 0.5569590643274854\n",
      "iteration 97 current loss: 0.7065337896347046 current acc: 0.5571118530884808\n",
      "iteration 98 current loss: 0.7825906276702881 current acc: 0.557231025854879\n",
      "iteration 99 current loss: 0.5811119079589844 current acc: 0.5574666666666667\n",
      "\t\tEpoch 11/100 complete. Epoch loss 0.7545311772823333 Epoch accuracy 0.7386\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 11, Validation Accuracy: 0.614625, Validation Loss: 1.090377314388752\n",
      "best loss 0.7545311772823333\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.9557592272758484 current acc: 0.55751873438801\n",
      "iteration 1 current loss: 0.7503653168678284 current acc: 0.5577038269550749\n",
      "iteration 2 current loss: 0.8642611503601074 current acc: 0.5578054862842893\n",
      "iteration 3 current loss: 0.7793694138526917 current acc: 0.5579401993355482\n",
      "iteration 4 current loss: 0.8094918727874756 current acc: 0.5581244813278008\n",
      "iteration 5 current loss: 0.6160234808921814 current acc: 0.5583250414593698\n",
      "iteration 6 current loss: 0.6558260917663574 current acc: 0.5585418392709196\n",
      "iteration 7 current loss: 0.7011727094650269 current acc: 0.5587417218543046\n",
      "iteration 8 current loss: 0.8244678974151611 current acc: 0.5588585607940446\n",
      "iteration 9 current loss: 0.537674605846405 current acc: 0.5590743801652892\n",
      "iteration 10 current loss: 0.6453188061714172 current acc: 0.5592402972749794\n",
      "iteration 11 current loss: 0.6104791760444641 current acc: 0.5594224422442244\n",
      "iteration 12 current loss: 0.6281642317771912 current acc: 0.5595877988458368\n",
      "iteration 13 current loss: 0.7233232259750366 current acc: 0.5597693574958814\n",
      "iteration 14 current loss: 0.7204445600509644 current acc: 0.5599670781893004\n",
      "iteration 15 current loss: 0.7528917789459229 current acc: 0.5601480263157895\n",
      "iteration 16 current loss: 0.8239418864250183 current acc: 0.5602629416598193\n",
      "iteration 17 current loss: 0.6450827717781067 current acc: 0.5604433497536946\n",
      "iteration 18 current loss: 0.6377151012420654 current acc: 0.5606398687448728\n",
      "iteration 19 current loss: 0.6604663133621216 current acc: 0.5608196721311476\n",
      "iteration 20 current loss: 0.6315547823905945 current acc: 0.561015561015561\n",
      "iteration 21 current loss: 0.515040934085846 current acc: 0.561227495908347\n",
      "iteration 22 current loss: 0.5918993949890137 current acc: 0.561471790678659\n",
      "iteration 23 current loss: 0.5752288699150085 current acc: 0.5616503267973856\n",
      "iteration 24 current loss: 0.6375883221626282 current acc: 0.5618285714285715\n",
      "iteration 25 current loss: 0.6993994116783142 current acc: 0.5619412724306688\n",
      "iteration 26 current loss: 0.7027269601821899 current acc: 0.5621678891605542\n",
      "iteration 27 current loss: 0.7234594821929932 current acc: 0.5623452768729642\n",
      "iteration 28 current loss: 0.7866478562355042 current acc: 0.5625061025223759\n",
      "iteration 29 current loss: 0.6002371907234192 current acc: 0.5627154471544715\n",
      "iteration 30 current loss: 0.6136687994003296 current acc: 0.5629082047116166\n",
      "iteration 31 current loss: 0.820931613445282 current acc: 0.5630357142857143\n",
      "iteration 32 current loss: 0.7057769298553467 current acc: 0.5631954582319546\n",
      "iteration 33 current loss: 0.7734705209732056 current acc: 0.563354943273906\n",
      "iteration 34 current loss: 0.7545466423034668 current acc: 0.5635141700404859\n",
      "iteration 35 current loss: 0.6272604465484619 current acc: 0.5636731391585761\n",
      "iteration 36 current loss: 0.9506498575210571 current acc: 0.5637348423605497\n",
      "iteration 37 current loss: 0.6708192229270935 current acc: 0.5638772213247173\n",
      "iteration 38 current loss: 0.6130180954933167 current acc: 0.5641000807102502\n",
      "iteration 39 current loss: 0.7050631642341614 current acc: 0.5642741935483871\n",
      "iteration 40 current loss: 0.826276421546936 current acc: 0.5643674456083804\n",
      "iteration 41 current loss: 0.6677210927009583 current acc: 0.5645249597423511\n",
      "iteration 42 current loss: 0.60216224193573 current acc: 0.5646983105390185\n",
      "iteration 43 current loss: 0.8287689089775085 current acc: 0.5648231511254019\n",
      "iteration 44 current loss: 0.7225205302238464 current acc: 0.5649638554216867\n",
      "iteration 45 current loss: 0.8967348337173462 current acc: 0.5650561797752809\n",
      "iteration 46 current loss: 0.9115231037139893 current acc: 0.5651483560545308\n",
      "iteration 47 current loss: 0.6475923657417297 current acc: 0.5653525641025641\n",
      "iteration 48 current loss: 0.6050832867622375 current acc: 0.5655084067253803\n",
      "iteration 49 current loss: 0.5577307939529419 current acc: 0.565728\n",
      "iteration 50 current loss: 0.5696725845336914 current acc: 0.5659152677857714\n",
      "iteration 51 current loss: 0.7064486742019653 current acc: 0.566038338658147\n",
      "iteration 52 current loss: 0.7544005513191223 current acc: 0.5662250598563447\n",
      "iteration 53 current loss: 0.7292565107345581 current acc: 0.5663795853269538\n",
      "iteration 54 current loss: 0.6424945592880249 current acc: 0.5665657370517928\n",
      "iteration 55 current loss: 0.6571815609931946 current acc: 0.5667197452229299\n",
      "iteration 56 current loss: 0.6435729265213013 current acc: 0.5669053301511535\n",
      "iteration 57 current loss: 0.7823817729949951 current acc: 0.5670588235294117\n",
      "iteration 58 current loss: 0.6701188683509827 current acc: 0.5671961874503574\n",
      "iteration 59 current loss: 0.4890190362930298 current acc: 0.5674603174603174\n",
      "iteration 60 current loss: 0.7475861310958862 current acc: 0.5675654242664552\n",
      "iteration 61 current loss: 0.9140121340751648 current acc: 0.5676545166402536\n",
      "iteration 62 current loss: 0.84367835521698 current acc: 0.5677909738717339\n",
      "iteration 63 current loss: 0.6232925653457642 current acc: 0.567990506329114\n",
      "iteration 64 current loss: 0.7712019085884094 current acc: 0.5680790513833992\n",
      "iteration 65 current loss: 0.6023485660552979 current acc: 0.5682306477093207\n",
      "iteration 66 current loss: 0.48154324293136597 current acc: 0.5684293606945541\n",
      "iteration 67 current loss: 0.7184405326843262 current acc: 0.5685488958990537\n",
      "iteration 68 current loss: 0.7999445199966431 current acc: 0.568715524034673\n",
      "iteration 69 current loss: 0.7876499891281128 current acc: 0.5687874015748031\n",
      "iteration 70 current loss: 0.6766700148582458 current acc: 0.5689221085759245\n",
      "iteration 71 current loss: 0.6763866543769836 current acc: 0.5690566037735849\n",
      "iteration 72 current loss: 0.6381668448448181 current acc: 0.569222309505106\n",
      "iteration 73 current loss: 0.7448976635932922 current acc: 0.5692778649921507\n",
      "iteration 74 current loss: 0.7743384838104248 current acc: 0.5694274509803922\n",
      "iteration 75 current loss: 0.6385001540184021 current acc: 0.5695924764890282\n",
      "iteration 76 current loss: 0.6280342936515808 current acc: 0.5697415818324197\n",
      "iteration 77 current loss: 0.8210896253585815 current acc: 0.5698278560250392\n",
      "iteration 78 current loss: 0.8053222894668579 current acc: 0.5699452697419859\n",
      "iteration 79 current loss: 0.6399490833282471 current acc: 0.5700625\n",
      "iteration 80 current loss: 0.4957728981971741 current acc: 0.5702732240437158\n",
      "iteration 81 current loss: 0.5698094367980957 current acc: 0.570421216848674\n",
      "iteration 82 current loss: 0.5646783709526062 current acc: 0.5705689789555729\n",
      "iteration 83 current loss: 0.5886103510856628 current acc: 0.5707476635514018\n",
      "iteration 84 current loss: 0.6718347668647766 current acc: 0.5708793774319066\n",
      "iteration 85 current loss: 0.4925864040851593 current acc: 0.5711197511664075\n",
      "iteration 86 current loss: 0.6720577478408813 current acc: 0.5712665112665113\n",
      "iteration 87 current loss: 0.7499654293060303 current acc: 0.5713975155279503\n",
      "iteration 88 current loss: 0.5503979325294495 current acc: 0.5716058960434446\n",
      "iteration 89 current loss: 0.6906746625900269 current acc: 0.5717674418604651\n",
      "iteration 90 current loss: 0.5467010140419006 current acc: 0.5719752130131681\n",
      "iteration 91 current loss: 0.6866154670715332 current acc: 0.5721207430340557\n",
      "iteration 92 current loss: 0.4997403621673584 current acc: 0.5723124516627996\n",
      "iteration 93 current loss: 0.773440957069397 current acc: 0.5724111282843894\n",
      "iteration 94 current loss: 0.6556388139724731 current acc: 0.5725714285714286\n",
      "iteration 95 current loss: 0.7514591813087463 current acc: 0.5727314814814815\n",
      "iteration 96 current loss: 0.660716712474823 current acc: 0.5728450269853508\n",
      "iteration 97 current loss: 0.7401453256607056 current acc: 0.5729892141756548\n",
      "iteration 98 current loss: 0.8652336001396179 current acc: 0.5731023864511162\n",
      "iteration 99 current loss: 0.6496781706809998 current acc: 0.5732769230769231\n",
      "\t\tEpoch 12/100 complete. Epoch loss 0.6916069939732552 Epoch accuracy 0.763\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 12, Validation Accuracy: 0.633, Validation Loss: 1.0356417257338761\n",
      "best loss 0.6916069939732552\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.48803141713142395 current acc: 0.5734973097617218\n",
      "iteration 1 current loss: 0.6445677876472473 current acc: 0.5736251920122888\n",
      "iteration 2 current loss: 0.4990296959877014 current acc: 0.5738449731389103\n",
      "iteration 3 current loss: 0.48048079013824463 current acc: 0.574079754601227\n",
      "iteration 4 current loss: 0.64368736743927 current acc: 0.5742222222222222\n",
      "iteration 5 current loss: 0.6142240166664124 current acc: 0.5743950995405819\n",
      "iteration 6 current loss: 0.48035597801208496 current acc: 0.5745983167559297\n",
      "iteration 7 current loss: 0.6181897521018982 current acc: 0.5747553516819572\n",
      "iteration 8 current loss: 0.379825234413147 current acc: 0.5750190985485103\n",
      "iteration 9 current loss: 0.6319456696510315 current acc: 0.5751908396946565\n",
      "iteration 10 current loss: 0.6554455757141113 current acc: 0.5753775743707094\n",
      "iteration 11 current loss: 0.5598909854888916 current acc: 0.5755792682926829\n",
      "iteration 12 current loss: 0.42580997943878174 current acc: 0.5757958872810358\n",
      "iteration 13 current loss: 0.5022658705711365 current acc: 0.5760121765601217\n",
      "iteration 14 current loss: 0.6041688323020935 current acc: 0.5761825095057034\n",
      "iteration 15 current loss: 0.7221233248710632 current acc: 0.5763221884498481\n",
      "iteration 16 current loss: 0.7011448740959167 current acc: 0.5764464692482916\n",
      "iteration 17 current loss: 0.5507600903511047 current acc: 0.5766009104704097\n",
      "iteration 18 current loss: 0.66420578956604 current acc: 0.5767551175132676\n",
      "iteration 19 current loss: 0.7650042772293091 current acc: 0.5768181818181818\n",
      "iteration 20 current loss: 0.4838312268257141 current acc: 0.5770174110522331\n",
      "iteration 21 current loss: 0.6379861831665039 current acc: 0.5772163388804841\n",
      "iteration 22 current loss: 0.6534841060638428 current acc: 0.5773847316704459\n",
      "iteration 23 current loss: 0.543001651763916 current acc: 0.5775679758308158\n",
      "iteration 24 current loss: 0.5711368322372437 current acc: 0.577766037735849\n",
      "iteration 25 current loss: 0.4328687787055969 current acc: 0.5779939668174963\n",
      "iteration 26 current loss: 0.5330917239189148 current acc: 0.57819140919367\n",
      "iteration 27 current loss: 0.4766356945037842 current acc: 0.5783885542168675\n",
      "iteration 28 current loss: 0.5496109127998352 current acc: 0.578510158013544\n",
      "iteration 29 current loss: 0.5580169558525085 current acc: 0.5787067669172933\n",
      "iteration 30 current loss: 0.7203577160835266 current acc: 0.5788279489105935\n",
      "iteration 31 current loss: 0.5270856618881226 current acc: 0.5790390390390391\n",
      "iteration 32 current loss: 0.6163160800933838 current acc: 0.5791747936984246\n",
      "iteration 33 current loss: 0.4674657881259918 current acc: 0.5793553223388306\n",
      "iteration 34 current loss: 0.6596621870994568 current acc: 0.5794756554307116\n",
      "iteration 35 current loss: 0.5113353133201599 current acc: 0.579625748502994\n",
      "iteration 36 current loss: 0.6427444219589233 current acc: 0.5797606581899776\n",
      "iteration 37 current loss: 0.617664635181427 current acc: 0.5798953662182361\n",
      "iteration 38 current loss: 0.5345693826675415 current acc: 0.5800896191187453\n",
      "iteration 39 current loss: 0.5517910122871399 current acc: 0.5802537313432836\n",
      "iteration 40 current loss: 0.48445412516593933 current acc: 0.5804175988068605\n",
      "iteration 41 current loss: 0.49345821142196655 current acc: 0.5805961251862891\n",
      "iteration 42 current loss: 0.6025935411453247 current acc: 0.5807594936708861\n",
      "iteration 43 current loss: 0.6366034150123596 current acc: 0.580922619047619\n",
      "iteration 44 current loss: 0.7114399671554565 current acc: 0.5810408921933086\n",
      "iteration 45 current loss: 0.445450097322464 current acc: 0.5812481426448737\n",
      "iteration 46 current loss: 0.6453348398208618 current acc: 0.581365998515219\n",
      "iteration 47 current loss: 0.5277197360992432 current acc: 0.581513353115727\n",
      "iteration 48 current loss: 0.4769110381603241 current acc: 0.5817642698295034\n",
      "iteration 49 current loss: 0.6678752303123474 current acc: 0.581925925925926\n",
      "iteration 50 current loss: 0.6589397192001343 current acc: 0.5820281273131014\n",
      "iteration 51 current loss: 0.7326534986495972 current acc: 0.5821449704142012\n",
      "iteration 52 current loss: 0.6128299832344055 current acc: 0.5823355506282335\n",
      "iteration 53 current loss: 0.6069696545600891 current acc: 0.5824963072378139\n",
      "iteration 54 current loss: 0.5844599604606628 current acc: 0.5826568265682657\n",
      "iteration 55 current loss: 0.695344865322113 current acc: 0.5828318584070796\n",
      "iteration 56 current loss: 0.5965675711631775 current acc: 0.5829771554900516\n",
      "iteration 57 current loss: 0.7232891321182251 current acc: 0.5830486008836524\n",
      "iteration 58 current loss: 0.661043643951416 current acc: 0.5831788079470198\n",
      "iteration 59 current loss: 0.7477253675460815 current acc: 0.5832794117647059\n",
      "iteration 60 current loss: 0.5505347847938538 current acc: 0.5834533431300515\n",
      "iteration 61 current loss: 0.5425871014595032 current acc: 0.5836563876651982\n",
      "iteration 62 current loss: 0.5920969843864441 current acc: 0.5838297872340426\n",
      "iteration 63 current loss: 0.6201872825622559 current acc: 0.583958944281525\n",
      "iteration 64 current loss: 0.48333561420440674 current acc: 0.5841318681318681\n",
      "iteration 65 current loss: 0.5796883702278137 current acc: 0.5842606149341142\n",
      "iteration 66 current loss: 0.8457126021385193 current acc: 0.5843160204828091\n",
      "iteration 67 current loss: 0.4780394434928894 current acc: 0.5844883040935672\n",
      "iteration 68 current loss: 0.621515154838562 current acc: 0.584645726807889\n",
      "iteration 69 current loss: 0.7820097208023071 current acc: 0.5847591240875912\n",
      "iteration 70 current loss: 0.8634641170501709 current acc: 0.5848869438366157\n",
      "iteration 71 current loss: 0.575355052947998 current acc: 0.5850145772594753\n",
      "iteration 72 current loss: 0.5689875483512878 current acc: 0.585142024763292\n",
      "iteration 73 current loss: 0.6366698741912842 current acc: 0.585254730713246\n",
      "iteration 74 current loss: 0.6900182962417603 current acc: 0.5853818181818182\n",
      "iteration 75 current loss: 0.6365808248519897 current acc: 0.5855232558139535\n",
      "iteration 76 current loss: 0.9255553483963013 current acc: 0.5855337690631808\n",
      "iteration 77 current loss: 0.6181467771530151 current acc: 0.5856313497822931\n",
      "iteration 78 current loss: 0.692700982093811 current acc: 0.5858158085569253\n",
      "iteration 79 current loss: 0.7736647129058838 current acc: 0.5859565217391305\n",
      "iteration 80 current loss: 0.6127780675888062 current acc: 0.5860970311368574\n",
      "iteration 81 current loss: 0.6552570462226868 current acc: 0.5862662807525325\n",
      "iteration 82 current loss: 0.5888254642486572 current acc: 0.5863919016630513\n",
      "iteration 83 current loss: 0.7371074557304382 current acc: 0.5865028901734104\n",
      "iteration 84 current loss: 0.5661824345588684 current acc: 0.5866714801444043\n",
      "iteration 85 current loss: 0.7499115467071533 current acc: 0.5867821067821067\n",
      "iteration 86 current loss: 0.6028075218200684 current acc: 0.5868925739005046\n",
      "iteration 87 current loss: 0.7997439503669739 current acc: 0.5870028818443804\n",
      "iteration 88 current loss: 0.6179203987121582 current acc: 0.5871418286537077\n",
      "iteration 89 current loss: 0.7048522233963013 current acc: 0.5872661870503597\n",
      "iteration 90 current loss: 0.8215638995170593 current acc: 0.5873472322070453\n",
      "iteration 91 current loss: 0.5722090005874634 current acc: 0.587514367816092\n",
      "iteration 92 current loss: 0.7144763469696045 current acc: 0.5876238334529792\n",
      "iteration 93 current loss: 0.5679581165313721 current acc: 0.5877905308464849\n",
      "iteration 94 current loss: 0.7968907356262207 current acc: 0.5879139784946237\n",
      "iteration 95 current loss: 0.6268177628517151 current acc: 0.588080229226361\n",
      "iteration 96 current loss: 0.7714895606040955 current acc: 0.5882032927702219\n",
      "iteration 97 current loss: 0.7510311007499695 current acc: 0.5882975679542203\n",
      "iteration 98 current loss: 0.8585888147354126 current acc: 0.5884203002144389\n",
      "iteration 99 current loss: 0.9343010187149048 current acc: 0.5885\n",
      "\t\tEpoch 13/100 complete. Epoch loss 0.6235904023051262 Epoch accuracy 0.7864\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 13, Validation Accuracy: 0.629, Validation Loss: 1.0568404529243707\n",
      "best loss 0.6235904023051262\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.6088277697563171 current acc: 0.588665239114918\n",
      "iteration 1 current loss: 0.39471766352653503 current acc: 0.5888873038516406\n",
      "iteration 2 current loss: 0.7080506682395935 current acc: 0.589023521026372\n",
      "iteration 3 current loss: 0.6011676788330078 current acc: 0.5891737891737892\n",
      "iteration 4 current loss: 0.3727385401725769 current acc: 0.5893807829181494\n",
      "iteration 5 current loss: 0.4751969575881958 current acc: 0.589601706970128\n",
      "iteration 6 current loss: 0.5048530697822571 current acc: 0.5897796730632552\n",
      "iteration 7 current loss: 0.43975475430488586 current acc: 0.5899857954545454\n",
      "iteration 8 current loss: 0.683115541934967 current acc: 0.5901064584811924\n",
      "iteration 9 current loss: 0.8011376261711121 current acc: 0.5902127659574468\n",
      "iteration 10 current loss: 0.4453648626804352 current acc: 0.5904181431608788\n",
      "iteration 11 current loss: 0.5600230693817139 current acc: 0.5905949008498583\n",
      "iteration 12 current loss: 0.5596091747283936 current acc: 0.5907289455060156\n",
      "iteration 13 current loss: 0.37193602323532104 current acc: 0.590975954738331\n",
      "iteration 14 current loss: 0.4159986078739166 current acc: 0.5911519434628976\n",
      "iteration 15 current loss: 0.7863821983337402 current acc: 0.5912994350282486\n",
      "iteration 16 current loss: 0.49680855870246887 current acc: 0.5915031757233592\n",
      "iteration 17 current loss: 0.4579907953739166 current acc: 0.5916784203102962\n",
      "iteration 18 current loss: 0.574277937412262 current acc: 0.5918252290345314\n",
      "iteration 19 current loss: 0.6472816467285156 current acc: 0.5919295774647887\n",
      "iteration 20 current loss: 0.5644595623016357 current acc: 0.5920760028149191\n",
      "iteration 21 current loss: 0.475544810295105 current acc: 0.5922784810126582\n",
      "iteration 22 current loss: 0.37939634919166565 current acc: 0.5924806746310611\n",
      "iteration 23 current loss: 0.38077718019485474 current acc: 0.5927106741573034\n",
      "iteration 24 current loss: 0.6690343618392944 current acc: 0.5928561403508772\n",
      "iteration 25 current loss: 0.6296331286430359 current acc: 0.5930014025245441\n",
      "iteration 26 current loss: 0.45843327045440674 current acc: 0.5932025227750526\n",
      "iteration 27 current loss: 0.5808272361755371 current acc: 0.593375350140056\n",
      "iteration 28 current loss: 0.7232540249824524 current acc: 0.5934639608117565\n",
      "iteration 29 current loss: 0.518170952796936 current acc: 0.5936503496503497\n",
      "iteration 30 current loss: 0.5209321975708008 current acc: 0.5937665967854647\n",
      "iteration 31 current loss: 0.4588117301464081 current acc: 0.5939245810055865\n",
      "iteration 32 current loss: 0.4844435155391693 current acc: 0.5940823447313329\n",
      "iteration 33 current loss: 0.5255323052406311 current acc: 0.5942398884239889\n",
      "iteration 34 current loss: 0.612032949924469 current acc: 0.594397212543554\n",
      "iteration 35 current loss: 0.5253760814666748 current acc: 0.5945821727019499\n",
      "iteration 36 current loss: 0.3873293399810791 current acc: 0.594794711203897\n",
      "iteration 37 current loss: 0.42442142963409424 current acc: 0.5949791376912378\n",
      "iteration 38 current loss: 0.6562557816505432 current acc: 0.5951077136900625\n",
      "iteration 39 current loss: 0.651957631111145 current acc: 0.59525\n",
      "iteration 40 current loss: 0.5626224279403687 current acc: 0.5953782095766829\n",
      "iteration 41 current loss: 0.5781556963920593 current acc: 0.5955478502080443\n",
      "iteration 42 current loss: 0.42029598355293274 current acc: 0.5957172557172558\n",
      "iteration 43 current loss: 0.573615550994873 current acc: 0.5958725761772853\n",
      "iteration 44 current loss: 0.5643710494041443 current acc: 0.5960276816608997\n",
      "iteration 45 current loss: 0.8002382516860962 current acc: 0.596058091286307\n",
      "iteration 46 current loss: 0.4466506540775299 current acc: 0.5962681409813407\n",
      "iteration 47 current loss: 0.4043770730495453 current acc: 0.5964364640883978\n",
      "iteration 48 current loss: 0.5922994613647461 current acc: 0.5965769496204278\n",
      "iteration 49 current loss: 0.5832936763763428 current acc: 0.596744827586207\n",
      "iteration 50 current loss: 0.5384846329689026 current acc: 0.5969262577532736\n",
      "iteration 51 current loss: 0.6212829351425171 current acc: 0.5970247933884297\n",
      "iteration 52 current loss: 0.5549542903900146 current acc: 0.5971920165175499\n",
      "iteration 53 current loss: 0.5571553707122803 current acc: 0.597331499312242\n",
      "iteration 54 current loss: 0.46432211995124817 current acc: 0.5975395189003436\n",
      "iteration 55 current loss: 0.683715283870697 current acc: 0.5976648351648352\n",
      "iteration 56 current loss: 0.6211761236190796 current acc: 0.5977899794097461\n",
      "iteration 57 current loss: 0.5259700417518616 current acc: 0.5979698216735254\n",
      "iteration 58 current loss: 0.47414442896842957 current acc: 0.5981494174091844\n",
      "iteration 59 current loss: 0.4575130045413971 current acc: 0.5983150684931506\n",
      "iteration 60 current loss: 0.4302619695663452 current acc: 0.598507871321013\n",
      "iteration 61 current loss: 0.5087698101997375 current acc: 0.598686730506156\n",
      "iteration 62 current loss: 0.45621997117996216 current acc: 0.5988106630211893\n",
      "iteration 63 current loss: 0.6047633290290833 current acc: 0.598948087431694\n",
      "iteration 64 current loss: 0.6744678616523743 current acc: 0.5990307167235495\n",
      "iteration 65 current loss: 0.6459652781486511 current acc: 0.5991678035470669\n",
      "iteration 66 current loss: 0.49986758828163147 current acc: 0.5993319700068166\n",
      "iteration 67 current loss: 0.564230740070343 current acc: 0.5994686648501363\n",
      "iteration 68 current loss: 0.47788524627685547 current acc: 0.5996187882913546\n",
      "iteration 69 current loss: 0.42983725666999817 current acc: 0.59978231292517\n",
      "iteration 70 current loss: 0.6270967721939087 current acc: 0.5999048266485384\n",
      "iteration 71 current loss: 0.5437373518943787 current acc: 0.6000271739130435\n",
      "iteration 72 current loss: 0.5968877673149109 current acc: 0.600162932790224\n",
      "iteration 73 current loss: 0.6636284589767456 current acc: 0.6002442333785617\n",
      "iteration 74 current loss: 0.5606876611709595 current acc: 0.6003661016949152\n",
      "iteration 75 current loss: 0.6930655837059021 current acc: 0.600460704607046\n",
      "iteration 76 current loss: 0.5049963593482971 current acc: 0.6006228842247799\n",
      "iteration 77 current loss: 0.5665409564971924 current acc: 0.6007713125845737\n",
      "iteration 78 current loss: 0.5179176926612854 current acc: 0.600919540229885\n",
      "iteration 79 current loss: 0.5405551791191101 current acc: 0.6010675675675675\n",
      "iteration 80 current loss: 0.6227260828018188 current acc: 0.6011883862255233\n",
      "iteration 81 current loss: 0.4408341348171234 current acc: 0.6013630229419703\n",
      "iteration 82 current loss: 0.7251476049423218 current acc: 0.601429534726905\n",
      "iteration 83 current loss: 0.5850936770439148 current acc: 0.6015363881401617\n",
      "iteration 84 current loss: 0.7772780060768127 current acc: 0.6015892255892256\n",
      "iteration 85 current loss: 0.5799715518951416 current acc: 0.6016958277254374\n",
      "iteration 86 current loss: 0.6068515181541443 current acc: 0.6018022864828514\n",
      "iteration 87 current loss: 0.4188954532146454 current acc: 0.6019758064516129\n",
      "iteration 88 current loss: 0.5324642062187195 current acc: 0.6020953660174614\n",
      "iteration 89 current loss: 0.7398550510406494 current acc: 0.6021879194630873\n",
      "iteration 90 current loss: 0.66948401927948 current acc: 0.6022669349429913\n",
      "iteration 91 current loss: 0.6524479389190674 current acc: 0.602372654155496\n",
      "iteration 92 current loss: 0.584826648235321 current acc: 0.6024916275954454\n",
      "iteration 93 current loss: 0.7082535028457642 current acc: 0.6026238286479251\n",
      "iteration 94 current loss: 0.6037705540657043 current acc: 0.6027692307692307\n",
      "iteration 95 current loss: 0.6339337229728699 current acc: 0.6028877005347594\n",
      "iteration 96 current loss: 0.530195951461792 current acc: 0.6030193720774883\n",
      "iteration 97 current loss: 0.5050542950630188 current acc: 0.6031642189586115\n",
      "iteration 98 current loss: 0.5234897136688232 current acc: 0.6032821881254169\n",
      "iteration 99 current loss: 0.755536675453186 current acc: 0.6034\n",
      "\t\tEpoch 14/100 complete. Epoch loss 0.5585998818278313 Epoch accuracy 0.812\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 14, Validation Accuracy: 0.6055, Validation Loss: 1.1699882104992867\n",
      "best loss 0.5585998818278313\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.6203134059906006 current acc: 0.6035043304463691\n",
      "iteration 1 current loss: 0.3898162841796875 current acc: 0.6036884154460719\n",
      "iteration 2 current loss: 0.46305733919143677 current acc: 0.6038456420492349\n",
      "iteration 3 current loss: 0.4181293249130249 current acc: 0.6040159574468085\n",
      "iteration 4 current loss: 0.45482337474823 current acc: 0.604172757475083\n",
      "iteration 5 current loss: 0.5321390628814697 current acc: 0.6043160690571049\n",
      "iteration 6 current loss: 0.6231579780578613 current acc: 0.6044326476443265\n",
      "iteration 7 current loss: 0.5356788039207458 current acc: 0.6045888594164456\n",
      "iteration 8 current loss: 0.42706963419914246 current acc: 0.6047448641484426\n",
      "iteration 9 current loss: 0.40019455552101135 current acc: 0.604953642384106\n",
      "iteration 10 current loss: 0.5303073525428772 current acc: 0.6050827266710788\n",
      "iteration 11 current loss: 0.42298197746276855 current acc: 0.6052645502645503\n",
      "iteration 12 current loss: 0.6511446237564087 current acc: 0.6053932584269663\n",
      "iteration 13 current loss: 0.48169514536857605 current acc: 0.6055878467635403\n",
      "iteration 14 current loss: 0.4796382188796997 current acc: 0.6057293729372937\n",
      "iteration 15 current loss: 0.42916980385780334 current acc: 0.605910290237467\n",
      "iteration 16 current loss: 0.5779219269752502 current acc: 0.6060250494396836\n",
      "iteration 17 current loss: 0.4029301404953003 current acc: 0.6061923583662714\n",
      "iteration 18 current loss: 0.5424844622612 current acc: 0.6063331138907175\n",
      "iteration 19 current loss: 0.486670583486557 current acc: 0.6064868421052632\n",
      "iteration 20 current loss: 0.5195923447608948 current acc: 0.6066403681788297\n",
      "iteration 21 current loss: 0.5115959048271179 current acc: 0.6067411300919843\n",
      "iteration 22 current loss: 0.5953856110572815 current acc: 0.606854891661195\n",
      "iteration 23 current loss: 0.5253437161445618 current acc: 0.6070078740157481\n",
      "iteration 24 current loss: 0.44587790966033936 current acc: 0.6071737704918033\n",
      "iteration 25 current loss: 0.34013885259628296 current acc: 0.6073787680209699\n",
      "iteration 26 current loss: 0.4179348051548004 current acc: 0.6075573018991487\n",
      "iteration 27 current loss: 0.5138441920280457 current acc: 0.6076832460732984\n",
      "iteration 28 current loss: 0.36249592900276184 current acc: 0.6078744277305428\n",
      "iteration 29 current loss: 0.33566081523895264 current acc: 0.6080915032679739\n",
      "iteration 30 current loss: 0.4126473367214203 current acc: 0.6082429784454605\n",
      "iteration 31 current loss: 0.44839152693748474 current acc: 0.6084073107049608\n",
      "iteration 32 current loss: 0.3911606967449188 current acc: 0.6085844748858448\n",
      "iteration 33 current loss: 0.5644384026527405 current acc: 0.608748370273794\n",
      "iteration 34 current loss: 0.7135487198829651 current acc: 0.6088338762214984\n",
      "iteration 35 current loss: 0.5793856382369995 current acc: 0.6089453125\n",
      "iteration 36 current loss: 0.42552483081817627 current acc: 0.6091086532205595\n",
      "iteration 37 current loss: 0.6008080840110779 current acc: 0.6092067620286086\n",
      "iteration 38 current loss: 0.5270659923553467 current acc: 0.6093697205977908\n",
      "iteration 39 current loss: 0.2904830873012543 current acc: 0.6095844155844156\n",
      "iteration 40 current loss: 0.47326990962028503 current acc: 0.6097209604153148\n",
      "iteration 41 current loss: 0.4308757483959198 current acc: 0.6098832684824903\n",
      "iteration 42 current loss: 0.4576048254966736 current acc: 0.6100583279325988\n",
      "iteration 43 current loss: 0.4051113426685333 current acc: 0.6102461139896374\n",
      "iteration 44 current loss: 0.2947153151035309 current acc: 0.6104595469255664\n",
      "iteration 45 current loss: 0.43706539273262024 current acc: 0.6106338939197931\n",
      "iteration 46 current loss: 0.5226700901985168 current acc: 0.6107563025210084\n",
      "iteration 47 current loss: 0.4873494803905487 current acc: 0.6109043927648579\n",
      "iteration 48 current loss: 0.5811414122581482 current acc: 0.6110393802453196\n",
      "iteration 49 current loss: 0.5623918771743774 current acc: 0.6112\n",
      "iteration 50 current loss: 0.5272594690322876 current acc: 0.6113475177304964\n",
      "iteration 51 current loss: 0.4891345500946045 current acc: 0.6114948453608248\n",
      "iteration 52 current loss: 0.5599212646484375 current acc: 0.6116548615582743\n",
      "iteration 53 current loss: 0.2988649606704712 current acc: 0.6118661518661519\n",
      "iteration 54 current loss: 0.5007883310317993 current acc: 0.6119871382636656\n",
      "iteration 55 current loss: 0.3951742947101593 current acc: 0.6121850899742931\n",
      "iteration 56 current loss: 0.429146945476532 current acc: 0.6123827874116892\n",
      "iteration 57 current loss: 0.3288208246231079 current acc: 0.6125673940949936\n",
      "iteration 58 current loss: 0.3822227418422699 current acc: 0.6127645926876203\n",
      "iteration 59 current loss: 0.48211464285850525 current acc: 0.6129230769230769\n",
      "iteration 60 current loss: 0.6222195625305176 current acc: 0.6130044843049327\n",
      "iteration 61 current loss: 0.34708067774772644 current acc: 0.6131882202304737\n",
      "iteration 62 current loss: 0.4977174997329712 current acc: 0.6133333333333333\n",
      "iteration 63 current loss: 0.41816428303718567 current acc: 0.61346547314578\n",
      "iteration 64 current loss: 0.3796490430831909 current acc: 0.6136102236421725\n",
      "iteration 65 current loss: 0.4141693115234375 current acc: 0.6137803320561941\n",
      "iteration 66 current loss: 0.389244943857193 current acc: 0.6139374601148692\n",
      "iteration 67 current loss: 0.25463828444480896 current acc: 0.6141198979591836\n",
      "iteration 68 current loss: 0.6220285892486572 current acc: 0.614238368387508\n",
      "iteration 69 current loss: 0.31524667143821716 current acc: 0.6144458598726115\n",
      "iteration 70 current loss: 0.5293749570846558 current acc: 0.6146021642266073\n",
      "iteration 71 current loss: 0.5737652778625488 current acc: 0.6146946564885496\n",
      "iteration 72 current loss: 0.4460471272468567 current acc: 0.6148760330578512\n",
      "iteration 73 current loss: 0.6935915350914001 current acc: 0.6149682337992376\n",
      "iteration 74 current loss: 0.5139784812927246 current acc: 0.6151111111111112\n",
      "iteration 75 current loss: 0.5006906986236572 current acc: 0.615241116751269\n",
      "iteration 76 current loss: 0.3395345211029053 current acc: 0.6154470513633481\n",
      "iteration 77 current loss: 0.4204801917076111 current acc: 0.6155893536121673\n",
      "iteration 78 current loss: 0.4010202884674072 current acc: 0.6157568081063964\n",
      "iteration 79 current loss: 0.5800331830978394 current acc: 0.6158860759493671\n",
      "iteration 80 current loss: 0.48912665247917175 current acc: 0.6160151802656546\n",
      "iteration 81 current loss: 0.3988521993160248 current acc: 0.6161820480404551\n",
      "iteration 82 current loss: 0.42030277848243713 current acc: 0.6163487049905243\n",
      "iteration 83 current loss: 0.361566424369812 current acc: 0.6165277777777778\n",
      "iteration 84 current loss: 0.5349900722503662 current acc: 0.6166182965299685\n",
      "iteration 85 current loss: 0.38834086060523987 current acc: 0.6167843631778058\n",
      "iteration 86 current loss: 0.5502580404281616 current acc: 0.6169124133585381\n",
      "iteration 87 current loss: 0.5208911299705505 current acc: 0.6170654911838791\n",
      "iteration 88 current loss: 0.4979574978351593 current acc: 0.6171806167400881\n",
      "iteration 89 current loss: 0.7144678235054016 current acc: 0.6172704402515723\n",
      "iteration 90 current loss: 0.5460108518600464 current acc: 0.6173978629792584\n",
      "iteration 91 current loss: 0.4884260296821594 current acc: 0.6175125628140703\n",
      "iteration 92 current loss: 0.6601389050483704 current acc: 0.6175894538606403\n",
      "iteration 93 current loss: 0.6418464779853821 current acc: 0.6176537013801756\n",
      "iteration 94 current loss: 0.4744831919670105 current acc: 0.6178056426332288\n",
      "iteration 95 current loss: 0.452185720205307 current acc: 0.6179824561403509\n",
      "iteration 96 current loss: 0.5815244913101196 current acc: 0.6180588603631809\n",
      "iteration 97 current loss: 0.7584699392318726 current acc: 0.6180976220275344\n",
      "iteration 98 current loss: 0.3815409541130066 current acc: 0.6182739212007504\n",
      "iteration 99 current loss: 0.47167137265205383 current acc: 0.6184125\n",
      "\t\tEpoch 15/100 complete. Epoch loss 0.48028018355369567 Epoch accuracy 0.8436\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 15, Validation Accuracy: 0.61075, Validation Loss: 1.2081136256456375\n",
      "best loss 0.48028018355369567\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.3327290415763855 current acc: 0.6186008744534666\n",
      "iteration 1 current loss: 0.2079317271709442 current acc: 0.6188139825218477\n",
      "iteration 2 current loss: 0.4167431592941284 current acc: 0.6189644416718653\n",
      "iteration 3 current loss: 0.44083255529403687 current acc: 0.6190773067331671\n",
      "iteration 4 current loss: 0.5173578858375549 current acc: 0.619177570093458\n",
      "iteration 5 current loss: 0.22883723676204681 current acc: 0.6193773349937733\n",
      "iteration 6 current loss: 0.3724829852581024 current acc: 0.6195519601742377\n",
      "iteration 7 current loss: 0.5370800495147705 current acc: 0.6196517412935323\n",
      "iteration 8 current loss: 0.36685943603515625 current acc: 0.6198508390304537\n",
      "iteration 9 current loss: 0.5905807614326477 current acc: 0.6199751552795031\n",
      "iteration 10 current loss: 0.3494347631931305 current acc: 0.6201241464928616\n",
      "iteration 11 current loss: 0.5640949010848999 current acc: 0.6202605459057072\n",
      "iteration 12 current loss: 0.32972776889801025 current acc: 0.6204215747055176\n",
      "iteration 13 current loss: 0.4994405508041382 current acc: 0.620545229244114\n",
      "iteration 14 current loss: 0.4355452060699463 current acc: 0.6207182662538699\n",
      "iteration 15 current loss: 0.31282055377960205 current acc: 0.6208910891089109\n",
      "iteration 16 current loss: 0.3524884879589081 current acc: 0.6210884353741497\n",
      "iteration 17 current loss: 0.4523236155509949 current acc: 0.6212360939431397\n",
      "iteration 18 current loss: 0.47583743929862976 current acc: 0.6213588634959851\n",
      "iteration 19 current loss: 0.3225891888141632 current acc: 0.6215432098765432\n",
      "iteration 20 current loss: 0.32313454151153564 current acc: 0.6217396668723011\n",
      "iteration 21 current loss: 0.3355473279953003 current acc: 0.6219112207151665\n",
      "iteration 22 current loss: 0.3176199197769165 current acc: 0.6221072088724584\n",
      "iteration 23 current loss: 0.6100162863731384 current acc: 0.6222290640394089\n",
      "iteration 24 current loss: 0.3171195387840271 current acc: 0.6224246153846154\n",
      "iteration 25 current loss: 0.31078436970710754 current acc: 0.6225953259532595\n",
      "iteration 26 current loss: 0.451473206281662 current acc: 0.6227658266748617\n",
      "iteration 27 current loss: 0.40197527408599854 current acc: 0.6229238329238329\n",
      "iteration 28 current loss: 0.3810247480869293 current acc: 0.6230693677102517\n",
      "iteration 29 current loss: 0.4231818914413452 current acc: 0.6232147239263803\n",
      "iteration 30 current loss: 0.43663376569747925 current acc: 0.6233476394849785\n",
      "iteration 31 current loss: 0.5790802240371704 current acc: 0.6234558823529411\n",
      "iteration 32 current loss: 0.37135693430900574 current acc: 0.6236252296387018\n",
      "iteration 33 current loss: 0.2714947462081909 current acc: 0.6238188494492044\n",
      "iteration 34 current loss: 0.3875104784965515 current acc: 0.6239633027522936\n",
      "iteration 35 current loss: 0.31910040974617004 current acc: 0.6241687041564792\n",
      "iteration 36 current loss: 0.39907124638557434 current acc: 0.6243494196701282\n",
      "iteration 37 current loss: 0.36462730169296265 current acc: 0.6245054945054945\n",
      "iteration 38 current loss: 0.30972766876220703 current acc: 0.624685784014643\n",
      "iteration 39 current loss: 0.47280627489089966 current acc: 0.6248048780487805\n",
      "iteration 40 current loss: 0.31750771403312683 current acc: 0.6249725776965265\n",
      "iteration 41 current loss: 0.5046677589416504 current acc: 0.6251035322777101\n",
      "iteration 42 current loss: 0.3692196309566498 current acc: 0.6252708460133901\n",
      "iteration 43 current loss: 0.36695781350135803 current acc: 0.6254501216545012\n",
      "iteration 44 current loss: 0.6075608730316162 current acc: 0.6255440729483283\n",
      "iteration 45 current loss: 0.3753972351551056 current acc: 0.6257108140947752\n",
      "iteration 46 current loss: 0.38448548316955566 current acc: 0.6258530661809351\n",
      "iteration 47 current loss: 0.350635826587677 current acc: 0.6260315533980583\n",
      "iteration 48 current loss: 0.43793246150016785 current acc: 0.6261855670103093\n",
      "iteration 49 current loss: 0.4173012673854828 current acc: 0.6263515151515151\n",
      "iteration 50 current loss: 0.42573824524879456 current acc: 0.6264930345245305\n",
      "iteration 51 current loss: 0.31582364439964294 current acc: 0.6266707021791768\n",
      "iteration 52 current loss: 0.643831729888916 current acc: 0.6267392619479734\n",
      "iteration 53 current loss: 0.5325239896774292 current acc: 0.6268681983071343\n",
      "iteration 54 current loss: 0.48207515478134155 current acc: 0.6269728096676738\n",
      "iteration 55 current loss: 0.46335574984550476 current acc: 0.6271256038647343\n",
      "iteration 56 current loss: 0.3513850271701813 current acc: 0.6272661436330719\n",
      "iteration 57 current loss: 0.4109964370727539 current acc: 0.6274185765983112\n",
      "iteration 58 current loss: 0.5695659518241882 current acc: 0.6275587703435804\n",
      "iteration 59 current loss: 0.4282633066177368 current acc: 0.6276867469879518\n",
      "iteration 60 current loss: 0.6099123954772949 current acc: 0.6277784467188441\n",
      "iteration 61 current loss: 0.3870994448661804 current acc: 0.6279061371841155\n",
      "iteration 62 current loss: 0.34109485149383545 current acc: 0.6280577269993987\n",
      "iteration 63 current loss: 0.37332862615585327 current acc: 0.6282091346153846\n",
      "iteration 64 current loss: 0.33315303921699524 current acc: 0.6283843843843844\n",
      "iteration 65 current loss: 0.3767188787460327 current acc: 0.6285234093637455\n",
      "iteration 66 current loss: 0.5565996766090393 current acc: 0.6286142771445711\n",
      "iteration 67 current loss: 0.4274415075778961 current acc: 0.6287410071942446\n",
      "iteration 68 current loss: 0.2885546088218689 current acc: 0.6289035350509287\n",
      "iteration 69 current loss: 0.4993419349193573 current acc: 0.6290419161676647\n",
      "iteration 70 current loss: 0.5414558053016663 current acc: 0.6291442250149611\n",
      "iteration 71 current loss: 0.5021560192108154 current acc: 0.6292703349282297\n",
      "iteration 72 current loss: 0.444459468126297 current acc: 0.6293962940824865\n",
      "iteration 73 current loss: 0.47396889328956604 current acc: 0.6295221027479092\n",
      "iteration 74 current loss: 0.5705862641334534 current acc: 0.6296716417910447\n",
      "iteration 75 current loss: 0.4734770655632019 current acc: 0.6297852028639618\n",
      "iteration 76 current loss: 0.5895979404449463 current acc: 0.6298628503279666\n",
      "iteration 77 current loss: 0.4431189000606537 current acc: 0.6299880810488677\n",
      "iteration 78 current loss: 0.41442397236824036 current acc: 0.6300893388921978\n",
      "iteration 79 current loss: 0.38047146797180176 current acc: 0.6302261904761904\n",
      "iteration 80 current loss: 0.35913902521133423 current acc: 0.6303985722784057\n",
      "iteration 81 current loss: 0.451401948928833 current acc: 0.6305112960760999\n",
      "iteration 82 current loss: 0.4732017517089844 current acc: 0.6306357694592989\n",
      "iteration 83 current loss: 0.5141144394874573 current acc: 0.6307363420427553\n",
      "iteration 84 current loss: 0.43246912956237793 current acc: 0.6308486646884273\n",
      "iteration 85 current loss: 0.35038256645202637 current acc: 0.6310083036773428\n",
      "iteration 86 current loss: 0.48665106296539307 current acc: 0.6311203319502074\n",
      "iteration 87 current loss: 0.45624151825904846 current acc: 0.6312440758293839\n",
      "iteration 88 current loss: 0.4259110987186432 current acc: 0.6313795145056247\n",
      "iteration 89 current loss: 0.3182713985443115 current acc: 0.6315384615384615\n",
      "iteration 90 current loss: 0.47025540471076965 current acc: 0.6316499112950916\n",
      "iteration 91 current loss: 0.42530497908592224 current acc: 0.63177304964539\n",
      "iteration 92 current loss: 0.45857322216033936 current acc: 0.6319432959243946\n",
      "iteration 93 current loss: 0.3759324550628662 current acc: 0.6320779220779221\n",
      "iteration 94 current loss: 0.5001247525215149 current acc: 0.6322005899705014\n",
      "iteration 95 current loss: 0.3517860472202301 current acc: 0.6323466981132075\n",
      "iteration 96 current loss: 0.3379229009151459 current acc: 0.6325279905715969\n",
      "iteration 97 current loss: 0.5036051869392395 current acc: 0.6326383981154299\n",
      "iteration 98 current loss: 0.5566970109939575 current acc: 0.6327251324308417\n",
      "iteration 99 current loss: 0.36236274242401123 current acc: 0.6328705882352941\n",
      "\t\tEpoch 16/100 complete. Epoch loss 0.4228155817091465 Epoch accuracy 0.8642\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 16, Validation Accuracy: 0.62075, Validation Loss: 1.1285452794283628\n",
      "best loss 0.4228155817091465\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.31772327423095703 current acc: 0.6330276308054086\n",
      "iteration 1 current loss: 0.3433449864387512 current acc: 0.6331962397179789\n",
      "iteration 2 current loss: 0.37943488359451294 current acc: 0.6333881385789782\n",
      "iteration 3 current loss: 0.381895512342453 current acc: 0.6335446009389671\n",
      "iteration 4 current loss: 0.5316999554634094 current acc: 0.6336656891495601\n",
      "iteration 5 current loss: 0.3756455183029175 current acc: 0.6338335287221571\n",
      "iteration 6 current loss: 0.30722418427467346 current acc: 0.6340011716461629\n",
      "iteration 7 current loss: 0.24380384385585785 current acc: 0.6341803278688525\n",
      "iteration 8 current loss: 0.27856165170669556 current acc: 0.6343709771796372\n",
      "iteration 9 current loss: 0.4129315912723541 current acc: 0.6345029239766082\n",
      "iteration 10 current loss: 0.30659207701683044 current acc: 0.6346697837521917\n",
      "iteration 11 current loss: 0.21557705104351044 current acc: 0.6348598130841121\n",
      "iteration 12 current loss: 0.4080128073692322 current acc: 0.6350029188558085\n",
      "iteration 13 current loss: 0.3127332627773285 current acc: 0.6351691948658109\n",
      "iteration 14 current loss: 0.18961447477340698 current acc: 0.6353586005830903\n",
      "iteration 15 current loss: 0.2618662714958191 current acc: 0.6355477855477856\n",
      "iteration 16 current loss: 0.3233664333820343 current acc: 0.635713453698311\n",
      "iteration 17 current loss: 0.2801785171031952 current acc: 0.6359022118742724\n",
      "iteration 18 current loss: 0.43999457359313965 current acc: 0.6360209424083769\n",
      "iteration 19 current loss: 0.2771165072917938 current acc: 0.6361976744186046\n",
      "iteration 20 current loss: 0.34128445386886597 current acc: 0.6363509587449158\n",
      "iteration 21 current loss: 0.35935309529304504 current acc: 0.6365156794425088\n",
      "iteration 22 current loss: 0.4150539040565491 current acc: 0.6366569936157864\n",
      "iteration 23 current loss: 0.283864289522171 current acc: 0.6368329466357309\n",
      "iteration 24 current loss: 0.3987652659416199 current acc: 0.6369739130434783\n",
      "iteration 25 current loss: 0.33208099007606506 current acc: 0.6371378910776362\n",
      "iteration 26 current loss: 0.41009125113487244 current acc: 0.6373016792125072\n",
      "iteration 27 current loss: 0.30804601311683655 current acc: 0.6374768518518519\n",
      "iteration 28 current loss: 0.4040209949016571 current acc: 0.6375824175824176\n",
      "iteration 29 current loss: 0.29942193627357483 current acc: 0.6377456647398844\n",
      "iteration 30 current loss: 0.30458366870880127 current acc: 0.6379202772963605\n",
      "iteration 31 current loss: 0.318482369184494 current acc: 0.6380831408775981\n",
      "iteration 32 current loss: 0.3058427572250366 current acc: 0.6382342758222735\n",
      "iteration 33 current loss: 0.4328376054763794 current acc: 0.6383391003460208\n",
      "iteration 34 current loss: 0.3526226282119751 current acc: 0.638478386167147\n",
      "iteration 35 current loss: 0.3231428563594818 current acc: 0.6386405529953917\n",
      "iteration 36 current loss: 0.4038814604282379 current acc: 0.6387910189982728\n",
      "iteration 37 current loss: 0.41866543889045715 current acc: 0.6389182968929804\n",
      "iteration 38 current loss: 0.2941894233226776 current acc: 0.6390914318573893\n",
      "iteration 39 current loss: 0.4598161280155182 current acc: 0.639183908045977\n",
      "iteration 40 current loss: 0.4166717231273651 current acc: 0.6393107409534751\n",
      "iteration 41 current loss: 0.29772087931632996 current acc: 0.6395063145809414\n",
      "iteration 42 current loss: 0.24905163049697876 current acc: 0.6397016637980494\n",
      "iteration 43 current loss: 0.28737616539001465 current acc: 0.6398509174311927\n",
      "iteration 44 current loss: 0.3205336332321167 current acc: 0.6400343839541547\n",
      "iteration 45 current loss: 0.22238300740718842 current acc: 0.6402176403207331\n",
      "iteration 46 current loss: 0.3023572862148285 current acc: 0.6403434459072697\n",
      "iteration 47 current loss: 0.22929997742176056 current acc: 0.640537757437071\n",
      "iteration 48 current loss: 0.25116831064224243 current acc: 0.6407432818753573\n",
      "iteration 49 current loss: 0.46097347140312195 current acc: 0.6408457142857142\n",
      "iteration 50 current loss: 0.2852659821510315 current acc: 0.6410165619645917\n",
      "iteration 51 current loss: 0.5183283090591431 current acc: 0.6411187214611872\n",
      "iteration 52 current loss: 0.3334091305732727 current acc: 0.6412892184826012\n",
      "iteration 53 current loss: 0.2526911199092865 current acc: 0.6414709236031927\n",
      "iteration 54 current loss: 0.2428244650363922 current acc: 0.6416638176638176\n",
      "iteration 55 current loss: 0.19550812244415283 current acc: 0.6418451025056947\n",
      "iteration 56 current loss: 0.2812471389770508 current acc: 0.6419806488332385\n",
      "iteration 57 current loss: 0.3400954306125641 current acc: 0.6421387940841866\n",
      "iteration 58 current loss: 0.351772278547287 current acc: 0.6422853894258101\n",
      "iteration 59 current loss: 0.4869609773159027 current acc: 0.6423977272727273\n",
      "iteration 60 current loss: 0.3307846188545227 current acc: 0.6425553662691652\n",
      "iteration 61 current loss: 0.38569822907447815 current acc: 0.6426901248581158\n",
      "iteration 62 current loss: 0.2725081145763397 current acc: 0.6428587634713556\n",
      "iteration 63 current loss: 0.4056350290775299 current acc: 0.6429931972789116\n",
      "iteration 64 current loss: 0.3785620629787445 current acc: 0.6431161473087819\n",
      "iteration 65 current loss: 0.5077164769172668 current acc: 0.6432049830124575\n",
      "iteration 66 current loss: 0.4128172993659973 current acc: 0.6433276740237691\n",
      "iteration 67 current loss: 0.42649850249290466 current acc: 0.6434502262443439\n",
      "iteration 68 current loss: 0.24213260412216187 current acc: 0.6436404748445449\n",
      "iteration 69 current loss: 0.3886447846889496 current acc: 0.643774011299435\n",
      "iteration 70 current loss: 0.2935926616191864 current acc: 0.6439299830604178\n",
      "iteration 71 current loss: 0.3010622262954712 current acc: 0.6440970654627539\n",
      "iteration 72 current loss: 0.2940603494644165 current acc: 0.6442413987591653\n",
      "iteration 73 current loss: 0.2815853953361511 current acc: 0.6443855693348365\n",
      "iteration 74 current loss: 0.43868115544319153 current acc: 0.6445295774647888\n",
      "iteration 75 current loss: 0.5578529238700867 current acc: 0.6446171171171171\n",
      "iteration 76 current loss: 0.34705662727355957 current acc: 0.6447495779403489\n",
      "iteration 77 current loss: 0.3244071304798126 current acc: 0.6448818897637796\n",
      "iteration 78 current loss: 0.4742721617221832 current acc: 0.6449803260258572\n",
      "iteration 79 current loss: 0.46131622791290283 current acc: 0.6450786516853932\n",
      "iteration 80 current loss: 0.3641926646232605 current acc: 0.6451768669286917\n",
      "iteration 81 current loss: 0.27507513761520386 current acc: 0.6453423120089786\n",
      "iteration 82 current loss: 0.521068811416626 current acc: 0.6454514862591139\n",
      "iteration 83 current loss: 0.6138852834701538 current acc: 0.6455269058295964\n",
      "iteration 84 current loss: 0.3556613624095917 current acc: 0.6456694677871149\n",
      "iteration 85 current loss: 0.41490134596824646 current acc: 0.6457894736842106\n",
      "iteration 86 current loss: 0.3422617018222809 current acc: 0.6459317291550084\n",
      "iteration 87 current loss: 0.43556272983551025 current acc: 0.6460402684563759\n",
      "iteration 88 current loss: 0.443231463432312 current acc: 0.6461486864169927\n",
      "iteration 89 current loss: 0.4814368784427643 current acc: 0.6462793296089385\n",
      "iteration 90 current loss: 0.2563118636608124 current acc: 0.6464209938581797\n",
      "iteration 91 current loss: 0.49256324768066406 current acc: 0.6465625\n",
      "iteration 92 current loss: 0.4405914545059204 current acc: 0.6466592303402119\n",
      "iteration 93 current loss: 0.3882659077644348 current acc: 0.6467781493868451\n",
      "iteration 94 current loss: 0.3300619423389435 current acc: 0.6469303621169916\n",
      "iteration 95 current loss: 0.43979737162590027 current acc: 0.6470824053452116\n",
      "iteration 96 current loss: 0.5021414756774902 current acc: 0.6471897607122983\n",
      "iteration 97 current loss: 0.3222135603427887 current acc: 0.6473081201334816\n",
      "iteration 98 current loss: 0.43264174461364746 current acc: 0.6474041133963313\n",
      "iteration 99 current loss: 0.3467806875705719 current acc: 0.6475222222222222\n",
      "\t\tEpoch 17/100 complete. Epoch loss 0.358025341629982 Epoch accuracy 0.8966\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 17, Validation Accuracy: 0.615125, Validation Loss: 1.2204148568212987\n",
      "best loss 0.358025341629982\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.2970689535140991 current acc: 0.6476624097723487\n",
      "iteration 1 current loss: 0.34148353338241577 current acc: 0.6477913429522753\n",
      "iteration 2 current loss: 0.34891277551651 current acc: 0.6479201331114809\n",
      "iteration 3 current loss: 0.26693102717399597 current acc: 0.648059866962306\n",
      "iteration 4 current loss: 0.258163720369339 current acc: 0.6482105263157895\n",
      "iteration 5 current loss: 0.3174543082714081 current acc: 0.6483499446290144\n",
      "iteration 6 current loss: 0.2616546154022217 current acc: 0.6485002767017155\n",
      "iteration 7 current loss: 0.37358206510543823 current acc: 0.6486393805309735\n",
      "iteration 8 current loss: 0.34440574049949646 current acc: 0.6487783305693754\n",
      "iteration 9 current loss: 0.16289643943309784 current acc: 0.6489613259668509\n",
      "iteration 10 current loss: 0.3229207992553711 current acc: 0.6491109884041966\n",
      "iteration 11 current loss: 0.2491457760334015 current acc: 0.6492604856512141\n",
      "iteration 12 current loss: 0.3816957175731659 current acc: 0.6493767236624379\n",
      "iteration 13 current loss: 0.24811214208602905 current acc: 0.6495589856670342\n",
      "iteration 14 current loss: 0.22823604941368103 current acc: 0.6497300275482094\n",
      "iteration 15 current loss: 0.3097303509712219 current acc: 0.6498788546255506\n",
      "iteration 16 current loss: 0.21109265089035034 current acc: 0.6500605393505778\n",
      "iteration 17 current loss: 0.24680085480213165 current acc: 0.6501980198019802\n",
      "iteration 18 current loss: 0.24393053352832794 current acc: 0.6503463441451347\n",
      "iteration 19 current loss: 0.33687472343444824 current acc: 0.6504835164835164\n",
      "iteration 20 current loss: 0.2932243049144745 current acc: 0.6506425041186161\n",
      "iteration 21 current loss: 0.4781224727630615 current acc: 0.6507683863885839\n",
      "iteration 22 current loss: 0.43003836274147034 current acc: 0.6508721886999451\n",
      "iteration 23 current loss: 0.31498804688453674 current acc: 0.6510087719298245\n",
      "iteration 24 current loss: 0.3142275810241699 current acc: 0.6511561643835616\n",
      "iteration 25 current loss: 0.41057610511779785 current acc: 0.6512376779846659\n",
      "iteration 26 current loss: 0.22603252530097961 current acc: 0.6514176245210728\n",
      "iteration 27 current loss: 0.1850307285785675 current acc: 0.6515864332603939\n",
      "iteration 28 current loss: 0.2544911503791809 current acc: 0.6517441224712958\n",
      "iteration 29 current loss: 0.18420332670211792 current acc: 0.6519234972677596\n",
      "iteration 30 current loss: 0.26501163840293884 current acc: 0.6520917531403605\n",
      "iteration 31 current loss: 0.2164686769247055 current acc: 0.6522598253275109\n",
      "iteration 32 current loss: 0.28632426261901855 current acc: 0.652416803055101\n",
      "iteration 33 current loss: 0.3247225880622864 current acc: 0.6525517993456925\n",
      "iteration 34 current loss: 0.31645697355270386 current acc: 0.6526975476839237\n",
      "iteration 35 current loss: 0.27849748730659485 current acc: 0.6528540305010894\n",
      "iteration 36 current loss: 0.32751548290252686 current acc: 0.6529994556341862\n",
      "iteration 37 current loss: 0.4202192723751068 current acc: 0.65310119695321\n",
      "iteration 38 current loss: 0.6152442693710327 current acc: 0.6532028276237085\n",
      "iteration 39 current loss: 0.28736451268196106 current acc: 0.6533260869565217\n",
      "iteration 40 current loss: 0.2707579433917999 current acc: 0.6534926670287887\n",
      "iteration 41 current loss: 0.28879907727241516 current acc: 0.6536482084690554\n",
      "iteration 42 current loss: 0.23959164321422577 current acc: 0.6538035811177428\n",
      "iteration 43 current loss: 0.22470508515834808 current acc: 0.6539587852494577\n",
      "iteration 44 current loss: 0.38289040327072144 current acc: 0.6540813008130081\n",
      "iteration 45 current loss: 0.3301415741443634 current acc: 0.6542145178764897\n",
      "iteration 46 current loss: 0.23732520639896393 current acc: 0.6543584190579318\n",
      "iteration 47 current loss: 0.28807443380355835 current acc: 0.6545021645021645\n",
      "iteration 48 current loss: 0.3016766607761383 current acc: 0.6546457544618712\n",
      "iteration 49 current loss: 0.29794442653656006 current acc: 0.6547891891891892\n",
      "iteration 50 current loss: 0.19110700488090515 current acc: 0.6549648838465694\n",
      "iteration 51 current loss: 0.34462279081344604 current acc: 0.6550863930885529\n",
      "iteration 52 current loss: 0.36637550592422485 current acc: 0.6551969778737183\n",
      "iteration 53 current loss: 0.282931387424469 current acc: 0.6553398058252428\n",
      "iteration 54 current loss: 0.32154834270477295 current acc: 0.6554609164420485\n",
      "iteration 55 current loss: 0.31322920322418213 current acc: 0.6555926724137932\n",
      "iteration 56 current loss: 0.4074900448322296 current acc: 0.6557135164243403\n",
      "iteration 57 current loss: 0.2988586723804474 current acc: 0.6558665231431647\n",
      "iteration 58 current loss: 0.4040193259716034 current acc: 0.6559763313609468\n",
      "iteration 59 current loss: 0.34195590019226074 current acc: 0.6560967741935484\n",
      "iteration 60 current loss: 0.26503100991249084 current acc: 0.6562385814078452\n",
      "iteration 61 current loss: 0.3370121121406555 current acc: 0.656358754027927\n",
      "iteration 62 current loss: 0.2712695598602295 current acc: 0.6564895330112721\n",
      "iteration 63 current loss: 0.26665133237838745 current acc: 0.6566416309012876\n",
      "iteration 64 current loss: 0.3623531758785248 current acc: 0.6567506702412869\n",
      "iteration 65 current loss: 0.25731003284454346 current acc: 0.6569024651661307\n",
      "iteration 66 current loss: 0.26294803619384766 current acc: 0.6570433851098019\n",
      "iteration 67 current loss: 0.2273600995540619 current acc: 0.6572162740899358\n",
      "iteration 68 current loss: 0.2972715198993683 current acc: 0.6573675762439808\n",
      "iteration 69 current loss: 0.2662104666233063 current acc: 0.6575187165775401\n",
      "iteration 70 current loss: 0.3274940550327301 current acc: 0.6576696953500801\n",
      "iteration 71 current loss: 0.15639729797840118 current acc: 0.6578418803418803\n",
      "iteration 72 current loss: 0.28242257237434387 current acc: 0.6579925253603844\n",
      "iteration 73 current loss: 0.21378055214881897 current acc: 0.6581536819637139\n",
      "iteration 74 current loss: 0.3115260899066925 current acc: 0.6582933333333333\n",
      "iteration 75 current loss: 0.2636018693447113 current acc: 0.6584434968017058\n",
      "iteration 76 current loss: 0.28130903840065 current acc: 0.6585828449653702\n",
      "iteration 77 current loss: 0.5100610256195068 current acc: 0.6587007454739084\n",
      "iteration 78 current loss: 0.3781222105026245 current acc: 0.658839808408728\n",
      "iteration 79 current loss: 0.34313517808914185 current acc: 0.6589787234042553\n",
      "iteration 80 current loss: 0.1860940158367157 current acc: 0.6591493886230728\n",
      "iteration 81 current loss: 0.3231300413608551 current acc: 0.659298618490967\n",
      "iteration 82 current loss: 0.22380605340003967 current acc: 0.6594476898566118\n",
      "iteration 83 current loss: 0.22343873977661133 current acc: 0.6596072186836518\n",
      "iteration 84 current loss: 0.26617276668548584 current acc: 0.6597453580901856\n",
      "iteration 85 current loss: 0.40347060561180115 current acc: 0.6598409331919406\n",
      "iteration 86 current loss: 0.40694373846054077 current acc: 0.6599364069952305\n",
      "iteration 87 current loss: 0.29798823595046997 current acc: 0.6600635593220339\n",
      "iteration 88 current loss: 0.4013371765613556 current acc: 0.6601694017998941\n",
      "iteration 89 current loss: 0.3017863631248474 current acc: 0.6603174603174603\n",
      "iteration 90 current loss: 0.29647374153137207 current acc: 0.6604547858276044\n",
      "iteration 91 current loss: 0.25145193934440613 current acc: 0.6606131078224101\n",
      "iteration 92 current loss: 0.23145851492881775 current acc: 0.6607606973058637\n",
      "iteration 93 current loss: 0.40140247344970703 current acc: 0.6608658922914467\n",
      "iteration 94 current loss: 0.2511175870895386 current acc: 0.6610131926121372\n",
      "iteration 95 current loss: 0.47125136852264404 current acc: 0.6610970464135021\n",
      "iteration 96 current loss: 0.5068002939224243 current acc: 0.6611913547706906\n",
      "iteration 97 current loss: 0.24915768206119537 current acc: 0.6613382507903056\n",
      "iteration 98 current loss: 0.24180303514003754 current acc: 0.6614955239599789\n",
      "iteration 99 current loss: 0.31017687916755676 current acc: 0.6616315789473685\n",
      "\t\tEpoch 18/100 complete. Epoch loss 0.30440425664186477 Epoch accuracy 0.9156\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 18, Validation Accuracy: 0.621375, Validation Loss: 1.190392603352666\n",
      "best loss 0.30440425664186477\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.311943382024765 current acc: 0.6617569700157812\n",
      "iteration 1 current loss: 0.20415672659873962 current acc: 0.6619137749737118\n",
      "iteration 2 current loss: 0.20781461894512177 current acc: 0.6620809248554913\n",
      "iteration 3 current loss: 0.2190849930047989 current acc: 0.6622163865546219\n",
      "iteration 4 current loss: 0.2188611924648285 current acc: 0.6623727034120734\n",
      "iteration 5 current loss: 0.23261995613574982 current acc: 0.6625183630640084\n",
      "iteration 6 current loss: 0.20753498375415802 current acc: 0.6626848453067645\n",
      "iteration 7 current loss: 0.1541391909122467 current acc: 0.6628616352201258\n",
      "iteration 8 current loss: 0.17156514525413513 current acc: 0.6630172865374542\n",
      "iteration 9 current loss: 0.3003554046154022 current acc: 0.663151832460733\n",
      "iteration 10 current loss: 0.2516796588897705 current acc: 0.6632862375719518\n",
      "iteration 11 current loss: 0.2378077507019043 current acc: 0.6634205020920502\n",
      "iteration 12 current loss: 0.2928406894207001 current acc: 0.6635546262415055\n",
      "iteration 13 current loss: 0.11643274128437042 current acc: 0.663730407523511\n",
      "iteration 14 current loss: 0.3083348274230957 current acc: 0.663864229765013\n",
      "iteration 15 current loss: 0.34849393367767334 current acc: 0.6639770354906054\n",
      "iteration 16 current loss: 0.2594377398490906 current acc: 0.6641418883672405\n",
      "iteration 17 current loss: 0.26012077927589417 current acc: 0.6642648592283629\n",
      "iteration 18 current loss: 0.21213266253471375 current acc: 0.6644398124022929\n",
      "iteration 19 current loss: 0.1722489297389984 current acc: 0.6646041666666667\n",
      "iteration 20 current loss: 0.2115860879421234 current acc: 0.6647579385736595\n",
      "iteration 21 current loss: 0.1798362135887146 current acc: 0.6649323621227887\n",
      "iteration 22 current loss: 0.36524006724357605 current acc: 0.6650442017680708\n",
      "iteration 23 current loss: 0.22137580811977386 current acc: 0.6651871101871102\n",
      "iteration 24 current loss: 0.25214219093322754 current acc: 0.6653194805194805\n",
      "iteration 25 current loss: 0.27921876311302185 current acc: 0.6654517133956386\n",
      "iteration 26 current loss: 0.32103225588798523 current acc: 0.6655838090295797\n",
      "iteration 27 current loss: 0.3012440502643585 current acc: 0.6657365145228216\n",
      "iteration 28 current loss: 0.21623270213603973 current acc: 0.6658786936236392\n",
      "iteration 29 current loss: 0.21400132775306702 current acc: 0.6660310880829016\n",
      "iteration 30 current loss: 0.27714622020721436 current acc: 0.6661729673744174\n",
      "iteration 31 current loss: 0.22586509585380554 current acc: 0.6662939958592132\n",
      "iteration 32 current loss: 0.21817821264266968 current acc: 0.6664459389549923\n",
      "iteration 33 current loss: 0.2116575390100479 current acc: 0.6665770423991727\n",
      "iteration 34 current loss: 0.18966414034366608 current acc: 0.666749354005168\n",
      "iteration 35 current loss: 0.30023014545440674 current acc: 0.6668801652892562\n",
      "iteration 36 current loss: 0.30968376994132996 current acc: 0.6670005162622612\n",
      "iteration 37 current loss: 0.2480064332485199 current acc: 0.6671104231166151\n",
      "iteration 38 current loss: 0.3474147319793701 current acc: 0.6672408457968024\n",
      "iteration 39 current loss: 0.17625673115253448 current acc: 0.6674020618556701\n",
      "iteration 40 current loss: 0.3062255084514618 current acc: 0.6675321998969603\n",
      "iteration 41 current loss: 0.11789266765117645 current acc: 0.6677033985581874\n",
      "iteration 42 current loss: 0.2564292550086975 current acc: 0.6678435409161091\n",
      "iteration 43 current loss: 0.449329137802124 current acc: 0.6679115226337449\n",
      "iteration 44 current loss: 0.23205479979515076 current acc: 0.6680719794344473\n",
      "iteration 45 current loss: 0.16363391280174255 current acc: 0.6682322713257965\n",
      "iteration 46 current loss: 0.16898541152477264 current acc: 0.668382126348228\n",
      "iteration 47 current loss: 0.18433693051338196 current acc: 0.6685215605749487\n",
      "iteration 48 current loss: 0.20995326340198517 current acc: 0.6686711133914828\n",
      "iteration 49 current loss: 0.3272797465324402 current acc: 0.6687897435897436\n",
      "iteration 50 current loss: 0.2742363512516022 current acc: 0.6689185033316248\n",
      "iteration 51 current loss: 0.2562422752380371 current acc: 0.669047131147541\n",
      "iteration 52 current loss: 0.21561729907989502 current acc: 0.6691858678955453\n",
      "iteration 53 current loss: 0.24801544845104218 current acc: 0.6693142272262027\n",
      "iteration 54 current loss: 0.2103816568851471 current acc: 0.669462915601023\n",
      "iteration 55 current loss: 0.2547850012779236 current acc: 0.669601226993865\n",
      "iteration 56 current loss: 0.17221803963184357 current acc: 0.6697700562084824\n",
      "iteration 57 current loss: 0.2630213499069214 current acc: 0.6699284984678243\n",
      "iteration 58 current loss: 0.28726962208747864 current acc: 0.6700561510974987\n",
      "iteration 59 current loss: 0.1894914209842682 current acc: 0.6702040816326531\n",
      "iteration 60 current loss: 0.2169611006975174 current acc: 0.670362060173381\n",
      "iteration 61 current loss: 0.27176493406295776 current acc: 0.6704892966360856\n",
      "iteration 62 current loss: 0.18659719824790955 current acc: 0.6706265919510953\n",
      "iteration 63 current loss: 0.17556896805763245 current acc: 0.6707535641547862\n",
      "iteration 64 current loss: 0.16577380895614624 current acc: 0.6709211195928754\n",
      "iteration 65 current loss: 0.2671261429786682 current acc: 0.6710681586978637\n",
      "iteration 66 current loss: 0.17658314108848572 current acc: 0.6712150482968988\n",
      "iteration 67 current loss: 0.22285166382789612 current acc: 0.6713516260162602\n",
      "iteration 68 current loss: 0.1656862050294876 current acc: 0.6714982224479431\n",
      "iteration 69 current loss: 0.16778706014156342 current acc: 0.6716548223350254\n",
      "iteration 70 current loss: 0.31512749195098877 current acc: 0.6717706747843735\n",
      "iteration 71 current loss: 0.28024011850357056 current acc: 0.671896551724138\n",
      "iteration 72 current loss: 0.14131483435630798 current acc: 0.6720527116066903\n",
      "iteration 73 current loss: 0.19564691185951233 current acc: 0.6722188449848024\n",
      "iteration 74 current loss: 0.14715561270713806 current acc: 0.6723746835443039\n",
      "iteration 75 current loss: 0.2659870982170105 current acc: 0.6725101214574899\n",
      "iteration 76 current loss: 0.18206165730953217 current acc: 0.6726555386949924\n",
      "iteration 77 current loss: 0.2028294801712036 current acc: 0.6727805864509606\n",
      "iteration 78 current loss: 0.1793539971113205 current acc: 0.6729358261748358\n",
      "iteration 79 current loss: 0.2165893018245697 current acc: 0.673060606060606\n",
      "iteration 80 current loss: 0.2924419343471527 current acc: 0.6732054517920242\n",
      "iteration 81 current loss: 0.3347622752189636 current acc: 0.6733097880928355\n",
      "iteration 82 current loss: 0.14362062513828278 current acc: 0.6734644478063541\n",
      "iteration 83 current loss: 0.35418131947517395 current acc: 0.6735887096774194\n",
      "iteration 84 current loss: 0.2386332005262375 current acc: 0.6737229219143577\n",
      "iteration 85 current loss: 0.2568725347518921 current acc: 0.6738368580060423\n",
      "iteration 86 current loss: 0.31450948119163513 current acc: 0.6739708102667338\n",
      "iteration 87 current loss: 0.22959941625595093 current acc: 0.6741046277665996\n",
      "iteration 88 current loss: 0.2082444131374359 current acc: 0.6742483660130719\n",
      "iteration 89 current loss: 0.16453762352466583 current acc: 0.6743819095477387\n",
      "iteration 90 current loss: 0.14431853592395782 current acc: 0.6745253641386239\n",
      "iteration 91 current loss: 0.19174303114414215 current acc: 0.6746686746987952\n",
      "iteration 92 current loss: 0.2665718197822571 current acc: 0.6748018063221275\n",
      "iteration 93 current loss: 0.24177995324134827 current acc: 0.6749247743229689\n",
      "iteration 94 current loss: 0.16860775649547577 current acc: 0.6750776942355889\n",
      "iteration 95 current loss: 0.2832014560699463 current acc: 0.6751803607214429\n",
      "iteration 96 current loss: 0.31392911076545715 current acc: 0.6753029544316475\n",
      "iteration 97 current loss: 0.236541286110878 current acc: 0.6754354354354354\n",
      "iteration 98 current loss: 0.2270185351371765 current acc: 0.6755677838919459\n",
      "iteration 99 current loss: 0.17913608253002167 current acc: 0.67572\n",
      "\t\tEpoch 19/100 complete. Epoch loss 0.2347427400946617 Epoch accuracy 0.9434\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 19, Validation Accuracy: 0.61975, Validation Loss: 1.2472803507000207\n",
      "best loss 0.2347427400946617\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.09115633368492126 current acc: 0.6758820589705148\n",
      "iteration 1 current loss: 0.21426044404506683 current acc: 0.676013986013986\n",
      "iteration 2 current loss: 0.2397085577249527 current acc: 0.6761357963055417\n",
      "iteration 3 current loss: 0.16919708251953125 current acc: 0.6762774451097804\n",
      "iteration 4 current loss: 0.19152851402759552 current acc: 0.6764089775561097\n",
      "iteration 5 current loss: 0.14088734984397888 current acc: 0.6765702891326022\n",
      "iteration 6 current loss: 0.14979086816310883 current acc: 0.676711509715994\n",
      "iteration 7 current loss: 0.2678099572658539 current acc: 0.6768227091633466\n",
      "iteration 8 current loss: 0.10667531937360764 current acc: 0.676973618715779\n",
      "iteration 9 current loss: 0.15438494086265564 current acc: 0.6771144278606965\n",
      "iteration 10 current loss: 0.345066636800766 current acc: 0.6772153157633019\n",
      "iteration 11 current loss: 0.11579162627458572 current acc: 0.677365805168986\n",
      "iteration 12 current loss: 0.1291157752275467 current acc: 0.6775161450571286\n",
      "iteration 13 current loss: 0.15758661925792694 current acc: 0.6776663356504469\n",
      "iteration 14 current loss: 0.12855438888072968 current acc: 0.6778163771712159\n",
      "iteration 15 current loss: 0.16276678442955017 current acc: 0.6779464285714286\n",
      "iteration 16 current loss: 0.14977070689201355 current acc: 0.6780961824491819\n",
      "iteration 17 current loss: 0.20689290761947632 current acc: 0.6782259663032706\n",
      "iteration 18 current loss: 0.20478937029838562 current acc: 0.678355621594849\n",
      "iteration 19 current loss: 0.1614828109741211 current acc: 0.6785049504950496\n",
      "iteration 20 current loss: 0.13616085052490234 current acc: 0.6786541316180109\n",
      "iteration 21 current loss: 0.1655307412147522 current acc: 0.6788031651829871\n",
      "iteration 22 current loss: 0.25251305103302 current acc: 0.6789223924864063\n",
      "iteration 23 current loss: 0.15932458639144897 current acc: 0.6790612648221344\n",
      "iteration 24 current loss: 0.2549675405025482 current acc: 0.6791802469135803\n",
      "iteration 25 current loss: 0.14765340089797974 current acc: 0.6793287265547877\n",
      "iteration 26 current loss: 0.25378456711769104 current acc: 0.6794573260976813\n",
      "iteration 27 current loss: 0.1640128493309021 current acc: 0.6796055226824458\n",
      "iteration 28 current loss: 0.21379691362380981 current acc: 0.679733859043864\n",
      "iteration 29 current loss: 0.19829271733760834 current acc: 0.679871921182266\n",
      "iteration 30 current loss: 0.12594248354434967 current acc: 0.6800196947316592\n",
      "iteration 31 current loss: 0.09679259359836578 current acc: 0.6801673228346456\n",
      "iteration 32 current loss: 0.22605925798416138 current acc: 0.6802951303492376\n",
      "iteration 33 current loss: 0.2235233187675476 current acc: 0.6804228121927237\n",
      "iteration 34 current loss: 0.14408931136131287 current acc: 0.6805601965601965\n",
      "iteration 35 current loss: 0.1952151507139206 current acc: 0.680697445972495\n",
      "iteration 36 current loss: 0.14365562796592712 current acc: 0.6808443789887089\n",
      "iteration 37 current loss: 0.2162134051322937 current acc: 0.6809715407262021\n",
      "iteration 38 current loss: 0.12398529052734375 current acc: 0.6811280039234919\n",
      "iteration 39 current loss: 0.1982312947511673 current acc: 0.6812745098039216\n",
      "iteration 40 current loss: 0.1725790798664093 current acc: 0.681420872121509\n",
      "iteration 41 current loss: 0.3161947727203369 current acc: 0.681537708129285\n",
      "iteration 42 current loss: 0.21678729355335236 current acc: 0.6816642192853647\n",
      "iteration 43 current loss: 0.16971944272518158 current acc: 0.6818101761252446\n",
      "iteration 44 current loss: 0.21264846622943878 current acc: 0.6819364303178485\n",
      "iteration 45 current loss: 0.1554907262325287 current acc: 0.6820625610948191\n",
      "iteration 46 current loss: 0.18940423429012299 current acc: 0.6821983390327309\n",
      "iteration 47 current loss: 0.16989494860172272 current acc: 0.68234375\n",
      "iteration 48 current loss: 0.1739375740289688 current acc: 0.6824792581747193\n",
      "iteration 49 current loss: 0.11210403591394424 current acc: 0.6826341463414635\n",
      "iteration 50 current loss: 0.15753789246082306 current acc: 0.682779132130668\n",
      "iteration 51 current loss: 0.11082489788532257 current acc: 0.6829337231968811\n",
      "iteration 52 current loss: 0.1312883049249649 current acc: 0.6830784218217243\n",
      "iteration 53 current loss: 0.2197888046503067 current acc: 0.6831937682570594\n",
      "iteration 54 current loss: 0.16362787783145905 current acc: 0.683338199513382\n",
      "iteration 55 current loss: 0.13396216928958893 current acc: 0.6834824902723735\n",
      "iteration 56 current loss: 0.17084075510501862 current acc: 0.6836169178415168\n",
      "iteration 57 current loss: 0.194437637925148 current acc: 0.6837414965986395\n",
      "iteration 58 current loss: 0.2598721385002136 current acc: 0.6838465274405051\n",
      "iteration 59 current loss: 0.20717811584472656 current acc: 0.6839611650485437\n",
      "iteration 60 current loss: 0.13313432037830353 current acc: 0.6841145075206211\n",
      "iteration 61 current loss: 0.14856013655662537 current acc: 0.6842677012609117\n",
      "iteration 62 current loss: 0.157258078455925 current acc: 0.6844110518662142\n",
      "iteration 63 current loss: 0.323761910200119 current acc: 0.684515503875969\n",
      "iteration 64 current loss: 0.1259484440088272 current acc: 0.6846682808716708\n",
      "iteration 65 current loss: 0.0824936106801033 current acc: 0.6848209099709583\n",
      "iteration 66 current loss: 0.1488095074892044 current acc: 0.6849637155297532\n",
      "iteration 67 current loss: 0.10778898000717163 current acc: 0.6851160541586073\n",
      "iteration 68 current loss: 0.19972728192806244 current acc: 0.6852392460125665\n",
      "iteration 69 current loss: 0.22025194764137268 current acc: 0.6853623188405797\n",
      "iteration 70 current loss: 0.1530158370733261 current acc: 0.6854852728150652\n",
      "iteration 71 current loss: 0.22537067532539368 current acc: 0.6855984555984556\n",
      "iteration 72 current loss: 0.2626892030239105 current acc: 0.6857018813314038\n",
      "iteration 73 current loss: 0.20216505229473114 current acc: 0.685834136933462\n",
      "iteration 74 current loss: 0.2445487380027771 current acc: 0.6859469879518072\n",
      "iteration 75 current loss: 0.20170456171035767 current acc: 0.6860693641618497\n",
      "iteration 76 current loss: 0.29443854093551636 current acc: 0.6861916225324988\n",
      "iteration 77 current loss: 0.18980035185813904 current acc: 0.6863233878729548\n",
      "iteration 78 current loss: 0.26587626338005066 current acc: 0.6864454064454064\n",
      "iteration 79 current loss: 0.2727032005786896 current acc: 0.6865576923076923\n",
      "iteration 80 current loss: 0.2158067524433136 current acc: 0.6866698702546853\n",
      "iteration 81 current loss: 0.20082324743270874 current acc: 0.6868011527377521\n",
      "iteration 82 current loss: 0.22519804537296295 current acc: 0.6869323091694671\n",
      "iteration 83 current loss: 0.18843846023082733 current acc: 0.6870537428023032\n",
      "iteration 84 current loss: 0.15602460503578186 current acc: 0.6871846522781775\n",
      "iteration 85 current loss: 0.1597868651151657 current acc: 0.6873154362416107\n",
      "iteration 86 current loss: 0.22133304178714752 current acc: 0.6874460948730234\n",
      "iteration 87 current loss: 0.2863677442073822 current acc: 0.6875478927203065\n",
      "iteration 88 current loss: 0.2755078971385956 current acc: 0.6876687410244136\n",
      "iteration 89 current loss: 0.12529467046260834 current acc: 0.687799043062201\n",
      "iteration 90 current loss: 0.18772760033607483 current acc: 0.6879292204686753\n",
      "iteration 91 current loss: 0.23236940801143646 current acc: 0.6880497131931166\n",
      "iteration 92 current loss: 0.22734354436397552 current acc: 0.6881700907787864\n",
      "iteration 93 current loss: 0.205694317817688 current acc: 0.6882903533906399\n",
      "iteration 94 current loss: 0.12671704590320587 current acc: 0.6884295942720764\n",
      "iteration 95 current loss: 0.2550794184207916 current acc: 0.6885400763358779\n",
      "iteration 96 current loss: 0.15975266695022583 current acc: 0.6886790653314259\n",
      "iteration 97 current loss: 0.17606911063194275 current acc: 0.6887988560533842\n",
      "iteration 98 current loss: 0.19703039526939392 current acc: 0.6889185326345879\n",
      "iteration 99 current loss: 0.15042269229888916 current acc: 0.689047619047619\n",
      "\t\tEpoch 20/100 complete. Epoch loss 0.18697915285825728 Epoch accuracy 0.9556\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 20, Validation Accuracy: 0.615875, Validation Loss: 1.31271254979074\n",
      "best loss 0.18697915285825728\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.1373840719461441 current acc: 0.689186101856259\n",
      "iteration 1 current loss: 0.10686223953962326 current acc: 0.6893339676498573\n",
      "iteration 2 current loss: 0.18895205855369568 current acc: 0.6894721825962911\n",
      "iteration 3 current loss: 0.16115595400333405 current acc: 0.6896007604562737\n",
      "iteration 4 current loss: 0.1685015857219696 current acc: 0.6897197149643706\n",
      "iteration 5 current loss: 0.18288205564022064 current acc: 0.6898480531813865\n",
      "iteration 6 current loss: 0.12409886717796326 current acc: 0.6899857617465591\n",
      "iteration 7 current loss: 0.1157250627875328 current acc: 0.6901138519924098\n",
      "iteration 8 current loss: 0.13361233472824097 current acc: 0.6902607871028924\n",
      "iteration 9 current loss: 0.10084302723407745 current acc: 0.6903981042654028\n",
      "iteration 10 current loss: 0.12785157561302185 current acc: 0.6905352913311227\n",
      "iteration 11 current loss: 0.11425623297691345 current acc: 0.6906818181818182\n",
      "iteration 12 current loss: 0.13733115792274475 current acc: 0.6908187411263607\n",
      "iteration 13 current loss: 0.11516939848661423 current acc: 0.6909555345316934\n",
      "iteration 14 current loss: 0.1665535867214203 current acc: 0.6910827423167849\n",
      "iteration 15 current loss: 0.176274374127388 current acc: 0.6912192816635161\n",
      "iteration 16 current loss: 0.22626516222953796 current acc: 0.6913084553613604\n",
      "iteration 17 current loss: 0.15579214692115784 current acc: 0.6914353163361662\n",
      "iteration 18 current loss: 0.14544996619224548 current acc: 0.6915809344030203\n",
      "iteration 19 current loss: 0.11699887365102768 current acc: 0.6917264150943396\n",
      "iteration 20 current loss: 0.08080187439918518 current acc: 0.6918717586044318\n",
      "iteration 21 current loss: 0.18013456463813782 current acc: 0.6919886899151744\n",
      "iteration 22 current loss: 0.11241820454597473 current acc: 0.6921337729627886\n",
      "iteration 23 current loss: 0.0672898143529892 current acc: 0.6922787193973635\n",
      "iteration 24 current loss: 0.10869858413934708 current acc: 0.6924141176470588\n",
      "iteration 25 current loss: 0.12246907502412796 current acc: 0.6925399811853246\n",
      "iteration 26 current loss: 0.15890030562877655 current acc: 0.692646920545369\n",
      "iteration 27 current loss: 0.10693945735692978 current acc: 0.6927913533834587\n",
      "iteration 28 current loss: 0.1465190052986145 current acc: 0.6929262564584312\n",
      "iteration 29 current loss: 0.11380806565284729 current acc: 0.6930610328638498\n",
      "iteration 30 current loss: 0.2054932713508606 current acc: 0.6931862975129047\n",
      "iteration 31 current loss: 0.14285048842430115 current acc: 0.6933208255159474\n",
      "iteration 32 current loss: 0.14279776811599731 current acc: 0.6934364744491327\n",
      "iteration 33 current loss: 0.11199994385242462 current acc: 0.6935707591377694\n",
      "iteration 34 current loss: 0.14099587500095367 current acc: 0.693695550351288\n",
      "iteration 35 current loss: 0.14889580011367798 current acc: 0.6938202247191011\n",
      "iteration 36 current loss: 0.08145049214363098 current acc: 0.6939635002339729\n",
      "iteration 37 current loss: 0.26651957631111145 current acc: 0.6940505144995323\n",
      "iteration 38 current loss: 0.2625364363193512 current acc: 0.6941654978962132\n",
      "iteration 39 current loss: 0.13653048872947693 current acc: 0.6942990654205607\n",
      "iteration 40 current loss: 0.1122027337551117 current acc: 0.6944418496029893\n",
      "iteration 41 current loss: 0.11586601287126541 current acc: 0.6945751633986929\n",
      "iteration 42 current loss: 0.18541422486305237 current acc: 0.694699020065329\n",
      "iteration 43 current loss: 0.13663987815380096 current acc: 0.6948320895522389\n",
      "iteration 44 current loss: 0.141567200422287 current acc: 0.6949650349650349\n",
      "iteration 45 current loss: 0.12197548896074295 current acc: 0.695107176141659\n",
      "iteration 46 current loss: 0.12823814153671265 current acc: 0.6952398695854681\n",
      "iteration 47 current loss: 0.20446252822875977 current acc: 0.6953538175046555\n",
      "iteration 48 current loss: 0.10402294248342514 current acc: 0.6954955793392276\n",
      "iteration 49 current loss: 0.09389651566743851 current acc: 0.6956279069767441\n",
      "iteration 50 current loss: 0.18980808556079865 current acc: 0.6957508135750814\n",
      "iteration 51 current loss: 0.09930859506130219 current acc: 0.6958921933085502\n",
      "iteration 52 current loss: 0.23795467615127563 current acc: 0.6960055736182071\n",
      "iteration 53 current loss: 0.07975248247385025 current acc: 0.6961467038068709\n",
      "iteration 54 current loss: 0.08791950345039368 current acc: 0.6962877030162413\n",
      "iteration 55 current loss: 0.08559359610080719 current acc: 0.6964285714285714\n",
      "iteration 56 current loss: 0.1339079737663269 current acc: 0.6965600370885489\n",
      "iteration 57 current loss: 0.14333777129650116 current acc: 0.6966913809082483\n",
      "iteration 58 current loss: 0.1661989837884903 current acc: 0.6968226030569709\n",
      "iteration 59 current loss: 0.18738943338394165 current acc: 0.6969444444444445\n",
      "iteration 60 current loss: 0.0929102674126625 current acc: 0.6970846830171217\n",
      "iteration 61 current loss: 0.15073566138744354 current acc: 0.6972062904717854\n",
      "iteration 62 current loss: 0.1245032474398613 current acc: 0.6973462783171521\n",
      "iteration 63 current loss: 0.19829295575618744 current acc: 0.6974584103512015\n",
      "iteration 64 current loss: 0.12808199226856232 current acc: 0.6975889145496535\n",
      "iteration 65 current loss: 0.18370674550533295 current acc: 0.6977192982456141\n",
      "iteration 66 current loss: 0.07955364882946014 current acc: 0.6978587909552376\n",
      "iteration 67 current loss: 0.15072175860404968 current acc: 0.6979704797047971\n",
      "iteration 68 current loss: 0.14729371666908264 current acc: 0.6981005071461504\n",
      "iteration 69 current loss: 0.15611931681632996 current acc: 0.698221198156682\n",
      "iteration 70 current loss: 0.11400879174470901 current acc: 0.6983602026715799\n",
      "iteration 71 current loss: 0.19117099046707153 current acc: 0.6984806629834254\n",
      "iteration 72 current loss: 0.164044588804245 current acc: 0.6986102162908422\n",
      "iteration 73 current loss: 0.13600963354110718 current acc: 0.6987304507819687\n",
      "iteration 74 current loss: 0.14568805694580078 current acc: 0.6988505747126437\n",
      "iteration 75 current loss: 0.1455627977848053 current acc: 0.6989889705882353\n",
      "iteration 76 current loss: 0.16122865676879883 current acc: 0.6991088654111162\n",
      "iteration 77 current loss: 0.22270868718624115 current acc: 0.699228650137741\n",
      "iteration 78 current loss: 0.09857648611068726 current acc: 0.6993483249196879\n",
      "iteration 79 current loss: 0.19255371391773224 current acc: 0.6994587155963303\n",
      "iteration 80 current loss: 0.15758635103702545 current acc: 0.6995873452544704\n",
      "iteration 81 current loss: 0.1245516836643219 current acc: 0.6997250229147571\n",
      "iteration 82 current loss: 0.12558120489120483 current acc: 0.6998534127347686\n",
      "iteration 83 current loss: 0.1086501032114029 current acc: 0.699981684981685\n",
      "iteration 84 current loss: 0.13242211937904358 current acc: 0.7001006864988558\n",
      "iteration 85 current loss: 0.17798756062984467 current acc: 0.7002195791399817\n",
      "iteration 86 current loss: 0.22423483431339264 current acc: 0.7003292181069959\n",
      "iteration 87 current loss: 0.1913868933916092 current acc: 0.7004387568555759\n",
      "iteration 88 current loss: 0.07995398342609406 current acc: 0.7005756052992234\n",
      "iteration 89 current loss: 0.1603567749261856 current acc: 0.7006940639269407\n",
      "iteration 90 current loss: 0.237005352973938 current acc: 0.7007941579187585\n",
      "iteration 91 current loss: 0.107309490442276 current acc: 0.7009215328467153\n",
      "iteration 92 current loss: 0.07837256789207458 current acc: 0.7010579115367077\n",
      "iteration 93 current loss: 0.207991823554039 current acc: 0.701175934366454\n",
      "iteration 94 current loss: 0.13680371642112732 current acc: 0.7013029612756264\n",
      "iteration 95 current loss: 0.14660221338272095 current acc: 0.7014389799635701\n",
      "iteration 96 current loss: 0.19002974033355713 current acc: 0.7015566681838871\n",
      "iteration 97 current loss: 0.10634269565343857 current acc: 0.7016924476797088\n",
      "iteration 98 current loss: 0.23175489902496338 current acc: 0.7017917235106866\n",
      "iteration 99 current loss: 0.15837855637073517 current acc: 0.7019090909090909\n",
      "\t\tEpoch 21/100 complete. Epoch loss 0.1456223987042904 Epoch accuracy 0.972\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 21, Validation Accuracy: 0.6215, Validation Loss: 1.3203844588249922\n",
      "best loss 0.1456223987042904\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.09331420063972473 current acc: 0.702035438437074\n",
      "iteration 1 current loss: 0.16781572997570038 current acc: 0.7021525885558583\n",
      "iteration 2 current loss: 0.07453760504722595 current acc: 0.7022877893781208\n",
      "iteration 3 current loss: 0.16300782561302185 current acc: 0.7023956442831216\n",
      "iteration 4 current loss: 0.11143425107002258 current acc: 0.702530612244898\n",
      "iteration 5 current loss: 0.12235446274280548 current acc: 0.7026654578422484\n",
      "iteration 6 current loss: 0.15093056857585907 current acc: 0.7027639329406434\n",
      "iteration 7 current loss: 0.13678041100502014 current acc: 0.7028804347826086\n",
      "iteration 8 current loss: 0.08372588455677032 current acc: 0.7030149388863739\n",
      "iteration 9 current loss: 0.13043218851089478 current acc: 0.7031402714932127\n",
      "iteration 10 current loss: 0.06711375713348389 current acc: 0.7032745364088647\n",
      "iteration 11 current loss: 0.0939575657248497 current acc: 0.7033996383363472\n",
      "iteration 12 current loss: 0.1324121505022049 current acc: 0.703524627202892\n",
      "iteration 13 current loss: 0.07372850924730301 current acc: 0.7036585365853658\n",
      "iteration 14 current loss: 0.060143474489450455 current acc: 0.7037923250564334\n",
      "iteration 15 current loss: 0.07767775654792786 current acc: 0.7039259927797834\n",
      "iteration 16 current loss: 0.14372669160366058 current acc: 0.7040505187189896\n",
      "iteration 17 current loss: 0.1091010645031929 current acc: 0.704165915238954\n",
      "iteration 18 current loss: 0.09900877624750137 current acc: 0.7042992338891393\n",
      "iteration 19 current loss: 0.11466529220342636 current acc: 0.7044234234234235\n",
      "iteration 20 current loss: 0.18673039972782135 current acc: 0.7045294912201711\n",
      "iteration 21 current loss: 0.0780465304851532 current acc: 0.7046534653465346\n",
      "iteration 22 current loss: 0.0739412009716034 current acc: 0.7047863247863247\n",
      "iteration 23 current loss: 0.06042947620153427 current acc: 0.7049190647482014\n",
      "iteration 24 current loss: 0.07537524402141571 current acc: 0.7050516853932585\n",
      "iteration 25 current loss: 0.16942526400089264 current acc: 0.7051662174303683\n",
      "iteration 26 current loss: 0.12784254550933838 current acc: 0.705280646609789\n",
      "iteration 27 current loss: 0.059118520468473434 current acc: 0.7054129263913824\n",
      "iteration 28 current loss: 0.052316948771476746 current acc: 0.7055450874831763\n",
      "iteration 29 current loss: 0.10777778923511505 current acc: 0.7056681614349776\n",
      "iteration 30 current loss: 0.05400076508522034 current acc: 0.7058000896458987\n",
      "iteration 31 current loss: 0.08623544871807098 current acc: 0.7059318996415771\n",
      "iteration 32 current loss: 0.0846705511212349 current acc: 0.706063591580833\n",
      "iteration 33 current loss: 0.13639387488365173 current acc: 0.706177260519248\n",
      "iteration 34 current loss: 0.06977622956037521 current acc: 0.7062997762863534\n",
      "iteration 35 current loss: 0.16067935526371002 current acc: 0.7064132379248659\n",
      "iteration 36 current loss: 0.06544879078865051 current acc: 0.7065355386678588\n",
      "iteration 37 current loss: 0.10841717571020126 current acc: 0.7066577301161752\n",
      "iteration 38 current loss: 0.07446375489234924 current acc: 0.7067887449754354\n",
      "iteration 39 current loss: 0.1391550898551941 current acc: 0.7069017857142857\n",
      "iteration 40 current loss: 0.11537119746208191 current acc: 0.7070147255689424\n",
      "iteration 41 current loss: 0.12710432708263397 current acc: 0.7071454058876003\n",
      "iteration 42 current loss: 0.08290012180805206 current acc: 0.7072759696834596\n",
      "iteration 43 current loss: 0.17459207773208618 current acc: 0.707379679144385\n",
      "iteration 44 current loss: 0.11428603529930115 current acc: 0.7075011135857461\n",
      "iteration 45 current loss: 0.07326129823923111 current acc: 0.7076313446126447\n",
      "iteration 46 current loss: 0.2171391248703003 current acc: 0.707743658210948\n",
      "iteration 47 current loss: 0.0695345476269722 current acc: 0.7078736654804271\n",
      "iteration 48 current loss: 0.10599930584430695 current acc: 0.7079946642952424\n",
      "iteration 49 current loss: 0.06522521376609802 current acc: 0.7081244444444444\n",
      "iteration 50 current loss: 0.08576029539108276 current acc: 0.7082452243447357\n",
      "iteration 51 current loss: 0.09696304053068161 current acc: 0.7083747779751333\n",
      "iteration 52 current loss: 0.2033424973487854 current acc: 0.7084775854416334\n",
      "iteration 53 current loss: 0.1451472043991089 current acc: 0.7085980479148181\n",
      "iteration 54 current loss: 0.13473953306674957 current acc: 0.7087184035476718\n",
      "iteration 55 current loss: 0.10224814713001251 current acc: 0.7088297872340426\n",
      "iteration 56 current loss: 0.06867587566375732 current acc: 0.7089587948604342\n",
      "iteration 57 current loss: 0.11142125725746155 current acc: 0.7090788308237378\n",
      "iteration 58 current loss: 0.08807646483182907 current acc: 0.7092076139884905\n",
      "iteration 59 current loss: 0.14165207743644714 current acc: 0.7093185840707965\n",
      "iteration 60 current loss: 0.08469715714454651 current acc: 0.7094471472799646\n",
      "iteration 61 current loss: 0.06980650126934052 current acc: 0.7095755968169761\n",
      "iteration 62 current loss: 0.05970562621951103 current acc: 0.7097039328325232\n",
      "iteration 63 current loss: 0.09016560763120651 current acc: 0.7098321554770318\n",
      "iteration 64 current loss: 0.1946175992488861 current acc: 0.7099426048565122\n",
      "iteration 65 current loss: 0.10682197660207748 current acc: 0.7100706090026478\n",
      "iteration 66 current loss: 0.10540896654129028 current acc: 0.7101985002205558\n",
      "iteration 67 current loss: 0.08105752617120743 current acc: 0.7103174603174603\n",
      "iteration 68 current loss: 0.16149471700191498 current acc: 0.710427501101807\n",
      "iteration 69 current loss: 0.11322535574436188 current acc: 0.7105462555066079\n",
      "iteration 70 current loss: 0.08895893394947052 current acc: 0.710673712021136\n",
      "iteration 71 current loss: 0.11914949119091034 current acc: 0.7108010563380281\n",
      "iteration 72 current loss: 0.130241259932518 current acc: 0.7109194896612406\n",
      "iteration 73 current loss: 0.06737694144248962 current acc: 0.7110466138962181\n",
      "iteration 74 current loss: 0.15862317383289337 current acc: 0.7111648351648352\n",
      "iteration 75 current loss: 0.09940559417009354 current acc: 0.7112917398945519\n",
      "iteration 76 current loss: 0.10156797617673874 current acc: 0.7114097496706192\n",
      "iteration 77 current loss: 0.05248696729540825 current acc: 0.7115364354697102\n",
      "iteration 78 current loss: 0.11989259719848633 current acc: 0.7116542343132953\n",
      "iteration 79 current loss: 0.12628717720508575 current acc: 0.7117631578947369\n",
      "iteration 80 current loss: 0.17605705559253693 current acc: 0.7118544498027181\n",
      "iteration 81 current loss: 0.11751417815685272 current acc: 0.7119719544259422\n",
      "iteration 82 current loss: 0.08466458320617676 current acc: 0.7120981165133596\n",
      "iteration 83 current loss: 0.08668667078018188 current acc: 0.7122241681260946\n",
      "iteration 84 current loss: 0.11172434687614441 current acc: 0.7123501094091904\n",
      "iteration 85 current loss: 0.09533903747797012 current acc: 0.7124759405074366\n",
      "iteration 86 current loss: 0.07225289195775986 current acc: 0.7126016615653695\n",
      "iteration 87 current loss: 0.1316722333431244 current acc: 0.7127185314685315\n",
      "iteration 88 current loss: 0.16115500032901764 current acc: 0.7128265618173875\n",
      "iteration 89 current loss: 0.20856115221977234 current acc: 0.7129257641921397\n",
      "iteration 90 current loss: 0.10831726342439651 current acc: 0.7130510694020079\n",
      "iteration 91 current loss: 0.23684073984622955 current acc: 0.7131500872600349\n",
      "iteration 92 current loss: 0.11177056282758713 current acc: 0.7132664631487134\n",
      "iteration 93 current loss: 0.12296499311923981 current acc: 0.7133914559721012\n",
      "iteration 94 current loss: 0.2068929523229599 current acc: 0.7134901960784313\n",
      "iteration 95 current loss: 0.053561754524707794 current acc: 0.7136149825783972\n",
      "iteration 96 current loss: 0.10890614241361618 current acc: 0.7137396604266435\n",
      "iteration 97 current loss: 0.144643634557724 current acc: 0.7138468233246301\n",
      "iteration 98 current loss: 0.09676866233348846 current acc: 0.713962592431492\n",
      "iteration 99 current loss: 0.12458964437246323 current acc: 0.7140608695652174\n",
      "\t\tEpoch 22/100 complete. Epoch loss 0.11191427409648895 Epoch accuracy 0.9814\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 22, Validation Accuracy: 0.612, Validation Loss: 1.4292200420051813\n",
      "best loss 0.11191427409648895\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.12006808072328568 current acc: 0.7141764450239027\n",
      "iteration 1 current loss: 0.11531275510787964 current acc: 0.7142832319721981\n",
      "iteration 2 current loss: 0.09582403302192688 current acc: 0.7144072948328267\n",
      "iteration 3 current loss: 0.13024264574050903 current acc: 0.7145138888888889\n",
      "iteration 4 current loss: 0.07946780323982239 current acc: 0.7146377440347071\n",
      "iteration 5 current loss: 0.06284528225660324 current acc: 0.7147614917606244\n",
      "iteration 6 current loss: 0.07243949174880981 current acc: 0.7148851322063285\n",
      "iteration 7 current loss: 0.05526609718799591 current acc: 0.7150086655112652\n",
      "iteration 8 current loss: 0.0821356475353241 current acc: 0.7151320918146383\n",
      "iteration 9 current loss: 0.07846827805042267 current acc: 0.7152554112554113\n",
      "iteration 10 current loss: 0.05057428032159805 current acc: 0.7153786239723063\n",
      "iteration 11 current loss: 0.0699738934636116 current acc: 0.7155017301038062\n",
      "iteration 12 current loss: 0.16370609402656555 current acc: 0.7156160830090791\n",
      "iteration 13 current loss: 0.07637795060873032 current acc: 0.7157389801210026\n",
      "iteration 14 current loss: 0.04081767424941063 current acc: 0.7158617710583154\n",
      "iteration 15 current loss: 0.04128074273467064 current acc: 0.7159844559585492\n",
      "iteration 16 current loss: 0.1173672303557396 current acc: 0.7160984031074665\n",
      "iteration 17 current loss: 0.06722622364759445 current acc: 0.7162208800690251\n",
      "iteration 18 current loss: 0.07019137591123581 current acc: 0.7163432514014662\n",
      "iteration 19 current loss: 0.05903949588537216 current acc: 0.7164655172413793\n",
      "iteration 20 current loss: 0.06903398036956787 current acc: 0.7165876777251184\n",
      "iteration 21 current loss: 0.09258820861577988 current acc: 0.7167011197243756\n",
      "iteration 22 current loss: 0.06024874746799469 current acc: 0.716823073611709\n",
      "iteration 23 current loss: 0.09159731864929199 current acc: 0.7169363166953528\n",
      "iteration 24 current loss: 0.03877429664134979 current acc: 0.7170580645161291\n",
      "iteration 25 current loss: 0.08142255991697311 current acc: 0.7171797076526225\n",
      "iteration 26 current loss: 0.04349314048886299 current acc: 0.7173012462397937\n",
      "iteration 27 current loss: 0.05933498218655586 current acc: 0.7174226804123711\n",
      "iteration 28 current loss: 0.056463103741407394 current acc: 0.7175440103048518\n",
      "iteration 29 current loss: 0.08805640041828156 current acc: 0.7176480686695279\n",
      "iteration 30 current loss: 0.08120602369308472 current acc: 0.7177691977691978\n",
      "iteration 31 current loss: 0.05837683379650116 current acc: 0.7178816466552316\n",
      "iteration 32 current loss: 0.07768408954143524 current acc: 0.7180025717959708\n",
      "iteration 33 current loss: 0.12077482044696808 current acc: 0.718114824335904\n",
      "iteration 34 current loss: 0.08018466085195541 current acc: 0.7182355460385439\n",
      "iteration 35 current loss: 0.08320806175470352 current acc: 0.7183561643835616\n",
      "iteration 36 current loss: 0.04600020498037338 current acc: 0.7184766795036371\n",
      "iteration 37 current loss: 0.21316160261631012 current acc: 0.718562874251497\n",
      "iteration 38 current loss: 0.0470089353621006 current acc: 0.718683197947841\n",
      "iteration 39 current loss: 0.06983605027198792 current acc: 0.7187948717948718\n",
      "iteration 40 current loss: 0.080312080681324 current acc: 0.7189064502349424\n",
      "iteration 41 current loss: 0.08032384514808655 current acc: 0.7190264730999146\n",
      "iteration 42 current loss: 0.06428830325603485 current acc: 0.7191463935125907\n",
      "iteration 43 current loss: 0.06386840343475342 current acc: 0.7192662116040955\n",
      "iteration 44 current loss: 0.10243678092956543 current acc: 0.7193859275053305\n",
      "iteration 45 current loss: 0.04765452444553375 current acc: 0.7195055413469735\n",
      "iteration 46 current loss: 0.1006300076842308 current acc: 0.7196080102258202\n",
      "iteration 47 current loss: 0.06451892107725143 current acc: 0.7197274275979557\n",
      "iteration 48 current loss: 0.11302975565195084 current acc: 0.7198467432950192\n",
      "iteration 49 current loss: 0.05745908245444298 current acc: 0.7199659574468085\n",
      "iteration 50 current loss: 0.051244571805000305 current acc: 0.7200850701829009\n",
      "iteration 51 current loss: 0.05888703837990761 current acc: 0.720204081632653\n",
      "iteration 52 current loss: 0.05657979100942612 current acc: 0.7203229919252019\n",
      "iteration 53 current loss: 0.06348679959774017 current acc: 0.7204418011894648\n",
      "iteration 54 current loss: 0.07673370838165283 current acc: 0.7205605095541401\n",
      "iteration 55 current loss: 0.06728467345237732 current acc: 0.7206791171477079\n",
      "iteration 56 current loss: 0.09551092982292175 current acc: 0.720789138735681\n",
      "iteration 57 current loss: 0.0696306899189949 current acc: 0.7209075487701442\n",
      "iteration 58 current loss: 0.07855678349733353 current acc: 0.7210173802458669\n",
      "iteration 59 current loss: 0.11154823005199432 current acc: 0.7211355932203389\n",
      "iteration 60 current loss: 0.08269312977790833 current acc: 0.7212452350698857\n",
      "iteration 61 current loss: 0.06764201074838638 current acc: 0.7213632514817951\n",
      "iteration 62 current loss: 0.053284548223018646 current acc: 0.7214727041895895\n",
      "iteration 63 current loss: 0.0913296714425087 current acc: 0.721590524534687\n",
      "iteration 64 current loss: 0.060341089963912964 current acc: 0.7217082452431289\n",
      "iteration 65 current loss: 0.11091382801532745 current acc: 0.7218174133558749\n",
      "iteration 66 current loss: 0.18085318803787231 current acc: 0.7219011406844107\n",
      "iteration 67 current loss: 0.09168152511119843 current acc: 0.7220101351351351\n",
      "iteration 68 current loss: 0.10328353941440582 current acc: 0.7221190375685943\n",
      "iteration 69 current loss: 0.05216081440448761 current acc: 0.7222362869198312\n",
      "iteration 70 current loss: 0.05789170414209366 current acc: 0.7223534373681991\n",
      "iteration 71 current loss: 0.12881633639335632 current acc: 0.7224704890387859\n",
      "iteration 72 current loss: 0.07170020788908005 current acc: 0.7225874420564686\n",
      "iteration 73 current loss: 0.13100317120552063 current acc: 0.7226790227464196\n",
      "iteration 74 current loss: 0.10003527998924255 current acc: 0.7227789473684211\n",
      "iteration 75 current loss: 0.09040164202451706 current acc: 0.7228787878787879\n",
      "iteration 76 current loss: 0.12070723623037338 current acc: 0.7229785443836769\n",
      "iteration 77 current loss: 0.09585187584161758 current acc: 0.7230866274179983\n",
      "iteration 78 current loss: 0.1055729016661644 current acc: 0.7231946195880622\n",
      "iteration 79 current loss: 0.06011962890625 current acc: 0.7233025210084033\n",
      "iteration 80 current loss: 0.05657307803630829 current acc: 0.7234187316253675\n",
      "iteration 81 current loss: 0.19282566010951996 current acc: 0.7235096557514693\n",
      "iteration 82 current loss: 0.11635401844978333 current acc: 0.7236256819135544\n",
      "iteration 83 current loss: 0.10144292563199997 current acc: 0.7237332214765101\n",
      "iteration 84 current loss: 0.07056453824043274 current acc: 0.7238490566037736\n",
      "iteration 85 current loss: 0.15750515460968018 current acc: 0.7239480301760268\n",
      "iteration 86 current loss: 0.05938870832324028 current acc: 0.7240636782572266\n",
      "iteration 87 current loss: 0.057218536734580994 current acc: 0.724179229480737\n",
      "iteration 88 current loss: 0.08459368348121643 current acc: 0.7242946839681875\n",
      "iteration 89 current loss: 0.0803280919790268 current acc: 0.7244100418410042\n",
      "iteration 90 current loss: 0.11273765563964844 current acc: 0.724508573818486\n",
      "iteration 91 current loss: 0.06469109654426575 current acc: 0.724623745819398\n",
      "iteration 92 current loss: 0.06312385201454163 current acc: 0.7247388215628918\n",
      "iteration 93 current loss: 0.0973268672823906 current acc: 0.7248538011695906\n",
      "iteration 94 current loss: 0.07027660310268402 current acc: 0.7249686847599165\n",
      "iteration 95 current loss: 0.1436629742383957 current acc: 0.7250667779632721\n",
      "iteration 96 current loss: 0.09294632822275162 current acc: 0.7251731330830204\n",
      "iteration 97 current loss: 0.08505593240261078 current acc: 0.725279399499583\n",
      "iteration 98 current loss: 0.15603329241275787 current acc: 0.7253689037098792\n",
      "iteration 99 current loss: 0.05109451338648796 current acc: 0.7254833333333334\n",
      "\t\tEpoch 23/100 complete. Epoch loss 0.08481090858578683 Epoch accuracy 0.9882\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 23, Validation Accuracy: 0.61175, Validation Loss: 1.4397170141339302\n",
      "best loss 0.08481090858578683\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.10678290575742722 current acc: 0.7255893377759267\n",
      "iteration 1 current loss: 0.04279729351401329 current acc: 0.7257035803497086\n",
      "iteration 2 current loss: 0.06918915361166 current acc: 0.7258177278401997\n",
      "iteration 3 current loss: 0.07463881373405457 current acc: 0.7259317803660565\n",
      "iteration 4 current loss: 0.05245337635278702 current acc: 0.726045738045738\n",
      "iteration 5 current loss: 0.044871602207422256 current acc: 0.7261596009975062\n",
      "iteration 6 current loss: 0.05662175267934799 current acc: 0.7262733693394267\n",
      "iteration 7 current loss: 0.04584506154060364 current acc: 0.7263870431893688\n",
      "iteration 8 current loss: 0.061473287642002106 current acc: 0.7265006226650063\n",
      "iteration 9 current loss: 0.03971100598573685 current acc: 0.7266141078838174\n",
      "iteration 10 current loss: 0.0688115805387497 current acc: 0.7267274989630859\n",
      "iteration 11 current loss: 0.0710734874010086 current acc: 0.726832504145937\n",
      "iteration 12 current loss: 0.06740670651197433 current acc: 0.7269457107335268\n",
      "iteration 13 current loss: 0.04995144531130791 current acc: 0.7270588235294118\n",
      "iteration 14 current loss: 0.05498166382312775 current acc: 0.7271718426501035\n",
      "iteration 15 current loss: 0.05390283465385437 current acc: 0.7272847682119206\n",
      "iteration 16 current loss: 0.037345822900533676 current acc: 0.7273976003309889\n",
      "iteration 17 current loss: 0.033975329250097275 current acc: 0.7275103391232424\n",
      "iteration 18 current loss: 0.02220984734594822 current acc: 0.7276229847044233\n",
      "iteration 19 current loss: 0.04978976398706436 current acc: 0.7277355371900827\n",
      "iteration 20 current loss: 0.035460542887449265 current acc: 0.7278479966955803\n",
      "iteration 21 current loss: 0.03748343884944916 current acc: 0.7279603633360858\n",
      "iteration 22 current loss: 0.06063368171453476 current acc: 0.7280726372265787\n",
      "iteration 23 current loss: 0.06020176783204079 current acc: 0.7281848184818482\n",
      "iteration 24 current loss: 0.024175191298127174 current acc: 0.7282969072164949\n",
      "iteration 25 current loss: 0.043800607323646545 current acc: 0.7284089035449299\n",
      "iteration 26 current loss: 0.047666870057582855 current acc: 0.7285125669550886\n",
      "iteration 27 current loss: 0.04498434066772461 current acc: 0.7286243822075783\n",
      "iteration 28 current loss: 0.05815593898296356 current acc: 0.7287361053931659\n",
      "iteration 29 current loss: 0.023631103336811066 current acc: 0.7288477366255144\n",
      "iteration 30 current loss: 0.02798430435359478 current acc: 0.7289592760180995\n",
      "iteration 31 current loss: 0.04300369694828987 current acc: 0.7290707236842106\n",
      "iteration 32 current loss: 0.05509203299880028 current acc: 0.7291820797369503\n",
      "iteration 33 current loss: 0.032426364719867706 current acc: 0.7292933442892359\n",
      "iteration 34 current loss: 0.04099314287304878 current acc: 0.7294045174537988\n",
      "iteration 35 current loss: 0.05256492272019386 current acc: 0.7295155993431856\n",
      "iteration 36 current loss: 0.08833997696638107 current acc: 0.7296183832581042\n",
      "iteration 37 current loss: 0.06324896216392517 current acc: 0.729721082854799\n",
      "iteration 38 current loss: 0.04391242563724518 current acc: 0.7298318983189832\n",
      "iteration 39 current loss: 0.044009093195199966 current acc: 0.7299426229508197\n",
      "iteration 40 current loss: 0.041323427110910416 current acc: 0.7300532568619418\n",
      "iteration 41 current loss: 0.07551359385251999 current acc: 0.7301556101556101\n",
      "iteration 42 current loss: 0.05367674306035042 current acc: 0.7302660663119116\n",
      "iteration 43 current loss: 0.05545961484313011 current acc: 0.7303682487725041\n",
      "iteration 44 current loss: 0.041510697454214096 current acc: 0.730478527607362\n",
      "iteration 45 current loss: 0.05139638110995293 current acc: 0.7305887162714636\n",
      "iteration 46 current loss: 0.056305743753910065 current acc: 0.7306906416019616\n",
      "iteration 47 current loss: 0.038143470883369446 current acc: 0.7308006535947712\n",
      "iteration 48 current loss: 0.026982169598340988 current acc: 0.7309105757452021\n",
      "iteration 49 current loss: 0.053629085421562195 current acc: 0.7310204081632653\n",
      "iteration 50 current loss: 0.06463176012039185 current acc: 0.7311219910240718\n",
      "iteration 51 current loss: 0.03454216569662094 current acc: 0.731231647634584\n",
      "iteration 52 current loss: 0.07961468398571014 current acc: 0.7313330615572768\n",
      "iteration 53 current loss: 0.028133731335401535 current acc: 0.731442542787286\n",
      "iteration 54 current loss: 0.055369552224874496 current acc: 0.731551934826884\n",
      "iteration 55 current loss: 0.07908550649881363 current acc: 0.7316530944625407\n",
      "iteration 56 current loss: 0.1163654550909996 current acc: 0.7317460317460317\n",
      "iteration 57 current loss: 0.07693377137184143 current acc: 0.731847030105777\n",
      "iteration 58 current loss: 0.12391930818557739 current acc: 0.7319479463196421\n",
      "iteration 59 current loss: 0.03603693097829819 current acc: 0.7320569105691057\n",
      "iteration 60 current loss: 0.05209670960903168 current acc: 0.7321657862657456\n",
      "iteration 61 current loss: 0.04178040474653244 current acc: 0.7322745735174655\n",
      "iteration 62 current loss: 0.06389503926038742 current acc: 0.7323751522533496\n",
      "iteration 63 current loss: 0.05210685729980469 current acc: 0.7324837662337662\n",
      "iteration 64 current loss: 0.12137270718812943 current acc: 0.7325841784989858\n",
      "iteration 65 current loss: 0.03181714937090874 current acc: 0.7326926196269262\n",
      "iteration 66 current loss: 0.05552508309483528 current acc: 0.732792865828942\n",
      "iteration 67 current loss: 0.09399665147066116 current acc: 0.7328930307941653\n",
      "iteration 68 current loss: 0.0921204537153244 current acc: 0.7329850141757797\n",
      "iteration 69 current loss: 0.04099665582180023 current acc: 0.7330931174089069\n",
      "iteration 70 current loss: 0.08849862217903137 current acc: 0.7332011331444759\n",
      "iteration 71 current loss: 0.07537970691919327 current acc: 0.7333009708737864\n",
      "iteration 72 current loss: 0.0285521037876606 current acc: 0.7334088152042054\n",
      "iteration 73 current loss: 0.05106375738978386 current acc: 0.7335165723524656\n",
      "iteration 74 current loss: 0.0716462954878807 current acc: 0.7336161616161616\n",
      "iteration 75 current loss: 0.05211587995290756 current acc: 0.7337237479806139\n",
      "iteration 76 current loss: 0.06625174731016159 current acc: 0.7338231731933791\n",
      "iteration 77 current loss: 0.0855591669678688 current acc: 0.7339225181598062\n",
      "iteration 78 current loss: 0.04790964722633362 current acc: 0.7340298507462687\n",
      "iteration 79 current loss: 0.07661419361829758 current acc: 0.7341290322580645\n",
      "iteration 80 current loss: 0.03738485649228096 current acc: 0.7342361950826279\n",
      "iteration 81 current loss: 0.10336517542600632 current acc: 0.7343352135374698\n",
      "iteration 82 current loss: 0.05517037957906723 current acc: 0.7344341522351994\n",
      "iteration 83 current loss: 0.08240395784378052 current acc: 0.7345330112721417\n",
      "iteration 84 current loss: 0.03505665436387062 current acc: 0.7346398390342053\n",
      "iteration 85 current loss: 0.0431230366230011 current acc: 0.7347465808527756\n",
      "iteration 86 current loss: 0.04770391434431076 current acc: 0.7348532368315239\n",
      "iteration 87 current loss: 0.04245748370885849 current acc: 0.734959807073955\n",
      "iteration 88 current loss: 0.04167838394641876 current acc: 0.735066291683407\n",
      "iteration 89 current loss: 0.05638422071933746 current acc: 0.7351726907630523\n",
      "iteration 90 current loss: 0.043963074684143066 current acc: 0.7352790044158972\n",
      "iteration 91 current loss: 0.06247050315141678 current acc: 0.7353852327447833\n",
      "iteration 92 current loss: 0.04208783805370331 current acc: 0.7354913758523867\n",
      "iteration 93 current loss: 0.06773664802312851 current acc: 0.7355974338412189\n",
      "iteration 94 current loss: 0.039481572806835175 current acc: 0.7357034068136272\n",
      "iteration 95 current loss: 0.08133544772863388 current acc: 0.735801282051282\n",
      "iteration 96 current loss: 0.019241997972130775 current acc: 0.7359070885062075\n",
      "iteration 97 current loss: 0.04704147204756737 current acc: 0.7360128102481985\n",
      "iteration 98 current loss: 0.07835795730352402 current acc: 0.736110444177671\n",
      "iteration 99 current loss: 0.08459853380918503 current acc: 0.736208\n",
      "\t\tEpoch 24/100 complete. Epoch loss 0.05620516873896122 Epoch accuracy 0.9936\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 24, Validation Accuracy: 0.62875, Validation Loss: 1.397157795727253\n",
      "best loss 0.05620516873896122\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.03610354661941528 current acc: 0.736313474610156\n",
      "iteration 1 current loss: 0.02508472464978695 current acc: 0.7364188649080735\n",
      "iteration 2 current loss: 0.028972942382097244 current acc: 0.7365241709948063\n",
      "iteration 3 current loss: 0.027849338948726654 current acc: 0.736629392971246\n",
      "iteration 4 current loss: 0.046041931957006454 current acc: 0.7367345309381238\n",
      "iteration 5 current loss: 0.032450441271066666 current acc: 0.7368395849960095\n",
      "iteration 6 current loss: 0.04712755233049393 current acc: 0.7369445552453131\n",
      "iteration 7 current loss: 0.05077115073800087 current acc: 0.7370494417862838\n",
      "iteration 8 current loss: 0.07127505540847778 current acc: 0.7371462734157035\n",
      "iteration 9 current loss: 0.026868069544434547 current acc: 0.7372509960159362\n",
      "iteration 10 current loss: 0.02880602329969406 current acc: 0.7373556352050976\n",
      "iteration 11 current loss: 0.05917087569832802 current acc: 0.7374601910828026\n",
      "iteration 12 current loss: 0.028172481805086136 current acc: 0.7375646637485077\n",
      "iteration 13 current loss: 0.03737347945570946 current acc: 0.7376690533015116\n",
      "iteration 14 current loss: 0.026935510337352753 current acc: 0.7377733598409543\n",
      "iteration 15 current loss: 0.060739483684301376 current acc: 0.7378696343402226\n",
      "iteration 16 current loss: 0.02228052355349064 current acc: 0.737973778307509\n",
      "iteration 17 current loss: 0.04353852570056915 current acc: 0.7380778395552026\n",
      "iteration 18 current loss: 0.038082096725702286 current acc: 0.7381818181818182\n",
      "iteration 19 current loss: 0.03270058333873749 current acc: 0.7382857142857143\n",
      "iteration 20 current loss: 0.038393307477235794 current acc: 0.7383895279650932\n",
      "iteration 21 current loss: 0.04337596520781517 current acc: 0.7384932593180016\n",
      "iteration 22 current loss: 0.017923064529895782 current acc: 0.7385969084423306\n",
      "iteration 23 current loss: 0.041236866265535355 current acc: 0.7387004754358162\n",
      "iteration 24 current loss: 0.039774276316165924 current acc: 0.7388039603960396\n",
      "iteration 25 current loss: 0.03614843264222145 current acc: 0.7389073634204275\n",
      "iteration 26 current loss: 0.0226802546530962 current acc: 0.7390106846062525\n",
      "iteration 27 current loss: 0.04337794706225395 current acc: 0.7391139240506329\n",
      "iteration 28 current loss: 0.04358768090605736 current acc: 0.7392170818505338\n",
      "iteration 29 current loss: 0.03682677075266838 current acc: 0.7393201581027667\n",
      "iteration 30 current loss: 0.03285296633839607 current acc: 0.7394231529039905\n",
      "iteration 31 current loss: 0.02810124307870865 current acc: 0.7395260663507109\n",
      "iteration 32 current loss: 0.03936281427741051 current acc: 0.7396288985392815\n",
      "iteration 33 current loss: 0.026862043887376785 current acc: 0.7397316495659038\n",
      "iteration 34 current loss: 0.02416314370930195 current acc: 0.7398343195266273\n",
      "iteration 35 current loss: 0.041936419904232025 current acc: 0.7399369085173502\n",
      "iteration 36 current loss: 0.029404370114207268 current acc: 0.7400394166338194\n",
      "iteration 37 current loss: 0.032269950956106186 current acc: 0.7401418439716312\n",
      "iteration 38 current loss: 0.023717891424894333 current acc: 0.7402441906262308\n",
      "iteration 39 current loss: 0.036254819482564926 current acc: 0.7403464566929134\n",
      "iteration 40 current loss: 0.029925579205155373 current acc: 0.740448642266824\n",
      "iteration 41 current loss: 0.023098928853869438 current acc: 0.7405507474429583\n",
      "iteration 42 current loss: 0.019554613158106804 current acc: 0.740652772316162\n",
      "iteration 43 current loss: 0.022916603833436966 current acc: 0.740754716981132\n",
      "iteration 44 current loss: 0.05561983212828636 current acc: 0.7408565815324165\n",
      "iteration 45 current loss: 0.04791173338890076 current acc: 0.7409583660644148\n",
      "iteration 46 current loss: 0.034689780324697495 current acc: 0.7410600706713781\n",
      "iteration 47 current loss: 0.033968303352594376 current acc: 0.7411616954474097\n",
      "iteration 48 current loss: 0.033441223204135895 current acc: 0.7412632404864653\n",
      "iteration 49 current loss: 0.04675918072462082 current acc: 0.7413647058823529\n",
      "iteration 50 current loss: 0.05896223708987236 current acc: 0.7414582516660133\n",
      "iteration 51 current loss: 0.02543812058866024 current acc: 0.7415595611285266\n",
      "iteration 52 current loss: 0.03131438046693802 current acc: 0.7416607912260086\n",
      "iteration 53 current loss: 0.0192545335739851 current acc: 0.7417619420516837\n",
      "iteration 54 current loss: 0.02968514896929264 current acc: 0.7418630136986302\n",
      "iteration 55 current loss: 0.037773337215185165 current acc: 0.7419640062597809\n",
      "iteration 56 current loss: 0.034032128751277924 current acc: 0.7420649198279233\n",
      "iteration 57 current loss: 0.03458268940448761 current acc: 0.7421657544956998\n",
      "iteration 58 current loss: 0.0339115746319294 current acc: 0.7422665103556076\n",
      "iteration 59 current loss: 0.018769068643450737 current acc: 0.7423671875\n",
      "iteration 60 current loss: 0.03708214312791824 current acc: 0.7424677860210855\n",
      "iteration 61 current loss: 0.05224095284938812 current acc: 0.7425683060109289\n",
      "iteration 62 current loss: 0.04505294933915138 current acc: 0.7426687475614514\n",
      "iteration 63 current loss: 0.02593141235411167 current acc: 0.7427691107644305\n",
      "iteration 64 current loss: 0.019329959526658058 current acc: 0.742869395711501\n",
      "iteration 65 current loss: 0.029947243630886078 current acc: 0.7429696024941543\n",
      "iteration 66 current loss: 0.02451903373003006 current acc: 0.7430697312037398\n",
      "iteration 67 current loss: 0.03284040093421936 current acc: 0.7431697819314642\n",
      "iteration 68 current loss: 0.06050742790102959 current acc: 0.7432697547683924\n",
      "iteration 69 current loss: 0.029128437861800194 current acc: 0.7433696498054475\n",
      "iteration 70 current loss: 0.05248359590768814 current acc: 0.7434694671334111\n",
      "iteration 71 current loss: 0.03021574579179287 current acc: 0.7435692068429238\n",
      "iteration 72 current loss: 0.046144090592861176 current acc: 0.743668869024485\n",
      "iteration 73 current loss: 0.023820918053388596 current acc: 0.7437684537684538\n",
      "iteration 74 current loss: 0.04238065332174301 current acc: 0.7438601941747572\n",
      "iteration 75 current loss: 0.10450419783592224 current acc: 0.7439518633540373\n",
      "iteration 76 current loss: 0.04462584853172302 current acc: 0.7440512223515716\n",
      "iteration 77 current loss: 0.05453106015920639 current acc: 0.7441505042668736\n",
      "iteration 78 current loss: 0.07328149676322937 current acc: 0.7442497091896084\n",
      "iteration 79 current loss: 0.034195948392152786 current acc: 0.7443488372093023\n",
      "iteration 80 current loss: 0.09991936385631561 current acc: 0.7444323905462998\n",
      "iteration 81 current loss: 0.03920989856123924 current acc: 0.7445313710302092\n",
      "iteration 82 current loss: 0.03474591299891472 current acc: 0.7446302748741773\n",
      "iteration 83 current loss: 0.05507931113243103 current acc: 0.7447291021671827\n",
      "iteration 84 current loss: 0.04399937763810158 current acc: 0.7448278529980658\n",
      "iteration 85 current loss: 0.03873734176158905 current acc: 0.7449265274555298\n",
      "iteration 86 current loss: 0.043701156973838806 current acc: 0.7450251256281407\n",
      "iteration 87 current loss: 0.04579056426882744 current acc: 0.7451236476043277\n",
      "iteration 88 current loss: 0.037859976291656494 current acc: 0.7452220934723831\n",
      "iteration 89 current loss: 0.04492230340838432 current acc: 0.7453204633204633\n",
      "iteration 90 current loss: 0.03130032494664192 current acc: 0.7454187572365882\n",
      "iteration 91 current loss: 0.03607241064310074 current acc: 0.745516975308642\n",
      "iteration 92 current loss: 0.026942336931824684 current acc: 0.7456151176243733\n",
      "iteration 93 current loss: 0.04571586102247238 current acc: 0.7457131842713955\n",
      "iteration 94 current loss: 0.03955156356096268 current acc: 0.7458111753371869\n",
      "iteration 95 current loss: 0.021323680877685547 current acc: 0.7459090909090909\n",
      "iteration 96 current loss: 0.055755615234375 current acc: 0.7459992298806315\n",
      "iteration 97 current loss: 0.07552666962146759 current acc: 0.7460892994611239\n",
      "iteration 98 current loss: 0.06443296372890472 current acc: 0.7461792997306657\n",
      "iteration 99 current loss: 0.03735116124153137 current acc: 0.7462769230769231\n",
      "\t\tEpoch 25/100 complete. Epoch loss 0.0390090487524867 Epoch accuracy 0.998\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 25, Validation Accuracy: 0.624125, Validation Loss: 1.4521912522614002\n",
      "best loss 0.0390090487524867\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.030890529975295067 current acc: 0.7463744713571703\n",
      "iteration 1 current loss: 0.03275236487388611 current acc: 0.7464719446579554\n",
      "iteration 2 current loss: 0.018970835953950882 current acc: 0.7465693430656934\n",
      "iteration 3 current loss: 0.037157878279685974 current acc: 0.7466666666666667\n",
      "iteration 4 current loss: 0.02761940471827984 current acc: 0.7467639155470249\n",
      "iteration 5 current loss: 0.041584912687540054 current acc: 0.7468534151957023\n",
      "iteration 6 current loss: 0.03390726447105408 current acc: 0.7469505178365938\n",
      "iteration 7 current loss: 0.02928260527551174 current acc: 0.74704754601227\n",
      "iteration 8 current loss: 0.030756091699004173 current acc: 0.7471444998083557\n",
      "iteration 9 current loss: 0.0183454230427742 current acc: 0.7472413793103448\n",
      "iteration 10 current loss: 0.02082575298845768 current acc: 0.7473381846036001\n",
      "iteration 11 current loss: 0.056692108511924744 current acc: 0.7474349157733537\n",
      "iteration 12 current loss: 0.022623468190431595 current acc: 0.7475315729047073\n",
      "iteration 13 current loss: 0.024346232414245605 current acc: 0.747628156082632\n",
      "iteration 14 current loss: 0.042511023581027985 current acc: 0.7477246653919694\n",
      "iteration 15 current loss: 0.026377607136964798 current acc: 0.7478211009174311\n",
      "iteration 16 current loss: 0.055807698518037796 current acc: 0.7479174627435995\n",
      "iteration 17 current loss: 0.02438683807849884 current acc: 0.7480137509549274\n",
      "iteration 18 current loss: 0.02518671751022339 current acc: 0.7481099656357388\n",
      "iteration 19 current loss: 0.017658453434705734 current acc: 0.748206106870229\n",
      "iteration 20 current loss: 0.018947459757328033 current acc: 0.7483021747424647\n",
      "iteration 21 current loss: 0.02746645361185074 current acc: 0.7483981693363845\n",
      "iteration 22 current loss: 0.03193742036819458 current acc: 0.7484940907357988\n",
      "iteration 23 current loss: 0.0306917242705822 current acc: 0.7485899390243902\n",
      "iteration 24 current loss: 0.07342296838760376 current acc: 0.7486780952380953\n",
      "iteration 25 current loss: 0.04544682428240776 current acc: 0.7487661843107387\n",
      "iteration 26 current loss: 0.04698441922664642 current acc: 0.7488618195660449\n",
      "iteration 27 current loss: 0.022122569382190704 current acc: 0.7489573820395738\n",
      "iteration 28 current loss: 0.04509073123335838 current acc: 0.7490528718143781\n",
      "iteration 29 current loss: 0.03287123143672943 current acc: 0.749148288973384\n",
      "iteration 30 current loss: 0.024030232802033424 current acc: 0.7492436335993918\n",
      "iteration 31 current loss: 0.0353904590010643 current acc: 0.749338905775076\n",
      "iteration 32 current loss: 0.021602122113108635 current acc: 0.7494341055829852\n",
      "iteration 33 current loss: 0.036630336195230484 current acc: 0.7495292331055429\n",
      "iteration 34 current loss: 0.05275707691907883 current acc: 0.7496166982922201\n",
      "iteration 35 current loss: 0.03536486625671387 current acc: 0.7497116843702579\n",
      "iteration 36 current loss: 0.023517746478319168 current acc: 0.749806598407281\n",
      "iteration 37 current loss: 0.026413677260279655 current acc: 0.7499014404852161\n",
      "iteration 38 current loss: 0.03942858427762985 current acc: 0.7499962106858659\n",
      "iteration 39 current loss: 0.03224828466773033 current acc: 0.7500909090909091\n",
      "iteration 40 current loss: 0.02754106931388378 current acc: 0.7501855357819008\n",
      "iteration 41 current loss: 0.022026164457201958 current acc: 0.7502800908402725\n",
      "iteration 42 current loss: 0.03585540130734444 current acc: 0.7503745743473326\n",
      "iteration 43 current loss: 0.02140159346163273 current acc: 0.7504689863842663\n",
      "iteration 44 current loss: 0.032606445252895355 current acc: 0.7505633270321361\n",
      "iteration 45 current loss: 0.06971848011016846 current acc: 0.750650037792895\n",
      "iteration 46 current loss: 0.02171485126018524 current acc: 0.7507442387608614\n",
      "iteration 47 current loss: 0.03224888816475868 current acc: 0.7508383685800605\n",
      "iteration 48 current loss: 0.0265584047883749 current acc: 0.7509324273310684\n",
      "iteration 49 current loss: 0.04332945868372917 current acc: 0.7510264150943396\n",
      "iteration 50 current loss: 0.06558886915445328 current acc: 0.7511203319502074\n",
      "iteration 51 current loss: 0.02757156454026699 current acc: 0.7512141779788839\n",
      "iteration 52 current loss: 0.03894155099987984 current acc: 0.7513079532604598\n",
      "iteration 53 current loss: 0.023275934159755707 current acc: 0.7514016578749058\n",
      "iteration 54 current loss: 0.028257539495825768 current acc: 0.7514952919020715\n",
      "iteration 55 current loss: 0.023488229140639305 current acc: 0.7515888554216867\n",
      "iteration 56 current loss: 0.030511701479554176 current acc: 0.7516823485133609\n",
      "iteration 57 current loss: 0.0441400520503521 current acc: 0.7517757712565839\n",
      "iteration 58 current loss: 0.03283853083848953 current acc: 0.7518691237307258\n",
      "iteration 59 current loss: 0.03769925236701965 current acc: 0.7519624060150376\n",
      "iteration 60 current loss: 0.027035975828766823 current acc: 0.7520556181886509\n",
      "iteration 61 current loss: 0.017822105437517166 current acc: 0.7521487603305785\n",
      "iteration 62 current loss: 0.026668112725019455 current acc: 0.7522418325197147\n",
      "iteration 63 current loss: 0.023760395124554634 current acc: 0.7523348348348349\n",
      "iteration 64 current loss: 0.030842192471027374 current acc: 0.7524277673545966\n",
      "iteration 65 current loss: 0.042924147099256516 current acc: 0.7525131282820705\n",
      "iteration 66 current loss: 0.0346846841275692 current acc: 0.7526059242594676\n",
      "iteration 67 current loss: 0.02128222957253456 current acc: 0.7526986506746627\n",
      "iteration 68 current loss: 0.037622515112161636 current acc: 0.7527913076058449\n",
      "iteration 69 current loss: 0.022640807554125786 current acc: 0.7528838951310861\n",
      "iteration 70 current loss: 0.019233720377087593 current acc: 0.7529764133283414\n",
      "iteration 71 current loss: 0.017967361956834793 current acc: 0.7530688622754491\n",
      "iteration 72 current loss: 0.04393647983670235 current acc: 0.7531612420501309\n",
      "iteration 73 current loss: 0.02572312392294407 current acc: 0.7532535527299925\n",
      "iteration 74 current loss: 0.02008199691772461 current acc: 0.7533457943925234\n",
      "iteration 75 current loss: 0.07313814759254456 current acc: 0.7534304932735426\n",
      "iteration 76 current loss: 0.03193045035004616 current acc: 0.7535225999252895\n",
      "iteration 77 current loss: 0.04495159536600113 current acc: 0.7536146377893951\n",
      "iteration 78 current loss: 0.03442312777042389 current acc: 0.7537066069428892\n",
      "iteration 79 current loss: 0.014214366674423218 current acc: 0.7537985074626866\n",
      "iteration 80 current loss: 0.03093268722295761 current acc: 0.7538903394255875\n",
      "iteration 81 current loss: 0.02570810355246067 current acc: 0.7539821029082774\n",
      "iteration 82 current loss: 0.0267267394810915 current acc: 0.7540737979873277\n",
      "iteration 83 current loss: 0.02955375425517559 current acc: 0.7541654247391952\n",
      "iteration 84 current loss: 0.0357842743396759 current acc: 0.7542569832402235\n",
      "iteration 85 current loss: 0.02835806831717491 current acc: 0.7543484735666418\n",
      "iteration 86 current loss: 0.03610730916261673 current acc: 0.7544398957945664\n",
      "iteration 87 current loss: 0.040709592401981354 current acc: 0.75453125\n",
      "iteration 88 current loss: 0.02437865175306797 current acc: 0.7546225362588322\n",
      "iteration 89 current loss: 0.02009056881070137 current acc: 0.7547137546468401\n",
      "iteration 90 current loss: 0.029836371541023254 current acc: 0.7548049052396878\n",
      "iteration 91 current loss: 0.019915079697966576 current acc: 0.7548959881129272\n",
      "iteration 92 current loss: 0.019107041880488396 current acc: 0.7549870033419978\n",
      "iteration 93 current loss: 0.029886983335018158 current acc: 0.7550779510022272\n",
      "iteration 94 current loss: 0.029863424599170685 current acc: 0.7551688311688312\n",
      "iteration 95 current loss: 0.02961602248251438 current acc: 0.7552596439169139\n",
      "iteration 96 current loss: 0.020654594525694847 current acc: 0.7553503893214683\n",
      "iteration 97 current loss: 0.022342020645737648 current acc: 0.7554410674573758\n",
      "iteration 98 current loss: 0.020055904984474182 current acc: 0.7555316783994072\n",
      "iteration 99 current loss: 0.02240404300391674 current acc: 0.7556222222222222\n",
      "\t\tEpoch 26/100 complete. Epoch loss 0.03158277152106166 Epoch accuracy 0.9986\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 26, Validation Accuracy: 0.627625, Validation Loss: 1.490268687158823\n",
      "best loss 0.03158277152106166\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.012371690012514591 current acc: 0.7557126990003702\n",
      "iteration 1 current loss: 0.015064460225403309 current acc: 0.7558031088082902\n",
      "iteration 2 current loss: 0.02464984357357025 current acc: 0.7558934517203107\n",
      "iteration 3 current loss: 0.037553571164608 current acc: 0.7559837278106509\n",
      "iteration 4 current loss: 0.01870802603662014 current acc: 0.7560739371534196\n",
      "iteration 5 current loss: 0.02864782139658928 current acc: 0.7561640798226164\n",
      "iteration 6 current loss: 0.018539724871516228 current acc: 0.7562541558921315\n",
      "iteration 7 current loss: 0.01612250506877899 current acc: 0.7563441654357459\n",
      "iteration 8 current loss: 0.041393257677555084 current acc: 0.7564267257290513\n",
      "iteration 9 current loss: 0.02176234871149063 current acc: 0.7565166051660517\n",
      "iteration 10 current loss: 0.011289495043456554 current acc: 0.7566064182958318\n",
      "iteration 11 current loss: 0.019060516729950905 current acc: 0.7566961651917404\n",
      "iteration 12 current loss: 0.014800764620304108 current acc: 0.756785845927018\n",
      "iteration 13 current loss: 0.01771552488207817 current acc: 0.7568754605747974\n",
      "iteration 14 current loss: 0.013509622775018215 current acc: 0.7569650092081032\n",
      "iteration 15 current loss: 0.013506699353456497 current acc: 0.7570544918998527\n",
      "iteration 16 current loss: 0.023054804652929306 current acc: 0.7571439087228561\n",
      "iteration 17 current loss: 0.013695323839783669 current acc: 0.757233259749816\n",
      "iteration 18 current loss: 0.019564848393201828 current acc: 0.7573225450533284\n",
      "iteration 19 current loss: 0.024228984490036964 current acc: 0.7574117647058823\n",
      "iteration 20 current loss: 0.012656155042350292 current acc: 0.7575009187798604\n",
      "iteration 21 current loss: 0.019723709672689438 current acc: 0.7575900073475386\n",
      "iteration 22 current loss: 0.02218220755457878 current acc: 0.7576790304810871\n",
      "iteration 23 current loss: 0.043734170496463776 current acc: 0.7577679882525697\n",
      "iteration 24 current loss: 0.022160273045301437 current acc: 0.757856880733945\n",
      "iteration 25 current loss: 0.020399272441864014 current acc: 0.7579457079970653\n",
      "iteration 26 current loss: 0.012316891923546791 current acc: 0.7580344701136781\n",
      "iteration 27 current loss: 0.015487431548535824 current acc: 0.7581231671554253\n",
      "iteration 28 current loss: 0.013533695600926876 current acc: 0.7582117991938438\n",
      "iteration 29 current loss: 0.009403757750988007 current acc: 0.7583003663003663\n",
      "iteration 30 current loss: 0.014650369063019753 current acc: 0.75838886854632\n",
      "iteration 31 current loss: 0.020229248329997063 current acc: 0.7584773060029283\n",
      "iteration 32 current loss: 0.014973532408475876 current acc: 0.7585656787413099\n",
      "iteration 33 current loss: 0.02418782189488411 current acc: 0.7586539868324799\n",
      "iteration 34 current loss: 0.022260425612330437 current acc: 0.7587422303473492\n",
      "iteration 35 current loss: 0.02241268940269947 current acc: 0.7588304093567252\n",
      "iteration 36 current loss: 0.012851403094828129 current acc: 0.7589185239313117\n",
      "iteration 37 current loss: 0.009134293533861637 current acc: 0.7590065741417092\n",
      "iteration 38 current loss: 0.017830532044172287 current acc: 0.7590945600584155\n",
      "iteration 39 current loss: 0.010790989734232426 current acc: 0.7591824817518248\n",
      "iteration 40 current loss: 0.013318211771547794 current acc: 0.7592703392922291\n",
      "iteration 41 current loss: 0.012236970476806164 current acc: 0.7593581327498177\n",
      "iteration 42 current loss: 0.03119124099612236 current acc: 0.7594458621946774\n",
      "iteration 43 current loss: 0.015419994480907917 current acc: 0.759533527696793\n",
      "iteration 44 current loss: 0.033869531005620956 current acc: 0.7596211293260473\n",
      "iteration 45 current loss: 0.011497294530272484 current acc: 0.7597086671522214\n",
      "iteration 46 current loss: 0.018368752673268318 current acc: 0.7597961412449945\n",
      "iteration 47 current loss: 0.015229694545269012 current acc: 0.7598835516739447\n",
      "iteration 48 current loss: 0.015383967198431492 current acc: 0.7599708985085486\n",
      "iteration 49 current loss: 0.024529241025447845 current acc: 0.7600581818181819\n",
      "iteration 50 current loss: 0.03255479410290718 current acc: 0.7601454016721192\n",
      "iteration 51 current loss: 0.018285121768712997 current acc: 0.7602325581395348\n",
      "iteration 52 current loss: 0.027992790564894676 current acc: 0.7603196512895024\n",
      "iteration 53 current loss: 0.0206757802516222 current acc: 0.760406681190995\n",
      "iteration 54 current loss: 0.013936416245996952 current acc: 0.7604936479128857\n",
      "iteration 55 current loss: 0.013970976695418358 current acc: 0.7605805515239478\n",
      "iteration 56 current loss: 0.016982577741146088 current acc: 0.7606673920928545\n",
      "iteration 57 current loss: 0.01545881386846304 current acc: 0.7607541696881799\n",
      "iteration 58 current loss: 0.01904710941016674 current acc: 0.7608408843783979\n",
      "iteration 59 current loss: 0.02189665101468563 current acc: 0.760927536231884\n",
      "iteration 60 current loss: 0.010626516304910183 current acc: 0.7610141253169141\n",
      "iteration 61 current loss: 0.012701939791440964 current acc: 0.7611006517016654\n",
      "iteration 62 current loss: 0.017556361854076385 current acc: 0.7611871154542165\n",
      "iteration 63 current loss: 0.013766102492809296 current acc: 0.761273516642547\n",
      "iteration 64 current loss: 0.024601414799690247 current acc: 0.7613598553345389\n",
      "iteration 65 current loss: 0.03103102743625641 current acc: 0.7614461315979754\n",
      "iteration 66 current loss: 0.02112969011068344 current acc: 0.761532345500542\n",
      "iteration 67 current loss: 0.01545082125812769 current acc: 0.7616184971098265\n",
      "iteration 68 current loss: 0.012913831509649754 current acc: 0.7617045864933188\n",
      "iteration 69 current loss: 0.01284366101026535 current acc: 0.7617906137184115\n",
      "iteration 70 current loss: 0.01646329089999199 current acc: 0.7618765788523999\n",
      "iteration 71 current loss: 0.011559341102838516 current acc: 0.761962481962482\n",
      "iteration 72 current loss: 0.03437166288495064 current acc: 0.7620483231157591\n",
      "iteration 73 current loss: 0.012846368364989758 current acc: 0.7621341023792357\n",
      "iteration 74 current loss: 0.015444193035364151 current acc: 0.7622198198198198\n",
      "iteration 75 current loss: 0.017733555287122726 current acc: 0.7623054755043228\n",
      "iteration 76 current loss: 0.013764245435595512 current acc: 0.7623910694994599\n",
      "iteration 77 current loss: 0.016948774456977844 current acc: 0.7624766018718503\n",
      "iteration 78 current loss: 0.01721208542585373 current acc: 0.7625620726880172\n",
      "iteration 79 current loss: 0.02819468453526497 current acc: 0.7626474820143885\n",
      "iteration 80 current loss: 0.021124770864844322 current acc: 0.762732829917296\n",
      "iteration 81 current loss: 0.022129595279693604 current acc: 0.7628181164629763\n",
      "iteration 82 current loss: 0.016701729968190193 current acc: 0.762903341717571\n",
      "iteration 83 current loss: 0.022518113255500793 current acc: 0.7629885057471264\n",
      "iteration 84 current loss: 0.012297552078962326 current acc: 0.7630736086175942\n",
      "iteration 85 current loss: 0.013718115165829659 current acc: 0.7631586503948313\n",
      "iteration 86 current loss: 0.016613200306892395 current acc: 0.7632436311445999\n",
      "iteration 87 current loss: 0.015385241247713566 current acc: 0.7633285509325681\n",
      "iteration 88 current loss: 0.03823636472225189 current acc: 0.7634062387952671\n",
      "iteration 89 current loss: 0.022510547190904617 current acc: 0.7634910394265233\n",
      "iteration 90 current loss: 0.01277773454785347 current acc: 0.7635757792905769\n",
      "iteration 91 current loss: 0.013476509600877762 current acc: 0.763660458452722\n",
      "iteration 92 current loss: 0.023169012740254402 current acc: 0.7637450769781596\n",
      "iteration 93 current loss: 0.018296703696250916 current acc: 0.7638296349319972\n",
      "iteration 94 current loss: 0.016353070735931396 current acc: 0.7639141323792487\n",
      "iteration 95 current loss: 0.04992098733782768 current acc: 0.7639914163090129\n",
      "iteration 96 current loss: 0.026145756244659424 current acc: 0.7640757954951733\n",
      "iteration 97 current loss: 0.011936026625335217 current acc: 0.7641601143674053\n",
      "iteration 98 current loss: 0.02393759787082672 current acc: 0.7642443729903537\n",
      "iteration 99 current loss: 0.020669035613536835 current acc: 0.7643285714285715\n",
      "\t\tEpoch 27/100 complete. Epoch loss 0.01933081867173314 Epoch accuracy 0.9994\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 27, Validation Accuracy: 0.6245, Validation Loss: 1.5629165310412645\n",
      "best loss 0.01933081867173314\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.018542582169175148 current acc: 0.7644127097465191\n",
      "iteration 1 current loss: 0.021476974710822105 current acc: 0.7644967880085654\n",
      "iteration 2 current loss: 0.0663772001862526 current acc: 0.7645736710667143\n",
      "iteration 3 current loss: 0.01800379529595375 current acc: 0.7646576319543509\n",
      "iteration 4 current loss: 0.015913065522909164 current acc: 0.7647415329768271\n",
      "iteration 5 current loss: 0.016445176675915718 current acc: 0.7648253741981468\n",
      "iteration 6 current loss: 0.022002816200256348 current acc: 0.7649091556822231\n",
      "iteration 7 current loss: 0.025759929791092873 current acc: 0.7649928774928775\n",
      "iteration 8 current loss: 0.024633390828967094 current acc: 0.7650765396938413\n",
      "iteration 9 current loss: 0.01197817362844944 current acc: 0.7651601423487544\n",
      "iteration 10 current loss: 0.010337828658521175 current acc: 0.7652436855211668\n",
      "iteration 11 current loss: 0.019345229491591454 current acc: 0.7653271692745377\n",
      "iteration 12 current loss: 0.01674998737871647 current acc: 0.7654105936722361\n",
      "iteration 13 current loss: 0.024755291640758514 current acc: 0.7654939587775409\n",
      "iteration 14 current loss: 0.010319712571799755 current acc: 0.7655772646536412\n",
      "iteration 15 current loss: 0.01967083290219307 current acc: 0.7656605113636363\n",
      "iteration 16 current loss: 0.01618999056518078 current acc: 0.7657436989705361\n",
      "iteration 17 current loss: 0.016753125935792923 current acc: 0.7658268275372605\n",
      "iteration 18 current loss: 0.02205088920891285 current acc: 0.7659098971266407\n",
      "iteration 19 current loss: 0.02330148033797741 current acc: 0.7659929078014185\n",
      "iteration 20 current loss: 0.02609025500714779 current acc: 0.7660758596242467\n",
      "iteration 21 current loss: 0.014736873097717762 current acc: 0.7661587526576896\n",
      "iteration 22 current loss: 0.02264815755188465 current acc: 0.7662415869642225\n",
      "iteration 23 current loss: 0.011825169436633587 current acc: 0.7663243626062323\n",
      "iteration 24 current loss: 0.011035313829779625 current acc: 0.7664070796460177\n",
      "iteration 25 current loss: 0.011419722810387611 current acc: 0.7664897381457891\n",
      "iteration 26 current loss: 0.013525374233722687 current acc: 0.7665723381676689\n",
      "iteration 27 current loss: 0.0114396121352911 current acc: 0.7666548797736916\n",
      "iteration 28 current loss: 0.048256710171699524 current acc: 0.7667373630258042\n",
      "iteration 29 current loss: 0.014636319130659103 current acc: 0.7668197879858657\n",
      "iteration 30 current loss: 0.01991528831422329 current acc: 0.7669021547156482\n",
      "iteration 31 current loss: 0.036064982414245605 current acc: 0.7669844632768361\n",
      "iteration 32 current loss: 0.011427853256464005 current acc: 0.7670667137310272\n",
      "iteration 33 current loss: 0.011401101015508175 current acc: 0.7671489061397319\n",
      "iteration 34 current loss: 0.015161871910095215 current acc: 0.7672310405643739\n",
      "iteration 35 current loss: 0.01204331498593092 current acc: 0.7673131170662906\n",
      "iteration 36 current loss: 0.010212835855782032 current acc: 0.7673951357067325\n",
      "iteration 37 current loss: 0.013403790071606636 current acc: 0.767477096546864\n",
      "iteration 38 current loss: 0.017712702974677086 current acc: 0.7675589996477633\n",
      "iteration 39 current loss: 0.017050262540578842 current acc: 0.7676408450704225\n",
      "iteration 40 current loss: 0.011320017278194427 current acc: 0.7677226328757479\n",
      "iteration 41 current loss: 0.015381526201963425 current acc: 0.7678043631245601\n",
      "iteration 42 current loss: 0.019241688773036003 current acc: 0.7678860358775941\n",
      "iteration 43 current loss: 0.012916752137243748 current acc: 0.7679676511954993\n",
      "iteration 44 current loss: 0.012214945629239082 current acc: 0.7680492091388401\n",
      "iteration 45 current loss: 0.020569810643792152 current acc: 0.7681307097680956\n",
      "iteration 46 current loss: 0.009713309817016125 current acc: 0.76821215314366\n",
      "iteration 47 current loss: 0.005681810434907675 current acc: 0.7682935393258427\n",
      "iteration 48 current loss: 0.017539674416184425 current acc: 0.7683748683748683\n",
      "iteration 49 current loss: 0.014137320220470428 current acc: 0.7684561403508772\n",
      "iteration 50 current loss: 0.020664310082793236 current acc: 0.768537355313925\n",
      "iteration 51 current loss: 0.011409788392484188 current acc: 0.7686185133239831\n",
      "iteration 52 current loss: 0.0250336192548275 current acc: 0.7686996144409394\n",
      "iteration 53 current loss: 0.010755334980785847 current acc: 0.7687806587245971\n",
      "iteration 54 current loss: 0.018434423953294754 current acc: 0.768861646234676\n",
      "iteration 55 current loss: 0.007739005144685507 current acc: 0.7689425770308124\n",
      "iteration 56 current loss: 0.0137969134375453 current acc: 0.7690234511725587\n",
      "iteration 57 current loss: 0.011963619850575924 current acc: 0.7691042687193842\n",
      "iteration 58 current loss: 0.017580153420567513 current acc: 0.769185029730675\n",
      "iteration 59 current loss: 0.010719082318246365 current acc: 0.7692657342657343\n",
      "iteration 60 current loss: 0.020069247111678123 current acc: 0.769346382383782\n",
      "iteration 61 current loss: 0.016978207975625992 current acc: 0.7694269741439552\n",
      "iteration 62 current loss: 0.027582485228776932 current acc: 0.7695075096053091\n",
      "iteration 63 current loss: 0.012004323303699493 current acc: 0.7695879888268157\n",
      "iteration 64 current loss: 0.025045137852430344 current acc: 0.7696684118673648\n",
      "iteration 65 current loss: 0.020276974886655807 current acc: 0.7697487787857641\n",
      "iteration 66 current loss: 0.014930604957044125 current acc: 0.7698290896407395\n",
      "iteration 67 current loss: 0.010281778872013092 current acc: 0.7699093444909344\n",
      "iteration 68 current loss: 0.013267475180327892 current acc: 0.7699895433949111\n",
      "iteration 69 current loss: 0.008064634166657925 current acc: 0.7700696864111498\n",
      "iteration 70 current loss: 0.01334304828196764 current acc: 0.7701497735980495\n",
      "iteration 71 current loss: 0.04339253902435303 current acc: 0.7702228412256268\n",
      "iteration 72 current loss: 0.01833127811551094 current acc: 0.7703028193525931\n",
      "iteration 73 current loss: 0.03285038471221924 current acc: 0.7703827418232428\n",
      "iteration 74 current loss: 0.014910268597304821 current acc: 0.7704626086956522\n",
      "iteration 75 current loss: 0.009569053538143635 current acc: 0.7705424200278164\n",
      "iteration 76 current loss: 0.01361630205065012 current acc: 0.7706221758776504\n",
      "iteration 77 current loss: 0.018415572121739388 current acc: 0.7707018763029881\n",
      "iteration 78 current loss: 0.01861623488366604 current acc: 0.7707815213615838\n",
      "iteration 79 current loss: 0.012842167168855667 current acc: 0.7708611111111111\n",
      "iteration 80 current loss: 0.01146951224654913 current acc: 0.7709406456091635\n",
      "iteration 81 current loss: 0.049918852746486664 current acc: 0.7710131852879945\n",
      "iteration 82 current loss: 0.022141676396131516 current acc: 0.7710926118626431\n",
      "iteration 83 current loss: 0.018104897812008858 current acc: 0.7711719833564494\n",
      "iteration 84 current loss: 0.03005528636276722 current acc: 0.7712512998266898\n",
      "iteration 85 current loss: 0.018758848309516907 current acc: 0.7713305613305613\n",
      "iteration 86 current loss: 0.023270776495337486 current acc: 0.7714097679251819\n",
      "iteration 87 current loss: 0.03557347133755684 current acc: 0.77148891966759\n",
      "iteration 88 current loss: 0.0421111136674881 current acc: 0.7715610938040844\n",
      "iteration 89 current loss: 0.018033260479569435 current acc: 0.7716401384083045\n",
      "iteration 90 current loss: 0.013360930606722832 current acc: 0.7717191283292978\n",
      "iteration 91 current loss: 0.027373628690838814 current acc: 0.7717980636237898\n",
      "iteration 92 current loss: 0.02690889872610569 current acc: 0.7718769443484272\n",
      "iteration 93 current loss: 0.01574944332242012 current acc: 0.7719557705597788\n",
      "iteration 94 current loss: 0.023243330419063568 current acc: 0.7720345423143351\n",
      "iteration 95 current loss: 0.016863157972693443 current acc: 0.7721132596685083\n",
      "iteration 96 current loss: 0.008789323270320892 current acc: 0.772191922678633\n",
      "iteration 97 current loss: 0.027062945067882538 current acc: 0.7722705314009661\n",
      "iteration 98 current loss: 0.021713970229029655 current acc: 0.7723490858916868\n",
      "iteration 99 current loss: 0.017451468855142593 current acc: 0.7724275862068966\n",
      "\t\tEpoch 28/100 complete. Epoch loss 0.0188977060187608 Epoch accuracy 0.9992\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 28, Validation Accuracy: 0.6215, Validation Loss: 1.606816253811121\n",
      "best loss 0.0188977060187608\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.01715397834777832 current acc: 0.7725060324026198\n",
      "iteration 1 current loss: 0.015390746295452118 current acc: 0.7725844245348036\n",
      "iteration 2 current loss: 0.01277242973446846 current acc: 0.772662762659318\n",
      "iteration 3 current loss: 0.012047011405229568 current acc: 0.7727410468319559\n",
      "iteration 4 current loss: 0.012794259004294872 current acc: 0.7728192771084338\n",
      "iteration 5 current loss: 0.010865914635360241 current acc: 0.7728974535443909\n",
      "iteration 6 current loss: 0.010495254769921303 current acc: 0.7729755761953905\n",
      "iteration 7 current loss: 0.008487772196531296 current acc: 0.7730536451169189\n",
      "iteration 8 current loss: 0.01470309030264616 current acc: 0.7731316603643864\n",
      "iteration 9 current loss: 0.008115694858133793 current acc: 0.7732096219931271\n",
      "iteration 10 current loss: 0.018606310710310936 current acc: 0.7732875300583992\n",
      "iteration 11 current loss: 0.01588892750442028 current acc: 0.7733653846153846\n",
      "iteration 12 current loss: 0.020649943500757217 current acc: 0.7734431857191898\n",
      "iteration 13 current loss: 0.01587483659386635 current acc: 0.7735209334248456\n",
      "iteration 14 current loss: 0.0073951478116214275 current acc: 0.773598627787307\n",
      "iteration 15 current loss: 0.011169392615556717 current acc: 0.773676268861454\n",
      "iteration 16 current loss: 0.026234157383441925 current acc: 0.7737538567020912\n",
      "iteration 17 current loss: 0.006319792941212654 current acc: 0.773831391363948\n",
      "iteration 18 current loss: 0.011315498501062393 current acc: 0.7739088729016786\n",
      "iteration 19 current loss: 0.017224838957190514 current acc: 0.773986301369863\n",
      "iteration 20 current loss: 0.015300632454454899 current acc: 0.7740636768230058\n",
      "iteration 21 current loss: 0.010790028609335423 current acc: 0.7741409993155373\n",
      "iteration 22 current loss: 0.010216685943305492 current acc: 0.7742182689018132\n",
      "iteration 23 current loss: 0.010669524781405926 current acc: 0.7742954856361149\n",
      "iteration 24 current loss: 0.012124481610953808 current acc: 0.7743726495726496\n",
      "iteration 25 current loss: 0.008948346599936485 current acc: 0.7744497607655503\n",
      "iteration 26 current loss: 0.012141886167228222 current acc: 0.774526819268876\n",
      "iteration 27 current loss: 0.01338818110525608 current acc: 0.774603825136612\n",
      "iteration 28 current loss: 0.024431724101305008 current acc: 0.7746807784226698\n",
      "iteration 29 current loss: 0.010488968342542648 current acc: 0.7747576791808873\n",
      "iteration 30 current loss: 0.007915346883237362 current acc: 0.774834527465029\n",
      "iteration 31 current loss: 0.009741337969899178 current acc: 0.7749113233287858\n",
      "iteration 32 current loss: 0.013764135539531708 current acc: 0.7749880668257757\n",
      "iteration 33 current loss: 0.010676976293325424 current acc: 0.7750647580095433\n",
      "iteration 34 current loss: 0.01083429902791977 current acc: 0.7751413969335604\n",
      "iteration 35 current loss: 0.012634193524718285 current acc: 0.7752179836512262\n",
      "iteration 36 current loss: 0.010718404315412045 current acc: 0.7752945182158665\n",
      "iteration 37 current loss: 0.01053191814571619 current acc: 0.7753710006807352\n",
      "iteration 38 current loss: 0.010099690407514572 current acc: 0.7754474310990133\n",
      "iteration 39 current loss: 0.010818054899573326 current acc: 0.7755238095238095\n",
      "iteration 40 current loss: 0.016984617337584496 current acc: 0.7756001360081605\n",
      "iteration 41 current loss: 0.007845055311918259 current acc: 0.7756764106050306\n",
      "iteration 42 current loss: 0.012159892357885838 current acc: 0.7757526333673123\n",
      "iteration 43 current loss: 0.006945592816919088 current acc: 0.7758288043478261\n",
      "iteration 44 current loss: 0.014886674471199512 current acc: 0.7759049235993208\n",
      "iteration 45 current loss: 0.013652610592544079 current acc: 0.7759809911744738\n",
      "iteration 46 current loss: 0.010915623977780342 current acc: 0.7760570071258908\n",
      "iteration 47 current loss: 0.011065023019909859 current acc: 0.7761329715061058\n",
      "iteration 48 current loss: 0.011797148734331131 current acc: 0.7762088843675823\n",
      "iteration 49 current loss: 0.008840576745569706 current acc: 0.7762847457627119\n",
      "iteration 50 current loss: 0.011332315392792225 current acc: 0.7763605557438157\n",
      "iteration 51 current loss: 0.007141417823731899 current acc: 0.7764363143631436\n",
      "iteration 52 current loss: 0.00949246808886528 current acc: 0.776512021672875\n",
      "iteration 53 current loss: 0.009205808863043785 current acc: 0.7765876777251185\n",
      "iteration 54 current loss: 0.006715606898069382 current acc: 0.776663282571912\n",
      "iteration 55 current loss: 0.010574284009635448 current acc: 0.7767388362652232\n",
      "iteration 56 current loss: 0.016281645745038986 current acc: 0.7768143388569496\n",
      "iteration 57 current loss: 0.008341409265995026 current acc: 0.7768897903989181\n",
      "iteration 58 current loss: 0.012215575203299522 current acc: 0.7769651909428861\n",
      "iteration 59 current loss: 0.006274008192121983 current acc: 0.7770405405405405\n",
      "iteration 60 current loss: 0.011725171469151974 current acc: 0.7771158392434988\n",
      "iteration 61 current loss: 0.01048907171934843 current acc: 0.7771910871033085\n",
      "iteration 62 current loss: 0.010230408981442451 current acc: 0.7772662841714478\n",
      "iteration 63 current loss: 0.015276198275387287 current acc: 0.7773414304993252\n",
      "iteration 64 current loss: 0.01524142362177372 current acc: 0.7774165261382799\n",
      "iteration 65 current loss: 0.016941344365477562 current acc: 0.7774915711395819\n",
      "iteration 66 current loss: 0.010505413636565208 current acc: 0.777566565554432\n",
      "iteration 67 current loss: 0.010614071972668171 current acc: 0.7776415094339623\n",
      "iteration 68 current loss: 0.008737784810364246 current acc: 0.7777164028292355\n",
      "iteration 69 current loss: 0.016812212765216827 current acc: 0.7777912457912458\n",
      "iteration 70 current loss: 0.008710494264960289 current acc: 0.7778660383709188\n",
      "iteration 71 current loss: 0.007091392297297716 current acc: 0.7779407806191118\n",
      "iteration 72 current loss: 0.008764798752963543 current acc: 0.7780154725866129\n",
      "iteration 73 current loss: 0.010686608962714672 current acc: 0.7780901143241425\n",
      "iteration 74 current loss: 0.00894396286457777 current acc: 0.778164705882353\n",
      "iteration 75 current loss: 0.008637682534754276 current acc: 0.7782392473118279\n",
      "iteration 76 current loss: 0.009072929620742798 current acc: 0.7783137386630836\n",
      "iteration 77 current loss: 0.014724881388247013 current acc: 0.7783881799865682\n",
      "iteration 78 current loss: 0.012034864164888859 current acc: 0.778462571332662\n",
      "iteration 79 current loss: 0.013827038928866386 current acc: 0.7785369127516778\n",
      "iteration 80 current loss: 0.00846034474670887 current acc: 0.7786112042938611\n",
      "iteration 81 current loss: 0.01306146290153265 current acc: 0.7786854460093897\n",
      "iteration 82 current loss: 0.012256573885679245 current acc: 0.7787596379483741\n",
      "iteration 83 current loss: 0.00657305121421814 current acc: 0.7788337801608579\n",
      "iteration 84 current loss: 0.03992815688252449 current acc: 0.7789078726968174\n",
      "iteration 85 current loss: 0.005919429007917643 current acc: 0.7789819156061621\n",
      "iteration 86 current loss: 0.00859779678285122 current acc: 0.7790559089387346\n",
      "iteration 87 current loss: 0.009763569571077824 current acc: 0.7791298527443106\n",
      "iteration 88 current loss: 0.014993790537118912 current acc: 0.7792037470725995\n",
      "iteration 89 current loss: 0.009975902736186981 current acc: 0.7792775919732442\n",
      "iteration 90 current loss: 0.006881956476718187 current acc: 0.7793513874958208\n",
      "iteration 91 current loss: 0.009744527749717236 current acc: 0.7794251336898396\n",
      "iteration 92 current loss: 0.012631280347704887 current acc: 0.7794988306047445\n",
      "iteration 93 current loss: 0.016967186704277992 current acc: 0.7795724782899132\n",
      "iteration 94 current loss: 0.020315533503890038 current acc: 0.7796460767946578\n",
      "iteration 95 current loss: 0.01638660952448845 current acc: 0.7797196261682243\n",
      "iteration 96 current loss: 0.012455383315682411 current acc: 0.7797931264597932\n",
      "iteration 97 current loss: 0.02241477183997631 current acc: 0.779866577718479\n",
      "iteration 98 current loss: 0.011509031988680363 current acc: 0.7799399799933311\n",
      "iteration 99 current loss: 0.00740736024454236 current acc: 0.7800133333333333\n",
      "\t\tEpoch 29/100 complete. Epoch loss 0.012177186449989677 Epoch accuracy 1.0\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 29, Validation Accuracy: 0.630625, Validation Loss: 1.5590905856341124\n",
      "best loss 0.012177186449989677\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.00737939216196537 current acc: 0.7800866377874042\n",
      "iteration 1 current loss: 0.007616931106895208 current acc: 0.7801598934043971\n",
      "iteration 2 current loss: 0.00583096407353878 current acc: 0.7802331002331002\n",
      "iteration 3 current loss: 0.006760324351489544 current acc: 0.780306258322237\n",
      "iteration 4 current loss: 0.009864594787359238 current acc: 0.7803793677204659\n",
      "iteration 5 current loss: 0.014121809974312782 current acc: 0.7804524284763805\n",
      "iteration 6 current loss: 0.01223994605243206 current acc: 0.7805254406385101\n",
      "iteration 7 current loss: 0.005913136061280966 current acc: 0.7805984042553191\n",
      "iteration 8 current loss: 0.012304937466979027 current acc: 0.7806713193752077\n",
      "iteration 9 current loss: 0.008759436197578907 current acc: 0.7807441860465116\n",
      "iteration 10 current loss: 0.006156629417091608 current acc: 0.7808170043175024\n",
      "iteration 11 current loss: 0.009263587184250355 current acc: 0.7808897742363878\n",
      "iteration 12 current loss: 0.010529999621212482 current acc: 0.780962495851311\n",
      "iteration 13 current loss: 0.007622617296874523 current acc: 0.7810351692103517\n",
      "iteration 14 current loss: 0.012364260852336884 current acc: 0.7811077943615257\n",
      "iteration 15 current loss: 0.007604868616908789 current acc: 0.7811803713527852\n",
      "iteration 16 current loss: 0.008360815234482288 current acc: 0.7812529002320185\n",
      "iteration 17 current loss: 0.006937870290130377 current acc: 0.7813253810470511\n",
      "iteration 18 current loss: 0.005262353457510471 current acc: 0.7813978138456442\n",
      "iteration 19 current loss: 0.009276922792196274 current acc: 0.7814701986754967\n",
      "iteration 20 current loss: 0.006152835674583912 current acc: 0.7815425355842436\n",
      "iteration 21 current loss: 0.006714177783578634 current acc: 0.7816148246194573\n",
      "iteration 22 current loss: 0.015256074257194996 current acc: 0.781687065828647\n",
      "iteration 23 current loss: 0.006248312070965767 current acc: 0.7817592592592593\n",
      "iteration 24 current loss: 0.007544579915702343 current acc: 0.7818314049586776\n",
      "iteration 25 current loss: 0.00797993689775467 current acc: 0.7819035029742234\n",
      "iteration 26 current loss: 0.007580257020890713 current acc: 0.7819755533531549\n",
      "iteration 27 current loss: 0.01454946305602789 current acc: 0.7820475561426684\n",
      "iteration 28 current loss: 0.007224192842841148 current acc: 0.7821195113898977\n",
      "iteration 29 current loss: 0.00996010098606348 current acc: 0.7821914191419141\n",
      "iteration 30 current loss: 0.014923453330993652 current acc: 0.7822632794457275\n",
      "iteration 31 current loss: 0.015024405904114246 current acc: 0.7823350923482849\n",
      "iteration 32 current loss: 0.005791220813989639 current acc: 0.7824068578964721\n",
      "iteration 33 current loss: 0.007372426800429821 current acc: 0.7824785761371127\n",
      "iteration 34 current loss: 0.012423254549503326 current acc: 0.7825502471169687\n",
      "iteration 35 current loss: 0.008336902596056461 current acc: 0.7826218708827405\n",
      "iteration 36 current loss: 0.00814721267670393 current acc: 0.7826934474810668\n",
      "iteration 37 current loss: 0.006820885464549065 current acc: 0.7827649769585253\n",
      "iteration 38 current loss: 0.009436585009098053 current acc: 0.7828364593616322\n",
      "iteration 39 current loss: 0.008173367008566856 current acc: 0.7829078947368421\n",
      "iteration 40 current loss: 0.010012054815888405 current acc: 0.7829792831305492\n",
      "iteration 41 current loss: 0.01636253483593464 current acc: 0.7830506245890861\n",
      "iteration 42 current loss: 0.006985682062804699 current acc: 0.783121919158725\n",
      "iteration 43 current loss: 0.006438306532800198 current acc: 0.7831931668856768\n",
      "iteration 44 current loss: 0.005492187570780516 current acc: 0.783264367816092\n",
      "iteration 45 current loss: 0.011189332231879234 current acc: 0.7833355219960604\n",
      "iteration 46 current loss: 0.009353332221508026 current acc: 0.7834066294716114\n",
      "iteration 47 current loss: 0.0074424161575734615 current acc: 0.783477690288714\n",
      "iteration 48 current loss: 0.008783107623457909 current acc: 0.7835487044932765\n",
      "iteration 49 current loss: 0.00862945057451725 current acc: 0.7836196721311476\n",
      "iteration 50 current loss: 0.00902857631444931 current acc: 0.7836905932481154\n",
      "iteration 51 current loss: 0.01443831529468298 current acc: 0.7837614678899083\n",
      "iteration 52 current loss: 0.008434474468231201 current acc: 0.7838322961021945\n",
      "iteration 53 current loss: 0.007100258022546768 current acc: 0.7839030779305829\n",
      "iteration 54 current loss: 0.005242743529379368 current acc: 0.7839738134206219\n",
      "iteration 55 current loss: 0.016587533056735992 current acc: 0.784044502617801\n",
      "iteration 56 current loss: 0.015928693115711212 current acc: 0.7841151455675499\n",
      "iteration 57 current loss: 0.007556159049272537 current acc: 0.7841857423152387\n",
      "iteration 58 current loss: 0.009842424653470516 current acc: 0.7842562929061785\n",
      "iteration 59 current loss: 0.004775300156325102 current acc: 0.7843267973856209\n",
      "iteration 60 current loss: 0.010749448090791702 current acc: 0.7843972557987586\n",
      "iteration 61 current loss: 0.01347555872052908 current acc: 0.784467668190725\n",
      "iteration 62 current loss: 0.018192511051893234 current acc: 0.7845380346065949\n",
      "iteration 63 current loss: 0.007575250696390867 current acc: 0.7846083550913838\n",
      "iteration 64 current loss: 0.006553791929036379 current acc: 0.784678629690049\n",
      "iteration 65 current loss: 0.03621822968125343 current acc: 0.7847423352902805\n",
      "iteration 66 current loss: 0.008620326407253742 current acc: 0.7848125203782198\n",
      "iteration 67 current loss: 0.008638158440589905 current acc: 0.7848826597131682\n",
      "iteration 68 current loss: 0.012053086422383785 current acc: 0.7849527533398502\n",
      "iteration 69 current loss: 0.011906009167432785 current acc: 0.7850228013029316\n",
      "iteration 70 current loss: 0.0069193607196211815 current acc: 0.7850928036470205\n",
      "iteration 71 current loss: 0.04391852766275406 current acc: 0.78515625\n",
      "iteration 72 current loss: 0.011908574029803276 current acc: 0.7852261633582818\n",
      "iteration 73 current loss: 0.011858928948640823 current acc: 0.7852960312296682\n",
      "iteration 74 current loss: 0.008652010932564735 current acc: 0.7853658536585366\n",
      "iteration 75 current loss: 0.010679618455469608 current acc: 0.7854356306892067\n",
      "iteration 76 current loss: 0.016792988404631615 current acc: 0.7855053623659408\n",
      "iteration 77 current loss: 0.010049373842775822 current acc: 0.7855750487329435\n",
      "iteration 78 current loss: 0.010156146250665188 current acc: 0.7856446898343618\n",
      "iteration 79 current loss: 0.008194290101528168 current acc: 0.7857142857142857\n",
      "iteration 80 current loss: 0.008942698128521442 current acc: 0.7857838364167478\n",
      "iteration 81 current loss: 0.007273441180586815 current acc: 0.7858533419857235\n",
      "iteration 82 current loss: 0.009680830873548985 current acc: 0.7859228024651314\n",
      "iteration 83 current loss: 0.007911945693194866 current acc: 0.7859922178988327\n",
      "iteration 84 current loss: 0.009042983874678612 current acc: 0.7860615883306321\n",
      "iteration 85 current loss: 0.006061552092432976 current acc: 0.7861309138042774\n",
      "iteration 86 current loss: 0.012335146777331829 current acc: 0.7862001943634597\n",
      "iteration 87 current loss: 0.007948726415634155 current acc: 0.7862694300518135\n",
      "iteration 88 current loss: 0.005427315831184387 current acc: 0.7863386209129168\n",
      "iteration 89 current loss: 0.007274623028934002 current acc: 0.7864077669902912\n",
      "iteration 90 current loss: 0.005771083757281303 current acc: 0.7864768683274022\n",
      "iteration 91 current loss: 0.007299771532416344 current acc: 0.7865459249676585\n",
      "iteration 92 current loss: 0.00885800551623106 current acc: 0.7866149369544132\n",
      "iteration 93 current loss: 0.008478096686303616 current acc: 0.7866839043309631\n",
      "iteration 94 current loss: 0.022417403757572174 current acc: 0.7867528271405493\n",
      "iteration 95 current loss: 0.013601127080619335 current acc: 0.7868217054263565\n",
      "iteration 96 current loss: 0.010485981591045856 current acc: 0.7868905392315144\n",
      "iteration 97 current loss: 0.00842585414648056 current acc: 0.7869593285990962\n",
      "iteration 98 current loss: 0.01858425699174404 current acc: 0.78702807357212\n",
      "iteration 99 current loss: 0.007202469278126955 current acc: 0.7870967741935484\n",
      "\t\tEpoch 30/100 complete. Epoch loss 0.010155894281342625 Epoch accuracy 0.9996\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 30, Validation Accuracy: 0.63075, Validation Loss: 1.5844599075615406\n",
      "best loss 0.010155894281342625\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.010706627741456032 current acc: 0.7871654305062883\n",
      "iteration 1 current loss: 0.008648703806102276 current acc: 0.7872340425531915\n",
      "iteration 2 current loss: 0.0053698038682341576 current acc: 0.7873026103770545\n",
      "iteration 3 current loss: 0.007399676367640495 current acc: 0.7873711340206185\n",
      "iteration 4 current loss: 0.009309517219662666 current acc: 0.7874396135265701\n",
      "iteration 5 current loss: 0.003655159380286932 current acc: 0.7875080489375402\n",
      "iteration 6 current loss: 0.009476064704358578 current acc: 0.7875764402961055\n",
      "iteration 7 current loss: 0.005783306434750557 current acc: 0.7876447876447876\n",
      "iteration 8 current loss: 0.007878162898123264 current acc: 0.7877130910260534\n",
      "iteration 9 current loss: 0.006590615026652813 current acc: 0.7877813504823151\n",
      "iteration 10 current loss: 0.00818504486232996 current acc: 0.7878495660559306\n",
      "iteration 11 current loss: 0.00815591961145401 current acc: 0.787917737789203\n",
      "iteration 12 current loss: 0.008960884064435959 current acc: 0.7879858657243817\n",
      "iteration 13 current loss: 0.00498676672577858 current acc: 0.7880539499036608\n",
      "iteration 14 current loss: 0.005575200542807579 current acc: 0.7881219903691814\n",
      "iteration 15 current loss: 0.00826207920908928 current acc: 0.7881899871630296\n",
      "iteration 16 current loss: 0.007063339930027723 current acc: 0.7882579403272377\n",
      "iteration 17 current loss: 0.008593359030783176 current acc: 0.7883258499037845\n",
      "iteration 18 current loss: 0.007044996600598097 current acc: 0.7883937159345944\n",
      "iteration 19 current loss: 0.007157370913773775 current acc: 0.7884615384615384\n",
      "iteration 20 current loss: 0.004406854044646025 current acc: 0.7885293175264338\n",
      "iteration 21 current loss: 0.005897132679820061 current acc: 0.7885970531710442\n",
      "iteration 22 current loss: 0.005172248929738998 current acc: 0.7886647454370798\n",
      "iteration 23 current loss: 0.011255975812673569 current acc: 0.7887323943661971\n",
      "iteration 24 current loss: 0.005685846321284771 current acc: 0.7888\n",
      "iteration 25 current loss: 0.008242779411375523 current acc: 0.7888675623800384\n",
      "iteration 26 current loss: 0.008560585789382458 current acc: 0.7889350815478094\n",
      "iteration 27 current loss: 0.006472096312791109 current acc: 0.789002557544757\n",
      "iteration 28 current loss: 0.005557146854698658 current acc: 0.7890699904122723\n",
      "iteration 29 current loss: 0.004886326380074024 current acc: 0.7891373801916933\n",
      "iteration 30 current loss: 0.007880284450948238 current acc: 0.7892047269243053\n",
      "iteration 31 current loss: 0.006523615214973688 current acc: 0.789272030651341\n",
      "iteration 32 current loss: 0.007944944314658642 current acc: 0.7893392914139802\n",
      "iteration 33 current loss: 0.01130080595612526 current acc: 0.7894065092533503\n",
      "iteration 34 current loss: 0.007738726679235697 current acc: 0.7894736842105263\n",
      "iteration 35 current loss: 0.004408612381666899 current acc: 0.7895408163265306\n",
      "iteration 36 current loss: 0.009443975985050201 current acc: 0.7896079056423334\n",
      "iteration 37 current loss: 0.006533813662827015 current acc: 0.7896749521988528\n",
      "iteration 38 current loss: 0.007426299620419741 current acc: 0.7897419560369544\n",
      "iteration 39 current loss: 0.0058609009720385075 current acc: 0.7898089171974523\n",
      "iteration 40 current loss: 0.009958834387362003 current acc: 0.789875835721108\n",
      "iteration 41 current loss: 0.009754868224263191 current acc: 0.7899427116486314\n",
      "iteration 42 current loss: 0.009004480205476284 current acc: 0.7900095450206809\n",
      "iteration 43 current loss: 0.004346424713730812 current acc: 0.7900763358778626\n",
      "iteration 44 current loss: 0.005044096149504185 current acc: 0.7901430842607313\n",
      "iteration 45 current loss: 0.004466477315872908 current acc: 0.7902097902097902\n",
      "iteration 46 current loss: 0.005900238174945116 current acc: 0.7902764537654909\n",
      "iteration 47 current loss: 0.00807985756546259 current acc: 0.7903430749682337\n",
      "iteration 48 current loss: 0.00789149571210146 current acc: 0.7904096538583677\n",
      "iteration 49 current loss: 0.009958636946976185 current acc: 0.7904761904761904\n",
      "iteration 50 current loss: 0.004133379552513361 current acc: 0.7905426848619486\n",
      "iteration 51 current loss: 0.00769048510119319 current acc: 0.7906091370558376\n",
      "iteration 52 current loss: 0.00532609224319458 current acc: 0.7906755470980019\n",
      "iteration 53 current loss: 0.00548482546582818 current acc: 0.7907419150285352\n",
      "iteration 54 current loss: 0.009153123944997787 current acc: 0.7908082408874801\n",
      "iteration 55 current loss: 0.00809958204627037 current acc: 0.7908745247148289\n",
      "iteration 56 current loss: 0.004971994087100029 current acc: 0.7909407665505227\n",
      "iteration 57 current loss: 0.005661288276314735 current acc: 0.7910069664344522\n",
      "iteration 58 current loss: 0.011321863159537315 current acc: 0.7910731244064577\n",
      "iteration 59 current loss: 0.011015026830136776 current acc: 0.7911392405063291\n",
      "iteration 60 current loss: 0.006837572902441025 current acc: 0.7912053147738057\n",
      "iteration 61 current loss: 0.005561204627156258 current acc: 0.7912713472485768\n",
      "iteration 62 current loss: 0.004361498169600964 current acc: 0.7913373379702814\n",
      "iteration 63 current loss: 0.00529142189770937 current acc: 0.7914032869785083\n",
      "iteration 64 current loss: 0.0076231746934354305 current acc: 0.7914691943127962\n",
      "iteration 65 current loss: 0.006459885276854038 current acc: 0.7915350600126342\n",
      "iteration 66 current loss: 0.006357286591082811 current acc: 0.7916008841174613\n",
      "iteration 67 current loss: 0.010880069807171822 current acc: 0.7916666666666666\n",
      "iteration 68 current loss: 0.008378499187529087 current acc: 0.7917324076995897\n",
      "iteration 69 current loss: 0.00803176499903202 current acc: 0.7917981072555205\n",
      "iteration 70 current loss: 0.003977407235652208 current acc: 0.7918637653736992\n",
      "iteration 71 current loss: 0.005575111601501703 current acc: 0.7919293820933165\n",
      "iteration 72 current loss: 0.004030340816825628 current acc: 0.791994957453514\n",
      "iteration 73 current loss: 0.004555298015475273 current acc: 0.7920604914933838\n",
      "iteration 74 current loss: 0.007851985283195972 current acc: 0.7921259842519685\n",
      "iteration 75 current loss: 0.006863607559353113 current acc: 0.792191435768262\n",
      "iteration 76 current loss: 0.009350936859846115 current acc: 0.7922568460812087\n",
      "iteration 77 current loss: 0.007153310813009739 current acc: 0.7923222152297043\n",
      "iteration 78 current loss: 0.009340094402432442 current acc: 0.7923875432525952\n",
      "iteration 79 current loss: 0.008630778640508652 current acc: 0.7924528301886793\n",
      "iteration 80 current loss: 0.0057015796191990376 current acc: 0.7925180760767054\n",
      "iteration 81 current loss: 0.004767053294926882 current acc: 0.792583280955374\n",
      "iteration 82 current loss: 0.004279246553778648 current acc: 0.7926484448633365\n",
      "iteration 83 current loss: 0.0067321802489459515 current acc: 0.792713567839196\n",
      "iteration 84 current loss: 0.011801528744399548 current acc: 0.792778649921507\n",
      "iteration 85 current loss: 0.005504739936441183 current acc: 0.7928436911487758\n",
      "iteration 86 current loss: 0.006618218030780554 current acc: 0.7929086915594603\n",
      "iteration 87 current loss: 0.005549869034439325 current acc: 0.7929736511919699\n",
      "iteration 88 current loss: 0.010568751022219658 current acc: 0.793038570084666\n",
      "iteration 89 current loss: 0.006608911789953709 current acc: 0.7931034482758621\n",
      "iteration 90 current loss: 0.004382237326353788 current acc: 0.7931682858038233\n",
      "iteration 91 current loss: 0.006787970196455717 current acc: 0.793233082706767\n",
      "iteration 92 current loss: 0.00208819517865777 current acc: 0.7932978390228625\n",
      "iteration 93 current loss: 0.006455837283283472 current acc: 0.7933625547902317\n",
      "iteration 94 current loss: 0.006858704145997763 current acc: 0.7934272300469484\n",
      "iteration 95 current loss: 0.006540393922477961 current acc: 0.7934918648310388\n",
      "iteration 96 current loss: 0.006163448095321655 current acc: 0.7935564591804817\n",
      "iteration 97 current loss: 0.01241939514875412 current acc: 0.7936210131332082\n",
      "iteration 98 current loss: 0.005404661875218153 current acc: 0.7936855267271022\n",
      "iteration 99 current loss: 0.009126304648816586 current acc: 0.79375\n",
      "\t\tEpoch 31/100 complete. Epoch loss 0.007087351162917912 Epoch accuracy 1.0\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 31, Validation Accuracy: 0.634, Validation Loss: 1.5864671994000674\n",
      "best loss 0.007087351162917912\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.008149713277816772 current acc: 0.7938144329896907\n",
      "iteration 1 current loss: 0.006282322574406862 current acc: 0.7938788257339163\n",
      "iteration 2 current loss: 0.005123840179294348 current acc: 0.7939431782703715\n",
      "iteration 3 current loss: 0.007017604075372219 current acc: 0.7940074906367042\n",
      "iteration 4 current loss: 0.0038977465592324734 current acc: 0.7940717628705148\n",
      "iteration 5 current loss: 0.00502718286588788 current acc: 0.7941359950093575\n",
      "iteration 6 current loss: 0.0037887010257691145 current acc: 0.794200187090739\n",
      "iteration 7 current loss: 0.005128927994519472 current acc: 0.7942643391521197\n",
      "iteration 8 current loss: 0.0054487609304487705 current acc: 0.794328451230913\n",
      "iteration 9 current loss: 0.004820085130631924 current acc: 0.794392523364486\n",
      "iteration 10 current loss: 0.005325996782630682 current acc: 0.7944565555901588\n",
      "iteration 11 current loss: 0.006110878195613623 current acc: 0.7945205479452054\n",
      "iteration 12 current loss: 0.004463864956051111 current acc: 0.7945845004668534\n",
      "iteration 13 current loss: 0.005421902984380722 current acc: 0.7946484131922837\n",
      "iteration 14 current loss: 0.007873289287090302 current acc: 0.7947122861586314\n",
      "iteration 15 current loss: 0.006909526418894529 current acc: 0.7947761194029851\n",
      "iteration 16 current loss: 0.00481669744476676 current acc: 0.7948399129623873\n",
      "iteration 17 current loss: 0.003667512908577919 current acc: 0.7949036668738346\n",
      "iteration 18 current loss: 0.01187028270214796 current acc: 0.7949673811742777\n",
      "iteration 19 current loss: 0.007666604593396187 current acc: 0.7950310559006211\n",
      "iteration 20 current loss: 0.006788043770939112 current acc: 0.7950946910897236\n",
      "iteration 21 current loss: 0.006574379745870829 current acc: 0.7951582867783985\n",
      "iteration 22 current loss: 0.005255390424281359 current acc: 0.7952218430034129\n",
      "iteration 23 current loss: 0.004445170052349567 current acc: 0.7952853598014888\n",
      "iteration 24 current loss: 0.0038397207390516996 current acc: 0.7953488372093023\n",
      "iteration 25 current loss: 0.006187823601067066 current acc: 0.7954122752634842\n",
      "iteration 26 current loss: 0.00509122246876359 current acc: 0.7954756740006198\n",
      "iteration 27 current loss: 0.004799221642315388 current acc: 0.7955390334572491\n",
      "iteration 28 current loss: 0.006255075801163912 current acc: 0.7956023536698669\n",
      "iteration 29 current loss: 0.004964782856404781 current acc: 0.7956656346749226\n",
      "iteration 30 current loss: 0.005341381300240755 current acc: 0.7957288765088208\n",
      "iteration 31 current loss: 0.006610304582864046 current acc: 0.7957920792079208\n",
      "iteration 32 current loss: 0.005246635526418686 current acc: 0.7958552428085369\n",
      "iteration 33 current loss: 0.004573236219584942 current acc: 0.7959183673469388\n",
      "iteration 34 current loss: 0.005550621543079615 current acc: 0.7959814528593508\n",
      "iteration 35 current loss: 0.004436836577951908 current acc: 0.796044499381953\n",
      "iteration 36 current loss: 0.009967323392629623 current acc: 0.7961075069508804\n",
      "iteration 37 current loss: 0.003558414988219738 current acc: 0.7961704756022236\n",
      "iteration 38 current loss: 0.0038639644626528025 current acc: 0.7962334053720284\n",
      "iteration 39 current loss: 0.005073127802461386 current acc: 0.7962962962962963\n",
      "iteration 40 current loss: 0.007095962762832642 current acc: 0.7963591484109843\n",
      "iteration 41 current loss: 0.005307834595441818 current acc: 0.796421961752005\n",
      "iteration 42 current loss: 0.005578892771154642 current acc: 0.7964847363552267\n",
      "iteration 43 current loss: 0.011579394340515137 current acc: 0.7965474722564735\n",
      "iteration 44 current loss: 0.0040091779083013535 current acc: 0.7966101694915254\n",
      "iteration 45 current loss: 0.005178537219762802 current acc: 0.7966728280961183\n",
      "iteration 46 current loss: 0.008028313517570496 current acc: 0.796735448105944\n",
      "iteration 47 current loss: 0.008170947432518005 current acc: 0.7967980295566502\n",
      "iteration 48 current loss: 0.004228152800351381 current acc: 0.7968605724838412\n",
      "iteration 49 current loss: 0.006172745954245329 current acc: 0.796923076923077\n",
      "iteration 50 current loss: 0.003654048778116703 current acc: 0.7969855429098739\n",
      "iteration 51 current loss: 0.006201114039868116 current acc: 0.7970479704797048\n",
      "iteration 52 current loss: 0.0051002902910113335 current acc: 0.7971103596679988\n",
      "iteration 53 current loss: 0.003719386411830783 current acc: 0.7971727105101414\n",
      "iteration 54 current loss: 0.004046175628900528 current acc: 0.7972350230414746\n",
      "iteration 55 current loss: 0.004425438120961189 current acc: 0.7972972972972973\n",
      "iteration 56 current loss: 0.003574463538825512 current acc: 0.7973595333128646\n",
      "iteration 57 current loss: 0.003472661366686225 current acc: 0.7974217311233885\n",
      "iteration 58 current loss: 0.005062257871031761 current acc: 0.797483890764038\n",
      "iteration 59 current loss: 0.006292134523391724 current acc: 0.7975460122699386\n",
      "iteration 60 current loss: 0.016347281634807587 current acc: 0.797608095676173\n",
      "iteration 61 current loss: 0.004725239239633083 current acc: 0.7976701410177806\n",
      "iteration 62 current loss: 0.00628197006881237 current acc: 0.7977321483297579\n",
      "iteration 63 current loss: 0.006010198500007391 current acc: 0.7977941176470589\n",
      "iteration 64 current loss: 0.011951292864978313 current acc: 0.7978560490045942\n",
      "iteration 65 current loss: 0.0067338417284190655 current acc: 0.7979179424372321\n",
      "iteration 66 current loss: 0.008464166894555092 current acc: 0.797979797979798\n",
      "iteration 67 current loss: 0.005113230552524328 current acc: 0.7980416156670747\n",
      "iteration 68 current loss: 0.004150182008743286 current acc: 0.7981033955338024\n",
      "iteration 69 current loss: 0.0054696714505553246 current acc: 0.7981651376146789\n",
      "iteration 70 current loss: 0.003248232416808605 current acc: 0.7982268419443596\n",
      "iteration 71 current loss: 0.006170691456645727 current acc: 0.7982885085574573\n",
      "iteration 72 current loss: 0.0033914127852767706 current acc: 0.7983501374885427\n",
      "iteration 73 current loss: 0.010731027461588383 current acc: 0.7984117287721442\n",
      "iteration 74 current loss: 0.006440198048949242 current acc: 0.7984732824427481\n",
      "iteration 75 current loss: 0.005150384735316038 current acc: 0.7985347985347986\n",
      "iteration 76 current loss: 0.007295014802366495 current acc: 0.7985962770826975\n",
      "iteration 77 current loss: 0.004083641339093447 current acc: 0.7986577181208053\n",
      "iteration 78 current loss: 0.008302103728055954 current acc: 0.7987191216834401\n",
      "iteration 79 current loss: 0.007097265683114529 current acc: 0.7987804878048781\n",
      "iteration 80 current loss: 0.007030772510915995 current acc: 0.7988418165193538\n",
      "iteration 81 current loss: 0.0054581500589847565 current acc: 0.7989031078610603\n",
      "iteration 82 current loss: 0.005460276734083891 current acc: 0.7989643618641487\n",
      "iteration 83 current loss: 0.009606486186385155 current acc: 0.7990255785627284\n",
      "iteration 84 current loss: 0.007000720594078302 current acc: 0.7990867579908676\n",
      "iteration 85 current loss: 0.007543633691966534 current acc: 0.7991479001825929\n",
      "iteration 86 current loss: 0.014459392987191677 current acc: 0.7992090051718893\n",
      "iteration 87 current loss: 0.005410248413681984 current acc: 0.7992700729927007\n",
      "iteration 88 current loss: 0.005665494594722986 current acc: 0.7993311036789298\n",
      "iteration 89 current loss: 0.006742333061993122 current acc: 0.7993920972644377\n",
      "iteration 90 current loss: 0.00852235034108162 current acc: 0.7994530537830447\n",
      "iteration 91 current loss: 0.0048669371753931046 current acc: 0.7995139732685298\n",
      "iteration 92 current loss: 0.0037214565090835094 current acc: 0.799574855754631\n",
      "iteration 93 current loss: 0.005625661462545395 current acc: 0.7996357012750456\n",
      "iteration 94 current loss: 0.004725997801870108 current acc: 0.7996965098634294\n",
      "iteration 95 current loss: 0.004830072168260813 current acc: 0.7997572815533981\n",
      "iteration 96 current loss: 0.00801847130060196 current acc: 0.7998180163785259\n",
      "iteration 97 current loss: 0.006480753887444735 current acc: 0.7998787143723469\n",
      "iteration 98 current loss: 0.016264429315924644 current acc: 0.799939375568354\n",
      "iteration 99 current loss: 0.008323325775563717 current acc: 0.8\n",
      "\t\tEpoch 32/100 complete. Epoch loss 0.0062104606023058295 Epoch accuracy 1.0\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 32, Validation Accuracy: 0.625875, Validation Loss: 1.613662376999855\n",
      "best loss 0.0062104606023058295\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.005350036080926657 current acc: 0.8000605877006968\n",
      "iteration 1 current loss: 0.004368670284748077 current acc: 0.8001211387038158\n",
      "iteration 2 current loss: 0.0056707183830440044 current acc: 0.8001816530426885\n",
      "iteration 3 current loss: 0.005436792969703674 current acc: 0.8002421307506054\n",
      "iteration 4 current loss: 0.003533993847668171 current acc: 0.800302571860817\n",
      "iteration 5 current loss: 0.0021115378476679325 current acc: 0.8003629764065335\n",
      "iteration 6 current loss: 0.00308721954934299 current acc: 0.8004233444209253\n",
      "iteration 7 current loss: 0.006569020915776491 current acc: 0.8004836759371221\n",
      "iteration 8 current loss: 0.0047903889790177345 current acc: 0.800543970988214\n",
      "iteration 9 current loss: 0.0052304076962172985 current acc: 0.8006042296072508\n",
      "iteration 10 current loss: 0.005361719522625208 current acc: 0.8006644518272426\n",
      "iteration 11 current loss: 0.004253437276929617 current acc: 0.8007246376811594\n",
      "iteration 12 current loss: 0.00337776611559093 current acc: 0.8007847872019318\n",
      "iteration 13 current loss: 0.0046193660236895084 current acc: 0.8008449004224502\n",
      "iteration 14 current loss: 0.004983812104910612 current acc: 0.8009049773755657\n",
      "iteration 15 current loss: 0.005687871016561985 current acc: 0.8009650180940893\n",
      "iteration 16 current loss: 0.005127931013703346 current acc: 0.8010250226107929\n",
      "iteration 17 current loss: 0.00478071253746748 current acc: 0.8010849909584087\n",
      "iteration 18 current loss: 0.004932507872581482 current acc: 0.8011449231696294\n",
      "iteration 19 current loss: 0.0041866423562169075 current acc: 0.8012048192771084\n",
      "iteration 20 current loss: 0.006018581800162792 current acc: 0.8012646793134598\n",
      "iteration 21 current loss: 0.0061361221596598625 current acc: 0.8013245033112583\n",
      "iteration 22 current loss: 0.0051363552920520306 current acc: 0.8013842913030395\n",
      "iteration 23 current loss: 0.004353884141892195 current acc: 0.8014440433212996\n",
      "iteration 24 current loss: 0.004491889383643866 current acc: 0.8015037593984963\n",
      "iteration 25 current loss: 0.0033063157461583614 current acc: 0.8015634395670475\n",
      "iteration 26 current loss: 0.005034040659666061 current acc: 0.8016230838593328\n",
      "iteration 27 current loss: 0.002753216540440917 current acc: 0.8016826923076923\n",
      "iteration 28 current loss: 0.006369394715875387 current acc: 0.8017422649444278\n",
      "iteration 29 current loss: 0.00420061219483614 current acc: 0.8018018018018018\n",
      "iteration 30 current loss: 0.0057485694997012615 current acc: 0.8018613029120384\n",
      "iteration 31 current loss: 0.003206869587302208 current acc: 0.801920768307323\n",
      "iteration 32 current loss: 0.007597129791975021 current acc: 0.801980198019802\n",
      "iteration 33 current loss: 0.0047978307120501995 current acc: 0.8020395920815837\n",
      "iteration 34 current loss: 0.0036613077390938997 current acc: 0.8020989505247377\n",
      "iteration 35 current loss: 0.003138120984658599 current acc: 0.802158273381295\n",
      "iteration 36 current loss: 0.005419563036412001 current acc: 0.8022175606832485\n",
      "iteration 37 current loss: 0.005853381939232349 current acc: 0.8022768124625524\n",
      "iteration 38 current loss: 0.00496276468038559 current acc: 0.8023360287511231\n",
      "iteration 39 current loss: 0.0046141911298036575 current acc: 0.8023952095808383\n",
      "iteration 40 current loss: 0.005052053369581699 current acc: 0.8024543549835379\n",
      "iteration 41 current loss: 0.005184146575629711 current acc: 0.8025134649910234\n",
      "iteration 42 current loss: 0.003998057916760445 current acc: 0.8025725396350584\n",
      "iteration 43 current loss: 0.004748872015625238 current acc: 0.8026315789473685\n",
      "iteration 44 current loss: 0.0036028081085532904 current acc: 0.8026905829596412\n",
      "iteration 45 current loss: 0.004179844167083502 current acc: 0.8027495517035266\n",
      "iteration 46 current loss: 0.0031010047532618046 current acc: 0.8028084852106364\n",
      "iteration 47 current loss: 0.005746111273765564 current acc: 0.8028673835125448\n",
      "iteration 48 current loss: 0.0033023974392563105 current acc: 0.8029262466407883\n",
      "iteration 49 current loss: 0.003325521945953369 current acc: 0.8029850746268656\n",
      "iteration 50 current loss: 0.004267319571226835 current acc: 0.8030438675022381\n",
      "iteration 51 current loss: 0.0046741715632379055 current acc: 0.8031026252983293\n",
      "iteration 52 current loss: 0.00394437275826931 current acc: 0.8031613480465255\n",
      "iteration 53 current loss: 0.006757901981472969 current acc: 0.8032200357781754\n",
      "iteration 54 current loss: 0.004357262048870325 current acc: 0.8032786885245902\n",
      "iteration 55 current loss: 0.005449257325381041 current acc: 0.8033373063170441\n",
      "iteration 56 current loss: 0.0033065546303987503 current acc: 0.803395889186774\n",
      "iteration 57 current loss: 0.004558388143777847 current acc: 0.8034544371649791\n",
      "iteration 58 current loss: 0.0034746790770441294 current acc: 0.8035129502828222\n",
      "iteration 59 current loss: 0.006225347053259611 current acc: 0.8035714285714286\n",
      "iteration 60 current loss: 0.004199379589408636 current acc: 0.8036298720618863\n",
      "iteration 61 current loss: 0.0029559102840721607 current acc: 0.8036882807852469\n",
      "iteration 62 current loss: 0.004943279083818197 current acc: 0.8037466547725245\n",
      "iteration 63 current loss: 0.004868441727012396 current acc: 0.8038049940546967\n",
      "iteration 64 current loss: 0.005122220143675804 current acc: 0.8038632986627043\n",
      "iteration 65 current loss: 0.004653391428291798 current acc: 0.803921568627451\n",
      "iteration 66 current loss: 0.003986978903412819 current acc: 0.803979803979804\n",
      "iteration 67 current loss: 0.006654292345046997 current acc: 0.8040380047505938\n",
      "iteration 68 current loss: 0.005093437619507313 current acc: 0.8040961709706145\n",
      "iteration 69 current loss: 0.003984321840107441 current acc: 0.8041543026706232\n",
      "iteration 70 current loss: 0.004938208963721991 current acc: 0.8042123998813409\n",
      "iteration 71 current loss: 0.004367207642644644 current acc: 0.8042704626334519\n",
      "iteration 72 current loss: 0.005210833624005318 current acc: 0.8043284909576045\n",
      "iteration 73 current loss: 0.004153575282543898 current acc: 0.8043864848844102\n",
      "iteration 74 current loss: 0.0030904754530638456 current acc: 0.8044444444444444\n",
      "iteration 75 current loss: 0.003780465107411146 current acc: 0.8045023696682464\n",
      "iteration 76 current loss: 0.0047091953456401825 current acc: 0.8045602605863192\n",
      "iteration 77 current loss: 0.004369435831904411 current acc: 0.8046181172291297\n",
      "iteration 78 current loss: 0.005633984692394733 current acc: 0.8046759396271086\n",
      "iteration 79 current loss: 0.003972218371927738 current acc: 0.8047337278106509\n",
      "iteration 80 current loss: 0.004937957972288132 current acc: 0.8047914818101154\n",
      "iteration 81 current loss: 0.003748310497030616 current acc: 0.804849201655825\n",
      "iteration 82 current loss: 0.0034362277947366238 current acc: 0.8049068873780668\n",
      "iteration 83 current loss: 0.006334805861115456 current acc: 0.8049645390070922\n",
      "iteration 84 current loss: 0.0027381256222724915 current acc: 0.8050221565731167\n",
      "iteration 85 current loss: 0.00449273781850934 current acc: 0.8050797401063201\n",
      "iteration 86 current loss: 0.005239551886916161 current acc: 0.8051372896368467\n",
      "iteration 87 current loss: 0.002271342324092984 current acc: 0.8051948051948052\n",
      "iteration 88 current loss: 0.010753696784377098 current acc: 0.8052522868102685\n",
      "iteration 89 current loss: 0.0022709574550390244 current acc: 0.8053097345132744\n",
      "iteration 90 current loss: 0.006723323371261358 current acc: 0.8053671483338248\n",
      "iteration 91 current loss: 0.003347259247675538 current acc: 0.8054245283018868\n",
      "iteration 92 current loss: 0.003111743601039052 current acc: 0.8054818744473917\n",
      "iteration 93 current loss: 0.004819782916456461 current acc: 0.8055391868002357\n",
      "iteration 94 current loss: 0.0033371360041201115 current acc: 0.8055964653902798\n",
      "iteration 95 current loss: 0.0030498539563268423 current acc: 0.8056537102473498\n",
      "iteration 96 current loss: 0.003835848765447736 current acc: 0.8057109214012363\n",
      "iteration 97 current loss: 0.0037691211327910423 current acc: 0.8057680988816951\n",
      "iteration 98 current loss: 0.002236167201772332 current acc: 0.8058252427184466\n",
      "iteration 99 current loss: 0.004572605714201927 current acc: 0.8058823529411765\n",
      "\t\tEpoch 33/100 complete. Epoch loss 0.00455230575054884 Epoch accuracy 1.0\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 33, Validation Accuracy: 0.634625, Validation Loss: 1.623806470632553\n",
      "best loss 0.00455230575054884\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.0029298393055796623 current acc: 0.8059394295795355\n",
      "iteration 1 current loss: 0.005260461010038853 current acc: 0.8059964726631393\n",
      "iteration 2 current loss: 0.0038413680158555508 current acc: 0.8060534822215693\n",
      "iteration 3 current loss: 0.01347009651362896 current acc: 0.8061104582843713\n",
      "iteration 4 current loss: 0.002984193852171302 current acc: 0.8061674008810573\n",
      "iteration 5 current loss: 0.005542665254324675 current acc: 0.806224310041104\n",
      "iteration 6 current loss: 0.0038442015647888184 current acc: 0.8062811857939536\n",
      "iteration 7 current loss: 0.004421922843903303 current acc: 0.8063380281690141\n",
      "iteration 8 current loss: 0.0037452559918165207 current acc: 0.8063948371956585\n",
      "iteration 9 current loss: 0.003330349223688245 current acc: 0.8064516129032258\n",
      "iteration 10 current loss: 0.001873345929197967 current acc: 0.8065083553210203\n",
      "iteration 11 current loss: 0.005264871288090944 current acc: 0.8065650644783119\n",
      "iteration 12 current loss: 0.0032897961791604757 current acc: 0.8066217404043363\n",
      "iteration 13 current loss: 0.0031269174069166183 current acc: 0.8066783831282952\n",
      "iteration 14 current loss: 0.0036866699811071157 current acc: 0.8067349926793558\n",
      "iteration 15 current loss: 0.004013145342469215 current acc: 0.8067915690866511\n",
      "iteration 16 current loss: 0.008065111003816128 current acc: 0.8068481123792801\n",
      "iteration 17 current loss: 0.006222383584827185 current acc: 0.8069046225863078\n",
      "iteration 18 current loss: 0.006191703490912914 current acc: 0.8069610997367651\n",
      "iteration 19 current loss: 0.005443672649562359 current acc: 0.8070175438596491\n",
      "iteration 20 current loss: 0.0033119639847427607 current acc: 0.8070739549839229\n",
      "iteration 21 current loss: 0.004077608231455088 current acc: 0.8071303331385155\n",
      "iteration 22 current loss: 0.008060186170041561 current acc: 0.8071866783523225\n",
      "iteration 23 current loss: 0.0071954126469790936 current acc: 0.8072429906542056\n",
      "iteration 24 current loss: 0.0035675575491040945 current acc: 0.8072992700729927\n",
      "iteration 25 current loss: 0.003880611155182123 current acc: 0.8073555166374781\n",
      "iteration 26 current loss: 0.002716043032705784 current acc: 0.8074117303764226\n",
      "iteration 27 current loss: 0.0038333090487867594 current acc: 0.8074679113185531\n",
      "iteration 28 current loss: 0.0036843600682914257 current acc: 0.8075240594925635\n",
      "iteration 29 current loss: 0.01142389327287674 current acc: 0.8075801749271136\n",
      "iteration 30 current loss: 0.0038015046156942844 current acc: 0.8076362576508307\n",
      "iteration 31 current loss: 0.007499517407268286 current acc: 0.8076923076923077\n",
      "iteration 32 current loss: 0.004743083380162716 current acc: 0.8077483250801049\n",
      "iteration 33 current loss: 0.004119932651519775 current acc: 0.807804309842749\n",
      "iteration 34 current loss: 0.002495351480320096 current acc: 0.8078602620087336\n",
      "iteration 35 current loss: 0.0049491943791508675 current acc: 0.8079161816065192\n",
      "iteration 36 current loss: 0.003367382800206542 current acc: 0.807972068664533\n",
      "iteration 37 current loss: 0.007262393832206726 current acc: 0.8080279232111692\n",
      "iteration 38 current loss: 0.006544780917465687 current acc: 0.8080837452747892\n",
      "iteration 39 current loss: 0.00601663114503026 current acc: 0.8081395348837209\n",
      "iteration 40 current loss: 0.004123223014175892 current acc: 0.8081952920662598\n",
      "iteration 41 current loss: 0.005034339614212513 current acc: 0.8082510168506682\n",
      "iteration 42 current loss: 0.00400289474055171 current acc: 0.8083067092651757\n",
      "iteration 43 current loss: 0.003069057362154126 current acc: 0.8083623693379791\n",
      "iteration 44 current loss: 0.0047165993601083755 current acc: 0.8084179970972424\n",
      "iteration 45 current loss: 0.004054052289575338 current acc: 0.808473592571097\n",
      "iteration 46 current loss: 0.0028242242988198996 current acc: 0.8085291557876414\n",
      "iteration 47 current loss: 0.005164704285562038 current acc: 0.808584686774942\n",
      "iteration 48 current loss: 0.003051862819120288 current acc: 0.8086401855610322\n",
      "iteration 49 current loss: 0.003768610768020153 current acc: 0.808695652173913\n",
      "iteration 50 current loss: 0.004400606267154217 current acc: 0.8087510866415532\n",
      "iteration 51 current loss: 0.003742433153092861 current acc: 0.8088064889918888\n",
      "iteration 52 current loss: 0.005445461254566908 current acc: 0.8088618592528236\n",
      "iteration 53 current loss: 0.0044961716048419476 current acc: 0.8089171974522293\n",
      "iteration 54 current loss: 0.004136188887059689 current acc: 0.808972503617945\n",
      "iteration 55 current loss: 0.00422827759757638 current acc: 0.8090277777777778\n",
      "iteration 56 current loss: 0.0046647218987345695 current acc: 0.8090830199595025\n",
      "iteration 57 current loss: 0.0033534523099660873 current acc: 0.8091382301908617\n",
      "iteration 58 current loss: 0.002474912442266941 current acc: 0.8091934084995663\n",
      "iteration 59 current loss: 0.00352667598053813 current acc: 0.8092485549132948\n",
      "iteration 60 current loss: 0.004541376605629921 current acc: 0.8093036694596937\n",
      "iteration 61 current loss: 0.003192325821146369 current acc: 0.8093587521663779\n",
      "iteration 62 current loss: 0.002106312895193696 current acc: 0.8094138030609298\n",
      "iteration 63 current loss: 0.005199171137064695 current acc: 0.8094688221709007\n",
      "iteration 64 current loss: 0.005489986389875412 current acc: 0.8095238095238095\n",
      "iteration 65 current loss: 0.007247752510011196 current acc: 0.8095787651471437\n",
      "iteration 66 current loss: 0.00503544369712472 current acc: 0.8096336890683589\n",
      "iteration 67 current loss: 0.004119268152862787 current acc: 0.8096885813148789\n",
      "iteration 68 current loss: 0.0032602513674646616 current acc: 0.8097434419140963\n",
      "iteration 69 current loss: 0.003988426178693771 current acc: 0.8097982708933718\n",
      "iteration 70 current loss: 0.0034878659062087536 current acc: 0.8098530682800346\n",
      "iteration 71 current loss: 0.0032831677235662937 current acc: 0.8099078341013825\n",
      "iteration 72 current loss: 0.003281456185504794 current acc: 0.8099625683846818\n",
      "iteration 73 current loss: 0.00782565213739872 current acc: 0.8100172711571675\n",
      "iteration 74 current loss: 0.003797848243266344 current acc: 0.8100719424460432\n",
      "iteration 75 current loss: 0.004520601127296686 current acc: 0.810126582278481\n",
      "iteration 76 current loss: 0.005246986635029316 current acc: 0.8101811906816221\n",
      "iteration 77 current loss: 0.0037093928549438715 current acc: 0.8102357676825762\n",
      "iteration 78 current loss: 0.006571193225681782 current acc: 0.8102903133084219\n",
      "iteration 79 current loss: 0.006834051106125116 current acc: 0.8103448275862069\n",
      "iteration 80 current loss: 0.00443462748080492 current acc: 0.8103993105429474\n",
      "iteration 81 current loss: 0.005585050676018 current acc: 0.8104537622056289\n",
      "iteration 82 current loss: 0.00636370200663805 current acc: 0.8105081826012058\n",
      "iteration 83 current loss: 0.006222863215953112 current acc: 0.8105625717566016\n",
      "iteration 84 current loss: 0.003089212579652667 current acc: 0.8106169296987088\n",
      "iteration 85 current loss: 0.0041056228801608086 current acc: 0.810671256454389\n",
      "iteration 86 current loss: 0.002719361102208495 current acc: 0.8107255520504731\n",
      "iteration 87 current loss: 0.005957434885203838 current acc: 0.8107798165137615\n",
      "iteration 88 current loss: 0.005728468298912048 current acc: 0.8108340498710233\n",
      "iteration 89 current loss: 0.00444964412599802 current acc: 0.8108882521489972\n",
      "iteration 90 current loss: 0.003855092916637659 current acc: 0.8109424233743913\n",
      "iteration 91 current loss: 0.003727142233401537 current acc: 0.8109965635738832\n",
      "iteration 92 current loss: 0.0037851566448807716 current acc: 0.8110506727741197\n",
      "iteration 93 current loss: 0.003100672271102667 current acc: 0.8111047510017172\n",
      "iteration 94 current loss: 0.0073670800775289536 current acc: 0.8111587982832618\n",
      "iteration 95 current loss: 0.0038141789846122265 current acc: 0.8112128146453089\n",
      "iteration 96 current loss: 0.004143223632127047 current acc: 0.8112668001143838\n",
      "iteration 97 current loss: 0.0033043657895177603 current acc: 0.8113207547169812\n",
      "iteration 98 current loss: 0.0034808211494237185 current acc: 0.8113746784795656\n",
      "iteration 99 current loss: 0.004396791569888592 current acc: 0.8114285714285714\n",
      "\t\tEpoch 34/100 complete. Epoch loss 0.004617462005699053 Epoch accuracy 1.0\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 34, Validation Accuracy: 0.635625, Validation Loss: 1.6618572764098645\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.0021480643190443516 current acc: 0.8114824335904027\n",
      "iteration 1 current loss: 0.0022065257653594017 current acc: 0.8115362649914335\n",
      "iteration 2 current loss: 0.0028692639898508787 current acc: 0.8115900656580074\n",
      "iteration 3 current loss: 0.0028365550097078085 current acc: 0.8116438356164384\n",
      "iteration 4 current loss: 0.002800805028527975 current acc: 0.81169757489301\n",
      "iteration 5 current loss: 0.0032708514481782913 current acc: 0.8117512835139761\n",
      "iteration 6 current loss: 0.00278990576043725 current acc: 0.8118049615055604\n",
      "iteration 7 current loss: 0.0036670290865004063 current acc: 0.8118586088939567\n",
      "iteration 8 current loss: 0.003712254110723734 current acc: 0.8119122257053292\n",
      "iteration 9 current loss: 0.003330324310809374 current acc: 0.811965811965812\n",
      "iteration 10 current loss: 0.0023491226602345705 current acc: 0.8120193677015095\n",
      "iteration 11 current loss: 0.0040839482098817825 current acc: 0.8120728929384966\n",
      "iteration 12 current loss: 0.002755002584308386 current acc: 0.8121263877028181\n",
      "iteration 13 current loss: 0.003998733591288328 current acc: 0.8121798520204895\n",
      "iteration 14 current loss: 0.0033383858390152454 current acc: 0.8122332859174964\n",
      "iteration 15 current loss: 0.0033486473839730024 current acc: 0.8122866894197952\n",
      "iteration 16 current loss: 0.0020577458199113607 current acc: 0.8123400625533125\n",
      "iteration 17 current loss: 0.0037667728029191494 current acc: 0.8123934053439454\n",
      "iteration 18 current loss: 0.004355242941528559 current acc: 0.8124467178175618\n",
      "iteration 19 current loss: 0.0026225396431982517 current acc: 0.8125\n",
      "iteration 20 current loss: 0.007484072353690863 current acc: 0.8125532519170691\n",
      "iteration 21 current loss: 0.002700495533645153 current acc: 0.8126064735945485\n",
      "iteration 22 current loss: 0.003788013942539692 current acc: 0.8126596650581891\n",
      "iteration 23 current loss: 0.002401184057816863 current acc: 0.8127128263337117\n",
      "iteration 24 current loss: 0.004016558639705181 current acc: 0.8127659574468085\n",
      "iteration 25 current loss: 0.0035558445379137993 current acc: 0.8128190584231424\n",
      "iteration 26 current loss: 0.003192303702235222 current acc: 0.8128721292883471\n",
      "iteration 27 current loss: 0.003663466777652502 current acc: 0.8129251700680272\n",
      "iteration 28 current loss: 0.003257547505199909 current acc: 0.8129781807877585\n",
      "iteration 29 current loss: 0.0042385742999613285 current acc: 0.8130311614730878\n",
      "iteration 30 current loss: 0.003292341250926256 current acc: 0.8130841121495327\n",
      "iteration 31 current loss: 0.003454327816143632 current acc: 0.8131370328425821\n",
      "iteration 32 current loss: 0.002701000776141882 current acc: 0.813189923577696\n",
      "iteration 33 current loss: 0.0034409253858029842 current acc: 0.8132427843803056\n",
      "iteration 34 current loss: 0.0031823571771383286 current acc: 0.8132956152758133\n",
      "iteration 35 current loss: 0.003796851262450218 current acc: 0.8133484162895928\n",
      "iteration 36 current loss: 0.0045160106383264065 current acc: 0.813401187446989\n",
      "iteration 37 current loss: 0.0035901141818612814 current acc: 0.8134539287733182\n",
      "iteration 38 current loss: 0.0031006811186671257 current acc: 0.8135066402938683\n",
      "iteration 39 current loss: 0.0030746120028197765 current acc: 0.8135593220338984\n",
      "iteration 40 current loss: 0.0025634062476456165 current acc: 0.8136119740186388\n",
      "iteration 41 current loss: 0.0023774155415594578 current acc: 0.8136645962732919\n",
      "iteration 42 current loss: 0.005443664267659187 current acc: 0.8137171888230313\n",
      "iteration 43 current loss: 0.004583217203617096 current acc: 0.8137697516930023\n",
      "iteration 44 current loss: 0.004008330870419741 current acc: 0.8138222849083215\n",
      "iteration 45 current loss: 0.004303850699216127 current acc: 0.8138747884940778\n",
      "iteration 46 current loss: 0.005361674353480339 current acc: 0.8139272624753313\n",
      "iteration 47 current loss: 0.003059623297303915 current acc: 0.8139797068771139\n",
      "iteration 48 current loss: 0.003096408909186721 current acc: 0.8140321217244294\n",
      "iteration 49 current loss: 0.004926176276057959 current acc: 0.8140845070422535\n",
      "iteration 50 current loss: 0.00507740443572402 current acc: 0.8141368628555337\n",
      "iteration 51 current loss: 0.003626109566539526 current acc: 0.8141891891891891\n",
      "iteration 52 current loss: 0.0027951074298471212 current acc: 0.8142414860681114\n",
      "iteration 53 current loss: 0.004555093124508858 current acc: 0.8142937535171637\n",
      "iteration 54 current loss: 0.002417260780930519 current acc: 0.8143459915611815\n",
      "iteration 55 current loss: 0.003081244882196188 current acc: 0.8143982002249719\n",
      "iteration 56 current loss: 0.003041687421500683 current acc: 0.8144503795333146\n",
      "iteration 57 current loss: 0.002599423285573721 current acc: 0.8145025295109612\n",
      "iteration 58 current loss: 0.0024243402294814587 current acc: 0.8145546501826356\n",
      "iteration 59 current loss: 0.003383159637451172 current acc: 0.8146067415730337\n",
      "iteration 60 current loss: 0.002294112229719758 current acc: 0.814658803706824\n",
      "iteration 61 current loss: 0.0037897182628512383 current acc: 0.8147108366086468\n",
      "iteration 62 current loss: 0.0023062843829393387 current acc: 0.8147628403031153\n",
      "iteration 63 current loss: 0.003908288665115833 current acc: 0.8148148148148148\n",
      "iteration 64 current loss: 0.0029709297232329845 current acc: 0.814866760168303\n",
      "iteration 65 current loss: 0.00582614541053772 current acc: 0.8149186763881099\n",
      "iteration 66 current loss: 0.004034103825688362 current acc: 0.8149705634987384\n",
      "iteration 67 current loss: 0.0028631091117858887 current acc: 0.8150224215246636\n",
      "iteration 68 current loss: 0.005380718037486076 current acc: 0.8150742504903334\n",
      "iteration 69 current loss: 0.005059122573584318 current acc: 0.8151260504201681\n",
      "iteration 70 current loss: 0.0034607257694005966 current acc: 0.8151778213385606\n",
      "iteration 71 current loss: 0.0032414558809250593 current acc: 0.8152295632698768\n",
      "iteration 72 current loss: 0.002523185685276985 current acc: 0.815281276238455\n",
      "iteration 73 current loss: 0.002627940382808447 current acc: 0.8153329602686066\n",
      "iteration 74 current loss: 0.005819113925099373 current acc: 0.8153846153846154\n",
      "iteration 75 current loss: 0.002594598801806569 current acc: 0.8154362416107382\n",
      "iteration 76 current loss: 0.0034014296252280474 current acc: 0.8154878389712049\n",
      "iteration 77 current loss: 0.006706867832690477 current acc: 0.815539407490218\n",
      "iteration 78 current loss: 0.0024524712935090065 current acc: 0.815590947191953\n",
      "iteration 79 current loss: 0.00332696083933115 current acc: 0.8156424581005587\n",
      "iteration 80 current loss: 0.0029594609513878822 current acc: 0.8156939402401564\n",
      "iteration 81 current loss: 0.0031280971597880125 current acc: 0.8157453936348409\n",
      "iteration 82 current loss: 0.003772749798372388 current acc: 0.8157968183086799\n",
      "iteration 83 current loss: 0.0037416231352835894 current acc: 0.8158482142857143\n",
      "iteration 84 current loss: 0.004486638121306896 current acc: 0.8158995815899581\n",
      "iteration 85 current loss: 0.0038254561368376017 current acc: 0.8159509202453987\n",
      "iteration 86 current loss: 0.0032914208713918924 current acc: 0.8160022302759966\n",
      "iteration 87 current loss: 0.004497869405895472 current acc: 0.8160535117056856\n",
      "iteration 88 current loss: 0.0021152780391275883 current acc: 0.8161047645583728\n",
      "iteration 89 current loss: 0.00360494083724916 current acc: 0.8161559888579387\n",
      "iteration 90 current loss: 0.0034055025316774845 current acc: 0.8162071846282373\n",
      "iteration 91 current loss: 0.0030610787216573954 current acc: 0.8162583518930958\n",
      "iteration 92 current loss: 0.004414733499288559 current acc: 0.8163094906763151\n",
      "iteration 93 current loss: 0.0029340265318751335 current acc: 0.8163606010016694\n",
      "iteration 94 current loss: 0.003937149420380592 current acc: 0.8164116828929068\n",
      "iteration 95 current loss: 0.0040532792918384075 current acc: 0.8164627363737486\n",
      "iteration 96 current loss: 0.0034302694257348776 current acc: 0.8165137614678899\n",
      "iteration 97 current loss: 0.0036181581672281027 current acc: 0.8165647581989994\n",
      "iteration 98 current loss: 0.0032798254396766424 current acc: 0.8166157265907197\n",
      "iteration 99 current loss: 0.003734425874426961 current acc: 0.8166666666666667\n",
      "\t\tEpoch 35/100 complete. Epoch loss 0.003533588673453778 Epoch accuracy 1.0\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 35, Validation Accuracy: 0.635375, Validation Loss: 1.6561513658612967\n",
      "best loss 0.003533588673453778\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.0019013986457139254 current acc: 0.8167175784504305\n",
      "iteration 1 current loss: 0.0022959148045629263 current acc: 0.8167684619655747\n",
      "iteration 2 current loss: 0.004356049466878176 current acc: 0.8168193172356369\n",
      "iteration 3 current loss: 0.001617741771042347 current acc: 0.8168701442841287\n",
      "iteration 4 current loss: 0.0024875567760318518 current acc: 0.8169209431345353\n",
      "iteration 5 current loss: 0.003684031078591943 current acc: 0.8169717138103162\n",
      "iteration 6 current loss: 0.002993748988956213 current acc: 0.8170224563349043\n",
      "iteration 7 current loss: 0.0032718577422201633 current acc: 0.8170731707317073\n",
      "iteration 8 current loss: 0.0027906927280128 current acc: 0.8171238570241064\n",
      "iteration 9 current loss: 0.002423439407721162 current acc: 0.817174515235457\n",
      "iteration 10 current loss: 0.003174775280058384 current acc: 0.8172251453890889\n",
      "iteration 11 current loss: 0.0028867998626083136 current acc: 0.8172757475083057\n",
      "iteration 12 current loss: 0.00326352147385478 current acc: 0.8173263216163853\n",
      "iteration 13 current loss: 0.00280945235863328 current acc: 0.8173768677365799\n",
      "iteration 14 current loss: 0.003435109741985798 current acc: 0.8174273858921162\n",
      "iteration 15 current loss: 0.002808299381285906 current acc: 0.8174778761061947\n",
      "iteration 16 current loss: 0.0024755613412708044 current acc: 0.8175283384019906\n",
      "iteration 17 current loss: 0.0021036318503320217 current acc: 0.8175787728026535\n",
      "iteration 18 current loss: 0.0023258356377482414 current acc: 0.817629179331307\n",
      "iteration 19 current loss: 0.0028357976116240025 current acc: 0.8176795580110497\n",
      "iteration 20 current loss: 0.0023812989238649607 current acc: 0.8177299088649544\n",
      "iteration 21 current loss: 0.002922181272879243 current acc: 0.8177802319160685\n",
      "iteration 22 current loss: 0.0049925874918699265 current acc: 0.8178305271874138\n",
      "iteration 23 current loss: 0.0022644212003797293 current acc: 0.8178807947019867\n",
      "iteration 24 current loss: 0.0035192447248846292 current acc: 0.8179310344827586\n",
      "iteration 25 current loss: 0.0027151433750987053 current acc: 0.8179812465526751\n",
      "iteration 26 current loss: 0.0017372636357322335 current acc: 0.8180314309346567\n",
      "iteration 27 current loss: 0.00297632347792387 current acc: 0.8180815876515987\n",
      "iteration 28 current loss: 0.0023330708499997854 current acc: 0.8181317167263709\n",
      "iteration 29 current loss: 0.0023787685204297304 current acc: 0.8181818181818182\n",
      "iteration 30 current loss: 0.00212494726292789 current acc: 0.8182318920407601\n",
      "iteration 31 current loss: 0.0032723958138376474 current acc: 0.8182819383259912\n",
      "iteration 32 current loss: 0.0027769804000854492 current acc: 0.8183319570602807\n",
      "iteration 33 current loss: 0.004448405932635069 current acc: 0.8183819482663731\n",
      "iteration 34 current loss: 0.0021371066104620695 current acc: 0.8184319119669876\n",
      "iteration 35 current loss: 0.002514989348128438 current acc: 0.8184818481848185\n",
      "iteration 36 current loss: 0.0030769454315304756 current acc: 0.8185317569425351\n",
      "iteration 37 current loss: 0.0017579927807673812 current acc: 0.8185816382627817\n",
      "iteration 38 current loss: 0.00208464777097106 current acc: 0.8186314921681781\n",
      "iteration 39 current loss: 0.003163814777508378 current acc: 0.8186813186813187\n",
      "iteration 40 current loss: 0.0025864585768431425 current acc: 0.8187311178247734\n",
      "iteration 41 current loss: 0.0025940837804228067 current acc: 0.8187808896210873\n",
      "iteration 42 current loss: 0.002469313330948353 current acc: 0.8188306340927807\n",
      "iteration 43 current loss: 0.0057052201591432095 current acc: 0.818880351262349\n",
      "iteration 44 current loss: 0.004945028107613325 current acc: 0.8189300411522634\n",
      "iteration 45 current loss: 0.003377825953066349 current acc: 0.8189797037849699\n",
      "iteration 46 current loss: 0.0028470647521317005 current acc: 0.81902933918289\n",
      "iteration 47 current loss: 0.0034193946048617363 current acc: 0.819078947368421\n",
      "iteration 48 current loss: 0.005238335579633713 current acc: 0.8191285283639353\n",
      "iteration 49 current loss: 0.0020546745508909225 current acc: 0.8191780821917808\n",
      "iteration 50 current loss: 0.0031384453177452087 current acc: 0.819227608874281\n",
      "iteration 51 current loss: 0.0028519367333501577 current acc: 0.8192771084337349\n",
      "iteration 52 current loss: 0.00253658858127892 current acc: 0.8193265808924172\n",
      "iteration 53 current loss: 0.0028459457680583 current acc: 0.819376026272578\n",
      "iteration 54 current loss: 0.0025173290632665157 current acc: 0.8194254445964432\n",
      "iteration 55 current loss: 0.0029082894325256348 current acc: 0.8194748358862144\n",
      "iteration 56 current loss: 0.0019540193025022745 current acc: 0.8195242001640689\n",
      "iteration 57 current loss: 0.0027039276901632547 current acc: 0.8195735374521597\n",
      "iteration 58 current loss: 0.004922355525195599 current acc: 0.8196228477726155\n",
      "iteration 59 current loss: 0.0046774884685873985 current acc: 0.819672131147541\n",
      "iteration 60 current loss: 0.0016696497332304716 current acc: 0.8197213875990167\n",
      "iteration 61 current loss: 0.0037731241900473833 current acc: 0.8197706171490988\n",
      "iteration 62 current loss: 0.003675170475617051 current acc: 0.8198198198198198\n",
      "iteration 63 current loss: 0.0024006934836506844 current acc: 0.8198689956331878\n",
      "iteration 64 current loss: 0.0028549733106046915 current acc: 0.819918144611187\n",
      "iteration 65 current loss: 0.0036869433242827654 current acc: 0.8199672667757774\n",
      "iteration 66 current loss: 0.003710924880579114 current acc: 0.8200163621488955\n",
      "iteration 67 current loss: 0.006036374717950821 current acc: 0.8200654307524536\n",
      "iteration 68 current loss: 0.005330045707523823 current acc: 0.8201144726083401\n",
      "iteration 69 current loss: 0.0031201140955090523 current acc: 0.8201634877384196\n",
      "iteration 70 current loss: 0.0031448248773813248 current acc: 0.8202124761645329\n",
      "iteration 71 current loss: 0.0028008020017296076 current acc: 0.8202614379084967\n",
      "iteration 72 current loss: 0.0036342472303658724 current acc: 0.8203103729921045\n",
      "iteration 73 current loss: 0.0026891171000897884 current acc: 0.8203592814371258\n",
      "iteration 74 current loss: 0.0030727246776223183 current acc: 0.8204081632653061\n",
      "iteration 75 current loss: 0.0042981794103980064 current acc: 0.8204570184983678\n",
      "iteration 76 current loss: 0.003052994841709733 current acc: 0.8205058471580092\n",
      "iteration 77 current loss: 0.0023714301642030478 current acc: 0.8205546492659054\n",
      "iteration 78 current loss: 0.002842589979991317 current acc: 0.8206034248437075\n",
      "iteration 79 current loss: 0.003540232079103589 current acc: 0.8206521739130435\n",
      "iteration 80 current loss: 0.002353660063818097 current acc: 0.8207008964955175\n",
      "iteration 81 current loss: 0.0023938522208482027 current acc: 0.8207495926127105\n",
      "iteration 82 current loss: 0.0018036566907539964 current acc: 0.8207982622861797\n",
      "iteration 83 current loss: 0.0035660748835653067 current acc: 0.8208469055374593\n",
      "iteration 84 current loss: 0.0022856302093714476 current acc: 0.8208955223880597\n",
      "iteration 85 current loss: 0.0020192593801766634 current acc: 0.8209441128594682\n",
      "iteration 86 current loss: 0.002518861321732402 current acc: 0.8209926769731489\n",
      "iteration 87 current loss: 0.003593174275010824 current acc: 0.8210412147505423\n",
      "iteration 88 current loss: 0.0017206191550940275 current acc: 0.8210897262130659\n",
      "iteration 89 current loss: 0.002649657428264618 current acc: 0.8211382113821138\n",
      "iteration 90 current loss: 0.0016537851188331842 current acc: 0.8211866702790571\n",
      "iteration 91 current loss: 0.003589646192267537 current acc: 0.8212351029252438\n",
      "iteration 92 current loss: 0.0024963573087006807 current acc: 0.8212835093419983\n",
      "iteration 93 current loss: 0.005489538423717022 current acc: 0.8213318895506226\n",
      "iteration 94 current loss: 0.002569835865870118 current acc: 0.8213802435723951\n",
      "iteration 95 current loss: 0.0028619628865271807 current acc: 0.8214285714285714\n",
      "iteration 96 current loss: 0.0028467325028032064 current acc: 0.8214768731403841\n",
      "iteration 97 current loss: 0.0026749297976493835 current acc: 0.8215251487290427\n",
      "iteration 98 current loss: 0.0018994659185409546 current acc: 0.8215733982157339\n",
      "iteration 99 current loss: 0.0033627981320023537 current acc: 0.8216216216216217\n",
      "\t\tEpoch 36/100 complete. Epoch loss 0.002998681248864159 Epoch accuracy 1.0\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 36, Validation Accuracy: 0.63225, Validation Loss: 1.6949983153492212\n",
      "best loss 0.002998681248864159\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.0016972769517451525 current acc: 0.8216698189678465\n",
      "iteration 1 current loss: 0.003012046916410327 current acc: 0.8217179902755267\n",
      "iteration 2 current loss: 0.0029237635899335146 current acc: 0.8217661355657575\n",
      "iteration 3 current loss: 0.002554181031882763 current acc: 0.8218142548596112\n",
      "iteration 4 current loss: 0.0021949487272650003 current acc: 0.8218623481781376\n",
      "iteration 5 current loss: 0.0028320944402366877 current acc: 0.8219104155423638\n",
      "iteration 6 current loss: 0.001781338476575911 current acc: 0.8219584569732937\n",
      "iteration 7 current loss: 0.002131872111931443 current acc: 0.8220064724919094\n",
      "iteration 8 current loss: 0.0019588014110922813 current acc: 0.8220544621191695\n",
      "iteration 9 current loss: 0.0029056116472929716 current acc: 0.8221024258760108\n",
      "iteration 10 current loss: 0.0029430100694298744 current acc: 0.8221503637833468\n",
      "iteration 11 current loss: 0.002758691320195794 current acc: 0.822198275862069\n",
      "iteration 12 current loss: 0.0020745478104799986 current acc: 0.822246162133046\n",
      "iteration 13 current loss: 0.001653540413826704 current acc: 0.8222940226171244\n",
      "iteration 14 current loss: 0.002387471031397581 current acc: 0.8223418573351279\n",
      "iteration 15 current loss: 0.00251935888081789 current acc: 0.8223896663078579\n",
      "iteration 16 current loss: 0.0021690065041184425 current acc: 0.8224374495560937\n",
      "iteration 17 current loss: 0.0027439892292022705 current acc: 0.8224852071005917\n",
      "iteration 18 current loss: 0.002483634278178215 current acc: 0.8225329389620866\n",
      "iteration 19 current loss: 0.001916787470690906 current acc: 0.8225806451612904\n",
      "iteration 20 current loss: 0.002633054042235017 current acc: 0.8226283257188928\n",
      "iteration 21 current loss: 0.0027590589597821236 current acc: 0.8226759806555616\n",
      "iteration 22 current loss: 0.001993999583646655 current acc: 0.8227236099919419\n",
      "iteration 23 current loss: 0.0020296310540288687 current acc: 0.8227712137486574\n",
      "iteration 24 current loss: 0.0033405027352273464 current acc: 0.8228187919463087\n",
      "iteration 25 current loss: 0.0017590284114703536 current acc: 0.822866344605475\n",
      "iteration 26 current loss: 0.0025268960744142532 current acc: 0.8229138717467132\n",
      "iteration 27 current loss: 0.0020946552976965904 current acc: 0.822961373390558\n",
      "iteration 28 current loss: 0.0025551426224410534 current acc: 0.8230088495575221\n",
      "iteration 29 current loss: 0.002372992690652609 current acc: 0.8230563002680965\n",
      "iteration 30 current loss: 0.002774355700239539 current acc: 0.8231037255427499\n",
      "iteration 31 current loss: 0.0022313997615128756 current acc: 0.8231511254019293\n",
      "iteration 32 current loss: 0.002895939629524946 current acc: 0.8231984998660594\n",
      "iteration 33 current loss: 0.0024261195212602615 current acc: 0.8232458489555436\n",
      "iteration 34 current loss: 0.00456358352676034 current acc: 0.8232931726907631\n",
      "iteration 35 current loss: 0.0031840456649661064 current acc: 0.8233404710920771\n",
      "iteration 36 current loss: 0.0021818848326802254 current acc: 0.8233877441798234\n",
      "iteration 37 current loss: 0.0025079408660531044 current acc: 0.8234349919743178\n",
      "iteration 38 current loss: 0.002792815677821636 current acc: 0.8234822144958545\n",
      "iteration 39 current loss: 0.003577977418899536 current acc: 0.8235294117647058\n",
      "iteration 40 current loss: 0.001855217618867755 current acc: 0.8235765838011226\n",
      "iteration 41 current loss: 0.004581362474709749 current acc: 0.823623730625334\n",
      "iteration 42 current loss: 0.0024918399285525084 current acc: 0.8236708522575474\n",
      "iteration 43 current loss: 0.0028130500577390194 current acc: 0.8237179487179487\n",
      "iteration 44 current loss: 0.0031024038325995207 current acc: 0.8237650200267023\n",
      "iteration 45 current loss: 0.002489732578396797 current acc: 0.8238120662039509\n",
      "iteration 46 current loss: 0.0055897110141813755 current acc: 0.8238590872698158\n",
      "iteration 47 current loss: 0.0026060387026518583 current acc: 0.823906083244397\n",
      "iteration 48 current loss: 0.002170285675674677 current acc: 0.8239530541477728\n",
      "iteration 49 current loss: 0.0021936222910881042 current acc: 0.824\n",
      "iteration 50 current loss: 0.0029787146486341953 current acc: 0.8240469208211144\n",
      "iteration 51 current loss: 0.0027735289186239243 current acc: 0.82409381663113\n",
      "iteration 52 current loss: 0.002032727235928178 current acc: 0.82414068745004\n",
      "iteration 53 current loss: 0.002821217756718397 current acc: 0.8241875332978157\n",
      "iteration 54 current loss: 0.004957303870469332 current acc: 0.8242343541944075\n",
      "iteration 55 current loss: 0.004564532544463873 current acc: 0.8242811501597445\n",
      "iteration 56 current loss: 0.0014636621344834566 current acc: 0.8243279212137343\n",
      "iteration 57 current loss: 0.0018751505995169282 current acc: 0.824374667376264\n",
      "iteration 58 current loss: 0.0035103228874504566 current acc: 0.8244213886671987\n",
      "iteration 59 current loss: 0.002365758875384927 current acc: 0.824468085106383\n",
      "iteration 60 current loss: 0.0022491090930998325 current acc: 0.8245147567136399\n",
      "iteration 61 current loss: 0.0016189448069781065 current acc: 0.8245614035087719\n",
      "iteration 62 current loss: 0.0022529843263328075 current acc: 0.8246080255115599\n",
      "iteration 63 current loss: 0.0015007694019004703 current acc: 0.824654622741764\n",
      "iteration 64 current loss: 0.002610766561701894 current acc: 0.8247011952191236\n",
      "iteration 65 current loss: 0.003179245861247182 current acc: 0.8247477429633564\n",
      "iteration 66 current loss: 0.0033539854921400547 current acc: 0.8247942659941598\n",
      "iteration 67 current loss: 0.001866868231445551 current acc: 0.8248407643312102\n",
      "iteration 68 current loss: 0.0014169412897899747 current acc: 0.8248872379941629\n",
      "iteration 69 current loss: 0.001359203946776688 current acc: 0.8249336870026526\n",
      "iteration 70 current loss: 0.003412008285522461 current acc: 0.8249801113762928\n",
      "iteration 71 current loss: 0.0025245503056794405 current acc: 0.8250265111346765\n",
      "iteration 72 current loss: 0.0015609875554218888 current acc: 0.8250728862973761\n",
      "iteration 73 current loss: 0.0032237053383141756 current acc: 0.8251192368839427\n",
      "iteration 74 current loss: 0.003080700058490038 current acc: 0.8251655629139073\n",
      "iteration 75 current loss: 0.002017638646066189 current acc: 0.8252118644067796\n",
      "iteration 76 current loss: 0.002513277344405651 current acc: 0.8252581413820492\n",
      "iteration 77 current loss: 0.002770652761682868 current acc: 0.8253043938591847\n",
      "iteration 78 current loss: 0.0036726403050124645 current acc: 0.8253506218576343\n",
      "iteration 79 current loss: 0.0023656331468373537 current acc: 0.8253968253968254\n",
      "iteration 80 current loss: 0.0024422055575996637 current acc: 0.825443004496165\n",
      "iteration 81 current loss: 0.0019980466458946466 current acc: 0.8254891591750396\n",
      "iteration 82 current loss: 0.002258257009088993 current acc: 0.8255352894528152\n",
      "iteration 83 current loss: 0.0024338483344763517 current acc: 0.8255813953488372\n",
      "iteration 84 current loss: 0.0019203110132366419 current acc: 0.8256274768824307\n",
      "iteration 85 current loss: 0.0033942488953471184 current acc: 0.8256735340729001\n",
      "iteration 86 current loss: 0.0018981131725013256 current acc: 0.82571956693953\n",
      "iteration 87 current loss: 0.0018023737939074636 current acc: 0.8257655755015839\n",
      "iteration 88 current loss: 0.0035427797120064497 current acc: 0.8258115597783057\n",
      "iteration 89 current loss: 0.0022104219533503056 current acc: 0.8258575197889182\n",
      "iteration 90 current loss: 0.0015980784082785249 current acc: 0.8259034555526247\n",
      "iteration 91 current loss: 0.0021252308506518602 current acc: 0.8259493670886076\n",
      "iteration 92 current loss: 0.0028493076097220182 current acc: 0.8259952544160295\n",
      "iteration 93 current loss: 0.0020441741216927767 current acc: 0.8260411175540326\n",
      "iteration 94 current loss: 0.002035244135186076 current acc: 0.8260869565217391\n",
      "iteration 95 current loss: 0.005773250479251146 current acc: 0.8261327713382508\n",
      "iteration 96 current loss: 0.0055574942380189896 current acc: 0.8261785620226495\n",
      "iteration 97 current loss: 0.002957940101623535 current acc: 0.8262243285939969\n",
      "iteration 98 current loss: 0.003298740368336439 current acc: 0.8262700710713345\n",
      "iteration 99 current loss: 0.003558805212378502 current acc: 0.8263157894736842\n",
      "\t\tEpoch 37/100 complete. Epoch loss 0.002643286644015461 Epoch accuracy 1.0\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 37, Validation Accuracy: 0.634375, Validation Loss: 1.6751601196825505\n",
      "best loss 0.002643286644015461\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.0036482573486864567 current acc: 0.8263614838200474\n",
      "iteration 1 current loss: 0.0018830634653568268 current acc: 0.8264071541294056\n",
      "iteration 2 current loss: 0.0013251873897388577 current acc: 0.8264528004207204\n",
      "iteration 3 current loss: 0.0020099927205592394 current acc: 0.8264984227129337\n",
      "iteration 4 current loss: 0.0023231443483382463 current acc: 0.8265440210249672\n",
      "iteration 5 current loss: 0.0014747845707461238 current acc: 0.8265895953757225\n",
      "iteration 6 current loss: 0.0022104401141405106 current acc: 0.8266351457840819\n",
      "iteration 7 current loss: 0.001870207954198122 current acc: 0.8266806722689075\n",
      "iteration 8 current loss: 0.0015272670425474644 current acc: 0.8267261748490418\n",
      "iteration 9 current loss: 0.0016094216844066978 current acc: 0.8267716535433071\n",
      "iteration 10 current loss: 0.0015124049969017506 current acc: 0.8268171083705065\n",
      "iteration 11 current loss: 0.0025795113760977983 current acc: 0.8268625393494229\n",
      "iteration 12 current loss: 0.002483976539224386 current acc: 0.8269079464988198\n",
      "iteration 13 current loss: 0.0015741786919534206 current acc: 0.826953329837441\n",
      "iteration 14 current loss: 0.0021549086086452007 current acc: 0.8269986893840104\n",
      "iteration 15 current loss: 0.0020682746544480324 current acc: 0.8270440251572327\n",
      "iteration 16 current loss: 0.0018139812164008617 current acc: 0.8270893371757925\n",
      "iteration 17 current loss: 0.0014445662964135408 current acc: 0.8271346254583551\n",
      "iteration 18 current loss: 0.003639964386820793 current acc: 0.8271798900235664\n",
      "iteration 19 current loss: 0.004132696893066168 current acc: 0.8272251308900523\n",
      "iteration 20 current loss: 0.003741656197234988 current acc: 0.8272703480764197\n",
      "iteration 21 current loss: 0.001601757132448256 current acc: 0.8273155416012559\n",
      "iteration 22 current loss: 0.00286862556822598 current acc: 0.8273607114831284\n",
      "iteration 23 current loss: 0.002274216851219535 current acc: 0.8274058577405857\n",
      "iteration 24 current loss: 0.0015842957654967904 current acc: 0.8274509803921568\n",
      "iteration 25 current loss: 0.002147470600903034 current acc: 0.8274960794563513\n",
      "iteration 26 current loss: 0.001707940362393856 current acc: 0.8275411549516593\n",
      "iteration 27 current loss: 0.0016166303539648652 current acc: 0.8275862068965517\n",
      "iteration 28 current loss: 0.0013316716067492962 current acc: 0.8276312353094802\n",
      "iteration 29 current loss: 0.0014717535814270377 current acc: 0.8276762402088773\n",
      "iteration 30 current loss: 0.0018359934911131859 current acc: 0.8277212216131559\n",
      "iteration 31 current loss: 0.0017677885480225086 current acc: 0.8277661795407099\n",
      "iteration 32 current loss: 0.0030618563760071993 current acc: 0.8278111140099139\n",
      "iteration 33 current loss: 0.0017974138027057052 current acc: 0.8278560250391236\n",
      "iteration 34 current loss: 0.0031086700037121773 current acc: 0.8279009126466753\n",
      "iteration 35 current loss: 0.0024963419418781996 current acc: 0.8279457768508863\n",
      "iteration 36 current loss: 0.004021842963993549 current acc: 0.8279906176700548\n",
      "iteration 37 current loss: 0.004507026169449091 current acc: 0.8280354351224596\n",
      "iteration 38 current loss: 0.0016902768984436989 current acc: 0.828080229226361\n",
      "iteration 39 current loss: 0.0028205846901983023 current acc: 0.828125\n",
      "iteration 40 current loss: 0.0021366106811910868 current acc: 0.8281697474615985\n",
      "iteration 41 current loss: 0.003104860894382 current acc: 0.8282144716293597\n",
      "iteration 42 current loss: 0.0014298934256657958 current acc: 0.8282591725214676\n",
      "iteration 43 current loss: 0.0015868303598836064 current acc: 0.8283038501560874\n",
      "iteration 44 current loss: 0.0019647348672151566 current acc: 0.8283485045513654\n",
      "iteration 45 current loss: 0.00477151433005929 current acc: 0.828393135725429\n",
      "iteration 46 current loss: 0.001310064923018217 current acc: 0.8284377436963868\n",
      "iteration 47 current loss: 0.004260361660271883 current acc: 0.8284823284823285\n",
      "iteration 48 current loss: 0.002152637578547001 current acc: 0.828526890101325\n",
      "iteration 49 current loss: 0.0030628060922026634 current acc: 0.8285714285714286\n",
      "iteration 50 current loss: 0.001403606845997274 current acc: 0.8286159439106725\n",
      "iteration 51 current loss: 0.0024201839696615934 current acc: 0.8286604361370716\n",
      "iteration 52 current loss: 0.0022396419662982225 current acc: 0.8287049052686218\n",
      "iteration 53 current loss: 0.0020542459096759558 current acc: 0.8287493513233005\n",
      "iteration 54 current loss: 0.0020227571949362755 current acc: 0.8287937743190662\n",
      "iteration 55 current loss: 0.002276194281876087 current acc: 0.828838174273859\n",
      "iteration 56 current loss: 0.0027269995771348476 current acc: 0.8288825512056002\n",
      "iteration 57 current loss: 0.0020641193259507418 current acc: 0.8289269051321928\n",
      "iteration 58 current loss: 0.0014526555314660072 current acc: 0.8289712360715211\n",
      "iteration 59 current loss: 0.004852209705859423 current acc: 0.8290155440414507\n",
      "iteration 60 current loss: 0.0017698891460895538 current acc: 0.8290598290598291\n",
      "iteration 61 current loss: 0.0017940361285582185 current acc: 0.8291040911444847\n",
      "iteration 62 current loss: 0.001212084200233221 current acc: 0.8291483303132281\n",
      "iteration 63 current loss: 0.0022582781966775656 current acc: 0.8291925465838509\n",
      "iteration 64 current loss: 0.0015855259262025356 current acc: 0.8292367399741267\n",
      "iteration 65 current loss: 0.0033513223752379417 current acc: 0.8292809105018106\n",
      "iteration 66 current loss: 0.0034136944450438023 current acc: 0.8293250581846393\n",
      "iteration 67 current loss: 0.001346378936432302 current acc: 0.8293691830403309\n",
      "iteration 68 current loss: 0.0022119146306067705 current acc: 0.8294132850865856\n",
      "iteration 69 current loss: 0.001969631528481841 current acc: 0.8294573643410853\n",
      "iteration 70 current loss: 0.002525999443605542 current acc: 0.8295014208214931\n",
      "iteration 71 current loss: 0.002620468381792307 current acc: 0.8295454545454546\n",
      "iteration 72 current loss: 0.0016686941962689161 current acc: 0.8295894655305964\n",
      "iteration 73 current loss: 0.001793439732864499 current acc: 0.8296334537945276\n",
      "iteration 74 current loss: 0.0027644888032227755 current acc: 0.8296774193548387\n",
      "iteration 75 current loss: 0.0015257704071700573 current acc: 0.8297213622291022\n",
      "iteration 76 current loss: 0.002236782805994153 current acc: 0.8297652824348724\n",
      "iteration 77 current loss: 0.0026995674706995487 current acc: 0.8298091799896854\n",
      "iteration 78 current loss: 0.003195652272552252 current acc: 0.8298530549110595\n",
      "iteration 79 current loss: 0.001887661055661738 current acc: 0.8298969072164949\n",
      "iteration 80 current loss: 0.0013436407316476107 current acc: 0.8299407369234734\n",
      "iteration 81 current loss: 0.0027933793608099222 current acc: 0.8299845440494591\n",
      "iteration 82 current loss: 0.002309587085619569 current acc: 0.830028328611898\n",
      "iteration 83 current loss: 0.0032537183724343777 current acc: 0.8300720906282183\n",
      "iteration 84 current loss: 0.0027216083835810423 current acc: 0.8301158301158301\n",
      "iteration 85 current loss: 0.0029537868686020374 current acc: 0.8301595470921256\n",
      "iteration 86 current loss: 0.002805424388498068 current acc: 0.830203241574479\n",
      "iteration 87 current loss: 0.002699810778722167 current acc: 0.8302469135802469\n",
      "iteration 88 current loss: 0.0018312043976038694 current acc: 0.8302905631267679\n",
      "iteration 89 current loss: 0.0028814617544412613 current acc: 0.8303341902313625\n",
      "iteration 90 current loss: 0.0018329377053305507 current acc: 0.8303777949113339\n",
      "iteration 91 current loss: 0.001843294594436884 current acc: 0.8304213771839671\n",
      "iteration 92 current loss: 0.003144186222925782 current acc: 0.8304649370665297\n",
      "iteration 93 current loss: 0.0017966144951060414 current acc: 0.8305084745762712\n",
      "iteration 94 current loss: 0.001654831226915121 current acc: 0.8305519897304237\n",
      "iteration 95 current loss: 0.004801966715604067 current acc: 0.8305954825462012\n",
      "iteration 96 current loss: 0.0017603466985747218 current acc: 0.8306389530408006\n",
      "iteration 97 current loss: 0.0018094059778377414 current acc: 0.8306824012314007\n",
      "iteration 98 current loss: 0.0027898408006876707 current acc: 0.8307258271351629\n",
      "iteration 99 current loss: 0.0029445216059684753 current acc: 0.8307692307692308\n",
      "\t\tEpoch 38/100 complete. Epoch loss 0.002338358205743134 Epoch accuracy 1.0\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 38, Validation Accuracy: 0.635375, Validation Loss: 1.6941156014800072\n",
      "best loss 0.002338358205743134\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.006617456674575806 current acc: 0.8308126121507305\n",
      "iteration 1 current loss: 0.0019227771554142237 current acc: 0.8308559712967709\n",
      "iteration 2 current loss: 0.002226646989583969 current acc: 0.8308993082244427\n",
      "iteration 3 current loss: 0.0018468804191797972 current acc: 0.8309426229508197\n",
      "iteration 4 current loss: 0.002690017456188798 current acc: 0.8309859154929577\n",
      "iteration 5 current loss: 0.0022886674851179123 current acc: 0.8310291858678955\n",
      "iteration 6 current loss: 0.0013391231186687946 current acc: 0.8310724340926542\n",
      "iteration 7 current loss: 0.0010351705132052302 current acc: 0.8311156601842374\n",
      "iteration 8 current loss: 0.0016594416229054332 current acc: 0.8311588641596316\n",
      "iteration 9 current loss: 0.0020643218886107206 current acc: 0.8312020460358056\n",
      "iteration 10 current loss: 0.002170996740460396 current acc: 0.8312452058297111\n",
      "iteration 11 current loss: 0.0020982236601412296 current acc: 0.8312883435582822\n",
      "iteration 12 current loss: 0.00385739142075181 current acc: 0.831331459238436\n",
      "iteration 13 current loss: 0.0014680622844025493 current acc: 0.8313745528870721\n",
      "iteration 14 current loss: 0.002521028509363532 current acc: 0.8314176245210728\n",
      "iteration 15 current loss: 0.0018821198027580976 current acc: 0.8314606741573034\n",
      "iteration 16 current loss: 0.0017599648563191295 current acc: 0.8315037018126117\n",
      "iteration 17 current loss: 0.00136095704510808 current acc: 0.8315467075038285\n",
      "iteration 18 current loss: 0.003019759664312005 current acc: 0.8315896912477673\n",
      "iteration 19 current loss: 0.0017587958136573434 current acc: 0.8316326530612245\n",
      "iteration 20 current loss: 0.002254869556054473 current acc: 0.8316755929609794\n",
      "iteration 21 current loss: 0.0013958171475678682 current acc: 0.831718510963794\n",
      "iteration 22 current loss: 0.002327581634745002 current acc: 0.8317614070864134\n",
      "iteration 23 current loss: 0.002290966920554638 current acc: 0.8318042813455657\n",
      "iteration 24 current loss: 0.0016505967359989882 current acc: 0.8318471337579618\n",
      "iteration 25 current loss: 0.0023078799713402987 current acc: 0.8318899643402955\n",
      "iteration 26 current loss: 0.0014370500575751066 current acc: 0.8319327731092437\n",
      "iteration 27 current loss: 0.0013172733597457409 current acc: 0.8319755600814664\n",
      "iteration 28 current loss: 0.002311422023922205 current acc: 0.8320183252736065\n",
      "iteration 29 current loss: 0.00222415872849524 current acc: 0.8320610687022901\n",
      "iteration 30 current loss: 0.0017559778643772006 current acc: 0.8321037903841262\n",
      "iteration 31 current loss: 0.0014943316346034408 current acc: 0.832146490335707\n",
      "iteration 32 current loss: 0.0021965240593999624 current acc: 0.8321891685736079\n",
      "iteration 33 current loss: 0.0020790724083781242 current acc: 0.8322318251143874\n",
      "iteration 34 current loss: 0.004941185005009174 current acc: 0.832274459974587\n",
      "iteration 35 current loss: 0.0024166537914425135 current acc: 0.8323170731707317\n",
      "iteration 36 current loss: 0.002864469075575471 current acc: 0.8323596647193294\n",
      "iteration 37 current loss: 0.0034437954891473055 current acc: 0.8324022346368715\n",
      "iteration 38 current loss: 0.0020655535627156496 current acc: 0.8324447829398325\n",
      "iteration 39 current loss: 0.0026225594338029623 current acc: 0.8324873096446701\n",
      "iteration 40 current loss: 0.0026458129286766052 current acc: 0.8325298147678254\n",
      "iteration 41 current loss: 0.0020748875103890896 current acc: 0.832572298325723\n",
      "iteration 42 current loss: 0.001551480614580214 current acc: 0.8326147603347704\n",
      "iteration 43 current loss: 0.001548140193335712 current acc: 0.8326572008113591\n",
      "iteration 44 current loss: 0.001859375974163413 current acc: 0.8326996197718631\n",
      "iteration 45 current loss: 0.0018283636309206486 current acc: 0.8327420172326406\n",
      "iteration 46 current loss: 0.0012528160586953163 current acc: 0.832784393210033\n",
      "iteration 47 current loss: 0.002106740139424801 current acc: 0.8328267477203647\n",
      "iteration 48 current loss: 0.00161121622659266 current acc: 0.8328690807799443\n",
      "iteration 49 current loss: 0.0021914513781666756 current acc: 0.8329113924050633\n",
      "iteration 50 current loss: 0.0023748744279146194 current acc: 0.832953682611997\n",
      "iteration 51 current loss: 0.0020744355861097574 current acc: 0.832995951417004\n",
      "iteration 52 current loss: 0.003140709362924099 current acc: 0.8330381988363268\n",
      "iteration 53 current loss: 0.0020590415224432945 current acc: 0.8330804248861912\n",
      "iteration 54 current loss: 0.0024554084520787 current acc: 0.8331226295828066\n",
      "iteration 55 current loss: 0.0018629394471645355 current acc: 0.833164812942366\n",
      "iteration 56 current loss: 0.0017240509623661637 current acc: 0.8332069749810462\n",
      "iteration 57 current loss: 0.0031423941254615784 current acc: 0.8332491157150076\n",
      "iteration 58 current loss: 0.004999552387744188 current acc: 0.833291235160394\n",
      "iteration 59 current loss: 0.001870858483016491 current acc: 0.8333333333333334\n",
      "iteration 60 current loss: 0.0026531387120485306 current acc: 0.8333754102499369\n",
      "iteration 61 current loss: 0.0036742014344781637 current acc: 0.8334174659262998\n",
      "iteration 62 current loss: 0.001897535752505064 current acc: 0.8334595003785011\n",
      "iteration 63 current loss: 0.0019759340211749077 current acc: 0.8335015136226034\n",
      "iteration 64 current loss: 0.0015329901361837983 current acc: 0.8335435056746532\n",
      "iteration 65 current loss: 0.001422253204509616 current acc: 0.8335854765506808\n",
      "iteration 66 current loss: 0.0017657217103987932 current acc: 0.8336274262667003\n",
      "iteration 67 current loss: 0.005181465297937393 current acc: 0.8336693548387096\n",
      "iteration 68 current loss: 0.006944339722394943 current acc: 0.8337112622826909\n",
      "iteration 69 current loss: 0.0035467692650854588 current acc: 0.8337531486146096\n",
      "iteration 70 current loss: 0.004211774095892906 current acc: 0.8337950138504155\n",
      "iteration 71 current loss: 0.002482239855453372 current acc: 0.8338368580060423\n",
      "iteration 72 current loss: 0.0026570898480713367 current acc: 0.8338786810974075\n",
      "iteration 73 current loss: 0.0020814179442822933 current acc: 0.8339204831404127\n",
      "iteration 74 current loss: 0.0025138070341199636 current acc: 0.8339622641509434\n",
      "iteration 75 current loss: 0.0019166449783369899 current acc: 0.8340040241448692\n",
      "iteration 76 current loss: 0.0021077417768538 current acc: 0.8340457631380438\n",
      "iteration 77 current loss: 0.00141018140129745 current acc: 0.8340874811463047\n",
      "iteration 78 current loss: 0.002155150519683957 current acc: 0.8341291781854737\n",
      "iteration 79 current loss: 0.0034040797036141157 current acc: 0.8341708542713567\n",
      "iteration 80 current loss: 0.002424859208986163 current acc: 0.8342125094197438\n",
      "iteration 81 current loss: 0.0014739721082150936 current acc: 0.8342541436464088\n",
      "iteration 82 current loss: 0.013003135100007057 current acc: 0.8342957569671102\n",
      "iteration 83 current loss: 0.0024553651455789804 current acc: 0.8343373493975904\n",
      "iteration 84 current loss: 0.002071400871500373 current acc: 0.8343789209535759\n",
      "iteration 85 current loss: 0.0018887920305132866 current acc: 0.8344204716507777\n",
      "iteration 86 current loss: 0.0024293086025863886 current acc: 0.8344620015048909\n",
      "iteration 87 current loss: 0.002057046862319112 current acc: 0.8345035105315948\n",
      "iteration 88 current loss: 0.002484391676262021 current acc: 0.834544998746553\n",
      "iteration 89 current loss: 0.002545067109167576 current acc: 0.8345864661654135\n",
      "iteration 90 current loss: 0.0019851436372846365 current acc: 0.8346279128038085\n",
      "iteration 91 current loss: 0.004444797523319721 current acc: 0.8346693386773547\n",
      "iteration 92 current loss: 0.005109828896820545 current acc: 0.8347107438016529\n",
      "iteration 93 current loss: 0.005444997455924749 current acc: 0.8347521281922884\n",
      "iteration 94 current loss: 0.0027245408855378628 current acc: 0.8347934918648311\n",
      "iteration 95 current loss: 0.0018153199926018715 current acc: 0.8348348348348348\n",
      "iteration 96 current loss: 0.001450869720429182 current acc: 0.8348761571178384\n",
      "iteration 97 current loss: 0.0035349929239600897 current acc: 0.8349174587293647\n",
      "iteration 98 current loss: 0.003943831194192171 current acc: 0.8349587396849212\n",
      "iteration 99 current loss: 0.0030522102024406195 current acc: 0.835\n",
      "\t\tEpoch 39/100 complete. Epoch loss 0.0025723443855531513 Epoch accuracy 1.0\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 39, Validation Accuracy: 0.628875, Validation Loss: 1.7607046999037266\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.0023772292770445347 current acc: 0.8350412396900775\n",
      "iteration 1 current loss: 0.0014017949579283595 current acc: 0.8350824587706147\n",
      "iteration 2 current loss: 0.002194533124566078 current acc: 0.8351236572570572\n",
      "iteration 3 current loss: 0.0020297809969633818 current acc: 0.8351648351648352\n",
      "iteration 4 current loss: 0.0020175783429294825 current acc: 0.8352059925093633\n",
      "iteration 5 current loss: 0.0012825626181438565 current acc: 0.835247129306041\n",
      "iteration 6 current loss: 0.0019021735060960054 current acc: 0.835288245570252\n",
      "iteration 7 current loss: 0.0015014542732387781 current acc: 0.8353293413173652\n",
      "iteration 8 current loss: 0.002213667379692197 current acc: 0.8353704165627338\n",
      "iteration 9 current loss: 0.0023767068050801754 current acc: 0.8354114713216958\n",
      "iteration 10 current loss: 0.00212596682831645 current acc: 0.8354525056095736\n",
      "iteration 11 current loss: 0.001984938280656934 current acc: 0.8354935194416749\n",
      "iteration 12 current loss: 0.002199562732130289 current acc: 0.8355345128332918\n",
      "iteration 13 current loss: 0.003760037710890174 current acc: 0.8355754857997011\n",
      "iteration 14 current loss: 0.003236179705709219 current acc: 0.8356164383561644\n",
      "iteration 15 current loss: 0.004238579422235489 current acc: 0.8356573705179283\n",
      "iteration 16 current loss: 0.0020152954384684563 current acc: 0.835698282300224\n",
      "iteration 17 current loss: 0.0022236329969018698 current acc: 0.8357391737182678\n",
      "iteration 18 current loss: 0.003410888370126486 current acc: 0.8357800447872605\n",
      "iteration 19 current loss: 0.0019930668640881777 current acc: 0.835820895522388\n",
      "iteration 20 current loss: 0.0037078920286148787 current acc: 0.8358617259388212\n",
      "iteration 21 current loss: 0.0028441434260457754 current acc: 0.8359025360517156\n",
      "iteration 22 current loss: 0.0024097643326967955 current acc: 0.8359433258762118\n",
      "iteration 23 current loss: 0.0016726377652958035 current acc: 0.8359840954274353\n",
      "iteration 24 current loss: 0.0017377528129145503 current acc: 0.8360248447204969\n",
      "iteration 25 current loss: 0.0022617289796471596 current acc: 0.8360655737704918\n",
      "iteration 26 current loss: 0.0023122779093682766 current acc: 0.8361062825925006\n",
      "iteration 27 current loss: 0.0015086978673934937 current acc: 0.8361469712015889\n",
      "iteration 28 current loss: 0.0023674361873418093 current acc: 0.8361876396128072\n",
      "iteration 29 current loss: 0.00557298306375742 current acc: 0.8362282878411911\n",
      "iteration 30 current loss: 0.0019889967516064644 current acc: 0.8362689159017613\n",
      "iteration 31 current loss: 0.0016517923213541508 current acc: 0.8363095238095238\n",
      "iteration 32 current loss: 0.002759029855951667 current acc: 0.8363501115794694\n",
      "iteration 33 current loss: 0.0023553804494440556 current acc: 0.8363906792265742\n",
      "iteration 34 current loss: 0.0018116828287020326 current acc: 0.8364312267657993\n",
      "iteration 35 current loss: 0.0021080973092466593 current acc: 0.8364717542120912\n",
      "iteration 36 current loss: 0.0038606771267950535 current acc: 0.8365122615803815\n",
      "iteration 37 current loss: 0.0022757318802177906 current acc: 0.836552748885587\n",
      "iteration 38 current loss: 0.0022581880912184715 current acc: 0.8365932161426095\n",
      "iteration 39 current loss: 0.002348426729440689 current acc: 0.8366336633663366\n",
      "iteration 40 current loss: 0.0015565749490633607 current acc: 0.8366740905716407\n",
      "iteration 41 current loss: 0.001813773182220757 current acc: 0.8367144977733795\n",
      "iteration 42 current loss: 0.0024453196674585342 current acc: 0.8367548849863963\n",
      "iteration 43 current loss: 0.0012119257589802146 current acc: 0.8367952522255193\n",
      "iteration 44 current loss: 0.0024999685119837523 current acc: 0.8368355995055624\n",
      "iteration 45 current loss: 0.0016973491292446852 current acc: 0.8368759268413247\n",
      "iteration 46 current loss: 0.0015415824018418789 current acc: 0.8369162342475908\n",
      "iteration 47 current loss: 0.0014990827767178416 current acc: 0.8369565217391305\n",
      "iteration 48 current loss: 0.0025306011084467173 current acc: 0.836996789330699\n",
      "iteration 49 current loss: 0.002052030758932233 current acc: 0.837037037037037\n",
      "iteration 50 current loss: 0.002501665148884058 current acc: 0.8370772648728709\n",
      "iteration 51 current loss: 0.0017796925967559218 current acc: 0.8371174728529122\n",
      "iteration 52 current loss: 0.0036760056391358376 current acc: 0.8371576609918578\n",
      "iteration 53 current loss: 0.0016141858650371432 current acc: 0.8371978293043907\n",
      "iteration 54 current loss: 0.0011564334854483604 current acc: 0.8372379778051788\n",
      "iteration 55 current loss: 0.0016603836556896567 current acc: 0.8372781065088757\n",
      "iteration 56 current loss: 0.0024869393091648817 current acc: 0.8373182154301207\n",
      "iteration 57 current loss: 0.0032516208011657 current acc: 0.8373583045835387\n",
      "iteration 58 current loss: 0.0018587852828204632 current acc: 0.8373983739837398\n",
      "iteration 59 current loss: 0.0018040022114291787 current acc: 0.8374384236453202\n",
      "iteration 60 current loss: 0.0016123552341014147 current acc: 0.8374784535828613\n",
      "iteration 61 current loss: 0.0020175385288894176 current acc: 0.8375184638109305\n",
      "iteration 62 current loss: 0.0015798363601788878 current acc: 0.8375584543440807\n",
      "iteration 63 current loss: 0.0030425358563661575 current acc: 0.8375984251968503\n",
      "iteration 64 current loss: 0.001500047743320465 current acc: 0.8376383763837638\n",
      "iteration 65 current loss: 0.002541904104873538 current acc: 0.8376783079193311\n",
      "iteration 66 current loss: 0.0019135500770062208 current acc: 0.8377182198180477\n",
      "iteration 67 current loss: 0.002200992312282324 current acc: 0.8377581120943953\n",
      "iteration 68 current loss: 0.0019265561131760478 current acc: 0.837797984762841\n",
      "iteration 69 current loss: 0.0016397106228396297 current acc: 0.8378378378378378\n",
      "iteration 70 current loss: 0.0015928121283650398 current acc: 0.8378776713338246\n",
      "iteration 71 current loss: 0.002269565360620618 current acc: 0.8379174852652259\n",
      "iteration 72 current loss: 0.0019870693795382977 current acc: 0.8379572796464523\n",
      "iteration 73 current loss: 0.0018276770133525133 current acc: 0.8379970544918999\n",
      "iteration 74 current loss: 0.001410576980561018 current acc: 0.838036809815951\n",
      "iteration 75 current loss: 0.002359071047976613 current acc: 0.8380765456329735\n",
      "iteration 76 current loss: 0.002895487006753683 current acc: 0.8381162619573216\n",
      "iteration 77 current loss: 0.0019464392680674791 current acc: 0.8381559588033349\n",
      "iteration 78 current loss: 0.0022884018253535032 current acc: 0.8381956361853395\n",
      "iteration 79 current loss: 0.002525398973375559 current acc: 0.8382352941176471\n",
      "iteration 80 current loss: 0.0018610571278259158 current acc: 0.8382749326145552\n",
      "iteration 81 current loss: 0.0023486290592700243 current acc: 0.8383145516903479\n",
      "iteration 82 current loss: 0.002165557350963354 current acc: 0.8383541513592946\n",
      "iteration 83 current loss: 0.0014355923049151897 current acc: 0.8383937316356513\n",
      "iteration 84 current loss: 0.0017115301452577114 current acc: 0.8384332925336597\n",
      "iteration 85 current loss: 0.0013305291067808867 current acc: 0.8384728340675477\n",
      "iteration 86 current loss: 0.0017237026477232575 current acc: 0.8385123562515292\n",
      "iteration 87 current loss: 0.001742136082611978 current acc: 0.8385518590998043\n",
      "iteration 88 current loss: 0.00120144197717309 current acc: 0.8385913426265591\n",
      "iteration 89 current loss: 0.002086850581690669 current acc: 0.8386308068459658\n",
      "iteration 90 current loss: 0.0021306565031409264 current acc: 0.8386702517721828\n",
      "iteration 91 current loss: 0.0034772774670273066 current acc: 0.8387096774193549\n",
      "iteration 92 current loss: 0.0018578689778223634 current acc: 0.8387490838016125\n",
      "iteration 93 current loss: 0.002291077747941017 current acc: 0.8387884709330727\n",
      "iteration 94 current loss: 0.0018984663765877485 current acc: 0.8388278388278388\n",
      "iteration 95 current loss: 0.0015788747696205974 current acc: 0.8388671875\n",
      "iteration 96 current loss: 0.0018859407864511013 current acc: 0.8389065169636319\n",
      "iteration 97 current loss: 0.0016682174755260348 current acc: 0.8389458272327965\n",
      "iteration 98 current loss: 0.003603095654398203 current acc: 0.8389851183215419\n",
      "iteration 99 current loss: 0.0024145334027707577 current acc: 0.8390243902439024\n",
      "\t\tEpoch 40/100 complete. Epoch loss 0.0021842743107117714 Epoch accuracy 1.0\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 40, Validation Accuracy: 0.632, Validation Loss: 1.7305149096995591\n",
      "best loss 0.0021842743107117714\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.0015758780064061284 current acc: 0.839063643013899\n",
      "iteration 1 current loss: 0.0012701167725026608 current acc: 0.8391028766455387\n",
      "iteration 2 current loss: 0.0013690126361325383 current acc: 0.839142091152815\n",
      "iteration 3 current loss: 0.0018870729254558682 current acc: 0.8391812865497076\n",
      "iteration 4 current loss: 0.0017186260083690286 current acc: 0.8392204628501827\n",
      "iteration 5 current loss: 0.001769242575392127 current acc: 0.8392596200681929\n",
      "iteration 6 current loss: 0.0015044568572193384 current acc: 0.8392987582176772\n",
      "iteration 7 current loss: 0.0013744838070124388 current acc: 0.8393378773125608\n",
      "iteration 8 current loss: 0.0022461016196757555 current acc: 0.8393769773667559\n",
      "iteration 9 current loss: 0.0016376185230910778 current acc: 0.8394160583941606\n",
      "iteration 10 current loss: 0.0013701674761250615 current acc: 0.8394551204086597\n",
      "iteration 11 current loss: 0.0018857738468796015 current acc: 0.8394941634241245\n",
      "iteration 12 current loss: 0.0021000925917178392 current acc: 0.8395331874544129\n",
      "iteration 13 current loss: 0.002139141783118248 current acc: 0.839572192513369\n",
      "iteration 14 current loss: 0.0019000672036781907 current acc: 0.8396111786148238\n",
      "iteration 15 current loss: 0.0026797845494002104 current acc: 0.8396501457725948\n",
      "iteration 16 current loss: 0.002197270980104804 current acc: 0.8396890940004857\n",
      "iteration 17 current loss: 0.0007322821184061468 current acc: 0.8397280233122875\n",
      "iteration 18 current loss: 0.0013928106054663658 current acc: 0.8397669337217771\n",
      "iteration 19 current loss: 0.002081867540255189 current acc: 0.8398058252427184\n",
      "iteration 20 current loss: 0.002682527294382453 current acc: 0.839844697888862\n",
      "iteration 21 current loss: 0.0016939410706982017 current acc: 0.8398835516739447\n",
      "iteration 22 current loss: 0.001580747659318149 current acc: 0.8399223866116905\n",
      "iteration 23 current loss: 0.0016159153310582042 current acc: 0.8399612027158099\n",
      "iteration 24 current loss: 0.001357743050903082 current acc: 0.84\n",
      "iteration 25 current loss: 0.0014835729962214828 current acc: 0.8400387784779447\n",
      "iteration 26 current loss: 0.0014588080812245607 current acc: 0.8400775381633148\n",
      "iteration 27 current loss: 0.0014579003909602761 current acc: 0.8401162790697675\n",
      "iteration 28 current loss: 0.0016467676032334566 current acc: 0.840155001210947\n",
      "iteration 29 current loss: 0.00110546313226223 current acc: 0.8401937046004843\n",
      "iteration 30 current loss: 0.0013282332802191377 current acc: 0.8402323892519971\n",
      "iteration 31 current loss: 0.0012348677264526486 current acc: 0.8402710551790901\n",
      "iteration 32 current loss: 0.00105206947773695 current acc: 0.8403097023953545\n",
      "iteration 33 current loss: 0.0016497726319357753 current acc: 0.8403483309143687\n",
      "iteration 34 current loss: 0.001216853386722505 current acc: 0.8403869407496977\n",
      "iteration 35 current loss: 0.0022382123861461878 current acc: 0.8404255319148937\n",
      "iteration 36 current loss: 0.0017718804301694036 current acc: 0.8404641044234953\n",
      "iteration 37 current loss: 0.0012256818590685725 current acc: 0.8405026582890285\n",
      "iteration 38 current loss: 0.0020660259760916233 current acc: 0.840541193525006\n",
      "iteration 39 current loss: 0.0019336389377713203 current acc: 0.8405797101449275\n",
      "iteration 40 current loss: 0.001723246998153627 current acc: 0.8406182081622796\n",
      "iteration 41 current loss: 0.0031698434613645077 current acc: 0.840656687590536\n",
      "iteration 42 current loss: 0.0012994431890547276 current acc: 0.8406951484431572\n",
      "iteration 43 current loss: 0.0012318993685767055 current acc: 0.8407335907335908\n",
      "iteration 44 current loss: 0.001198130426928401 current acc: 0.8407720144752714\n",
      "iteration 45 current loss: 0.0016092477599158883 current acc: 0.8408104196816208\n",
      "iteration 46 current loss: 0.0013332216767594218 current acc: 0.8408488063660478\n",
      "iteration 47 current loss: 0.001455775462090969 current acc: 0.840887174541948\n",
      "iteration 48 current loss: 0.001585041405633092 current acc: 0.8409255242227043\n",
      "iteration 49 current loss: 0.001736963982693851 current acc: 0.8409638554216867\n",
      "iteration 50 current loss: 0.0013986771227791905 current acc: 0.8410021681522525\n",
      "iteration 51 current loss: 0.0014058757806196809 current acc: 0.8410404624277457\n",
      "iteration 52 current loss: 0.002377380384132266 current acc: 0.8410787382614977\n",
      "iteration 53 current loss: 0.0021273507736623287 current acc: 0.8411169956668272\n",
      "iteration 54 current loss: 0.001833832124248147 current acc: 0.8411552346570397\n",
      "iteration 55 current loss: 0.001377095584757626 current acc: 0.8411934552454283\n",
      "iteration 56 current loss: 0.0019235893851146102 current acc: 0.8412316574452731\n",
      "iteration 57 current loss: 0.0023540447000414133 current acc: 0.8412698412698413\n",
      "iteration 58 current loss: 0.001583737088367343 current acc: 0.8413080067323876\n",
      "iteration 59 current loss: 0.0010734949028119445 current acc: 0.8413461538461539\n",
      "iteration 60 current loss: 0.0011722688795998693 current acc: 0.8413842826243692\n",
      "iteration 61 current loss: 0.0009429958299733698 current acc: 0.8414223930802499\n",
      "iteration 62 current loss: 0.0012719741789624095 current acc: 0.8414604852269998\n",
      "iteration 63 current loss: 0.00107075204141438 current acc: 0.8414985590778098\n",
      "iteration 64 current loss: 0.0009747776202857494 current acc: 0.8415366146458584\n",
      "iteration 65 current loss: 0.0011030151508748531 current acc: 0.8415746519443111\n",
      "iteration 66 current loss: 0.001683795708231628 current acc: 0.8416126709863211\n",
      "iteration 67 current loss: 0.0012306035496294498 current acc: 0.8416506717850288\n",
      "iteration 68 current loss: 0.0015364226419478655 current acc: 0.841688654353562\n",
      "iteration 69 current loss: 0.0022735162638127804 current acc: 0.841726618705036\n",
      "iteration 70 current loss: 0.0010497057810425758 current acc: 0.8417645648525534\n",
      "iteration 71 current loss: 0.0014645764604210854 current acc: 0.8418024928092043\n",
      "iteration 72 current loss: 0.0013531050644814968 current acc: 0.8418404025880661\n",
      "iteration 73 current loss: 0.0013560898369178176 current acc: 0.8418782942022042\n",
      "iteration 74 current loss: 0.0016476858872920275 current acc: 0.8419161676646707\n",
      "iteration 75 current loss: 0.0012412997893989086 current acc: 0.8419540229885057\n",
      "iteration 76 current loss: 0.0014645254705101252 current acc: 0.8419918601867369\n",
      "iteration 77 current loss: 0.0015005043242126703 current acc: 0.8420296792723792\n",
      "iteration 78 current loss: 0.0012567720841616392 current acc: 0.842067480258435\n",
      "iteration 79 current loss: 0.001643660943955183 current acc: 0.8421052631578947\n",
      "iteration 80 current loss: 0.0014076258521527052 current acc: 0.842143027983736\n",
      "iteration 81 current loss: 0.0017189610516652465 current acc: 0.8421807747489239\n",
      "iteration 82 current loss: 0.0015824059955775738 current acc: 0.8422185034664117\n",
      "iteration 83 current loss: 0.0013148857979103923 current acc: 0.8422562141491395\n",
      "iteration 84 current loss: 0.0019879178144037724 current acc: 0.8422939068100358\n",
      "iteration 85 current loss: 0.0012272222666069865 current acc: 0.8423315814620163\n",
      "iteration 86 current loss: 0.0018476604018360376 current acc: 0.8423692381179843\n",
      "iteration 87 current loss: 0.001150875468738377 current acc: 0.8424068767908309\n",
      "iteration 88 current loss: 0.002189158694818616 current acc: 0.8424444974934352\n",
      "iteration 89 current loss: 0.0012615998275578022 current acc: 0.8424821002386634\n",
      "iteration 90 current loss: 0.0014372580917552114 current acc: 0.84251968503937\n",
      "iteration 91 current loss: 0.0014876526547595859 current acc: 0.8425572519083969\n",
      "iteration 92 current loss: 0.0015056696720421314 current acc: 0.8425948008585739\n",
      "iteration 93 current loss: 0.0014073869679123163 current acc: 0.8426323319027181\n",
      "iteration 94 current loss: 0.0013787670759484172 current acc: 0.8426698450536353\n",
      "iteration 95 current loss: 0.0011548460461199284 current acc: 0.8427073403241182\n",
      "iteration 96 current loss: 0.0014665488852187991 current acc: 0.8427448177269479\n",
      "iteration 97 current loss: 0.0010999968508258462 current acc: 0.8427822772748929\n",
      "iteration 98 current loss: 0.0016736106481403112 current acc: 0.8428197189807097\n",
      "iteration 99 current loss: 0.0009803913999348879 current acc: 0.8428571428571429\n",
      "\t\tEpoch 41/100 complete. Epoch loss 0.001569488716777414 Epoch accuracy 1.0\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 41, Validation Accuracy: 0.634625, Validation Loss: 1.7380268055945636\n",
      "best loss 0.001569488716777414\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.0014301111223176122 current acc: 0.8428945489169245\n",
      "iteration 1 current loss: 0.0013922300422564149 current acc: 0.8429319371727748\n",
      "iteration 2 current loss: 0.0013831997057422996 current acc: 0.8429693076374019\n",
      "iteration 3 current loss: 0.0022761744912713766 current acc: 0.8430066603235015\n",
      "iteration 4 current loss: 0.001403532805852592 current acc: 0.8430439952437574\n",
      "iteration 5 current loss: 0.0018297001952305436 current acc: 0.8430813124108416\n",
      "iteration 6 current loss: 0.001530130859464407 current acc: 0.8431186118374139\n",
      "iteration 7 current loss: 0.0008519229013472795 current acc: 0.8431558935361216\n",
      "iteration 8 current loss: 0.001024893717840314 current acc: 0.8431931575196009\n",
      "iteration 9 current loss: 0.0012452341616153717 current acc: 0.8432304038004751\n",
      "iteration 10 current loss: 0.001442081294953823 current acc: 0.8432676323913559\n",
      "iteration 11 current loss: 0.0014306810917332768 current acc: 0.8433048433048433\n",
      "iteration 12 current loss: 0.0013237323146313429 current acc: 0.8433420365535248\n",
      "iteration 13 current loss: 0.0016422702465206385 current acc: 0.8433792121499762\n",
      "iteration 14 current loss: 0.0015008823247626424 current acc: 0.8434163701067615\n",
      "iteration 15 current loss: 0.0013950675493106246 current acc: 0.8434535104364327\n",
      "iteration 16 current loss: 0.001814081915654242 current acc: 0.8434906331515295\n",
      "iteration 17 current loss: 0.0015708454884588718 current acc: 0.8435277382645804\n",
      "iteration 18 current loss: 0.0012586374068632722 current acc: 0.8435648257881014\n",
      "iteration 19 current loss: 0.0023190381471067667 current acc: 0.8436018957345972\n",
      "iteration 20 current loss: 0.0013778964057564735 current acc: 0.84363894811656\n",
      "iteration 21 current loss: 0.0012393536744639277 current acc: 0.8436759829464708\n",
      "iteration 22 current loss: 0.0014710243558511138 current acc: 0.8437130002367985\n",
      "iteration 23 current loss: 0.0010618790984153748 current acc: 0.84375\n",
      "iteration 24 current loss: 0.0016123073874041438 current acc: 0.8437869822485207\n",
      "iteration 25 current loss: 0.0012892521917819977 current acc: 0.8438239469947941\n",
      "iteration 26 current loss: 0.0011717347661033273 current acc: 0.843860894251242\n",
      "iteration 27 current loss: 0.0011359156342223287 current acc: 0.8438978240302744\n",
      "iteration 28 current loss: 0.0009351345943287015 current acc: 0.8439347363442894\n",
      "iteration 29 current loss: 0.0018184781074523926 current acc: 0.8439716312056738\n",
      "iteration 30 current loss: 0.0015018905978649855 current acc: 0.8440085086268022\n",
      "iteration 31 current loss: 0.0012482220772653818 current acc: 0.8440453686200378\n",
      "iteration 32 current loss: 0.0009647049009799957 current acc: 0.8440822111977321\n",
      "iteration 33 current loss: 0.0014200465520843863 current acc: 0.8441190363722249\n",
      "iteration 34 current loss: 0.0011210452066734433 current acc: 0.8441558441558441\n",
      "iteration 35 current loss: 0.0012970052193850279 current acc: 0.8441926345609065\n",
      "iteration 36 current loss: 0.0011834532488137484 current acc: 0.8442294075997168\n",
      "iteration 37 current loss: 0.00163693365175277 current acc: 0.8442661632845682\n",
      "iteration 38 current loss: 0.0015251707518473268 current acc: 0.8443029016277424\n",
      "iteration 39 current loss: 0.0012199720367789268 current acc: 0.8443396226415094\n",
      "iteration 40 current loss: 0.0008471377077512443 current acc: 0.8443763263381278\n",
      "iteration 41 current loss: 0.0011583416489884257 current acc: 0.8444130127298444\n",
      "iteration 42 current loss: 0.0013009917456656694 current acc: 0.8444496818288947\n",
      "iteration 43 current loss: 0.0020567409228533506 current acc: 0.8444863336475024\n",
      "iteration 44 current loss: 0.001141079468652606 current acc: 0.8445229681978799\n",
      "iteration 45 current loss: 0.0012956474674865603 current acc: 0.844559585492228\n",
      "iteration 46 current loss: 0.0009256797493435442 current acc: 0.8445961855427361\n",
      "iteration 47 current loss: 0.0014373287558555603 current acc: 0.844632768361582\n",
      "iteration 48 current loss: 0.0018828986212611198 current acc: 0.844669333960932\n",
      "iteration 49 current loss: 0.0010427755769342184 current acc: 0.8447058823529412\n",
      "iteration 50 current loss: 0.0021529782097786665 current acc: 0.844742413549753\n",
      "iteration 51 current loss: 0.0014828321291133761 current acc: 0.8447789275634995\n",
      "iteration 52 current loss: 0.0015391260385513306 current acc: 0.8448154244063014\n",
      "iteration 53 current loss: 0.0010961266234517097 current acc: 0.844851904090268\n",
      "iteration 54 current loss: 0.001376317348331213 current acc: 0.8448883666274971\n",
      "iteration 55 current loss: 0.0016915908781811595 current acc: 0.8449248120300752\n",
      "iteration 56 current loss: 0.0026440531946718693 current acc: 0.8449612403100775\n",
      "iteration 57 current loss: 0.001011691871099174 current acc: 0.8449976514795678\n",
      "iteration 58 current loss: 0.001551333931274712 current acc: 0.8450340455505987\n",
      "iteration 59 current loss: 0.0014209689106792212 current acc: 0.8450704225352113\n",
      "iteration 60 current loss: 0.0010137921199202538 current acc: 0.8451067824454354\n",
      "iteration 61 current loss: 0.0011988236801698804 current acc: 0.8451431252932895\n",
      "iteration 62 current loss: 0.0008495606598444283 current acc: 0.8451794510907812\n",
      "iteration 63 current loss: 0.0013273003278300166 current acc: 0.8452157598499062\n",
      "iteration 64 current loss: 0.0018798368982970715 current acc: 0.8452520515826495\n",
      "iteration 65 current loss: 0.001271515735425055 current acc: 0.8452883263009845\n",
      "iteration 66 current loss: 0.001863320474512875 current acc: 0.8453245840168737\n",
      "iteration 67 current loss: 0.0013647186569869518 current acc: 0.845360824742268\n",
      "iteration 68 current loss: 0.0011720554903149605 current acc: 0.8453970484891076\n",
      "iteration 69 current loss: 0.0010191615438088775 current acc: 0.8454332552693209\n",
      "iteration 70 current loss: 0.001196672092191875 current acc: 0.8454694450948256\n",
      "iteration 71 current loss: 0.0021765141282230616 current acc: 0.8455056179775281\n",
      "iteration 72 current loss: 0.001424890710040927 current acc: 0.8455417739293236\n",
      "iteration 73 current loss: 0.001329914783127606 current acc: 0.8455779129620964\n",
      "iteration 74 current loss: 0.001419980195350945 current acc: 0.8456140350877193\n",
      "iteration 75 current loss: 0.0026062382385134697 current acc: 0.8456501403180543\n",
      "iteration 76 current loss: 0.001371239311993122 current acc: 0.845686228664952\n",
      "iteration 77 current loss: 0.0015363465063273907 current acc: 0.8457223001402524\n",
      "iteration 78 current loss: 0.0014420957304537296 current acc: 0.8457583547557841\n",
      "iteration 79 current loss: 0.0010742980521172285 current acc: 0.8457943925233645\n",
      "iteration 80 current loss: 0.000723667093552649 current acc: 0.8458304134548003\n",
      "iteration 81 current loss: 0.0029706889763474464 current acc: 0.845866417561887\n",
      "iteration 82 current loss: 0.0013912710128352046 current acc: 0.8459024048564091\n",
      "iteration 83 current loss: 0.00191295996773988 current acc: 0.84593837535014\n",
      "iteration 84 current loss: 0.0011176601983606815 current acc: 0.8459743290548425\n",
      "iteration 85 current loss: 0.002159842988476157 current acc: 0.8460102659822678\n",
      "iteration 86 current loss: 0.0016724071465432644 current acc: 0.8460461861441567\n",
      "iteration 87 current loss: 0.0013197092339396477 current acc: 0.8460820895522388\n",
      "iteration 88 current loss: 0.0019844661001116037 current acc: 0.8461179762182327\n",
      "iteration 89 current loss: 0.0011785189853981137 current acc: 0.8461538461538461\n",
      "iteration 90 current loss: 0.0015501400921493769 current acc: 0.846189699370776\n",
      "iteration 91 current loss: 0.001328572747297585 current acc: 0.8462255358807083\n",
      "iteration 92 current loss: 0.0012828189646825194 current acc: 0.8462613556953179\n",
      "iteration 93 current loss: 0.0011647411156445742 current acc: 0.8462971588262692\n",
      "iteration 94 current loss: 0.0018979280721396208 current acc: 0.8463329452852154\n",
      "iteration 95 current loss: 0.002390472451224923 current acc: 0.8463687150837989\n",
      "iteration 96 current loss: 0.002118959091603756 current acc: 0.8464044682336513\n",
      "iteration 97 current loss: 0.0014579326380044222 current acc: 0.8464402047463937\n",
      "iteration 98 current loss: 0.001351926475763321 current acc: 0.8464759246336357\n",
      "iteration 99 current loss: 0.0006869860808365047 current acc: 0.8465116279069768\n",
      "\t\tEpoch 42/100 complete. Epoch loss 0.001455514538101852 Epoch accuracy 1.0\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 42, Validation Accuracy: 0.6325, Validation Loss: 1.7544691052287817\n",
      "best loss 0.001455514538101852\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.0013333684764802456 current acc: 0.8465473145780051\n",
      "iteration 1 current loss: 0.0012939205626025796 current acc: 0.8465829846582985\n",
      "iteration 2 current loss: 0.0012215384049341083 current acc: 0.8466186381594236\n",
      "iteration 3 current loss: 0.002222060225903988 current acc: 0.8466542750929368\n",
      "iteration 4 current loss: 0.0010611258912831545 current acc: 0.8466898954703833\n",
      "iteration 5 current loss: 0.001918027875944972 current acc: 0.8467254993032978\n",
      "iteration 6 current loss: 0.0010270377388224006 current acc: 0.846761086603204\n",
      "iteration 7 current loss: 0.0012463098391890526 current acc: 0.8467966573816156\n",
      "iteration 8 current loss: 0.0010130987502634525 current acc: 0.8468322116500349\n",
      "iteration 9 current loss: 0.0013401370961219072 current acc: 0.8468677494199536\n",
      "iteration 10 current loss: 0.0009853732772171497 current acc: 0.8469032707028532\n",
      "iteration 11 current loss: 0.0014885648852214217 current acc: 0.8469387755102041\n",
      "iteration 12 current loss: 0.001290059881284833 current acc: 0.8469742638534663\n",
      "iteration 13 current loss: 0.0010654927464202046 current acc: 0.847009735744089\n",
      "iteration 14 current loss: 0.0008390251314267516 current acc: 0.847045191193511\n",
      "iteration 15 current loss: 0.0011376721085980535 current acc: 0.8470806302131604\n",
      "iteration 16 current loss: 0.0010119981598109007 current acc: 0.8471160528144545\n",
      "iteration 17 current loss: 0.0015427006874233484 current acc: 0.8471514590088004\n",
      "iteration 18 current loss: 0.0009550589602440596 current acc: 0.8471868488075943\n",
      "iteration 19 current loss: 0.001051121624186635 current acc: 0.8472222222222222\n",
      "iteration 20 current loss: 0.001090907957404852 current acc: 0.8472575792640592\n",
      "iteration 21 current loss: 0.000969393877312541 current acc: 0.8472929199444702\n",
      "iteration 22 current loss: 0.001454611076042056 current acc: 0.8473282442748091\n",
      "iteration 23 current loss: 0.0016404372872784734 current acc: 0.84736355226642\n",
      "iteration 24 current loss: 0.0010245919693261385 current acc: 0.8473988439306358\n",
      "iteration 25 current loss: 0.0011081264819949865 current acc: 0.8474341192787794\n",
      "iteration 26 current loss: 0.0009364786674268544 current acc: 0.8474693783221632\n",
      "iteration 27 current loss: 0.0008391256560571492 current acc: 0.8475046210720887\n",
      "iteration 28 current loss: 0.0010216752998530865 current acc: 0.8475398475398476\n",
      "iteration 29 current loss: 0.00193092692643404 current acc: 0.8475750577367206\n",
      "iteration 30 current loss: 0.0025901233311742544 current acc: 0.8476102516739783\n",
      "iteration 31 current loss: 0.0011706622317433357 current acc: 0.8476454293628809\n",
      "iteration 32 current loss: 0.000687517283950001 current acc: 0.8476805908146781\n",
      "iteration 33 current loss: 0.001587091712281108 current acc: 0.8477157360406091\n",
      "iteration 34 current loss: 0.0014438740909099579 current acc: 0.8477508650519031\n",
      "iteration 35 current loss: 0.0018735260237008333 current acc: 0.8477859778597786\n",
      "iteration 36 current loss: 0.0016428977251052856 current acc: 0.8478210744754439\n",
      "iteration 37 current loss: 0.0014774580486118793 current acc: 0.8478561549100968\n",
      "iteration 38 current loss: 0.0008826428675092757 current acc: 0.847891219174925\n",
      "iteration 39 current loss: 0.0015736776404082775 current acc: 0.847926267281106\n",
      "iteration 40 current loss: 0.0012673973105847836 current acc: 0.8479612992398065\n",
      "iteration 41 current loss: 0.001224192907102406 current acc: 0.8479963150621833\n",
      "iteration 42 current loss: 0.001972437836229801 current acc: 0.8480313147593829\n",
      "iteration 43 current loss: 0.0010059188352897763 current acc: 0.8480662983425414\n",
      "iteration 44 current loss: 0.001211807131767273 current acc: 0.8481012658227848\n",
      "iteration 45 current loss: 0.001049605431035161 current acc: 0.8481362172112287\n",
      "iteration 46 current loss: 0.002056807279586792 current acc: 0.8481711525189786\n",
      "iteration 47 current loss: 0.0020026254933327436 current acc: 0.8482060717571297\n",
      "iteration 48 current loss: 0.0012467957567423582 current acc: 0.8482409749367671\n",
      "iteration 49 current loss: 0.0015323639381676912 current acc: 0.8482758620689655\n",
      "iteration 50 current loss: 0.001509197405539453 current acc: 0.8483107331647897\n",
      "iteration 51 current loss: 0.0008423278341069818 current acc: 0.8483455882352942\n",
      "iteration 52 current loss: 0.0021144349593669176 current acc: 0.8483804272915231\n",
      "iteration 53 current loss: 0.0013060251949355006 current acc: 0.8484152503445108\n",
      "iteration 54 current loss: 0.0013622489059343934 current acc: 0.8484500574052812\n",
      "iteration 55 current loss: 0.002339019440114498 current acc: 0.8484848484848485\n",
      "iteration 56 current loss: 0.0010560714872553945 current acc: 0.8485196235942162\n",
      "iteration 57 current loss: 0.0016125562833622098 current acc: 0.8485543827443781\n",
      "iteration 58 current loss: 0.0016603836556896567 current acc: 0.848589125946318\n",
      "iteration 59 current loss: 0.00107587652746588 current acc: 0.8486238532110092\n",
      "iteration 60 current loss: 0.0014407639391720295 current acc: 0.8486585645494152\n",
      "iteration 61 current loss: 0.0012652644654735923 current acc: 0.8486932599724897\n",
      "iteration 62 current loss: 0.0012123024789616466 current acc: 0.8487279394911758\n",
      "iteration 63 current loss: 0.0019230357138440013 current acc: 0.8487626031164069\n",
      "iteration 64 current loss: 0.0012120041064918041 current acc: 0.8487972508591065\n",
      "iteration 65 current loss: 0.0008717392920516431 current acc: 0.8488318827301878\n",
      "iteration 66 current loss: 0.0008390973671339452 current acc: 0.8488664987405542\n",
      "iteration 67 current loss: 0.002376171527430415 current acc: 0.8489010989010989\n",
      "iteration 68 current loss: 0.002036571968346834 current acc: 0.8489356832227054\n",
      "iteration 69 current loss: 0.0020434707403182983 current acc: 0.8489702517162472\n",
      "iteration 70 current loss: 0.00103241135366261 current acc: 0.8490048043925875\n",
      "iteration 71 current loss: 0.0010359128937125206 current acc: 0.8490393412625801\n",
      "iteration 72 current loss: 0.0011719715548679233 current acc: 0.8490738623370684\n",
      "iteration 73 current loss: 0.0007783619221299887 current acc: 0.8491083676268861\n",
      "iteration 74 current loss: 0.0012238159542903304 current acc: 0.8491428571428571\n",
      "iteration 75 current loss: 0.00080638169310987 current acc: 0.8491773308957953\n",
      "iteration 76 current loss: 0.001390152843669057 current acc: 0.8492117888965045\n",
      "iteration 77 current loss: 0.0008678337908349931 current acc: 0.8492462311557789\n",
      "iteration 78 current loss: 0.001354113221168518 current acc: 0.8492806576844029\n",
      "iteration 79 current loss: 0.0017642517341300845 current acc: 0.8493150684931506\n",
      "iteration 80 current loss: 0.001384500414133072 current acc: 0.8493494635927871\n",
      "iteration 81 current loss: 0.003583327867090702 current acc: 0.8493838429940667\n",
      "iteration 82 current loss: 0.0018283805111423135 current acc: 0.8494182067077344\n",
      "iteration 83 current loss: 0.001625636825338006 current acc: 0.8494525547445255\n",
      "iteration 84 current loss: 0.0013934588059782982 current acc: 0.8494868871151653\n",
      "iteration 85 current loss: 0.0016806743806228042 current acc: 0.8495212038303693\n",
      "iteration 86 current loss: 0.0019379938021302223 current acc: 0.8495555049008434\n",
      "iteration 87 current loss: 0.0011872899485751987 current acc: 0.8495897903372835\n",
      "iteration 88 current loss: 0.0010614272905513644 current acc: 0.849624060150376\n",
      "iteration 89 current loss: 0.001312133390456438 current acc: 0.8496583143507973\n",
      "iteration 90 current loss: 0.001224742503836751 current acc: 0.8496925529492143\n",
      "iteration 91 current loss: 0.0009483700268901885 current acc: 0.8497267759562842\n",
      "iteration 92 current loss: 0.0013398135779425502 current acc: 0.8497609833826543\n",
      "iteration 93 current loss: 0.0007607879233546555 current acc: 0.8497951752389622\n",
      "iteration 94 current loss: 0.0007906679529696703 current acc: 0.8498293515358362\n",
      "iteration 95 current loss: 0.0013208542950451374 current acc: 0.8498635122838945\n",
      "iteration 96 current loss: 0.0010606171563267708 current acc: 0.8498976574937457\n",
      "iteration 97 current loss: 0.001666865311563015 current acc: 0.849931787175989\n",
      "iteration 98 current loss: 0.0018941485323011875 current acc: 0.8499659013412139\n",
      "iteration 99 current loss: 0.0012275681365281343 current acc: 0.85\n",
      "\t\tEpoch 43/100 complete. Epoch loss 0.0013731241930508987 Epoch accuracy 1.0\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 43, Validation Accuracy: 0.635875, Validation Loss: 1.7589797638356686\n",
      "best loss 0.0013731241930508987\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.0015214962186291814 current acc: 0.8500340831629175\n",
      "iteration 1 current loss: 0.0011047092266380787 current acc: 0.850068150840527\n",
      "iteration 2 current loss: 0.00128941610455513 current acc: 0.8501022030433795\n",
      "iteration 3 current loss: 0.0007358396542258561 current acc: 0.8501362397820164\n",
      "iteration 4 current loss: 0.0010151313617825508 current acc: 0.8501702610669694\n",
      "iteration 5 current loss: 0.0009110886603593826 current acc: 0.8502042669087608\n",
      "iteration 6 current loss: 0.0012866605538874865 current acc: 0.8502382573179034\n",
      "iteration 7 current loss: 0.0009925852064043283 current acc: 0.8502722323049002\n",
      "iteration 8 current loss: 0.0009804071160033345 current acc: 0.8503061918802449\n",
      "iteration 9 current loss: 0.0016925744712352753 current acc: 0.8503401360544217\n",
      "iteration 10 current loss: 0.0009284132975153625 current acc: 0.8503740648379052\n",
      "iteration 11 current loss: 0.0011939737014472485 current acc: 0.8504079782411604\n",
      "iteration 12 current loss: 0.0008728325483389199 current acc: 0.8504418762746431\n",
      "iteration 13 current loss: 0.0010014761937782168 current acc: 0.8504757589487992\n",
      "iteration 14 current loss: 0.0009893561946228147 current acc: 0.8505096262740657\n",
      "iteration 15 current loss: 0.0008998060948215425 current acc: 0.8505434782608695\n",
      "iteration 16 current loss: 0.001317542977631092 current acc: 0.8505773149196287\n",
      "iteration 17 current loss: 0.0011243767803534865 current acc: 0.8506111362607515\n",
      "iteration 18 current loss: 0.0012099536834284663 current acc: 0.8506449422946368\n",
      "iteration 19 current loss: 0.001396874780766666 current acc: 0.8506787330316742\n",
      "iteration 20 current loss: 0.0012038367567583919 current acc: 0.8507125084822439\n",
      "iteration 21 current loss: 0.001139516127295792 current acc: 0.8507462686567164\n",
      "iteration 22 current loss: 0.0007763683679513633 current acc: 0.8507800135654533\n",
      "iteration 23 current loss: 0.0018491256050765514 current acc: 0.8508137432188065\n",
      "iteration 24 current loss: 0.001661851885728538 current acc: 0.8508474576271187\n",
      "iteration 25 current loss: 0.0010003100614994764 current acc: 0.850881156800723\n",
      "iteration 26 current loss: 0.0015151388943195343 current acc: 0.8509148407499435\n",
      "iteration 27 current loss: 0.0008458875236101449 current acc: 0.8509485094850948\n",
      "iteration 28 current loss: 0.0008492012857459486 current acc: 0.8509821630164823\n",
      "iteration 29 current loss: 0.001689500524662435 current acc: 0.8510158013544018\n",
      "iteration 30 current loss: 0.0009453861275687814 current acc: 0.8510494245091401\n",
      "iteration 31 current loss: 0.0010926774702966213 current acc: 0.8510830324909747\n",
      "iteration 32 current loss: 0.0009534833370707929 current acc: 0.8511166253101737\n",
      "iteration 33 current loss: 0.0007240179693326354 current acc: 0.851150202976996\n",
      "iteration 34 current loss: 0.001440607593394816 current acc: 0.8511837655016911\n",
      "iteration 35 current loss: 0.000782745482865721 current acc: 0.8512173128944995\n",
      "iteration 36 current loss: 0.0013492899015545845 current acc: 0.8512508451656524\n",
      "iteration 37 current loss: 0.0008191653178073466 current acc: 0.8512843623253717\n",
      "iteration 38 current loss: 0.001054501160979271 current acc: 0.8513178643838702\n",
      "iteration 39 current loss: 0.0010945798130705953 current acc: 0.8513513513513513\n",
      "iteration 40 current loss: 0.0012036805273965001 current acc: 0.8513848232380095\n",
      "iteration 41 current loss: 0.0010407385416328907 current acc: 0.8514182800540298\n",
      "iteration 42 current loss: 0.0009104127530008554 current acc: 0.8514517218095882\n",
      "iteration 43 current loss: 0.0010661266278475523 current acc: 0.8514851485148515\n",
      "iteration 44 current loss: 0.0009910066146403551 current acc: 0.8515185601799775\n",
      "iteration 45 current loss: 0.001191267860122025 current acc: 0.8515519568151148\n",
      "iteration 46 current loss: 0.0012260888470336795 current acc: 0.8515853384304025\n",
      "iteration 47 current loss: 0.0014691948890686035 current acc: 0.8516187050359713\n",
      "iteration 48 current loss: 0.0012799366377294064 current acc: 0.851652056641942\n",
      "iteration 49 current loss: 0.0009499979205429554 current acc: 0.851685393258427\n",
      "iteration 50 current loss: 0.0015165552031248808 current acc: 0.8517187148955291\n",
      "iteration 51 current loss: 0.0012856712564826012 current acc: 0.8517520215633423\n",
      "iteration 52 current loss: 0.0010207286104559898 current acc: 0.8517853132719515\n",
      "iteration 53 current loss: 0.0013336630072444677 current acc: 0.8518185900314325\n",
      "iteration 54 current loss: 0.0009326628060080111 current acc: 0.8518518518518519\n",
      "iteration 55 current loss: 0.0008070791373029351 current acc: 0.8518850987432675\n",
      "iteration 56 current loss: 0.0007878600154072046 current acc: 0.8519183307157281\n",
      "iteration 57 current loss: 0.0016395843122154474 current acc: 0.8519515477792732\n",
      "iteration 58 current loss: 0.0008934044162742794 current acc: 0.8519847499439336\n",
      "iteration 59 current loss: 0.0010407326044514775 current acc: 0.852017937219731\n",
      "iteration 60 current loss: 0.002276783110573888 current acc: 0.8520511096166778\n",
      "iteration 61 current loss: 0.0008287857635878026 current acc: 0.8520842671447781\n",
      "iteration 62 current loss: 0.0012664790265262127 current acc: 0.8521174098140264\n",
      "iteration 63 current loss: 0.0020651770755648613 current acc: 0.8521505376344086\n",
      "iteration 64 current loss: 0.0006716831121593714 current acc: 0.8521836506159015\n",
      "iteration 65 current loss: 0.0011421600356698036 current acc: 0.8522167487684729\n",
      "iteration 66 current loss: 0.0009379460825584829 current acc: 0.8522498321020819\n",
      "iteration 67 current loss: 0.0008748452528379858 current acc: 0.8522829006266786\n",
      "iteration 68 current loss: 0.0006991683621890843 current acc: 0.8523159543522041\n",
      "iteration 69 current loss: 0.0009766167495399714 current acc: 0.8523489932885906\n",
      "iteration 70 current loss: 0.0009214444435201585 current acc: 0.8523820174457616\n",
      "iteration 71 current loss: 0.0008214060217142105 current acc: 0.8524150268336315\n",
      "iteration 72 current loss: 0.0015218581538647413 current acc: 0.852448021462106\n",
      "iteration 73 current loss: 0.001736119040288031 current acc: 0.8524810013410818\n",
      "iteration 74 current loss: 0.0010082731023430824 current acc: 0.852513966480447\n",
      "iteration 75 current loss: 0.0012617463944479823 current acc: 0.8525469168900804\n",
      "iteration 76 current loss: 0.0011358092306181788 current acc: 0.8525798525798526\n",
      "iteration 77 current loss: 0.0010768163483589888 current acc: 0.8526127735596248\n",
      "iteration 78 current loss: 0.0010087197879329324 current acc: 0.8526456798392499\n",
      "iteration 79 current loss: 0.0009127226658165455 current acc: 0.8526785714285714\n",
      "iteration 80 current loss: 0.0005660941242240369 current acc: 0.8527114483374246\n",
      "iteration 81 current loss: 0.001422980334609747 current acc: 0.8527443105756358\n",
      "iteration 82 current loss: 0.0011934349313378334 current acc: 0.8527771581530226\n",
      "iteration 83 current loss: 0.0015187709359452128 current acc: 0.8528099910793934\n",
      "iteration 84 current loss: 0.0013630324974656105 current acc: 0.8528428093645485\n",
      "iteration 85 current loss: 0.0010669926414266229 current acc: 0.8528756130182791\n",
      "iteration 86 current loss: 0.001381969777867198 current acc: 0.8529084020503678\n",
      "iteration 87 current loss: 0.0016147653805091977 current acc: 0.8529411764705882\n",
      "iteration 88 current loss: 0.0010667025344446301 current acc: 0.8529739362887058\n",
      "iteration 89 current loss: 0.0015157264424487948 current acc: 0.8530066815144766\n",
      "iteration 90 current loss: 0.0007831832044757903 current acc: 0.8530394121576487\n",
      "iteration 91 current loss: 0.0009100924362428486 current acc: 0.8530721282279609\n",
      "iteration 92 current loss: 0.0021660621277987957 current acc: 0.8531048297351436\n",
      "iteration 93 current loss: 0.00172661361284554 current acc: 0.8531375166889186\n",
      "iteration 94 current loss: 0.0012539587914943695 current acc: 0.8531701890989989\n",
      "iteration 95 current loss: 0.0008949445327743888 current acc: 0.853202846975089\n",
      "iteration 96 current loss: 0.0017248860094696283 current acc: 0.8532354903268846\n",
      "iteration 97 current loss: 0.0011020263191312551 current acc: 0.853268119164073\n",
      "iteration 98 current loss: 0.0015409928746521473 current acc: 0.8533007334963325\n",
      "iteration 99 current loss: 0.0008829075959511101 current acc: 0.8533333333333334\n",
      "\t\tEpoch 44/100 complete. Epoch loss 0.0011667879548622295 Epoch accuracy 1.0\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 44, Validation Accuracy: 0.632, Validation Loss: 1.7677225343883038\n",
      "best loss 0.0011667879548622295\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.0021694174502044916 current acc: 0.8533659186847368\n",
      "iteration 1 current loss: 0.0013894465519115329 current acc: 0.8533984895601955\n",
      "iteration 2 current loss: 0.0007633392233401537 current acc: 0.8534310459693538\n",
      "iteration 3 current loss: 0.0014278306625783443 current acc: 0.8534635879218473\n",
      "iteration 4 current loss: 0.0017840418731793761 current acc: 0.853496115427303\n",
      "iteration 5 current loss: 0.0012401314452290535 current acc: 0.8535286284953395\n",
      "iteration 6 current loss: 0.0012952298857271671 current acc: 0.8535611271355669\n",
      "iteration 7 current loss: 0.0005275734583847225 current acc: 0.8535936113575865\n",
      "iteration 8 current loss: 0.0010095101315528154 current acc: 0.8536260811709914\n",
      "iteration 9 current loss: 0.001245028804987669 current acc: 0.8536585365853658\n",
      "iteration 10 current loss: 0.000776315457187593 current acc: 0.853690977610286\n",
      "iteration 11 current loss: 0.0025579887442290783 current acc: 0.8537234042553191\n",
      "iteration 12 current loss: 0.0008595894905738533 current acc: 0.8537558165300244\n",
      "iteration 13 current loss: 0.0012036613188683987 current acc: 0.8537882144439521\n",
      "iteration 14 current loss: 0.0009957263246178627 current acc: 0.8538205980066446\n",
      "iteration 15 current loss: 0.0016622482798993587 current acc: 0.853852967227635\n",
      "iteration 16 current loss: 0.0010874696308746934 current acc: 0.853885322116449\n",
      "iteration 17 current loss: 0.0009131450206041336 current acc: 0.8539176626826029\n",
      "iteration 18 current loss: 0.0015696260379627347 current acc: 0.8539499889356053\n",
      "iteration 19 current loss: 0.000867325346916914 current acc: 0.8539823008849557\n",
      "iteration 20 current loss: 0.0007084187236614525 current acc: 0.8540145985401459\n",
      "iteration 21 current loss: 0.0014991440111771226 current acc: 0.854046881910659\n",
      "iteration 22 current loss: 0.0024599682074040174 current acc: 0.8540791510059695\n",
      "iteration 23 current loss: 0.0008798193302936852 current acc: 0.8541114058355438\n",
      "iteration 24 current loss: 0.0006472338573075831 current acc: 0.8541436464088398\n",
      "iteration 25 current loss: 0.0012863054871559143 current acc: 0.8541758727353072\n",
      "iteration 26 current loss: 0.0009367649327032268 current acc: 0.854208084824387\n",
      "iteration 27 current loss: 0.0010404649656265974 current acc: 0.8542402826855123\n",
      "iteration 28 current loss: 0.0009969329694285989 current acc: 0.8542724663281077\n",
      "iteration 29 current loss: 0.0021154936403036118 current acc: 0.8543046357615894\n",
      "iteration 30 current loss: 0.001125237555243075 current acc: 0.8543367909953653\n",
      "iteration 31 current loss: 0.0007017386960797012 current acc: 0.8543689320388349\n",
      "iteration 32 current loss: 0.0009781046537682414 current acc: 0.8544010589013898\n",
      "iteration 33 current loss: 0.00063949084142223 current acc: 0.8544331715924128\n",
      "iteration 34 current loss: 0.0012018787674605846 current acc: 0.8544652701212789\n",
      "iteration 35 current loss: 0.0009814536897465587 current acc: 0.8544973544973545\n",
      "iteration 36 current loss: 0.0017353551229462028 current acc: 0.8545294247299978\n",
      "iteration 37 current loss: 0.0015779611421748996 current acc: 0.8545614808285589\n",
      "iteration 38 current loss: 0.0008748809923417866 current acc: 0.8545935228023793\n",
      "iteration 39 current loss: 0.0006965880165807903 current acc: 0.8546255506607929\n",
      "iteration 40 current loss: 0.0016289527993649244 current acc: 0.8546575644131249\n",
      "iteration 41 current loss: 0.0014255880378186703 current acc: 0.8546895640686922\n",
      "iteration 42 current loss: 0.0018633331637829542 current acc: 0.8547215496368039\n",
      "iteration 43 current loss: 0.0011768219992518425 current acc: 0.8547535211267606\n",
      "iteration 44 current loss: 0.0011043723206967115 current acc: 0.8547854785478548\n",
      "iteration 45 current loss: 0.0009749589371494949 current acc: 0.8548174219093708\n",
      "iteration 46 current loss: 0.0009264099644497037 current acc: 0.854849351220585\n",
      "iteration 47 current loss: 0.0014110526535660028 current acc: 0.8548812664907651\n",
      "iteration 48 current loss: 0.0015740995295345783 current acc: 0.8549131677291713\n",
      "iteration 49 current loss: 0.001000814838334918 current acc: 0.8549450549450549\n",
      "iteration 50 current loss: 0.0010208797175437212 current acc: 0.8549769281476599\n",
      "iteration 51 current loss: 0.0006295123021118343 current acc: 0.8550087873462214\n",
      "iteration 52 current loss: 0.0011149502824991941 current acc: 0.8550406325499671\n",
      "iteration 53 current loss: 0.001407862058840692 current acc: 0.855072463768116\n",
      "iteration 54 current loss: 0.0024267134722322226 current acc: 0.8551042810098792\n",
      "iteration 55 current loss: 0.001166667090728879 current acc: 0.8551360842844601\n",
      "iteration 56 current loss: 0.0024994316045194864 current acc: 0.8551678736010533\n",
      "iteration 57 current loss: 0.0010039933258667588 current acc: 0.855199648968846\n",
      "iteration 58 current loss: 0.0006767375161871314 current acc: 0.8552314103970169\n",
      "iteration 59 current loss: 0.001124848029576242 current acc: 0.8552631578947368\n",
      "iteration 60 current loss: 0.0008810756844468415 current acc: 0.8552948914711686\n",
      "iteration 61 current loss: 0.0013184467097744346 current acc: 0.8553266111354669\n",
      "iteration 62 current loss: 0.0009070649975910783 current acc: 0.8553583168967784\n",
      "iteration 63 current loss: 0.0007634391076862812 current acc: 0.8553900087642419\n",
      "iteration 64 current loss: 0.0015567122027277946 current acc: 0.8554216867469879\n",
      "iteration 65 current loss: 0.0015743827680125833 current acc: 0.8554533508541393\n",
      "iteration 66 current loss: 0.001116276835091412 current acc: 0.8554850010948106\n",
      "iteration 67 current loss: 0.0010783171746879816 current acc: 0.8555166374781086\n",
      "iteration 68 current loss: 0.0012198303593322635 current acc: 0.855548260013132\n",
      "iteration 69 current loss: 0.000986349186860025 current acc: 0.8555798687089715\n",
      "iteration 70 current loss: 0.0007292956579476595 current acc: 0.8556114635747101\n",
      "iteration 71 current loss: 0.0006689399015158415 current acc: 0.8556430446194225\n",
      "iteration 72 current loss: 0.0010705760214477777 current acc: 0.8556746118521759\n",
      "iteration 73 current loss: 0.000828411488328129 current acc: 0.8557061652820288\n",
      "iteration 74 current loss: 0.000730248517356813 current acc: 0.8557377049180328\n",
      "iteration 75 current loss: 0.0013718113768845797 current acc: 0.8557692307692307\n",
      "iteration 76 current loss: 0.0008174689719453454 current acc: 0.8558007428446581\n",
      "iteration 77 current loss: 0.0011735227890312672 current acc: 0.8558322411533421\n",
      "iteration 78 current loss: 0.0009296236094087362 current acc: 0.8558637257043022\n",
      "iteration 79 current loss: 0.001050055492669344 current acc: 0.8558951965065502\n",
      "iteration 80 current loss: 0.0011333051370456815 current acc: 0.8559266535690897\n",
      "iteration 81 current loss: 0.000911777897272259 current acc: 0.8559580969009166\n",
      "iteration 82 current loss: 0.0008627154165878892 current acc: 0.855989526511019\n",
      "iteration 83 current loss: 0.0008357393671758473 current acc: 0.856020942408377\n",
      "iteration 84 current loss: 0.000798255146946758 current acc: 0.8560523446019629\n",
      "iteration 85 current loss: 0.0009803742868825793 current acc: 0.8560837331007414\n",
      "iteration 86 current loss: 0.0009638084447942674 current acc: 0.8561151079136691\n",
      "iteration 87 current loss: 0.0010997157078236341 current acc: 0.8561464690496948\n",
      "iteration 88 current loss: 0.0007897127652540803 current acc: 0.8561778165177598\n",
      "iteration 89 current loss: 0.0007343400502577424 current acc: 0.8562091503267973\n",
      "iteration 90 current loss: 0.0013352547539398074 current acc: 0.8562404704857329\n",
      "iteration 91 current loss: 0.0010680181439965963 current acc: 0.8562717770034843\n",
      "iteration 92 current loss: 0.0007941219955682755 current acc: 0.8563030698889614\n",
      "iteration 93 current loss: 0.0008066881564445794 current acc: 0.8563343491510667\n",
      "iteration 94 current loss: 0.0009723617695271969 current acc: 0.8563656147986942\n",
      "iteration 95 current loss: 0.0008764564990997314 current acc: 0.856396866840731\n",
      "iteration 96 current loss: 0.0018700904911383986 current acc: 0.8564281052860562\n",
      "iteration 97 current loss: 0.0007628684979863465 current acc: 0.8564593301435407\n",
      "iteration 98 current loss: 0.0008571121143177152 current acc: 0.8564905414220483\n",
      "iteration 99 current loss: 0.002449526684358716 current acc: 0.8565217391304348\n",
      "\t\tEpoch 45/100 complete. Epoch loss 0.0011625997256487608 Epoch accuracy 1.0\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 45, Validation Accuracy: 0.63225, Validation Loss: 1.783832174539566\n",
      "best loss 0.0011625997256487608\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.0006399336270987988 current acc: 0.8565529232775484\n",
      "iteration 1 current loss: 0.001053225714713335 current acc: 0.8565840938722294\n",
      "iteration 2 current loss: 0.0009406788740307093 current acc: 0.8566152509233109\n",
      "iteration 3 current loss: 0.0009660613723099232 current acc: 0.8566463944396178\n",
      "iteration 4 current loss: 0.0013064844533801079 current acc: 0.8566775244299675\n",
      "iteration 5 current loss: 0.0007464264053851366 current acc: 0.8567086409031698\n",
      "iteration 6 current loss: 0.0012588694225996733 current acc: 0.8567397438680269\n",
      "iteration 7 current loss: 0.0006447788910008967 current acc: 0.8567708333333334\n",
      "iteration 8 current loss: 0.0008481759577989578 current acc: 0.8568019093078759\n",
      "iteration 9 current loss: 0.0009694616310298443 current acc: 0.8568329718004338\n",
      "iteration 10 current loss: 0.0016257939860224724 current acc: 0.8568640208197787\n",
      "iteration 11 current loss: 0.0008788058185018599 current acc: 0.8568950563746748\n",
      "iteration 12 current loss: 0.0008953344076871872 current acc: 0.8569260784738781\n",
      "iteration 13 current loss: 0.0007925244281068444 current acc: 0.8569570871261378\n",
      "iteration 14 current loss: 0.0006989006069488823 current acc: 0.856988082340195\n",
      "iteration 15 current loss: 0.0012311855098232627 current acc: 0.8570190641247833\n",
      "iteration 16 current loss: 0.0012026241747662425 current acc: 0.857050032488629\n",
      "iteration 17 current loss: 0.0008542510913684964 current acc: 0.8570809874404504\n",
      "iteration 18 current loss: 0.0008300500921905041 current acc: 0.8571119289889586\n",
      "iteration 19 current loss: 0.0005935738445259631 current acc: 0.8571428571428571\n",
      "iteration 20 current loss: 0.0006537868175655603 current acc: 0.8571737719108418\n",
      "iteration 21 current loss: 0.0008946529706008732 current acc: 0.857204673301601\n",
      "iteration 22 current loss: 0.0022073506843298674 current acc: 0.8572355613238157\n",
      "iteration 23 current loss: 0.0015122295590117574 current acc: 0.8572664359861591\n",
      "iteration 24 current loss: 0.0014063124544918537 current acc: 0.8572972972972973\n",
      "iteration 25 current loss: 0.0008968114270828664 current acc: 0.8573281452658884\n",
      "iteration 26 current loss: 0.0011729954276233912 current acc: 0.8573589799005835\n",
      "iteration 27 current loss: 0.0011262986809015274 current acc: 0.857389801210026\n",
      "iteration 28 current loss: 0.0006572127458639443 current acc: 0.8574206092028516\n",
      "iteration 29 current loss: 0.0009670367580838501 current acc: 0.857451403887689\n",
      "iteration 30 current loss: 0.0010531165171414614 current acc: 0.8574821852731591\n",
      "iteration 31 current loss: 0.001983275171369314 current acc: 0.8575129533678757\n",
      "iteration 32 current loss: 0.0008471575565636158 current acc: 0.8575437081804447\n",
      "iteration 33 current loss: 0.0009138249442912638 current acc: 0.8575744497194648\n",
      "iteration 34 current loss: 0.000802918104454875 current acc: 0.8576051779935275\n",
      "iteration 35 current loss: 0.0017963866703212261 current acc: 0.8576358930112166\n",
      "iteration 36 current loss: 0.0008675744174979627 current acc: 0.8576665947811085\n",
      "iteration 37 current loss: 0.0007527691195718944 current acc: 0.8576972833117723\n",
      "iteration 38 current loss: 0.0010882159695029259 current acc: 0.8577279586117698\n",
      "iteration 39 current loss: 0.00098792661447078 current acc: 0.8577586206896551\n",
      "iteration 40 current loss: 0.002486115088686347 current acc: 0.8577892695539754\n",
      "iteration 41 current loss: 0.0014011103194206953 current acc: 0.8578199052132701\n",
      "iteration 42 current loss: 0.0007006019586697221 current acc: 0.8578505276760715\n",
      "iteration 43 current loss: 0.0005996310501359403 current acc: 0.8578811369509044\n",
      "iteration 44 current loss: 0.000907432462554425 current acc: 0.8579117330462863\n",
      "iteration 45 current loss: 0.0010137815261259675 current acc: 0.8579423159707275\n",
      "iteration 46 current loss: 0.0014660469023510814 current acc: 0.8579728857327308\n",
      "iteration 47 current loss: 0.0005516182282008231 current acc: 0.8580034423407917\n",
      "iteration 48 current loss: 0.0006670292932540178 current acc: 0.8580339858033986\n",
      "iteration 49 current loss: 0.0009875132236629725 current acc: 0.8580645161290322\n",
      "iteration 50 current loss: 0.0006962901097722352 current acc: 0.8580950333261664\n",
      "iteration 51 current loss: 0.0010890323901548982 current acc: 0.8581255374032674\n",
      "iteration 52 current loss: 0.0007776407292112708 current acc: 0.8581560283687943\n",
      "iteration 53 current loss: 0.0009803110733628273 current acc: 0.858186506231199\n",
      "iteration 54 current loss: 0.0006418779958039522 current acc: 0.8582169709989259\n",
      "iteration 55 current loss: 0.000981701072305441 current acc: 0.8582474226804123\n",
      "iteration 56 current loss: 0.0006870114011690021 current acc: 0.8582778612840885\n",
      "iteration 57 current loss: 0.0007160519598983228 current acc: 0.858308286818377\n",
      "iteration 58 current loss: 0.0009626355022192001 current acc: 0.8583386992916935\n",
      "iteration 59 current loss: 0.0009265623521059752 current acc: 0.8583690987124464\n",
      "iteration 60 current loss: 0.0012193325674161315 current acc: 0.8583994850890367\n",
      "iteration 61 current loss: 0.0010215797228738666 current acc: 0.8584298584298584\n",
      "iteration 62 current loss: 0.0009817740647122264 current acc: 0.8584602187432983\n",
      "iteration 63 current loss: 0.0009030908113345504 current acc: 0.8584905660377359\n",
      "iteration 64 current loss: 0.0008732127607800066 current acc: 0.8585209003215434\n",
      "iteration 65 current loss: 0.001598332659341395 current acc: 0.8585512216030862\n",
      "iteration 66 current loss: 0.0012603109935298562 current acc: 0.8585815298907221\n",
      "iteration 67 current loss: 0.0013402980985119939 current acc: 0.8586118251928021\n",
      "iteration 68 current loss: 0.0007761713350191712 current acc: 0.8586421075176698\n",
      "iteration 69 current loss: 0.0006927886861376464 current acc: 0.8586723768736617\n",
      "iteration 70 current loss: 0.0009889265056699514 current acc: 0.8587026332691072\n",
      "iteration 71 current loss: 0.0009230377618223429 current acc: 0.8587328767123288\n",
      "iteration 72 current loss: 0.0010180044919252396 current acc: 0.8587631072116414\n",
      "iteration 73 current loss: 0.001344827702268958 current acc: 0.858793324775353\n",
      "iteration 74 current loss: 0.0011483700945973396 current acc: 0.8588235294117647\n",
      "iteration 75 current loss: 0.0017923394916579127 current acc: 0.8588537211291702\n",
      "iteration 76 current loss: 0.0007365223136730492 current acc: 0.8588838999358563\n",
      "iteration 77 current loss: 0.00048303467337973416 current acc: 0.8589140658401027\n",
      "iteration 78 current loss: 0.0013200038811191916 current acc: 0.8589442188501817\n",
      "iteration 79 current loss: 0.000838940090034157 current acc: 0.8589743589743589\n",
      "iteration 80 current loss: 0.0005934963119216263 current acc: 0.859004486220893\n",
      "iteration 81 current loss: 0.000985845923423767 current acc: 0.859034600598035\n",
      "iteration 82 current loss: 0.0008120913989841938 current acc: 0.8590647021140295\n",
      "iteration 83 current loss: 0.0008333277073688805 current acc: 0.8590947907771136\n",
      "iteration 84 current loss: 0.0009398215333931148 current acc: 0.8591248665955176\n",
      "iteration 85 current loss: 0.000739722978323698 current acc: 0.8591549295774648\n",
      "iteration 86 current loss: 0.0009599864715710282 current acc: 0.8591849797311714\n",
      "iteration 87 current loss: 0.0008191596134565771 current acc: 0.8592150170648464\n",
      "iteration 88 current loss: 0.0007957947091199458 current acc: 0.8592450415866922\n",
      "iteration 89 current loss: 0.0030068533960729837 current acc: 0.8592750533049041\n",
      "iteration 90 current loss: 0.0006526874494738877 current acc: 0.85930505222767\n",
      "iteration 91 current loss: 0.0010788657236844301 current acc: 0.8593350383631714\n",
      "iteration 92 current loss: 0.0008684591157361865 current acc: 0.8593650117195824\n",
      "iteration 93 current loss: 0.0008096916135400534 current acc: 0.8593949723050703\n",
      "iteration 94 current loss: 0.0011341869831085205 current acc: 0.8594249201277955\n",
      "iteration 95 current loss: 0.0011425701668486 current acc: 0.8594548551959114\n",
      "iteration 96 current loss: 0.0007558319484815001 current acc: 0.8594847775175644\n",
      "iteration 97 current loss: 0.0020282408222556114 current acc: 0.859514687100894\n",
      "iteration 98 current loss: 0.0005315210437402129 current acc: 0.8595445839540328\n",
      "iteration 99 current loss: 0.0011716715525835752 current acc: 0.8595744680851064\n",
      "\t\tEpoch 46/100 complete. Epoch loss 0.0010272571272798815 Epoch accuracy 1.0\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 46, Validation Accuracy: 0.631375, Validation Loss: 1.7964341100305319\n",
      "best loss 0.0010272571272798815\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.0010233669308945537 current acc: 0.8596043395022336\n",
      "iteration 1 current loss: 0.0007560229278169572 current acc: 0.8596341982135262\n",
      "iteration 2 current loss: 0.0006082426989451051 current acc: 0.8596640442270891\n",
      "iteration 3 current loss: 0.0007574143237434328 current acc: 0.8596938775510204\n",
      "iteration 4 current loss: 0.0006603340152651072 current acc: 0.8597236981934112\n",
      "iteration 5 current loss: 0.0007997106295078993 current acc: 0.8597535061623459\n",
      "iteration 6 current loss: 0.0005214705597609282 current acc: 0.8597833014659019\n",
      "iteration 7 current loss: 0.0007046511746011674 current acc: 0.8598130841121495\n",
      "iteration 8 current loss: 0.002108660526573658 current acc: 0.8598428541091527\n",
      "iteration 9 current loss: 0.0006171370041556656 current acc: 0.8598726114649682\n",
      "iteration 10 current loss: 0.0011372881708666682 current acc: 0.859902356187646\n",
      "iteration 11 current loss: 0.0017454352928325534 current acc: 0.8599320882852292\n",
      "iteration 12 current loss: 0.0007188015151768923 current acc: 0.8599618077657543\n",
      "iteration 13 current loss: 0.0035242068115621805 current acc: 0.8599915146372508\n",
      "iteration 14 current loss: 0.0007241888088174164 current acc: 0.8600212089077413\n",
      "iteration 15 current loss: 0.0011180199217051268 current acc: 0.8600508905852418\n",
      "iteration 16 current loss: 0.0005692368722520769 current acc: 0.8600805596777613\n",
      "iteration 17 current loss: 0.0009238461498171091 current acc: 0.8601102161933023\n",
      "iteration 18 current loss: 0.0007970449514687061 current acc: 0.8601398601398601\n",
      "iteration 19 current loss: 0.0007251043571159244 current acc: 0.8601694915254238\n",
      "iteration 20 current loss: 0.0008111601928249002 current acc: 0.860199110357975\n",
      "iteration 21 current loss: 0.0014559883857145905 current acc: 0.8602287166454892\n",
      "iteration 22 current loss: 0.0009001751313917339 current acc: 0.8602583103959348\n",
      "iteration 23 current loss: 0.0005908451275900006 current acc: 0.8602878916172735\n",
      "iteration 24 current loss: 0.0019401914905756712 current acc: 0.8603174603174604\n",
      "iteration 25 current loss: 0.0009739212691783905 current acc: 0.8603470165044435\n",
      "iteration 26 current loss: 0.00223699607886374 current acc: 0.8603765601861646\n",
      "iteration 27 current loss: 0.0017700730822980404 current acc: 0.8604060913705583\n",
      "iteration 28 current loss: 0.0011708905221894383 current acc: 0.860435610065553\n",
      "iteration 29 current loss: 0.0008292795973829925 current acc: 0.8604651162790697\n",
      "iteration 30 current loss: 0.0007882123463787138 current acc: 0.8604946100190235\n",
      "iteration 31 current loss: 0.0006143387290649116 current acc: 0.8605240912933221\n",
      "iteration 32 current loss: 0.0009533234406262636 current acc: 0.8605535601098669\n",
      "iteration 33 current loss: 0.0009445521282032132 current acc: 0.8605830164765526\n",
      "iteration 34 current loss: 0.0008279100293293595 current acc: 0.8606124604012672\n",
      "iteration 35 current loss: 0.000749812345020473 current acc: 0.8606418918918919\n",
      "iteration 36 current loss: 0.0010220651747658849 current acc: 0.8606713109563014\n",
      "iteration 37 current loss: 0.0007182666449807584 current acc: 0.8607007176023639\n",
      "iteration 38 current loss: 0.0009759988752193749 current acc: 0.8607301118379405\n",
      "iteration 39 current loss: 0.0006707185530103743 current acc: 0.8607594936708861\n",
      "iteration 40 current loss: 0.0006994589348323643 current acc: 0.8607888631090487\n",
      "iteration 41 current loss: 0.0006568289827555418 current acc: 0.8608182201602699\n",
      "iteration 42 current loss: 0.0008216279093176126 current acc: 0.8608475648323846\n",
      "iteration 43 current loss: 0.0011856493074446917 current acc: 0.8608768971332209\n",
      "iteration 44 current loss: 0.0011232375400140882 current acc: 0.8609062170706007\n",
      "iteration 45 current loss: 0.0007873140857554972 current acc: 0.8609355246523388\n",
      "iteration 46 current loss: 0.0014509788015857339 current acc: 0.860964819886244\n",
      "iteration 47 current loss: 0.0005066634039394557 current acc: 0.8609941027801179\n",
      "iteration 48 current loss: 0.0007946997066028416 current acc: 0.8610233733417562\n",
      "iteration 49 current loss: 0.0015608310932293534 current acc: 0.8610526315789474\n",
      "iteration 50 current loss: 0.0007647698512300849 current acc: 0.8610818774994738\n",
      "iteration 51 current loss: 0.0007091287989169359 current acc: 0.8611111111111112\n",
      "iteration 52 current loss: 0.000789315381553024 current acc: 0.8611403324216285\n",
      "iteration 53 current loss: 0.0007363673066720366 current acc: 0.8611695414387884\n",
      "iteration 54 current loss: 0.000859767897054553 current acc: 0.861198738170347\n",
      "iteration 55 current loss: 0.0017995641101151705 current acc: 0.8612279226240538\n",
      "iteration 56 current loss: 0.0007157651125453413 current acc: 0.8612570948076519\n",
      "iteration 57 current loss: 0.0008902827394194901 current acc: 0.8612862547288777\n",
      "iteration 58 current loss: 0.0007304854807443917 current acc: 0.8613154023954612\n",
      "iteration 59 current loss: 0.000511095451656729 current acc: 0.8613445378151261\n",
      "iteration 60 current loss: 0.0007792280521243811 current acc: 0.8613736609955892\n",
      "iteration 61 current loss: 0.0007294726092368364 current acc: 0.8614027719445612\n",
      "iteration 62 current loss: 0.0008966813329607248 current acc: 0.8614318706697459\n",
      "iteration 63 current loss: 0.0006823894218541682 current acc: 0.8614609571788413\n",
      "iteration 64 current loss: 0.001324881799519062 current acc: 0.8614900314795383\n",
      "iteration 65 current loss: 0.0008553708903491497 current acc: 0.8615190935795216\n",
      "iteration 66 current loss: 0.0008452481124550104 current acc: 0.8615481434864695\n",
      "iteration 67 current loss: 0.0012018117122352123 current acc: 0.8615771812080537\n",
      "iteration 68 current loss: 0.0006184843368828297 current acc: 0.8616062067519397\n",
      "iteration 69 current loss: 0.0007398211164399981 current acc: 0.8616352201257862\n",
      "iteration 70 current loss: 0.0007123076939024031 current acc: 0.8616642213372459\n",
      "iteration 71 current loss: 0.000593475007917732 current acc: 0.8616932103939648\n",
      "iteration 72 current loss: 0.0012754249619320035 current acc: 0.8617221873035826\n",
      "iteration 73 current loss: 0.0009810085175558925 current acc: 0.8617511520737328\n",
      "iteration 74 current loss: 0.0004293450037948787 current acc: 0.8617801047120419\n",
      "iteration 75 current loss: 0.0008306567906402051 current acc: 0.8618090452261307\n",
      "iteration 76 current loss: 0.0006222883239388466 current acc: 0.8618379736236131\n",
      "iteration 77 current loss: 0.0010373168624937534 current acc: 0.8618668899120971\n",
      "iteration 78 current loss: 0.0009162035421468318 current acc: 0.8618957940991839\n",
      "iteration 79 current loss: 0.0008097745012491941 current acc: 0.8619246861924686\n",
      "iteration 80 current loss: 0.0007748946663923562 current acc: 0.8619535661995399\n",
      "iteration 81 current loss: 0.0007681783754378557 current acc: 0.8619824341279799\n",
      "iteration 82 current loss: 0.0008055765647441149 current acc: 0.8620112899853648\n",
      "iteration 83 current loss: 0.0009067428181879222 current acc: 0.8620401337792643\n",
      "iteration 84 current loss: 0.0015513324178755283 current acc: 0.8620689655172413\n",
      "iteration 85 current loss: 0.0006850092322565615 current acc: 0.8620977852068533\n",
      "iteration 86 current loss: 0.0010904152877628803 current acc: 0.8621265928556507\n",
      "iteration 87 current loss: 0.0006876689731143415 current acc: 0.8621553884711779\n",
      "iteration 88 current loss: 0.0009627364343032241 current acc: 0.8621841720609731\n",
      "iteration 89 current loss: 0.0012283135438337922 current acc: 0.8622129436325678\n",
      "iteration 90 current loss: 0.0012370174517855048 current acc: 0.8622417031934878\n",
      "iteration 91 current loss: 0.0015381531557068229 current acc: 0.8622704507512521\n",
      "iteration 92 current loss: 0.0009091573301702738 current acc: 0.8622991863133737\n",
      "iteration 93 current loss: 0.0009041716693900526 current acc: 0.8623279098873592\n",
      "iteration 94 current loss: 0.000979817588813603 current acc: 0.862356621480709\n",
      "iteration 95 current loss: 0.0010393316624686122 current acc: 0.8623853211009175\n",
      "iteration 96 current loss: 0.0008298682514578104 current acc: 0.8624140087554721\n",
      "iteration 97 current loss: 0.0005516152596101165 current acc: 0.8624426844518549\n",
      "iteration 98 current loss: 0.0009365978185087442 current acc: 0.8624713481975411\n",
      "iteration 99 current loss: 0.0016411688411608338 current acc: 0.8625\n",
      "\t\tEpoch 47/100 complete. Epoch loss 0.0009648769261548295 Epoch accuracy 1.0\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 47, Validation Accuracy: 0.632125, Validation Loss: 1.8107887145131827\n",
      "best loss 0.0009648769261548295\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.0007308636559173465 current acc: 0.8625286398666945\n",
      "iteration 1 current loss: 0.0006476014968939126 current acc: 0.8625572678050812\n",
      "iteration 2 current loss: 0.0008691305411048234 current acc: 0.8625858838226109\n",
      "iteration 3 current loss: 0.000953805458266288 current acc: 0.8626144879267277\n",
      "iteration 4 current loss: 0.0007122666575014591 current acc: 0.8626430801248699\n",
      "iteration 5 current loss: 0.0006698505021631718 current acc: 0.8626716604244694\n",
      "iteration 6 current loss: 0.0011517893290147185 current acc: 0.8627002288329519\n",
      "iteration 7 current loss: 0.0004616021760739386 current acc: 0.8627287853577371\n",
      "iteration 8 current loss: 0.0007926516700536013 current acc: 0.8627573300062383\n",
      "iteration 9 current loss: 0.0004115987103432417 current acc: 0.8627858627858628\n",
      "iteration 10 current loss: 0.0007324130856432021 current acc: 0.8628143837040116\n",
      "iteration 11 current loss: 0.0010533768218010664 current acc: 0.8628428927680798\n",
      "iteration 12 current loss: 0.0005852674948982894 current acc: 0.8628713899854561\n",
      "iteration 13 current loss: 0.0008342963410541415 current acc: 0.8628998753635231\n",
      "iteration 14 current loss: 0.0008405180997215211 current acc: 0.8629283489096573\n",
      "iteration 15 current loss: 0.001011566026136279 current acc: 0.8629568106312292\n",
      "iteration 16 current loss: 0.0005658394657075405 current acc: 0.8629852605356031\n",
      "iteration 17 current loss: 0.0008655783021822572 current acc: 0.863013698630137\n",
      "iteration 18 current loss: 0.0007403582567349076 current acc: 0.863042124922183\n",
      "iteration 19 current loss: 0.0006570217665284872 current acc: 0.8630705394190872\n",
      "iteration 20 current loss: 0.000840259890537709 current acc: 0.8630989421281892\n",
      "iteration 21 current loss: 0.0005317191244103014 current acc: 0.8631273330568229\n",
      "iteration 22 current loss: 0.0005010648746974766 current acc: 0.863155712212316\n",
      "iteration 23 current loss: 0.0008774425368756056 current acc: 0.8631840796019901\n",
      "iteration 24 current loss: 0.0007502255612052977 current acc: 0.8632124352331606\n",
      "iteration 25 current loss: 0.0009097370784729719 current acc: 0.8632407791131371\n",
      "iteration 26 current loss: 0.0009973449632525444 current acc: 0.8632691112492231\n",
      "iteration 27 current loss: 0.0005678515299223363 current acc: 0.8632974316487159\n",
      "iteration 28 current loss: 0.0005589062930084765 current acc: 0.8633257403189066\n",
      "iteration 29 current loss: 0.0009289145236834884 current acc: 0.8633540372670807\n",
      "iteration 30 current loss: 0.0009654514724388719 current acc: 0.8633823225005175\n",
      "iteration 31 current loss: 0.0005771873984485865 current acc: 0.8634105960264901\n",
      "iteration 32 current loss: 0.0005862373509444296 current acc: 0.8634388578522657\n",
      "iteration 33 current loss: 0.0008841429371386766 current acc: 0.8634671079851055\n",
      "iteration 34 current loss: 0.0008094717049971223 current acc: 0.8634953464322648\n",
      "iteration 35 current loss: 0.0006348897586576641 current acc: 0.8635235732009926\n",
      "iteration 36 current loss: 0.0010210369946435094 current acc: 0.8635517882985322\n",
      "iteration 37 current loss: 0.0005600785952992737 current acc: 0.8635799917321207\n",
      "iteration 38 current loss: 0.000631019240245223 current acc: 0.8636081835089895\n",
      "iteration 39 current loss: 0.0004897455801256001 current acc: 0.8636363636363636\n",
      "iteration 40 current loss: 0.0006576539017260075 current acc: 0.8636645321214625\n",
      "iteration 41 current loss: 0.0011556580429896712 current acc: 0.8636926889714994\n",
      "iteration 42 current loss: 0.0005661273025907576 current acc: 0.8637208341936816\n",
      "iteration 43 current loss: 0.000949988083448261 current acc: 0.8637489677952106\n",
      "iteration 44 current loss: 0.0027600291650742292 current acc: 0.8637770897832817\n",
      "iteration 45 current loss: 0.0007064440869726241 current acc: 0.8638052001650846\n",
      "iteration 46 current loss: 0.0010689758928492665 current acc: 0.8638332989478028\n",
      "iteration 47 current loss: 0.0009025970357470214 current acc: 0.8638613861386139\n",
      "iteration 48 current loss: 0.000812043494079262 current acc: 0.8638894617446896\n",
      "iteration 49 current loss: 0.0004479716590140015 current acc: 0.8639175257731959\n",
      "iteration 50 current loss: 0.0009031813824549317 current acc: 0.8639455782312925\n",
      "iteration 51 current loss: 0.0005085072480142117 current acc: 0.8639736191261336\n",
      "iteration 52 current loss: 0.0005769896670244634 current acc: 0.864001648464867\n",
      "iteration 53 current loss: 0.0005708735552616417 current acc: 0.8640296662546354\n",
      "iteration 54 current loss: 0.0008951528579927981 current acc: 0.8640576725025747\n",
      "iteration 55 current loss: 0.0006420778227038682 current acc: 0.8640856672158155\n",
      "iteration 56 current loss: 0.000659200013615191 current acc: 0.8641136504014824\n",
      "iteration 57 current loss: 0.000876137928571552 current acc: 0.8641416220666941\n",
      "iteration 58 current loss: 0.0008527619647793472 current acc: 0.8641695822185634\n",
      "iteration 59 current loss: 0.0004190169565845281 current acc: 0.8641975308641975\n",
      "iteration 60 current loss: 0.0006353107746690512 current acc: 0.8642254680106974\n",
      "iteration 61 current loss: 0.0005789511487819254 current acc: 0.8642533936651584\n",
      "iteration 62 current loss: 0.0007280718418769538 current acc: 0.8642813078346699\n",
      "iteration 63 current loss: 0.0009084382909350097 current acc: 0.8643092105263158\n",
      "iteration 64 current loss: 0.0011640890734270215 current acc: 0.8643371017471737\n",
      "iteration 65 current loss: 0.0005372026353143156 current acc: 0.8643649815043156\n",
      "iteration 66 current loss: 0.0007463720394298434 current acc: 0.8643928498048079\n",
      "iteration 67 current loss: 0.000914788746740669 current acc: 0.8644207066557108\n",
      "iteration 68 current loss: 0.0009009508066810668 current acc: 0.8644485520640789\n",
      "iteration 69 current loss: 0.0010008313693106174 current acc: 0.864476386036961\n",
      "iteration 70 current loss: 0.0005457845982164145 current acc: 0.8645042085814001\n",
      "iteration 71 current loss: 0.0007195166544988751 current acc: 0.8645320197044335\n",
      "iteration 72 current loss: 0.000606039771810174 current acc: 0.8645598194130926\n",
      "iteration 73 current loss: 0.0006628099363297224 current acc: 0.8645876077144029\n",
      "iteration 74 current loss: 0.0009541434701532125 current acc: 0.8646153846153846\n",
      "iteration 75 current loss: 0.0009338627569377422 current acc: 0.8646431501230517\n",
      "iteration 76 current loss: 0.0005703313509002328 current acc: 0.8646709042444125\n",
      "iteration 77 current loss: 0.0007972416933625937 current acc: 0.8646986469864698\n",
      "iteration 78 current loss: 0.0006679508369415998 current acc: 0.8647263783562206\n",
      "iteration 79 current loss: 0.0008792968583293259 current acc: 0.8647540983606558\n",
      "iteration 80 current loss: 0.0008289319230243564 current acc: 0.8647818070067609\n",
      "iteration 81 current loss: 0.0009974107379093766 current acc: 0.8648095043015158\n",
      "iteration 82 current loss: 0.0010445949155837297 current acc: 0.8648371902518943\n",
      "iteration 83 current loss: 0.0019215689972043037 current acc: 0.8648648648648649\n",
      "iteration 84 current loss: 0.0006701087695546448 current acc: 0.8648925281473899\n",
      "iteration 85 current loss: 0.0010113308671861887 current acc: 0.8649201801064266\n",
      "iteration 86 current loss: 0.0011535019148141146 current acc: 0.8649478207489257\n",
      "iteration 87 current loss: 0.00046976495650596917 current acc: 0.8649754500818331\n",
      "iteration 88 current loss: 0.0007648832397535443 current acc: 0.8650030681120884\n",
      "iteration 89 current loss: 0.0005361789953894913 current acc: 0.8650306748466258\n",
      "iteration 90 current loss: 0.0007924538804218173 current acc: 0.8650582702923737\n",
      "iteration 91 current loss: 0.0006693518371321261 current acc: 0.8650858544562551\n",
      "iteration 92 current loss: 0.0012070238590240479 current acc: 0.865113427345187\n",
      "iteration 93 current loss: 0.0006120954058133066 current acc: 0.8651409889660809\n",
      "iteration 94 current loss: 0.0006665783585049212 current acc: 0.8651685393258427\n",
      "iteration 95 current loss: 0.0006643044762313366 current acc: 0.8651960784313726\n",
      "iteration 96 current loss: 0.000847689516376704 current acc: 0.8652236062895651\n",
      "iteration 97 current loss: 0.0007892149151302874 current acc: 0.8652511229073091\n",
      "iteration 98 current loss: 0.0007808322552591562 current acc: 0.865278628291488\n",
      "iteration 99 current loss: 0.0006192717119120061 current acc: 0.8653061224489796\n",
      "\t\tEpoch 48/100 complete. Epoch loss 0.0007934058751561679 Epoch accuracy 1.0\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 48, Validation Accuracy: 0.63425, Validation Loss: 1.8174721993505956\n",
      "best loss 0.0007934058751561679\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.0010581191163510084 current acc: 0.8653336053866558\n",
      "iteration 1 current loss: 0.0006542764604091644 current acc: 0.8653610771113831\n",
      "iteration 2 current loss: 0.0008700774633325636 current acc: 0.8653885376300224\n",
      "iteration 3 current loss: 0.0005273402202874422 current acc: 0.865415986949429\n",
      "iteration 4 current loss: 0.0011187532218173146 current acc: 0.8654434250764526\n",
      "iteration 5 current loss: 0.0004833697166759521 current acc: 0.8654708520179372\n",
      "iteration 6 current loss: 0.0009288772125728428 current acc: 0.8654982677807214\n",
      "iteration 7 current loss: 0.0007774196565151215 current acc: 0.8655256723716381\n",
      "iteration 8 current loss: 0.0005405376432463527 current acc: 0.8655530657975148\n",
      "iteration 9 current loss: 0.0007365093915723264 current acc: 0.8655804480651731\n",
      "iteration 10 current loss: 0.0003920220478903502 current acc: 0.8656078191814295\n",
      "iteration 11 current loss: 0.0006909496150910854 current acc: 0.8656351791530945\n",
      "iteration 12 current loss: 0.0008934374782256782 current acc: 0.8656625279869733\n",
      "iteration 13 current loss: 0.0005619301227852702 current acc: 0.8656898656898657\n",
      "iteration 14 current loss: 0.0008833958418108523 current acc: 0.8657171922685656\n",
      "iteration 15 current loss: 0.0013601422542706132 current acc: 0.8657445077298617\n",
      "iteration 16 current loss: 0.0005533585208468139 current acc: 0.8657718120805369\n",
      "iteration 17 current loss: 0.0005305116646923125 current acc: 0.8657991053273688\n",
      "iteration 18 current loss: 0.0008469550521112978 current acc: 0.8658263874771295\n",
      "iteration 19 current loss: 0.0006067294743843377 current acc: 0.8658536585365854\n",
      "iteration 20 current loss: 0.0008799775969237089 current acc: 0.8658809185124975\n",
      "iteration 21 current loss: 0.0005453776102513075 current acc: 0.8659081674116212\n",
      "iteration 22 current loss: 0.0009220704669132829 current acc: 0.8659354052407069\n",
      "iteration 23 current loss: 0.0007873316644690931 current acc: 0.8659626320064988\n",
      "iteration 24 current loss: 0.0007675100350752473 current acc: 0.8659898477157361\n",
      "iteration 25 current loss: 0.0005644824705086648 current acc: 0.8660170523751523\n",
      "iteration 26 current loss: 0.0009179218322969973 current acc: 0.8660442459914756\n",
      "iteration 27 current loss: 0.0005429729935713112 current acc: 0.8660714285714286\n",
      "iteration 28 current loss: 0.0010171816684305668 current acc: 0.8660986001217286\n",
      "iteration 29 current loss: 0.00039003731217235327 current acc: 0.8661257606490872\n",
      "iteration 30 current loss: 0.0005011057946830988 current acc: 0.8661529101602109\n",
      "iteration 31 current loss: 0.000723772740457207 current acc: 0.8661800486618005\n",
      "iteration 32 current loss: 0.0010265227174386382 current acc: 0.8662071761605514\n",
      "iteration 33 current loss: 0.0004678304248955101 current acc: 0.8662342926631537\n",
      "iteration 34 current loss: 0.0006658036727458239 current acc: 0.8662613981762918\n",
      "iteration 35 current loss: 0.000815122970379889 current acc: 0.8662884927066451\n",
      "iteration 36 current loss: 0.0008897522930055857 current acc: 0.8663155762608872\n",
      "iteration 37 current loss: 0.0009540144237689674 current acc: 0.8663426488456865\n",
      "iteration 38 current loss: 0.00043088156962767243 current acc: 0.8663697104677061\n",
      "iteration 39 current loss: 0.001177821308374405 current acc: 0.8663967611336032\n",
      "iteration 40 current loss: 0.0005249971291050315 current acc: 0.8664238008500303\n",
      "iteration 41 current loss: 0.0008025819552130997 current acc: 0.8664508296236342\n",
      "iteration 42 current loss: 0.0008374563767574728 current acc: 0.866477847461056\n",
      "iteration 43 current loss: 0.0009773685596883297 current acc: 0.866504854368932\n",
      "iteration 44 current loss: 0.0005773568409495056 current acc: 0.8665318503538928\n",
      "iteration 45 current loss: 0.0004984614788554609 current acc: 0.8665588354225637\n",
      "iteration 46 current loss: 0.000817553314846009 current acc: 0.8665858095815646\n",
      "iteration 47 current loss: 0.0007157886284403503 current acc: 0.8666127728375101\n",
      "iteration 48 current loss: 0.0005399514338932931 current acc: 0.8666397251970095\n",
      "iteration 49 current loss: 0.0004561171226669103 current acc: 0.8666666666666667\n",
      "iteration 50 current loss: 0.0007746839546598494 current acc: 0.8666935972530802\n",
      "iteration 51 current loss: 0.000730829662643373 current acc: 0.8667205169628432\n",
      "iteration 52 current loss: 0.00046459923032671213 current acc: 0.8667474258025439\n",
      "iteration 53 current loss: 0.0006122647318989038 current acc: 0.8667743237787646\n",
      "iteration 54 current loss: 0.0003540388133842498 current acc: 0.8668012108980827\n",
      "iteration 55 current loss: 0.0005608962383121252 current acc: 0.8668280871670703\n",
      "iteration 56 current loss: 0.0004425705992616713 current acc: 0.8668549525922937\n",
      "iteration 57 current loss: 0.0006601845961995423 current acc: 0.8668818071803146\n",
      "iteration 58 current loss: 0.0006461412995122373 current acc: 0.8669086509376891\n",
      "iteration 59 current loss: 0.0006976653821766376 current acc: 0.8669354838709677\n",
      "iteration 60 current loss: 0.0006375197554007173 current acc: 0.8669623059866962\n",
      "iteration 61 current loss: 0.0007391783874481916 current acc: 0.8669891172914147\n",
      "iteration 62 current loss: 0.0005185482441447675 current acc: 0.8670159177916583\n",
      "iteration 63 current loss: 0.000733929337002337 current acc: 0.8670427074939565\n",
      "iteration 64 current loss: 0.0006620674394071102 current acc: 0.8670694864048338\n",
      "iteration 65 current loss: 0.0008884380804374814 current acc: 0.8670962545308095\n",
      "iteration 66 current loss: 0.001101703499443829 current acc: 0.8671230118783975\n",
      "iteration 67 current loss: 0.0006033487734384835 current acc: 0.8671497584541062\n",
      "iteration 68 current loss: 0.0008919681422412395 current acc: 0.8671764942644395\n",
      "iteration 69 current loss: 0.0003793790237978101 current acc: 0.8672032193158954\n",
      "iteration 70 current loss: 0.0007546024862676859 current acc: 0.8672299336149668\n",
      "iteration 71 current loss: 0.0005324446829035878 current acc: 0.8672566371681416\n",
      "iteration 72 current loss: 0.0007495495956391096 current acc: 0.8672833299819023\n",
      "iteration 73 current loss: 0.0006655158358626068 current acc: 0.8673100120627262\n",
      "iteration 74 current loss: 0.0005261602345854044 current acc: 0.8673366834170855\n",
      "iteration 75 current loss: 0.0007595055503770709 current acc: 0.867363344051447\n",
      "iteration 76 current loss: 0.0007530981092713773 current acc: 0.8673899939722725\n",
      "iteration 77 current loss: 0.0008732867427170277 current acc: 0.8674166331860185\n",
      "iteration 78 current loss: 0.0009603766957297921 current acc: 0.8674432616991363\n",
      "iteration 79 current loss: 0.0006424564635381103 current acc: 0.8674698795180723\n",
      "iteration 80 current loss: 0.0007682184805162251 current acc: 0.8674964866492673\n",
      "iteration 81 current loss: 0.001659057685174048 current acc: 0.8675230830991569\n",
      "iteration 82 current loss: 0.0005470127798616886 current acc: 0.8675496688741722\n",
      "iteration 83 current loss: 0.0005793596501462162 current acc: 0.8675762439807384\n",
      "iteration 84 current loss: 0.0005559829878620803 current acc: 0.8676028084252758\n",
      "iteration 85 current loss: 0.0004256413085386157 current acc: 0.8676293622141997\n",
      "iteration 86 current loss: 0.0009032904636114836 current acc: 0.8676559053539202\n",
      "iteration 87 current loss: 0.0004411736153997481 current acc: 0.867682437850842\n",
      "iteration 88 current loss: 0.0005703855422325432 current acc: 0.867708959711365\n",
      "iteration 89 current loss: 0.0005821648519486189 current acc: 0.8677354709418837\n",
      "iteration 90 current loss: 0.0006235814653337002 current acc: 0.8677619715487879\n",
      "iteration 91 current loss: 0.0006908381474204361 current acc: 0.8677884615384616\n",
      "iteration 92 current loss: 0.0006531003164127469 current acc: 0.8678149409172842\n",
      "iteration 93 current loss: 0.0004298718413338065 current acc: 0.8678414096916299\n",
      "iteration 94 current loss: 0.0008395171025767922 current acc: 0.8678678678678678\n",
      "iteration 95 current loss: 0.0007364521734416485 current acc: 0.8678943154523618\n",
      "iteration 96 current loss: 0.000749870901927352 current acc: 0.8679207524514709\n",
      "iteration 97 current loss: 0.0024258201010525227 current acc: 0.8679471788715486\n",
      "iteration 98 current loss: 0.0007852983544580638 current acc: 0.8679735947189438\n",
      "iteration 99 current loss: 0.0007232879288494587 current acc: 0.868\n",
      "\t\tEpoch 49/100 complete. Epoch loss 0.0007275671386742033 Epoch accuracy 1.0\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 49, Validation Accuracy: 0.632875, Validation Loss: 1.840717275813222\n",
      "best loss 0.0007275671386742033\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.0003673291066661477 current acc: 0.8680263947210558\n",
      "iteration 1 current loss: 0.0007805140339769423 current acc: 0.8680527788884447\n",
      "iteration 2 current loss: 0.0004760595038533211 current acc: 0.8680791525084949\n",
      "iteration 3 current loss: 0.000678060227073729 current acc: 0.86810551558753\n",
      "iteration 4 current loss: 0.0007270404021255672 current acc: 0.8681318681318682\n",
      "iteration 5 current loss: 0.0003956557484343648 current acc: 0.8681582101478226\n",
      "iteration 6 current loss: 0.0008600889705121517 current acc: 0.8681845416417017\n",
      "iteration 7 current loss: 0.0006397366523742676 current acc: 0.8682108626198083\n",
      "iteration 8 current loss: 0.0006346460431814194 current acc: 0.8682371730884408\n",
      "iteration 9 current loss: 0.0005269030225463212 current acc: 0.8682634730538922\n",
      "iteration 10 current loss: 0.0005227169604040682 current acc: 0.8682897625224506\n",
      "iteration 11 current loss: 0.0007675879169255495 current acc: 0.8683160415003991\n",
      "iteration 12 current loss: 0.0008392664603888988 current acc: 0.8683423099940155\n",
      "iteration 13 current loss: 0.0005407406715676188 current acc: 0.8683685680095732\n",
      "iteration 14 current loss: 0.0006241635419428349 current acc: 0.86839481555334\n",
      "iteration 15 current loss: 0.0013967817649245262 current acc: 0.868421052631579\n",
      "iteration 16 current loss: 0.0009121730690822005 current acc: 0.8684472792505481\n",
      "iteration 17 current loss: 0.000546677503734827 current acc: 0.8684734954165007\n",
      "iteration 18 current loss: 0.0005590387154370546 current acc: 0.8684997011356844\n",
      "iteration 19 current loss: 0.0005671928520314395 current acc: 0.8685258964143426\n",
      "iteration 20 current loss: 0.0005360673530958593 current acc: 0.8685520812587134\n",
      "iteration 21 current loss: 0.000604263273999095 current acc: 0.8685782556750299\n",
      "iteration 22 current loss: 0.00080069899559021 current acc: 0.8686044196695202\n",
      "iteration 23 current loss: 0.0006565935327671468 current acc: 0.8686305732484076\n",
      "iteration 24 current loss: 0.0006615084130316973 current acc: 0.8686567164179104\n",
      "iteration 25 current loss: 0.0005756021128036082 current acc: 0.868682849184242\n",
      "iteration 26 current loss: 0.0005722217028960586 current acc: 0.8687089715536105\n",
      "iteration 27 current loss: 0.0005489496397785842 current acc: 0.8687350835322196\n",
      "iteration 28 current loss: 0.0009092583786696196 current acc: 0.8687611851262677\n",
      "iteration 29 current loss: 0.0005116109969094396 current acc: 0.8687872763419483\n",
      "iteration 30 current loss: 0.0007935581379570067 current acc: 0.8688133571854502\n",
      "iteration 31 current loss: 0.0003095413267146796 current acc: 0.8688394276629571\n",
      "iteration 32 current loss: 0.0006742365658283234 current acc: 0.8688654877806478\n",
      "iteration 33 current loss: 0.0007793216500431299 current acc: 0.8688915375446961\n",
      "iteration 34 current loss: 0.0007283443701453507 current acc: 0.8689175769612711\n",
      "iteration 35 current loss: 0.00033709348645061255 current acc: 0.868943606036537\n",
      "iteration 36 current loss: 0.0011050417087972164 current acc: 0.8689696247766527\n",
      "iteration 37 current loss: 0.0009034057729877532 current acc: 0.868995633187773\n",
      "iteration 38 current loss: 0.0006915777921676636 current acc: 0.8690216312760468\n",
      "iteration 39 current loss: 0.0005527602625079453 current acc: 0.8690476190476191\n",
      "iteration 40 current loss: 0.0008931903284974396 current acc: 0.8690735965086293\n",
      "iteration 41 current loss: 0.0006046053022146225 current acc: 0.8690995636652122\n",
      "iteration 42 current loss: 0.0005165696493349969 current acc: 0.8691255205234979\n",
      "iteration 43 current loss: 0.00039839468081481755 current acc: 0.8691514670896114\n",
      "iteration 44 current loss: 0.0004307309864088893 current acc: 0.869177403369673\n",
      "iteration 45 current loss: 0.0010381853207945824 current acc: 0.8692033293697978\n",
      "iteration 46 current loss: 0.00048702515778131783 current acc: 0.8692292450960967\n",
      "iteration 47 current loss: 0.0007002599304541945 current acc: 0.8692551505546752\n",
      "iteration 48 current loss: 0.0005123790469951928 current acc: 0.869281045751634\n",
      "iteration 49 current loss: 0.000868284550961107 current acc: 0.8693069306930693\n",
      "iteration 50 current loss: 0.000657654192764312 current acc: 0.8693328053850723\n",
      "iteration 51 current loss: 0.0004497696354519576 current acc: 0.8693586698337292\n",
      "iteration 52 current loss: 0.0005411066231317818 current acc: 0.8693845240451217\n",
      "iteration 53 current loss: 0.0006718721124343574 current acc: 0.8694103680253265\n",
      "iteration 54 current loss: 0.0005181159358471632 current acc: 0.8694362017804155\n",
      "iteration 55 current loss: 0.0006883882451802492 current acc: 0.8694620253164557\n",
      "iteration 56 current loss: 0.0006326953298412263 current acc: 0.8694878386395096\n",
      "iteration 57 current loss: 0.00040494499262422323 current acc: 0.8695136417556346\n",
      "iteration 58 current loss: 0.0008661139290779829 current acc: 0.8695394346708836\n",
      "iteration 59 current loss: 0.0005849977605976164 current acc: 0.8695652173913043\n",
      "iteration 60 current loss: 0.000485750351799652 current acc: 0.8695909899229401\n",
      "iteration 61 current loss: 0.0004765034536831081 current acc: 0.8696167522718293\n",
      "iteration 62 current loss: 0.00038258612039498985 current acc: 0.8696425044440055\n",
      "iteration 63 current loss: 0.00045617533032782376 current acc: 0.8696682464454977\n",
      "iteration 64 current loss: 0.001433482626453042 current acc: 0.8696939782823297\n",
      "iteration 65 current loss: 0.002361712045967579 current acc: 0.8697196999605211\n",
      "iteration 66 current loss: 0.0007376985740847886 current acc: 0.8697454114860864\n",
      "iteration 67 current loss: 0.0008216029964387417 current acc: 0.8697711128650355\n",
      "iteration 68 current loss: 0.0006580791086889803 current acc: 0.8697968041033735\n",
      "iteration 69 current loss: 0.0007055511814542115 current acc: 0.8698224852071006\n",
      "iteration 70 current loss: 0.0006760659161955118 current acc: 0.8698481561822126\n",
      "iteration 71 current loss: 0.0006530021782964468 current acc: 0.8698738170347003\n",
      "iteration 72 current loss: 0.0009196624741889536 current acc: 0.8698994677705499\n",
      "iteration 73 current loss: 0.0008198001305572689 current acc: 0.869925108395743\n",
      "iteration 74 current loss: 0.0006084214546717703 current acc: 0.8699507389162562\n",
      "iteration 75 current loss: 0.000653209222946316 current acc: 0.8699763593380615\n",
      "iteration 76 current loss: 0.001125501119531691 current acc: 0.8700019696671263\n",
      "iteration 77 current loss: 0.0007844104547984898 current acc: 0.8700275699094131\n",
      "iteration 78 current loss: 0.0019092102302238345 current acc: 0.8700531600708801\n",
      "iteration 79 current loss: 0.0007161422981880605 current acc: 0.8700787401574803\n",
      "iteration 80 current loss: 0.0006698824581690133 current acc: 0.8701043101751623\n",
      "iteration 81 current loss: 0.0014535519294440746 current acc: 0.8701298701298701\n",
      "iteration 82 current loss: 0.0010555308545008302 current acc: 0.8701554200275428\n",
      "iteration 83 current loss: 0.0014218587893992662 current acc: 0.8701809598741149\n",
      "iteration 84 current loss: 0.0008588092750869691 current acc: 0.8702064896755162\n",
      "iteration 85 current loss: 0.0009734045015648007 current acc: 0.8702320094376721\n",
      "iteration 86 current loss: 0.0008950704941526055 current acc: 0.8702575191665028\n",
      "iteration 87 current loss: 0.001184331951662898 current acc: 0.8702830188679245\n",
      "iteration 88 current loss: 0.0007791508687660098 current acc: 0.8703085085478482\n",
      "iteration 89 current loss: 0.0007736228872090578 current acc: 0.8703339882121808\n",
      "iteration 90 current loss: 0.0009471721714362502 current acc: 0.8703594578668238\n",
      "iteration 91 current loss: 0.0008886523428373039 current acc: 0.8703849175176748\n",
      "iteration 92 current loss: 0.001119694672524929 current acc: 0.8704103671706264\n",
      "iteration 93 current loss: 0.0007504919194616377 current acc: 0.8704358068315665\n",
      "iteration 94 current loss: 0.0007235064404085279 current acc: 0.8704612365063789\n",
      "iteration 95 current loss: 0.0007300323341041803 current acc: 0.8704866562009419\n",
      "iteration 96 current loss: 0.0005514622898772359 current acc: 0.8705120659211301\n",
      "iteration 97 current loss: 0.0005501598352566361 current acc: 0.8705374656728129\n",
      "iteration 98 current loss: 0.0010546628618612885 current acc: 0.8705628554618553\n",
      "iteration 99 current loss: 0.001280084135942161 current acc: 0.8705882352941177\n",
      "\t\tEpoch 50/100 complete. Epoch loss 0.0007512501263408921 Epoch accuracy 1.0\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 50, Validation Accuracy: 0.633, Validation Loss: 1.8499843701720238\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.0005369507125578821 current acc: 0.8706136051754558\n",
      "iteration 1 current loss: 0.0004056620236951858 current acc: 0.8706389651117209\n",
      "iteration 2 current loss: 0.0006979216122999787 current acc: 0.8706643151087595\n",
      "iteration 3 current loss: 0.0013419452589005232 current acc: 0.8706896551724138\n",
      "iteration 4 current loss: 0.00083049206295982 current acc: 0.8707149853085211\n",
      "iteration 5 current loss: 0.00043868247303180397 current acc: 0.8707403055229143\n",
      "iteration 6 current loss: 0.0003919102600775659 current acc: 0.8707656158214215\n",
      "iteration 7 current loss: 0.00044174474896863103 current acc: 0.8707909162098669\n",
      "iteration 8 current loss: 0.0006127587985247374 current acc: 0.8708162066940693\n",
      "iteration 9 current loss: 0.0006821794668212533 current acc: 0.8708414872798435\n",
      "iteration 10 current loss: 0.0006610211567021906 current acc: 0.8708667579729994\n",
      "iteration 11 current loss: 0.0003998660540673882 current acc: 0.8708920187793427\n",
      "iteration 12 current loss: 0.00047311995876953006 current acc: 0.8709172697046743\n",
      "iteration 13 current loss: 0.0004807611694559455 current acc: 0.8709425107547908\n",
      "iteration 14 current loss: 0.00040416463161818683 current acc: 0.8709677419354839\n",
      "iteration 15 current loss: 0.0003721691609825939 current acc: 0.870992963252541\n",
      "iteration 16 current loss: 0.0005437820800580084 current acc: 0.8710181747117451\n",
      "iteration 17 current loss: 0.0006151230772957206 current acc: 0.8710433763188745\n",
      "iteration 18 current loss: 0.0006114975549280643 current acc: 0.8710685680797031\n",
      "iteration 19 current loss: 0.0008373851305805147 current acc: 0.87109375\n",
      "iteration 20 current loss: 0.0003865733742713928 current acc: 0.8711189220855302\n",
      "iteration 21 current loss: 0.0004555702907964587 current acc: 0.8711440843420539\n",
      "iteration 22 current loss: 0.0006743569392710924 current acc: 0.871169236775327\n",
      "iteration 23 current loss: 0.0004986987332813442 current acc: 0.8711943793911007\n",
      "iteration 24 current loss: 0.0004673193325288594 current acc: 0.871219512195122\n",
      "iteration 25 current loss: 0.0007084793760441244 current acc: 0.871244635193133\n",
      "iteration 26 current loss: 0.000448338920250535 current acc: 0.8712697483908719\n",
      "iteration 27 current loss: 0.0012827053433284163 current acc: 0.8712948517940717\n",
      "iteration 28 current loss: 0.001329332240857184 current acc: 0.8713199454084617\n",
      "iteration 29 current loss: 0.00031974061857908964 current acc: 0.8713450292397661\n",
      "iteration 30 current loss: 0.0005849061417393386 current acc: 0.8713701032937049\n",
      "iteration 31 current loss: 0.00044704845640808344 current acc: 0.8713951675759938\n",
      "iteration 32 current loss: 0.000640109705273062 current acc: 0.8714202220923437\n",
      "iteration 33 current loss: 0.0006046033231541514 current acc: 0.8714452668484612\n",
      "iteration 34 current loss: 0.00043108564568683505 current acc: 0.8714703018500487\n",
      "iteration 35 current loss: 0.00042739699711091816 current acc: 0.8714953271028038\n",
      "iteration 36 current loss: 0.0005192442331463099 current acc: 0.8715203426124197\n",
      "iteration 37 current loss: 0.0003217248886357993 current acc: 0.8715453483845854\n",
      "iteration 38 current loss: 0.0004974069306626916 current acc: 0.8715703444249854\n",
      "iteration 39 current loss: 0.00045212305849418044 current acc: 0.8715953307392996\n",
      "iteration 40 current loss: 0.0007590105524286628 current acc: 0.8716203073332036\n",
      "iteration 41 current loss: 0.0004975103656761348 current acc: 0.8716452742123687\n",
      "iteration 42 current loss: 0.0005092968931421638 current acc: 0.8716702313824616\n",
      "iteration 43 current loss: 0.00044865990639664233 current acc: 0.8716951788491446\n",
      "iteration 44 current loss: 0.0004354244447313249 current acc: 0.8717201166180758\n",
      "iteration 45 current loss: 0.00034536406747065485 current acc: 0.8717450446949087\n",
      "iteration 46 current loss: 0.0008230541716329753 current acc: 0.8717699630852924\n",
      "iteration 47 current loss: 0.0006541058537550271 current acc: 0.8717948717948718\n",
      "iteration 48 current loss: 0.00044081269879825413 current acc: 0.8718197708292872\n",
      "iteration 49 current loss: 0.0006136285956017673 current acc: 0.8718446601941747\n",
      "iteration 50 current loss: 0.000509503239300102 current acc: 0.871869539895166\n",
      "iteration 51 current loss: 0.0008134259842336178 current acc: 0.8718944099378882\n",
      "iteration 52 current loss: 0.0003868612984661013 current acc: 0.8719192703279643\n",
      "iteration 53 current loss: 0.0005078451940789819 current acc: 0.8719441210710128\n",
      "iteration 54 current loss: 0.0005626084748655558 current acc: 0.871968962172648\n",
      "iteration 55 current loss: 0.00048736052121967077 current acc: 0.8719937936384794\n",
      "iteration 56 current loss: 0.0005468290182761848 current acc: 0.8720186154741129\n",
      "iteration 57 current loss: 0.00024994061095640063 current acc: 0.8720434276851493\n",
      "iteration 58 current loss: 0.0007885758532211185 current acc: 0.8720682302771855\n",
      "iteration 59 current loss: 0.000722012366168201 current acc: 0.872093023255814\n",
      "iteration 60 current loss: 0.000451747328042984 current acc: 0.8721178066266227\n",
      "iteration 61 current loss: 0.0006986726075410843 current acc: 0.8721425803951957\n",
      "iteration 62 current loss: 0.0006213834858499467 current acc: 0.8721673445671121\n",
      "iteration 63 current loss: 0.0005932769272476435 current acc: 0.8721920991479474\n",
      "iteration 64 current loss: 0.0009024441824294627 current acc: 0.872216844143272\n",
      "iteration 65 current loss: 0.00037738311220891774 current acc: 0.8722415795586528\n",
      "iteration 66 current loss: 0.0005174762918613851 current acc: 0.8722663053996517\n",
      "iteration 67 current loss: 0.00082785525592044 current acc: 0.8722910216718266\n",
      "iteration 68 current loss: 0.0005626989295706153 current acc: 0.8723157283807312\n",
      "iteration 69 current loss: 0.00031123022199608386 current acc: 0.8723404255319149\n",
      "iteration 70 current loss: 0.0005683802883140743 current acc: 0.8723651131309225\n",
      "iteration 71 current loss: 0.0008645044290460646 current acc: 0.8723897911832946\n",
      "iteration 72 current loss: 0.0005085515440441668 current acc: 0.872414459694568\n",
      "iteration 73 current loss: 0.0006475373520515859 current acc: 0.8724391186702745\n",
      "iteration 74 current loss: 0.0004549121076706797 current acc: 0.8724637681159421\n",
      "iteration 75 current loss: 0.0009512681281194091 current acc: 0.8724884080370943\n",
      "iteration 76 current loss: 0.0005023975390940905 current acc: 0.8725130384392505\n",
      "iteration 77 current loss: 0.0006131345289759338 current acc: 0.8725376593279258\n",
      "iteration 78 current loss: 0.00046704005217179656 current acc: 0.872562270708631\n",
      "iteration 79 current loss: 0.0004709821951109916 current acc: 0.8725868725868726\n",
      "iteration 80 current loss: 0.0004992170725017786 current acc: 0.8726114649681529\n",
      "iteration 81 current loss: 0.0008187246276065707 current acc: 0.8726360478579699\n",
      "iteration 82 current loss: 0.0006913759279996157 current acc: 0.8726606212618174\n",
      "iteration 83 current loss: 0.0004411255067680031 current acc: 0.8726851851851852\n",
      "iteration 84 current loss: 0.0005103749572299421 current acc: 0.8727097396335584\n",
      "iteration 85 current loss: 0.000522859685588628 current acc: 0.8727342846124181\n",
      "iteration 86 current loss: 0.0006158456089906394 current acc: 0.8727588201272412\n",
      "iteration 87 current loss: 0.00045768992276862264 current acc: 0.8727833461835004\n",
      "iteration 88 current loss: 0.0006868464406579733 current acc: 0.8728078627866641\n",
      "iteration 89 current loss: 0.00046043150359764695 current acc: 0.8728323699421965\n",
      "iteration 90 current loss: 0.0008844130206853151 current acc: 0.8728568676555577\n",
      "iteration 91 current loss: 0.000707252649590373 current acc: 0.8728813559322034\n",
      "iteration 92 current loss: 0.0010085717076435685 current acc: 0.8729058347775852\n",
      "iteration 93 current loss: 0.0005672286497429013 current acc: 0.8729303041971506\n",
      "iteration 94 current loss: 0.0004551119345705956 current acc: 0.8729547641963427\n",
      "iteration 95 current loss: 0.00045186385978013277 current acc: 0.8729792147806005\n",
      "iteration 96 current loss: 0.0014010946033522487 current acc: 0.8730036559553589\n",
      "iteration 97 current loss: 0.0005507661262527108 current acc: 0.8730280877260485\n",
      "iteration 98 current loss: 0.0002853078185580671 current acc: 0.8730525100980958\n",
      "iteration 99 current loss: 0.0004609871539287269 current acc: 0.8730769230769231\n",
      "\t\tEpoch 51/100 complete. Epoch loss 0.0005876769733731635 Epoch accuracy 1.0\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 51, Validation Accuracy: 0.632375, Validation Loss: 1.8407667465507984\n",
      "best loss 0.0005876769733731635\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.00039189428207464516 current acc: 0.8731013266679485\n",
      "iteration 1 current loss: 0.0006374133517965674 current acc: 0.8731257208765859\n",
      "iteration 2 current loss: 0.00042683881474658847 current acc: 0.8731501057082452\n",
      "iteration 3 current loss: 0.0004736272385343909 current acc: 0.8731744811683321\n",
      "iteration 4 current loss: 0.0009090466774068773 current acc: 0.8731988472622478\n",
      "iteration 5 current loss: 0.0005017041694372892 current acc: 0.8732232039953899\n",
      "iteration 6 current loss: 0.0008398501668125391 current acc: 0.8732475513731516\n",
      "iteration 7 current loss: 0.0005644344491884112 current acc: 0.8732718894009217\n",
      "iteration 8 current loss: 0.0004980419180355966 current acc: 0.8732962180840852\n",
      "iteration 9 current loss: 0.001517823664471507 current acc: 0.8733205374280231\n",
      "iteration 10 current loss: 0.0006846440373919904 current acc: 0.8733448474381117\n",
      "iteration 11 current loss: 0.00034945018705911934 current acc: 0.8733691481197237\n",
      "iteration 12 current loss: 0.0005015571950934827 current acc: 0.8733934394782276\n",
      "iteration 13 current loss: 0.0005203346372582018 current acc: 0.8734177215189873\n",
      "iteration 14 current loss: 0.0012052321108058095 current acc: 0.8734419942473634\n",
      "iteration 15 current loss: 0.0008497294038534164 current acc: 0.8734662576687117\n",
      "iteration 16 current loss: 0.0010521841468289495 current acc: 0.8734905117883841\n",
      "iteration 17 current loss: 0.0010002790950238705 current acc: 0.8735147566117286\n",
      "iteration 18 current loss: 0.0006430785288102925 current acc: 0.8735389921440889\n",
      "iteration 19 current loss: 0.00038366077933460474 current acc: 0.8735632183908046\n",
      "iteration 20 current loss: 0.0005979171255603433 current acc: 0.8735874353572113\n",
      "iteration 21 current loss: 0.0007695150561630726 current acc: 0.8736116430486404\n",
      "iteration 22 current loss: 0.00042109680362045765 current acc: 0.8736358414704193\n",
      "iteration 23 current loss: 0.000621645653154701 current acc: 0.8736600306278713\n",
      "iteration 24 current loss: 0.0008875555358827114 current acc: 0.8736842105263158\n",
      "iteration 25 current loss: 0.0006885089678689837 current acc: 0.8737083811710677\n",
      "iteration 26 current loss: 0.0006120342877693474 current acc: 0.8737325425674383\n",
      "iteration 27 current loss: 0.0005784686654806137 current acc: 0.8737566947207345\n",
      "iteration 28 current loss: 0.00044381257612258196 current acc: 0.8737808376362594\n",
      "iteration 29 current loss: 0.0005330020794644952 current acc: 0.8738049713193117\n",
      "iteration 30 current loss: 0.000626652326900512 current acc: 0.8738290957751864\n",
      "iteration 31 current loss: 0.0003653680032584816 current acc: 0.8738532110091743\n",
      "iteration 32 current loss: 0.0006789900944568217 current acc: 0.8738773170265622\n",
      "iteration 33 current loss: 0.0005480145919136703 current acc: 0.8739014138326328\n",
      "iteration 34 current loss: 0.0006165056838653982 current acc: 0.8739255014326648\n",
      "iteration 35 current loss: 0.0005001120734959841 current acc: 0.8739495798319328\n",
      "iteration 36 current loss: 0.00044716300908476114 current acc: 0.8739736490357075\n",
      "iteration 37 current loss: 0.0005198126891627908 current acc: 0.8739977090492554\n",
      "iteration 38 current loss: 0.0006179861957207322 current acc: 0.8740217598778393\n",
      "iteration 39 current loss: 0.0015120175667107105 current acc: 0.8740458015267175\n",
      "iteration 40 current loss: 0.0007056838949210942 current acc: 0.8740698340011448\n",
      "iteration 41 current loss: 0.0005544922314584255 current acc: 0.8740938573063716\n",
      "iteration 42 current loss: 0.0004052268050145358 current acc: 0.8741178714476445\n",
      "iteration 43 current loss: 0.00042191799730062485 current acc: 0.8741418764302059\n",
      "iteration 44 current loss: 0.0010083186207339168 current acc: 0.8741658722592945\n",
      "iteration 45 current loss: 0.0008881146204657853 current acc: 0.8741898589401449\n",
      "iteration 46 current loss: 0.0004015311715193093 current acc: 0.8742138364779874\n",
      "iteration 47 current loss: 0.000502044684253633 current acc: 0.8742378048780488\n",
      "iteration 48 current loss: 0.0006272824248299003 current acc: 0.8742617641455516\n",
      "iteration 49 current loss: 0.0004920774954371154 current acc: 0.8742857142857143\n",
      "iteration 50 current loss: 0.002456922782585025 current acc: 0.8743096553037517\n",
      "iteration 51 current loss: 0.0004736497940029949 current acc: 0.8743335872048743\n",
      "iteration 52 current loss: 0.0006601341301575303 current acc: 0.874357509994289\n",
      "iteration 53 current loss: 0.0004928093403577805 current acc: 0.8743814236771983\n",
      "iteration 54 current loss: 0.0006238133646547794 current acc: 0.8744053282588011\n",
      "iteration 55 current loss: 0.0014913403429090977 current acc: 0.8744292237442922\n",
      "iteration 56 current loss: 0.0005189918447285891 current acc: 0.8744531101388625\n",
      "iteration 57 current loss: 0.0006298503139987588 current acc: 0.8744769874476988\n",
      "iteration 58 current loss: 0.0015634940937161446 current acc: 0.874500855675984\n",
      "iteration 59 current loss: 0.0008279638714157045 current acc: 0.8745247148288974\n",
      "iteration 60 current loss: 0.00045451908954419196 current acc: 0.8745485649116138\n",
      "iteration 61 current loss: 0.0019158705836161971 current acc: 0.8745724059293044\n",
      "iteration 62 current loss: 0.0005372969899326563 current acc: 0.8745962378871366\n",
      "iteration 63 current loss: 0.000528996461071074 current acc: 0.8746200607902735\n",
      "iteration 64 current loss: 0.000753938453271985 current acc: 0.8746438746438746\n",
      "iteration 65 current loss: 0.000788290926720947 current acc: 0.8746676794530953\n",
      "iteration 66 current loss: 0.00037910245009697974 current acc: 0.8746914752230871\n",
      "iteration 67 current loss: 0.0005929919425398111 current acc: 0.8747152619589977\n",
      "iteration 68 current loss: 0.000473650055937469 current acc: 0.8747390396659708\n",
      "iteration 69 current loss: 0.0007876424351707101 current acc: 0.8747628083491461\n",
      "iteration 70 current loss: 0.0007243831641972065 current acc: 0.8747865680136596\n",
      "iteration 71 current loss: 0.0008925633737817407 current acc: 0.8748103186646434\n",
      "iteration 72 current loss: 0.0005894420901313424 current acc: 0.8748340603072254\n",
      "iteration 73 current loss: 0.0008850914891809225 current acc: 0.8748577929465301\n",
      "iteration 74 current loss: 0.0005541377468034625 current acc: 0.8748815165876778\n",
      "iteration 75 current loss: 0.00037683863774873316 current acc: 0.8749052312357847\n",
      "iteration 76 current loss: 0.001101282425224781 current acc: 0.8749289368959636\n",
      "iteration 77 current loss: 0.0012012163642793894 current acc: 0.8749526335733232\n",
      "iteration 78 current loss: 0.0006086498615331948 current acc: 0.8749763212729683\n",
      "iteration 79 current loss: 0.0005319179035723209 current acc: 0.875\n",
      "iteration 80 current loss: 0.002615233650431037 current acc: 0.8750236697595153\n",
      "iteration 81 current loss: 0.0006311775650829077 current acc: 0.8750473305566073\n",
      "iteration 82 current loss: 0.0013684647856280208 current acc: 0.8750709823963657\n",
      "iteration 83 current loss: 0.0007623679121024907 current acc: 0.8750946252838758\n",
      "iteration 84 current loss: 0.0005706087686121464 current acc: 0.8751182592242195\n",
      "iteration 85 current loss: 0.0017749783582985401 current acc: 0.8751418842224744\n",
      "iteration 86 current loss: 0.0004990556626580656 current acc: 0.8751655002837148\n",
      "iteration 87 current loss: 0.00047423422802239656 current acc: 0.8751891074130106\n",
      "iteration 88 current loss: 0.0008151769870892167 current acc: 0.8752127056154283\n",
      "iteration 89 current loss: 0.0007957909838296473 current acc: 0.8752362948960303\n",
      "iteration 90 current loss: 0.0006024073227308691 current acc: 0.8752598752598753\n",
      "iteration 91 current loss: 0.001175811979919672 current acc: 0.8752834467120182\n",
      "iteration 92 current loss: 0.0005503284046426415 current acc: 0.8753070092575099\n",
      "iteration 93 current loss: 0.0007513940217904747 current acc: 0.8753305629013978\n",
      "iteration 94 current loss: 0.0008323174552060664 current acc: 0.8753541076487252\n",
      "iteration 95 current loss: 0.0005202139145694673 current acc: 0.8753776435045317\n",
      "iteration 96 current loss: 0.00104410364292562 current acc: 0.8754011704738531\n",
      "iteration 97 current loss: 0.00041080493247136474 current acc: 0.8754246885617214\n",
      "iteration 98 current loss: 0.0006996585289016366 current acc: 0.8754481977731647\n",
      "iteration 99 current loss: 0.0007299598073586822 current acc: 0.8754716981132076\n",
      "\t\tEpoch 52/100 complete. Epoch loss 0.0007458457502070814 Epoch accuracy 1.0\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 52, Validation Accuracy: 0.63575, Validation Loss: 1.8770134583115579\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.000543705013114959 current acc: 0.8754951895868704\n",
      "iteration 1 current loss: 0.0008334594313055277 current acc: 0.8755186721991701\n",
      "iteration 2 current loss: 0.0005865009152330458 current acc: 0.8755421459551197\n",
      "iteration 3 current loss: 0.000654219591524452 current acc: 0.8755656108597285\n",
      "iteration 4 current loss: 0.0007945881807245314 current acc: 0.8755890669180019\n",
      "iteration 5 current loss: 0.0004508894926402718 current acc: 0.8756125141349416\n",
      "iteration 6 current loss: 0.0004964328836649656 current acc: 0.8756359525155455\n",
      "iteration 7 current loss: 0.00033835656358860433 current acc: 0.8756593820648079\n",
      "iteration 8 current loss: 0.0005074550863355398 current acc: 0.8756828027877189\n",
      "iteration 9 current loss: 0.0007764244801364839 current acc: 0.8757062146892656\n",
      "iteration 10 current loss: 0.0016951458528637886 current acc: 0.8757296177744305\n",
      "iteration 11 current loss: 0.0008778421324677765 current acc: 0.8757530120481928\n",
      "iteration 12 current loss: 0.003073454834520817 current acc: 0.8757763975155279\n",
      "iteration 13 current loss: 0.0006781796109862626 current acc: 0.8757997741814076\n",
      "iteration 14 current loss: 0.0004916980396956205 current acc: 0.8758231420507996\n",
      "iteration 15 current loss: 0.00035785738145932555 current acc: 0.8758465011286681\n",
      "iteration 16 current loss: 0.0004430467961356044 current acc: 0.8758698514199736\n",
      "iteration 17 current loss: 0.0007616635994054377 current acc: 0.8758931929296728\n",
      "iteration 18 current loss: 0.0007037703762762249 current acc: 0.8759165256627186\n",
      "iteration 19 current loss: 0.0007828465895727277 current acc: 0.8759398496240601\n",
      "iteration 20 current loss: 0.0008798885392025113 current acc: 0.8759631648186431\n",
      "iteration 21 current loss: 0.0006449204520322382 current acc: 0.8759864712514093\n",
      "iteration 22 current loss: 0.00042462002602405846 current acc: 0.8760097689272967\n",
      "iteration 23 current loss: 0.0006725977291353047 current acc: 0.8760330578512396\n",
      "iteration 24 current loss: 0.00048482284182682633 current acc: 0.8760563380281691\n",
      "iteration 25 current loss: 0.0015309847658500075 current acc: 0.8760796094630117\n",
      "iteration 26 current loss: 0.0005900513497181237 current acc: 0.8761028721606908\n",
      "iteration 27 current loss: 0.0007145873387344182 current acc: 0.8761261261261262\n",
      "iteration 28 current loss: 0.000863195862621069 current acc: 0.8761493713642334\n",
      "iteration 29 current loss: 0.000500704743899405 current acc: 0.8761726078799249\n",
      "iteration 30 current loss: 0.0012577292509377003 current acc: 0.8761958356781092\n",
      "iteration 31 current loss: 0.0006526458892039955 current acc: 0.8762190547636909\n",
      "iteration 32 current loss: 0.0008027462754398584 current acc: 0.8762422651415713\n",
      "iteration 33 current loss: 0.0006061741732992232 current acc: 0.876265466816648\n",
      "iteration 34 current loss: 0.0006587222451344132 current acc: 0.8762886597938144\n",
      "iteration 35 current loss: 0.00073716213228181 current acc: 0.876311844077961\n",
      "iteration 36 current loss: 0.0004540905065368861 current acc: 0.8763350196739741\n",
      "iteration 37 current loss: 0.0005402205279096961 current acc: 0.8763581865867366\n",
      "iteration 38 current loss: 0.0009676150511950254 current acc: 0.8763813448211275\n",
      "iteration 39 current loss: 0.00037536132731474936 current acc: 0.8764044943820225\n",
      "iteration 40 current loss: 0.0006701176753267646 current acc: 0.8764276352742932\n",
      "iteration 41 current loss: 0.0004879820626229048 current acc: 0.8764507675028079\n",
      "iteration 42 current loss: 0.0006595429149456322 current acc: 0.8764738910724312\n",
      "iteration 43 current loss: 0.000589104020036757 current acc: 0.8764970059880239\n",
      "iteration 44 current loss: 0.0006366313900798559 current acc: 0.8765201122544434\n",
      "iteration 45 current loss: 0.000670798122882843 current acc: 0.8765432098765432\n",
      "iteration 46 current loss: 0.0008127448963932693 current acc: 0.8765662988591734\n",
      "iteration 47 current loss: 0.0011255055433139205 current acc: 0.8765893792071803\n",
      "iteration 48 current loss: 0.0006143038626760244 current acc: 0.8766124509254066\n",
      "iteration 49 current loss: 0.0005434408667497337 current acc: 0.8766355140186916\n",
      "iteration 50 current loss: 0.00037231677561067045 current acc: 0.8766585684918706\n",
      "iteration 51 current loss: 0.0006672162562608719 current acc: 0.8766816143497758\n",
      "iteration 52 current loss: 0.0007951873703859746 current acc: 0.8767046515972352\n",
      "iteration 53 current loss: 0.0003121495828963816 current acc: 0.8767276802390735\n",
      "iteration 54 current loss: 0.0006060723098926246 current acc: 0.876750700280112\n",
      "iteration 55 current loss: 0.00041194038931280375 current acc: 0.876773711725168\n",
      "iteration 56 current loss: 0.0011641642777249217 current acc: 0.8767967145790554\n",
      "iteration 57 current loss: 0.0006643924862146378 current acc: 0.8768197088465846\n",
      "iteration 58 current loss: 0.000549008313100785 current acc: 0.876842694532562\n",
      "iteration 59 current loss: 0.0011270097456872463 current acc: 0.8768656716417911\n",
      "iteration 60 current loss: 0.0005077386740595102 current acc: 0.8768886401790711\n",
      "iteration 61 current loss: 0.0006212982116267085 current acc: 0.8769116001491981\n",
      "iteration 62 current loss: 0.0005168972420506179 current acc: 0.8769345515569644\n",
      "iteration 63 current loss: 0.0009248857968486845 current acc: 0.8769574944071589\n",
      "iteration 64 current loss: 0.0005490857874974608 current acc: 0.8769804287045666\n",
      "iteration 65 current loss: 0.0004962629172950983 current acc: 0.8770033544539695\n",
      "iteration 66 current loss: 0.0005911437328904867 current acc: 0.8770262716601454\n",
      "iteration 67 current loss: 0.0006003849557600915 current acc: 0.8770491803278688\n",
      "iteration 68 current loss: 0.0005174714606255293 current acc: 0.877072080461911\n",
      "iteration 69 current loss: 0.0008197446004487574 current acc: 0.8770949720670391\n",
      "iteration 70 current loss: 0.0005097263492643833 current acc: 0.8771178551480171\n",
      "iteration 71 current loss: 0.0005566359031945467 current acc: 0.8771407297096053\n",
      "iteration 72 current loss: 0.0004519299545791 current acc: 0.8771635957565606\n",
      "iteration 73 current loss: 0.000607809517532587 current acc: 0.877186453293636\n",
      "iteration 74 current loss: 0.0005935695953667164 current acc: 0.8772093023255814\n",
      "iteration 75 current loss: 0.0005900823161937296 current acc: 0.8772321428571429\n",
      "iteration 76 current loss: 0.00042490431223995984 current acc: 0.877254974893063\n",
      "iteration 77 current loss: 0.0009745263960212469 current acc: 0.8772777984380811\n",
      "iteration 78 current loss: 0.0005237911827862263 current acc: 0.8773006134969326\n",
      "iteration 79 current loss: 0.00055145681835711 current acc: 0.8773234200743495\n",
      "iteration 80 current loss: 0.0006910694646649063 current acc: 0.8773462181750604\n",
      "iteration 81 current loss: 0.0005566051113419235 current acc: 0.8773690078037905\n",
      "iteration 82 current loss: 0.0006343779969029129 current acc: 0.877391788965261\n",
      "iteration 83 current loss: 0.0004648583708330989 current acc: 0.8774145616641902\n",
      "iteration 84 current loss: 0.0005714648868888617 current acc: 0.8774373259052924\n",
      "iteration 85 current loss: 0.0009108705562539399 current acc: 0.8774600816932788\n",
      "iteration 86 current loss: 0.0005402056849561632 current acc: 0.8774828290328569\n",
      "iteration 87 current loss: 0.0008105225278995931 current acc: 0.8775055679287305\n",
      "iteration 88 current loss: 0.0006082950276322663 current acc: 0.8775282983856003\n",
      "iteration 89 current loss: 0.0005022651166655123 current acc: 0.8775510204081632\n",
      "iteration 90 current loss: 0.0006452708621509373 current acc: 0.8775737340011129\n",
      "iteration 91 current loss: 0.0007088551064953208 current acc: 0.8775964391691394\n",
      "iteration 92 current loss: 0.0004797929141204804 current acc: 0.8776191359169293\n",
      "iteration 93 current loss: 0.0007992806495167315 current acc: 0.8776418242491657\n",
      "iteration 94 current loss: 0.0004635860677808523 current acc: 0.8776645041705282\n",
      "iteration 95 current loss: 0.0004019713669549674 current acc: 0.8776871756856931\n",
      "iteration 96 current loss: 0.00039619856397621334 current acc: 0.877709838799333\n",
      "iteration 97 current loss: 0.0007037263130769134 current acc: 0.877732493516117\n",
      "iteration 98 current loss: 0.0005180584266781807 current acc: 0.8777551398407113\n",
      "iteration 99 current loss: 0.0003835094685200602 current acc: 0.8777777777777778\n",
      "\t\tEpoch 53/100 complete. Epoch loss 0.000674061369500123 Epoch accuracy 1.0\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 53, Validation Accuracy: 0.631875, Validation Loss: 1.8877726137638091\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.0006552902050316334 current acc: 0.8778004073319755\n",
      "iteration 1 current loss: 0.00045246470835991204 current acc: 0.87782302850796\n",
      "iteration 2 current loss: 0.00032883533276617527 current acc: 0.8778456413103831\n",
      "iteration 3 current loss: 0.0004432114656083286 current acc: 0.8778682457438934\n",
      "iteration 4 current loss: 0.00039050125633366406 current acc: 0.877890841813136\n",
      "iteration 5 current loss: 0.0003127382951788604 current acc: 0.8779134295227525\n",
      "iteration 6 current loss: 0.0004552002646960318 current acc: 0.8779360088773812\n",
      "iteration 7 current loss: 0.0003106841177213937 current acc: 0.8779585798816568\n",
      "iteration 8 current loss: 0.0002866448776330799 current acc: 0.8779811425402108\n",
      "iteration 9 current loss: 0.00036617874866351485 current acc: 0.878003696857671\n",
      "iteration 10 current loss: 0.00027475683600641787 current acc: 0.8780262428386619\n",
      "iteration 11 current loss: 0.0004039921914227307 current acc: 0.8780487804878049\n",
      "iteration 12 current loss: 0.000368570996215567 current acc: 0.8780713098097174\n",
      "iteration 13 current loss: 0.0005536193493753672 current acc: 0.8780938308090137\n",
      "iteration 14 current loss: 0.0003312279295641929 current acc: 0.8781163434903048\n",
      "iteration 15 current loss: 0.0003420878201723099 current acc: 0.878138847858198\n",
      "iteration 16 current loss: 0.0005710234981961548 current acc: 0.8781613439172974\n",
      "iteration 17 current loss: 0.0003924074408132583 current acc: 0.8781838316722038\n",
      "iteration 18 current loss: 0.00035217267577536404 current acc: 0.8782063111275143\n",
      "iteration 19 current loss: 0.0003413113590795547 current acc: 0.8782287822878229\n",
      "iteration 20 current loss: 0.00030083919409662485 current acc: 0.87825124515772\n",
      "iteration 21 current loss: 0.0003937249130103737 current acc: 0.8782736997417927\n",
      "iteration 22 current loss: 0.00041274327668361366 current acc: 0.8782961460446247\n",
      "iteration 23 current loss: 0.00041441628127358854 current acc: 0.8783185840707964\n",
      "iteration 24 current loss: 0.00040694308700039983 current acc: 0.8783410138248848\n",
      "iteration 25 current loss: 0.00064801424741745 current acc: 0.8783634353114633\n",
      "iteration 26 current loss: 0.0004313079989515245 current acc: 0.8783858485351023\n",
      "iteration 27 current loss: 0.0002685942454263568 current acc: 0.8784082535003684\n",
      "iteration 28 current loss: 0.00037787214387208223 current acc: 0.8784306502118254\n",
      "iteration 29 current loss: 0.00044587982119992375 current acc: 0.8784530386740331\n",
      "iteration 30 current loss: 0.00047613464994356036 current acc: 0.8784754188915486\n",
      "iteration 31 current loss: 0.0002998259733431041 current acc: 0.8784977908689249\n",
      "iteration 32 current loss: 0.0004044316301587969 current acc: 0.8785201546107123\n",
      "iteration 33 current loss: 0.0004165539867244661 current acc: 0.8785425101214575\n",
      "iteration 34 current loss: 0.0005799924838356674 current acc: 0.8785648574057038\n",
      "iteration 35 current loss: 0.0007118976791389287 current acc: 0.8785871964679912\n",
      "iteration 36 current loss: 0.0006122664781287313 current acc: 0.8786095273128564\n",
      "iteration 37 current loss: 0.000486529286717996 current acc: 0.8786318499448327\n",
      "iteration 38 current loss: 0.00038074879557825625 current acc: 0.8786541643684501\n",
      "iteration 39 current loss: 0.00035076405038125813 current acc: 0.8786764705882353\n",
      "iteration 40 current loss: 0.0004043990047648549 current acc: 0.8786987686087117\n",
      "iteration 41 current loss: 0.0006036736303940415 current acc: 0.8787210584343991\n",
      "iteration 42 current loss: 0.00045722720096819103 current acc: 0.8787433400698145\n",
      "iteration 43 current loss: 0.000531101250089705 current acc: 0.878765613519471\n",
      "iteration 44 current loss: 0.00043607898987829685 current acc: 0.8787878787878788\n",
      "iteration 45 current loss: 0.00034835655242204666 current acc: 0.8788101358795446\n",
      "iteration 46 current loss: 0.00042171074892394245 current acc: 0.878832384798972\n",
      "iteration 47 current loss: 0.0005444344715215266 current acc: 0.8788546255506607\n",
      "iteration 48 current loss: 0.0003970083489548415 current acc: 0.8788768581391081\n",
      "iteration 49 current loss: 0.0004632951458916068 current acc: 0.8788990825688073\n",
      "iteration 50 current loss: 0.0006410665810108185 current acc: 0.8789212988442487\n",
      "iteration 51 current loss: 0.0003176641184836626 current acc: 0.8789435069699193\n",
      "iteration 52 current loss: 0.00036094439565204084 current acc: 0.8789657069503026\n",
      "iteration 53 current loss: 0.0003566133673302829 current acc: 0.878987898789879\n",
      "iteration 54 current loss: 0.00022406000061891973 current acc: 0.8790100824931256\n",
      "iteration 55 current loss: 0.00030562354368157685 current acc: 0.8790322580645161\n",
      "iteration 56 current loss: 0.0005301166092976928 current acc: 0.8790544255085212\n",
      "iteration 57 current loss: 0.0003464037145022303 current acc: 0.8790765848296079\n",
      "iteration 58 current loss: 0.00038698039134033024 current acc: 0.8790987360322403\n",
      "iteration 59 current loss: 0.00026720535242930055 current acc: 0.8791208791208791\n",
      "iteration 60 current loss: 0.00038217357359826565 current acc: 0.8791430140999816\n",
      "iteration 61 current loss: 0.0005360180512070656 current acc: 0.8791651409740022\n",
      "iteration 62 current loss: 0.00030634665745310485 current acc: 0.8791872597473915\n",
      "iteration 63 current loss: 0.0003045812773052603 current acc: 0.8792093704245973\n",
      "iteration 64 current loss: 0.0004080216458532959 current acc: 0.879231473010064\n",
      "iteration 65 current loss: 0.00040100771002471447 current acc: 0.8792535675082327\n",
      "iteration 66 current loss: 0.0002657797303982079 current acc: 0.8792756539235412\n",
      "iteration 67 current loss: 0.0004840217297896743 current acc: 0.8792977322604243\n",
      "iteration 68 current loss: 0.0004443173238541931 current acc: 0.8793198025233132\n",
      "iteration 69 current loss: 0.0004759986768476665 current acc: 0.8793418647166362\n",
      "iteration 70 current loss: 0.0004229804908391088 current acc: 0.8793639188448181\n",
      "iteration 71 current loss: 0.0003175708989147097 current acc: 0.8793859649122807\n",
      "iteration 72 current loss: 0.0005350409774109721 current acc: 0.8794080029234423\n",
      "iteration 73 current loss: 0.0004892166471108794 current acc: 0.8794300328827183\n",
      "iteration 74 current loss: 0.00044675529352389276 current acc: 0.8794520547945206\n",
      "iteration 75 current loss: 0.0004760372976306826 current acc: 0.8794740686632578\n",
      "iteration 76 current loss: 0.00041997444350272417 current acc: 0.8794960744933358\n",
      "iteration 77 current loss: 0.00039296792238019407 current acc: 0.8795180722891566\n",
      "iteration 78 current loss: 0.0002519895788282156 current acc: 0.8795400620551196\n",
      "iteration 79 current loss: 0.0003740557294804603 current acc: 0.8795620437956204\n",
      "iteration 80 current loss: 0.0004373824631329626 current acc: 0.879584017515052\n",
      "iteration 81 current loss: 0.00036042521242052317 current acc: 0.8796059832178037\n",
      "iteration 82 current loss: 0.00031705352012068033 current acc: 0.8796279409082619\n",
      "iteration 83 current loss: 0.0004207575984764844 current acc: 0.8796498905908097\n",
      "iteration 84 current loss: 0.0005987505428493023 current acc: 0.8796718322698268\n",
      "iteration 85 current loss: 0.0003548669337760657 current acc: 0.8796937659496901\n",
      "iteration 86 current loss: 0.00044026836985722184 current acc: 0.8797156916347731\n",
      "iteration 87 current loss: 0.0008450124296359718 current acc: 0.8797376093294461\n",
      "iteration 88 current loss: 0.0007155734929256141 current acc: 0.8797595190380761\n",
      "iteration 89 current loss: 0.0004933098098263144 current acc: 0.8797814207650273\n",
      "iteration 90 current loss: 0.0003169233677908778 current acc: 0.8798033145146603\n",
      "iteration 91 current loss: 0.0003474435652606189 current acc: 0.8798252002913328\n",
      "iteration 92 current loss: 0.0003474250843282789 current acc: 0.8798470780993992\n",
      "iteration 93 current loss: 0.0003852372756227851 current acc: 0.8798689479432108\n",
      "iteration 94 current loss: 0.0003097921144217253 current acc: 0.8798908098271155\n",
      "iteration 95 current loss: 0.0005770870484411716 current acc: 0.8799126637554585\n",
      "iteration 96 current loss: 0.0003169532574247569 current acc: 0.8799345097325814\n",
      "iteration 97 current loss: 0.0007586239953525364 current acc: 0.8799563477628228\n",
      "iteration 98 current loss: 0.0003776527300942689 current acc: 0.8799781778505182\n",
      "iteration 99 current loss: 0.0003646225086413324 current acc: 0.88\n",
      "\t\tEpoch 54/100 complete. Epoch loss 0.000423163799860049 Epoch accuracy 1.0\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 54, Validation Accuracy: 0.63625, Validation Loss: 1.8780932750552892\n",
      "best loss 0.000423163799860049\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.00032749242382124066 current acc: 0.8800218142155971\n",
      "iteration 1 current loss: 0.0003800283884629607 current acc: 0.8800436205016358\n",
      "iteration 2 current loss: 0.0006017129053361714 current acc: 0.8800654188624387\n",
      "iteration 3 current loss: 0.00037736367085017264 current acc: 0.8800872093023255\n",
      "iteration 4 current loss: 0.00028696912340819836 current acc: 0.8801089918256131\n",
      "iteration 5 current loss: 0.00042185711208730936 current acc: 0.8801307664366146\n",
      "iteration 6 current loss: 0.0002958440745715052 current acc: 0.8801525331396405\n",
      "iteration 7 current loss: 0.00039205941720865667 current acc: 0.8801742919389978\n",
      "iteration 8 current loss: 0.0002642897015903145 current acc: 0.8801960428389908\n",
      "iteration 9 current loss: 0.0003836615360341966 current acc: 0.8802177858439202\n",
      "iteration 10 current loss: 0.00038259982829913497 current acc: 0.8802395209580839\n",
      "iteration 11 current loss: 0.0005547675536945462 current acc: 0.8802612481857764\n",
      "iteration 12 current loss: 0.0011967060854658484 current acc: 0.8802829675312897\n",
      "iteration 13 current loss: 0.00041810888797044754 current acc: 0.8803046789989118\n",
      "iteration 14 current loss: 0.00024162241606973112 current acc: 0.8803263825929284\n",
      "iteration 15 current loss: 0.00043095950968563557 current acc: 0.8803480783176215\n",
      "iteration 16 current loss: 0.00040233286563307047 current acc: 0.8803697661772703\n",
      "iteration 17 current loss: 0.0006912703975103796 current acc: 0.8803914461761507\n",
      "iteration 18 current loss: 0.0002723263169173151 current acc: 0.880413118318536\n",
      "iteration 19 current loss: 0.0003204657114110887 current acc: 0.8804347826086957\n",
      "iteration 20 current loss: 0.00034673555637709796 current acc: 0.8804564390508965\n",
      "iteration 21 current loss: 0.00027644200599752367 current acc: 0.8804780876494024\n",
      "iteration 22 current loss: 0.00027966455672867596 current acc: 0.8804997284084737\n",
      "iteration 23 current loss: 0.0003181879874318838 current acc: 0.8805213613323678\n",
      "iteration 24 current loss: 0.00024007275351323187 current acc: 0.8805429864253393\n",
      "iteration 25 current loss: 0.0003908909566234797 current acc: 0.8805646036916395\n",
      "iteration 26 current loss: 0.0005716783343814313 current acc: 0.8805862131355165\n",
      "iteration 27 current loss: 0.00044874465675093234 current acc: 0.8806078147612156\n",
      "iteration 28 current loss: 0.000466566882096231 current acc: 0.8806294085729789\n",
      "iteration 29 current loss: 0.0003614566521719098 current acc: 0.8806509945750453\n",
      "iteration 30 current loss: 0.0013990446459501982 current acc: 0.8806725727716507\n",
      "iteration 31 current loss: 0.0003882290911860764 current acc: 0.8806941431670282\n",
      "iteration 32 current loss: 0.0003925007476937026 current acc: 0.8807157057654076\n",
      "iteration 33 current loss: 0.0005382363451644778 current acc: 0.8807372605710155\n",
      "iteration 34 current loss: 0.0003151427081320435 current acc: 0.8807588075880759\n",
      "iteration 35 current loss: 0.0004901178763248026 current acc: 0.8807803468208093\n",
      "iteration 36 current loss: 0.00031878799200057983 current acc: 0.8808018782734333\n",
      "iteration 37 current loss: 0.00030794835765846074 current acc: 0.8808234019501625\n",
      "iteration 38 current loss: 0.0003624143428169191 current acc: 0.8808449178552086\n",
      "iteration 39 current loss: 0.0002906384179368615 current acc: 0.8808664259927798\n",
      "iteration 40 current loss: 0.0002883465785998851 current acc: 0.8808879263670818\n",
      "iteration 41 current loss: 0.0003439810825511813 current acc: 0.8809094189823169\n",
      "iteration 42 current loss: 0.00042840803507715464 current acc: 0.8809309038426845\n",
      "iteration 43 current loss: 0.00043458686559461057 current acc: 0.8809523809523809\n",
      "iteration 44 current loss: 0.0005926446756348014 current acc: 0.8809738503155996\n",
      "iteration 45 current loss: 0.0005146377952769399 current acc: 0.8809953119365308\n",
      "iteration 46 current loss: 0.0003289228770881891 current acc: 0.8810167658193618\n",
      "iteration 47 current loss: 0.0003550094261299819 current acc: 0.8810382119682768\n",
      "iteration 48 current loss: 0.0004242636205162853 current acc: 0.8810596503874571\n",
      "iteration 49 current loss: 0.00042324859532527626 current acc: 0.8810810810810811\n",
      "iteration 50 current loss: 0.000589467235840857 current acc: 0.8811025040533237\n",
      "iteration 51 current loss: 0.0003180087369401008 current acc: 0.8811239193083573\n",
      "iteration 52 current loss: 0.00026599690318107605 current acc: 0.8811453268503512\n",
      "iteration 53 current loss: 0.0002483566931914538 current acc: 0.8811667266834714\n",
      "iteration 54 current loss: 0.0005398369394242764 current acc: 0.8811881188118812\n",
      "iteration 55 current loss: 0.0005326031241565943 current acc: 0.8812095032397408\n",
      "iteration 56 current loss: 0.00033908814657479525 current acc: 0.8812308799712075\n",
      "iteration 57 current loss: 0.00031894363928586245 current acc: 0.8812522490104354\n",
      "iteration 58 current loss: 0.0007876961026340723 current acc: 0.8812736103615758\n",
      "iteration 59 current loss: 0.0005007850704714656 current acc: 0.8812949640287769\n",
      "iteration 60 current loss: 0.00041017108014784753 current acc: 0.8813163100161842\n",
      "iteration 61 current loss: 0.0006887129857204854 current acc: 0.8813376483279396\n",
      "iteration 62 current loss: 0.0004226731543894857 current acc: 0.8813589789681826\n",
      "iteration 63 current loss: 0.0003202954540029168 current acc: 0.8813803019410497\n",
      "iteration 64 current loss: 0.0004515683394856751 current acc: 0.8814016172506739\n",
      "iteration 65 current loss: 0.0005411567399278283 current acc: 0.8814229249011858\n",
      "iteration 66 current loss: 0.0005455540376715362 current acc: 0.8814442248967128\n",
      "iteration 67 current loss: 0.000350688467733562 current acc: 0.8814655172413793\n",
      "iteration 68 current loss: 0.0003975575091317296 current acc: 0.8814868019393068\n",
      "iteration 69 current loss: 0.0007133500184863806 current acc: 0.881508078994614\n",
      "iteration 70 current loss: 0.00038321904139593244 current acc: 0.8815293484114163\n",
      "iteration 71 current loss: 0.0006599467014893889 current acc: 0.8815506101938263\n",
      "iteration 72 current loss: 0.00040535684092901647 current acc: 0.8815718643459537\n",
      "iteration 73 current loss: 0.00036442663986235857 current acc: 0.8815931108719053\n",
      "iteration 74 current loss: 0.0004050217685289681 current acc: 0.8816143497757848\n",
      "iteration 75 current loss: 0.00043331526103429496 current acc: 0.8816355810616929\n",
      "iteration 76 current loss: 0.00033882263232953846 current acc: 0.8816568047337278\n",
      "iteration 77 current loss: 0.0003046253405045718 current acc: 0.8816780207959842\n",
      "iteration 78 current loss: 0.0002559827407822013 current acc: 0.8816992292525542\n",
      "iteration 79 current loss: 0.0005573804955929518 current acc: 0.8817204301075269\n",
      "iteration 80 current loss: 0.0003779852413572371 current acc: 0.8817416233649884\n",
      "iteration 81 current loss: 0.0003703570691868663 current acc: 0.8817628090290218\n",
      "iteration 82 current loss: 0.0002347280242247507 current acc: 0.8817839871037076\n",
      "iteration 83 current loss: 0.0004186428268440068 current acc: 0.8818051575931232\n",
      "iteration 84 current loss: 0.000455957546364516 current acc: 0.8818263205013429\n",
      "iteration 85 current loss: 0.0003853927191812545 current acc: 0.8818474758324383\n",
      "iteration 86 current loss: 0.0002512064529582858 current acc: 0.8818686235904779\n",
      "iteration 87 current loss: 0.00047142626135610044 current acc: 0.8818897637795275\n",
      "iteration 88 current loss: 0.0006940559833310544 current acc: 0.8819108964036501\n",
      "iteration 89 current loss: 0.00047488429117947817 current acc: 0.8819320214669052\n",
      "iteration 90 current loss: 0.00021952722454443574 current acc: 0.88195313897335\n",
      "iteration 91 current loss: 0.00043879434815607965 current acc: 0.8819742489270386\n",
      "iteration 92 current loss: 0.00037720531690865755 current acc: 0.8819953513320222\n",
      "iteration 93 current loss: 0.00024820351973176 current acc: 0.882016446192349\n",
      "iteration 94 current loss: 0.0004233847139403224 current acc: 0.8820375335120644\n",
      "iteration 95 current loss: 0.00032317242585122585 current acc: 0.8820586132952108\n",
      "iteration 96 current loss: 0.0004489943676162511 current acc: 0.8820796855458282\n",
      "iteration 97 current loss: 0.0002661094185896218 current acc: 0.8821007502679529\n",
      "iteration 98 current loss: 0.00029617949621751904 current acc: 0.8821218074656189\n",
      "iteration 99 current loss: 0.0003130780823994428 current acc: 0.8821428571428571\n",
      "\t\tEpoch 55/100 complete. Epoch loss 0.0004215988151554484 Epoch accuracy 1.0\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 55, Validation Accuracy: 0.633125, Validation Loss: 1.8981376189738512\n",
      "best loss 0.0004215988151554484\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.0002865210408344865 current acc: 0.8821638993036958\n",
      "iteration 1 current loss: 0.00032431387808173895 current acc: 0.8821849339521599\n",
      "iteration 2 current loss: 0.0003660197544377297 current acc: 0.882205961092272\n",
      "iteration 3 current loss: 0.0004232298815622926 current acc: 0.8822269807280514\n",
      "iteration 4 current loss: 0.0009202667279168963 current acc: 0.8822479928635147\n",
      "iteration 5 current loss: 0.0002679470635484904 current acc: 0.8822689975026757\n",
      "iteration 6 current loss: 0.0003112873819191009 current acc: 0.8822899946495452\n",
      "iteration 7 current loss: 0.0005409774021245539 current acc: 0.8823109843081313\n",
      "iteration 8 current loss: 0.00030190908000804484 current acc: 0.882331966482439\n",
      "iteration 9 current loss: 0.0002943977597169578 current acc: 0.8823529411764706\n",
      "iteration 10 current loss: 0.00021561808534897864 current acc: 0.8823739083942256\n",
      "iteration 11 current loss: 0.00032305633067153394 current acc: 0.8823948681397007\n",
      "iteration 12 current loss: 0.00022763351444154978 current acc: 0.8824158204168894\n",
      "iteration 13 current loss: 0.0004106641572434455 current acc: 0.8824367652297826\n",
      "iteration 14 current loss: 0.0003469955991022289 current acc: 0.8824577025823687\n",
      "iteration 15 current loss: 0.00023183185840025544 current acc: 0.8824786324786325\n",
      "iteration 16 current loss: 0.00040010790689848363 current acc: 0.8824995549225565\n",
      "iteration 17 current loss: 0.00046494140406139195 current acc: 0.8825204699181203\n",
      "iteration 18 current loss: 0.00041177927050739527 current acc: 0.8825413774693006\n",
      "iteration 19 current loss: 0.0002717742172535509 current acc: 0.8825622775800712\n",
      "iteration 20 current loss: 0.0007149495068006217 current acc: 0.8825831702544031\n",
      "iteration 21 current loss: 0.0006165310624055564 current acc: 0.8826040554962646\n",
      "iteration 22 current loss: 0.00035997258964926004 current acc: 0.8826249333096212\n",
      "iteration 23 current loss: 0.0005312376306392252 current acc: 0.8826458036984353\n",
      "iteration 24 current loss: 0.0003523861232679337 current acc: 0.8826666666666667\n",
      "iteration 25 current loss: 0.0002599378349259496 current acc: 0.8826875222182723\n",
      "iteration 26 current loss: 0.00031533592846244574 current acc: 0.8827083703572063\n",
      "iteration 27 current loss: 0.0003204752574674785 current acc: 0.8827292110874201\n",
      "iteration 28 current loss: 0.0003068302175961435 current acc: 0.882750044412862\n",
      "iteration 29 current loss: 0.000368317065294832 current acc: 0.8827708703374778\n",
      "iteration 30 current loss: 0.00030793753103353083 current acc: 0.8827916888652104\n",
      "iteration 31 current loss: 0.0002074584481306374 current acc: 0.8828125\n",
      "iteration 32 current loss: 0.00024621724151074886 current acc: 0.8828333037457837\n",
      "iteration 33 current loss: 0.0003463597677182406 current acc: 0.8828541001064962\n",
      "iteration 34 current loss: 0.000279416999546811 current acc: 0.8828748890860693\n",
      "iteration 35 current loss: 0.0007061532814987004 current acc: 0.8828956706884316\n",
      "iteration 36 current loss: 0.00034723765566013753 current acc: 0.8829164449175093\n",
      "iteration 37 current loss: 0.0002589435316622257 current acc: 0.882937211777226\n",
      "iteration 38 current loss: 0.0006501007010228932 current acc: 0.882957971271502\n",
      "iteration 39 current loss: 0.0003814570955000818 current acc: 0.8829787234042553\n",
      "iteration 40 current loss: 0.00039084895979613066 current acc: 0.8829994681794008\n",
      "iteration 41 current loss: 0.00031496401061303914 current acc: 0.8830202056008508\n",
      "iteration 42 current loss: 0.0003035185218323022 current acc: 0.8830409356725146\n",
      "iteration 43 current loss: 0.00022880343021824956 current acc: 0.8830616583982991\n",
      "iteration 44 current loss: 0.0002502279239706695 current acc: 0.8830823737821081\n",
      "iteration 45 current loss: 0.00026189134223386645 current acc: 0.8831030818278427\n",
      "iteration 46 current loss: 0.0002670477260835469 current acc: 0.8831237825394015\n",
      "iteration 47 current loss: 0.0007475013844668865 current acc: 0.8831444759206799\n",
      "iteration 48 current loss: 0.00028718923567794263 current acc: 0.8831651619755709\n",
      "iteration 49 current loss: 0.0003681916859932244 current acc: 0.8831858407079646\n",
      "iteration 50 current loss: 0.0002519729605410248 current acc: 0.8832065121217484\n",
      "iteration 51 current loss: 0.00033779925433918834 current acc: 0.8832271762208068\n",
      "iteration 52 current loss: 0.00035430092248134315 current acc: 0.8832478330090218\n",
      "iteration 53 current loss: 0.0003080171882174909 current acc: 0.8832684824902723\n",
      "iteration 54 current loss: 0.0003712715988513082 current acc: 0.883289124668435\n",
      "iteration 55 current loss: 0.000208778161322698 current acc: 0.8833097595473833\n",
      "iteration 56 current loss: 0.0006535029970109463 current acc: 0.8833303871309881\n",
      "iteration 57 current loss: 0.0003003949241247028 current acc: 0.8833510074231177\n",
      "iteration 58 current loss: 0.00042047502938658 current acc: 0.8833716204276374\n",
      "iteration 59 current loss: 0.00040049978997558355 current acc: 0.8833922261484098\n",
      "iteration 60 current loss: 0.0002260129840578884 current acc: 0.8834128245892952\n",
      "iteration 61 current loss: 0.0004525422409642488 current acc: 0.8834334157541505\n",
      "iteration 62 current loss: 0.0003599288174882531 current acc: 0.8834539996468304\n",
      "iteration 63 current loss: 0.00027890646015293896 current acc: 0.8834745762711864\n",
      "iteration 64 current loss: 0.0005136061809025705 current acc: 0.883495145631068\n",
      "iteration 65 current loss: 0.0003395424864720553 current acc: 0.8835157077303212\n",
      "iteration 66 current loss: 0.00018652643484529108 current acc: 0.8835362625727898\n",
      "iteration 67 current loss: 0.0004533207684289664 current acc: 0.8835568101623148\n",
      "iteration 68 current loss: 0.00031637653592042625 current acc: 0.8835773505027341\n",
      "iteration 69 current loss: 0.00031404991750605404 current acc: 0.8835978835978836\n",
      "iteration 70 current loss: 0.0004012382705695927 current acc: 0.8836184094515959\n",
      "iteration 71 current loss: 0.00028552976436913013 current acc: 0.883638928067701\n",
      "iteration 72 current loss: 0.0005265361978672445 current acc: 0.8836594394500265\n",
      "iteration 73 current loss: 0.0005458822706714272 current acc: 0.8836799436023969\n",
      "iteration 74 current loss: 0.0002951400529127568 current acc: 0.8837004405286344\n",
      "iteration 75 current loss: 0.0002715436858125031 current acc: 0.8837209302325582\n",
      "iteration 76 current loss: 0.00045795083860866725 current acc: 0.8837414127179849\n",
      "iteration 77 current loss: 0.0002985744795296341 current acc: 0.8837618879887285\n",
      "iteration 78 current loss: 0.0003711720637511462 current acc: 0.8837823560486001\n",
      "iteration 79 current loss: 0.00033955692197196186 current acc: 0.8838028169014085\n",
      "iteration 80 current loss: 0.00031477693119086325 current acc: 0.8838232705509593\n",
      "iteration 81 current loss: 0.0002583011519163847 current acc: 0.883843717001056\n",
      "iteration 82 current loss: 0.00043118506437167525 current acc: 0.8838641562554989\n",
      "iteration 83 current loss: 0.00023735103604849428 current acc: 0.8838845883180858\n",
      "iteration 84 current loss: 0.00030011104536242783 current acc: 0.8839050131926122\n",
      "iteration 85 current loss: 0.00038987494190223515 current acc: 0.8839254308828702\n",
      "iteration 86 current loss: 0.00036948081105947495 current acc: 0.8839458413926499\n",
      "iteration 87 current loss: 0.00028213701443746686 current acc: 0.8839662447257384\n",
      "iteration 88 current loss: 0.0005287191597744823 current acc: 0.8839866408859202\n",
      "iteration 89 current loss: 0.00026120012626051903 current acc: 0.8840070298769771\n",
      "iteration 90 current loss: 0.00039597589056938887 current acc: 0.8840274117026885\n",
      "iteration 91 current loss: 0.00021000120614189655 current acc: 0.8840477863668307\n",
      "iteration 92 current loss: 0.00042809091974049807 current acc: 0.8840681538731776\n",
      "iteration 93 current loss: 0.00025322288274765015 current acc: 0.8840885142255005\n",
      "iteration 94 current loss: 0.00030983774922788143 current acc: 0.884108867427568\n",
      "iteration 95 current loss: 0.0003881111624650657 current acc: 0.8841292134831461\n",
      "iteration 96 current loss: 0.00029700720915570855 current acc: 0.8841495523959979\n",
      "iteration 97 current loss: 0.0002971484500449151 current acc: 0.8841698841698842\n",
      "iteration 98 current loss: 0.00035559397656470537 current acc: 0.8841902088085629\n",
      "iteration 99 current loss: 0.0003501763567328453 current acc: 0.8842105263157894\n",
      "\t\tEpoch 56/100 complete. Epoch loss 0.00036137965289526617 Epoch accuracy 1.0\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 56, Validation Accuracy: 0.634625, Validation Loss: 1.8959120515733958\n",
      "best loss 0.00036137965289526617\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.0002100356505252421 current acc: 0.8842308366953167\n",
      "iteration 1 current loss: 0.0003036147973034531 current acc: 0.8842511399508944\n",
      "iteration 2 current loss: 0.00021816318621858954 current acc: 0.8842714360862703\n",
      "iteration 3 current loss: 0.0003125453076791018 current acc: 0.8842917251051894\n",
      "iteration 4 current loss: 0.00019805938063655049 current acc: 0.8843120070113936\n",
      "iteration 5 current loss: 0.00031638547079637647 current acc: 0.8843322818086226\n",
      "iteration 6 current loss: 0.0002319070481462404 current acc: 0.8843525495006133\n",
      "iteration 7 current loss: 0.0003329378378111869 current acc: 0.8843728100911002\n",
      "iteration 8 current loss: 0.00024635824956931174 current acc: 0.884393063583815\n",
      "iteration 9 current loss: 0.00025625628768466413 current acc: 0.8844133099824869\n",
      "iteration 10 current loss: 0.00024572896654717624 current acc: 0.8844335492908423\n",
      "iteration 11 current loss: 0.00019770057406276464 current acc: 0.884453781512605\n",
      "iteration 12 current loss: 0.00031638293876312673 current acc: 0.8844740066514966\n",
      "iteration 13 current loss: 0.0003652157902251929 current acc: 0.8844942247112355\n",
      "iteration 14 current loss: 0.0003690847079269588 current acc: 0.884514435695538\n",
      "iteration 15 current loss: 0.0007243736763484776 current acc: 0.8845346396081175\n",
      "iteration 16 current loss: 0.000365108426194638 current acc: 0.884554836452685\n",
      "iteration 17 current loss: 0.00026443329988978803 current acc: 0.8845750262329486\n",
      "iteration 18 current loss: 0.0003007447230629623 current acc: 0.8845952089526141\n",
      "iteration 19 current loss: 0.00029665083275176585 current acc: 0.8846153846153846\n",
      "iteration 20 current loss: 0.000320503837428987 current acc: 0.8846355532249607\n",
      "iteration 21 current loss: 0.00018649456615094095 current acc: 0.8846557147850402\n",
      "iteration 22 current loss: 0.00023150230117607862 current acc: 0.8846758692993185\n",
      "iteration 23 current loss: 0.00039756347541697323 current acc: 0.8846960167714885\n",
      "iteration 24 current loss: 0.00047527195420116186 current acc: 0.8847161572052402\n",
      "iteration 25 current loss: 0.00042336375918239355 current acc: 0.8847362906042613\n",
      "iteration 26 current loss: 0.00027382225380279124 current acc: 0.8847564169722367\n",
      "iteration 27 current loss: 0.00023026729468256235 current acc: 0.8847765363128491\n",
      "iteration 28 current loss: 0.000239579429035075 current acc: 0.8847966486297784\n",
      "iteration 29 current loss: 0.00028846648638136685 current acc: 0.8848167539267016\n",
      "iteration 30 current loss: 0.0002676157746464014 current acc: 0.8848368522072937\n",
      "iteration 31 current loss: 0.00039661931805312634 current acc: 0.8848569434752268\n",
      "iteration 32 current loss: 0.0002670206013135612 current acc: 0.8848770277341705\n",
      "iteration 33 current loss: 0.0004196808149572462 current acc: 0.8848971049877922\n",
      "iteration 34 current loss: 0.00034958546166308224 current acc: 0.8849171752397559\n",
      "iteration 35 current loss: 0.0003488257934805006 current acc: 0.8849372384937239\n",
      "iteration 36 current loss: 0.00044777870061807334 current acc: 0.8849572947533554\n",
      "iteration 37 current loss: 0.0004557870561257005 current acc: 0.8849773440223074\n",
      "iteration 38 current loss: 0.00026369444094598293 current acc: 0.8849973863042342\n",
      "iteration 39 current loss: 0.0006650249124504626 current acc: 0.8850174216027874\n",
      "iteration 40 current loss: 0.0004196269728709012 current acc: 0.8850374499216165\n",
      "iteration 41 current loss: 0.00022877013543620706 current acc: 0.8850574712643678\n",
      "iteration 42 current loss: 0.00047934349277056754 current acc: 0.8850774856346857\n",
      "iteration 43 current loss: 0.000475917273433879 current acc: 0.8850974930362117\n",
      "iteration 44 current loss: 0.0004936715122312307 current acc: 0.8851174934725848\n",
      "iteration 45 current loss: 0.000321269704727456 current acc: 0.8851374869474417\n",
      "iteration 46 current loss: 0.0003995790611952543 current acc: 0.8851574734644162\n",
      "iteration 47 current loss: 0.00028442288748919964 current acc: 0.8851774530271399\n",
      "iteration 48 current loss: 0.0004057076293975115 current acc: 0.8851974256392416\n",
      "iteration 49 current loss: 0.00041237581172026694 current acc: 0.8852173913043478\n",
      "iteration 50 current loss: 0.00026873211027123034 current acc: 0.8852373500260824\n",
      "iteration 51 current loss: 0.00021745018602814525 current acc: 0.8852573018080667\n",
      "iteration 52 current loss: 0.00029875434120185673 current acc: 0.8852772466539197\n",
      "iteration 53 current loss: 0.00040438742144033313 current acc: 0.8852971845672576\n",
      "iteration 54 current loss: 0.00025012134574353695 current acc: 0.8853171155516942\n",
      "iteration 55 current loss: 0.0004080233338754624 current acc: 0.8853370396108409\n",
      "iteration 56 current loss: 0.00029811178683303297 current acc: 0.8853569567483064\n",
      "iteration 57 current loss: 0.00029251279192976654 current acc: 0.8853768669676971\n",
      "iteration 58 current loss: 0.0003027648781426251 current acc: 0.8853967702726168\n",
      "iteration 59 current loss: 0.00024780852254480124 current acc: 0.8854166666666666\n",
      "iteration 60 current loss: 0.000352441449649632 current acc: 0.8854365561534456\n",
      "iteration 61 current loss: 0.00026783018256537616 current acc: 0.8854564387365498\n",
      "iteration 62 current loss: 0.0002976501709781587 current acc: 0.8854763144195731\n",
      "iteration 63 current loss: 0.00033494498347863555 current acc: 0.8854961832061069\n",
      "iteration 64 current loss: 0.0004129348380956799 current acc: 0.8855160450997398\n",
      "iteration 65 current loss: 0.00029743544291704893 current acc: 0.8855359001040582\n",
      "iteration 66 current loss: 0.00030505022732540965 current acc: 0.885555748222646\n",
      "iteration 67 current loss: 0.0006528861122205853 current acc: 0.8855755894590847\n",
      "iteration 68 current loss: 0.0002505581942386925 current acc: 0.8855954238169527\n",
      "iteration 69 current loss: 0.00030935590621083975 current acc: 0.8856152512998267\n",
      "iteration 70 current loss: 0.00033045862801373005 current acc: 0.8856350719112805\n",
      "iteration 71 current loss: 0.00047328099026344717 current acc: 0.8856548856548857\n",
      "iteration 72 current loss: 0.0003478367580100894 current acc: 0.8856746925342109\n",
      "iteration 73 current loss: 0.0003959707682952285 current acc: 0.885694492552823\n",
      "iteration 74 current loss: 0.00027730772853828967 current acc: 0.8857142857142857\n",
      "iteration 75 current loss: 0.0002924891887232661 current acc: 0.8857340720221607\n",
      "iteration 76 current loss: 0.0003522431943565607 current acc: 0.885753851480007\n",
      "iteration 77 current loss: 0.00028243224369361997 current acc: 0.885773624091381\n",
      "iteration 78 current loss: 0.00020337033492978662 current acc: 0.8857933898598374\n",
      "iteration 79 current loss: 0.0003727891598828137 current acc: 0.8858131487889274\n",
      "iteration 80 current loss: 0.00023811310529708862 current acc: 0.8858329008822003\n",
      "iteration 81 current loss: 0.0002468976017553359 current acc: 0.885852646143203\n",
      "iteration 82 current loss: 0.00037319542025215924 current acc: 0.8858723845754799\n",
      "iteration 83 current loss: 0.0002768375270534307 current acc: 0.8858921161825726\n",
      "iteration 84 current loss: 0.0002691012341529131 current acc: 0.8859118409680208\n",
      "iteration 85 current loss: 0.0002750773273874074 current acc: 0.8859315589353612\n",
      "iteration 86 current loss: 0.0003314272325951606 current acc: 0.8859512700881286\n",
      "iteration 87 current loss: 0.00045460218098014593 current acc: 0.8859709744298548\n",
      "iteration 88 current loss: 0.0003472411772236228 current acc: 0.8859906719640698\n",
      "iteration 89 current loss: 0.00025229447055608034 current acc: 0.8860103626943006\n",
      "iteration 90 current loss: 0.0004278860578779131 current acc: 0.8860300466240718\n",
      "iteration 91 current loss: 0.0005125579773448408 current acc: 0.886049723756906\n",
      "iteration 92 current loss: 0.00026512492331676185 current acc: 0.8860693940963231\n",
      "iteration 93 current loss: 0.00041753961704671383 current acc: 0.8860890576458406\n",
      "iteration 94 current loss: 0.00021997667499817908 current acc: 0.8861087144089732\n",
      "iteration 95 current loss: 0.0003791908093262464 current acc: 0.8861283643892339\n",
      "iteration 96 current loss: 0.00024081893207039684 current acc: 0.8861480075901328\n",
      "iteration 97 current loss: 0.00022528678528033197 current acc: 0.8861676440151777\n",
      "iteration 98 current loss: 0.00025801616720855236 current acc: 0.8861872736678738\n",
      "iteration 99 current loss: 0.000509408360812813 current acc: 0.8862068965517241\n",
      "\t\tEpoch 57/100 complete. Epoch loss 0.0003325936666806228 Epoch accuracy 1.0\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 57, Validation Accuracy: 0.636375, Validation Loss: 1.884536663070321\n",
      "best loss 0.0003325936666806228\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.00028553171432577074 current acc: 0.8862265126702292\n",
      "iteration 1 current loss: 0.0002096823591273278 current acc: 0.8862461220268872\n",
      "iteration 2 current loss: 0.00019226617587264627 current acc: 0.8862657246251938\n",
      "iteration 3 current loss: 0.00023067434085533023 current acc: 0.8862853204686423\n",
      "iteration 4 current loss: 0.0003379939589649439 current acc: 0.8863049095607235\n",
      "iteration 5 current loss: 0.00023891111777629703 current acc: 0.8863244919049259\n",
      "iteration 6 current loss: 0.00026167379110120237 current acc: 0.8863440675047357\n",
      "iteration 7 current loss: 0.00035048063728027046 current acc: 0.8863636363636364\n",
      "iteration 8 current loss: 0.00047077159979380667 current acc: 0.8863831984851093\n",
      "iteration 9 current loss: 0.00031569163547828794 current acc: 0.8864027538726333\n",
      "iteration 10 current loss: 0.00025747905601747334 current acc: 0.8864223025296851\n",
      "iteration 11 current loss: 0.00028541506617330015 current acc: 0.8864418444597385\n",
      "iteration 12 current loss: 0.0002898296224884689 current acc: 0.8864613796662653\n",
      "iteration 13 current loss: 0.00025997572811320424 current acc: 0.8864809081527347\n",
      "iteration 14 current loss: 0.00020922705880366266 current acc: 0.8865004299226139\n",
      "iteration 15 current loss: 0.0004319125146139413 current acc: 0.8865199449793673\n",
      "iteration 16 current loss: 0.00028061200282536447 current acc: 0.8865394533264569\n",
      "iteration 17 current loss: 0.0006394304218702018 current acc: 0.8865589549673427\n",
      "iteration 18 current loss: 0.00021990913955960423 current acc: 0.8865784499054821\n",
      "iteration 19 current loss: 0.0001740792067721486 current acc: 0.8865979381443299\n",
      "iteration 20 current loss: 0.00022292065841611475 current acc: 0.886617419687339\n",
      "iteration 21 current loss: 0.00022918020840734243 current acc: 0.8866368945379595\n",
      "iteration 22 current loss: 0.00023361768398899585 current acc: 0.8866563626996393\n",
      "iteration 23 current loss: 0.00030008680187165737 current acc: 0.8866758241758241\n",
      "iteration 24 current loss: 0.0002076795499306172 current acc: 0.886695278969957\n",
      "iteration 25 current loss: 0.000298282946459949 current acc: 0.8867147270854789\n",
      "iteration 26 current loss: 0.00024935186957009137 current acc: 0.8867341685258281\n",
      "iteration 27 current loss: 0.00025126009131781757 current acc: 0.8867536032944406\n",
      "iteration 28 current loss: 0.00019328769121784717 current acc: 0.8867730313947504\n",
      "iteration 29 current loss: 0.0002652851981110871 current acc: 0.8867924528301887\n",
      "iteration 30 current loss: 0.0002987661282531917 current acc: 0.8868118676041845\n",
      "iteration 31 current loss: 0.0002934411750175059 current acc: 0.8868312757201646\n",
      "iteration 32 current loss: 0.00024639524053782225 current acc: 0.8868506771815532\n",
      "iteration 33 current loss: 0.0005511703784577549 current acc: 0.8868700719917724\n",
      "iteration 34 current loss: 0.00021649223344866186 current acc: 0.8868894601542416\n",
      "iteration 35 current loss: 0.000327829533489421 current acc: 0.8869088416723784\n",
      "iteration 36 current loss: 0.00030852408963255584 current acc: 0.8869282165495974\n",
      "iteration 37 current loss: 0.00031869893427938223 current acc: 0.8869475847893115\n",
      "iteration 38 current loss: 0.00028506378293968737 current acc: 0.8869669463949307\n",
      "iteration 39 current loss: 0.00025578122586011887 current acc: 0.886986301369863\n",
      "iteration 40 current loss: 0.0001696827239356935 current acc: 0.8870056497175142\n",
      "iteration 41 current loss: 0.0003843815647996962 current acc: 0.8870249914412872\n",
      "iteration 42 current loss: 0.00023820837668608874 current acc: 0.8870443265445833\n",
      "iteration 43 current loss: 0.00037829906796105206 current acc: 0.8870636550308009\n",
      "iteration 44 current loss: 0.000492895720526576 current acc: 0.8870829769033362\n",
      "iteration 45 current loss: 0.00026153368526138365 current acc: 0.8871022921655833\n",
      "iteration 46 current loss: 0.0002724819933064282 current acc: 0.8871216008209338\n",
      "iteration 47 current loss: 0.0003582976059988141 current acc: 0.887140902872777\n",
      "iteration 48 current loss: 0.0002967076434288174 current acc: 0.8871601983244999\n",
      "iteration 49 current loss: 0.0003442854795139283 current acc: 0.8871794871794871\n",
      "iteration 50 current loss: 0.0002046450535999611 current acc: 0.8871987694411212\n",
      "iteration 51 current loss: 0.0002869945310521871 current acc: 0.8872180451127819\n",
      "iteration 52 current loss: 0.00021673578885383904 current acc: 0.8872373141978472\n",
      "iteration 53 current loss: 0.0003524164203554392 current acc: 0.8872565766996925\n",
      "iteration 54 current loss: 0.0002063926076516509 current acc: 0.8872758326216909\n",
      "iteration 55 current loss: 0.0002566855982877314 current acc: 0.8872950819672131\n",
      "iteration 56 current loss: 0.0004410864785313606 current acc: 0.8873143247396278\n",
      "iteration 57 current loss: 0.00021672379807569087 current acc: 0.8873335609423011\n",
      "iteration 58 current loss: 0.000333795411279425 current acc: 0.8873527905785971\n",
      "iteration 59 current loss: 0.00033904571318998933 current acc: 0.8873720136518771\n",
      "iteration 60 current loss: 0.00024378299713134766 current acc: 0.8873912301655008\n",
      "iteration 61 current loss: 0.0003438615531194955 current acc: 0.887410440122825\n",
      "iteration 62 current loss: 0.00024234576267190278 current acc: 0.8874296435272045\n",
      "iteration 63 current loss: 0.00037881097523495555 current acc: 0.8874488403819918\n",
      "iteration 64 current loss: 0.0001737800193950534 current acc: 0.887468030690537\n",
      "iteration 65 current loss: 0.00026086182333528996 current acc: 0.8874872144561882\n",
      "iteration 66 current loss: 0.0007108188583515584 current acc: 0.8875063916822907\n",
      "iteration 67 current loss: 0.0002154884277842939 current acc: 0.8875255623721882\n",
      "iteration 68 current loss: 0.00027140797465108335 current acc: 0.8875447265292213\n",
      "iteration 69 current loss: 0.0004247360920999199 current acc: 0.8875638841567292\n",
      "iteration 70 current loss: 0.0001745637273415923 current acc: 0.887583035258048\n",
      "iteration 71 current loss: 0.00028566207038238645 current acc: 0.8876021798365122\n",
      "iteration 72 current loss: 0.00023228580539580435 current acc: 0.8876213178954537\n",
      "iteration 73 current loss: 0.0002753820735961199 current acc: 0.8876404494382022\n",
      "iteration 74 current loss: 0.0002289304102305323 current acc: 0.8876595744680851\n",
      "iteration 75 current loss: 0.0004003297653980553 current acc: 0.8876786929884275\n",
      "iteration 76 current loss: 0.0023577886167913675 current acc: 0.8876978050025524\n",
      "iteration 77 current loss: 0.0005808360292576253 current acc: 0.8877169105137802\n",
      "iteration 78 current loss: 0.0003251461312174797 current acc: 0.8877360095254295\n",
      "iteration 79 current loss: 0.00025270829792134464 current acc: 0.8877551020408163\n",
      "iteration 80 current loss: 0.0002944131847470999 current acc: 0.8877741880632546\n",
      "iteration 81 current loss: 0.0003362435381859541 current acc: 0.8877932675960558\n",
      "iteration 82 current loss: 0.000594199460465461 current acc: 0.8878123406425293\n",
      "iteration 83 current loss: 0.0010079698404297233 current acc: 0.8878314072059823\n",
      "iteration 84 current loss: 0.0005052695632912219 current acc: 0.8878504672897196\n",
      "iteration 85 current loss: 0.0013991236919537187 current acc: 0.8878695208970439\n",
      "iteration 86 current loss: 0.0006760148680768907 current acc: 0.8878885680312553\n",
      "iteration 87 current loss: 0.0011239585001021624 current acc: 0.8879076086956522\n",
      "iteration 88 current loss: 0.0003456712875049561 current acc: 0.8879266428935303\n",
      "iteration 89 current loss: 0.00044591911137104034 current acc: 0.8879456706281834\n",
      "iteration 90 current loss: 0.00024968580692075193 current acc: 0.8879646919029027\n",
      "iteration 91 current loss: 0.00047127791913226247 current acc: 0.8879837067209776\n",
      "iteration 92 current loss: 0.0006795363151468337 current acc: 0.8880027150856948\n",
      "iteration 93 current loss: 0.00038503791438415647 current acc: 0.8880217170003394\n",
      "iteration 94 current loss: 0.0006494633271358907 current acc: 0.8880407124681934\n",
      "iteration 95 current loss: 0.0008207710925489664 current acc: 0.8880597014925373\n",
      "iteration 96 current loss: 0.000710917403921485 current acc: 0.8880786840766491\n",
      "iteration 97 current loss: 0.00033632040140219033 current acc: 0.8880976602238047\n",
      "iteration 98 current loss: 0.0005394549807533622 current acc: 0.8881166299372775\n",
      "iteration 99 current loss: 0.0004656971141230315 current acc: 0.888135593220339\n",
      "\t\tEpoch 58/100 complete. Epoch loss 0.00037962430564220996 Epoch accuracy 1.0\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 58, Validation Accuracy: 0.6265, Validation Loss: 1.9786304213106631\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.0010123400716111064 current acc: 0.8881545500762582\n",
      "iteration 1 current loss: 0.0002399084041826427 current acc: 0.8881735005083022\n",
      "iteration 2 current loss: 0.0013000847538933158 current acc: 0.8881924445197358\n",
      "iteration 3 current loss: 0.0003200065693818033 current acc: 0.8882113821138211\n",
      "iteration 4 current loss: 0.00029543612617999315 current acc: 0.8882303132938189\n",
      "iteration 5 current loss: 0.0001376684376737103 current acc: 0.8882492380629868\n",
      "iteration 6 current loss: 0.0002828373690135777 current acc: 0.888268156424581\n",
      "iteration 7 current loss: 0.0003123231581412256 current acc: 0.8882870683818551\n",
      "iteration 8 current loss: 0.000291922566248104 current acc: 0.8883059739380605\n",
      "iteration 9 current loss: 0.000288793962681666 current acc: 0.8883248730964467\n",
      "iteration 10 current loss: 0.00038260730798356235 current acc: 0.8883437658602605\n",
      "iteration 11 current loss: 0.0005480509134940803 current acc: 0.8883626522327469\n",
      "iteration 12 current loss: 0.0006152522400952876 current acc: 0.8883815322171487\n",
      "iteration 13 current loss: 0.0006143372156657279 current acc: 0.8884004058167061\n",
      "iteration 14 current loss: 0.00029154380899854004 current acc: 0.8884192730346576\n",
      "iteration 15 current loss: 0.0003918969596270472 current acc: 0.8884381338742393\n",
      "iteration 16 current loss: 0.00024779781233519316 current acc: 0.8884569883386851\n",
      "iteration 17 current loss: 0.00027349867741577327 current acc: 0.8884758364312267\n",
      "iteration 18 current loss: 0.000306850706692785 current acc: 0.8884946781550938\n",
      "iteration 19 current loss: 0.000653990195132792 current acc: 0.8885135135135135\n",
      "iteration 20 current loss: 0.0004280217399355024 current acc: 0.8885323425097112\n",
      "iteration 21 current loss: 0.0005214567063376307 current acc: 0.8885511651469098\n",
      "iteration 22 current loss: 0.00024307983403559774 current acc: 0.8885699814283302\n",
      "iteration 23 current loss: 0.00044479340431280434 current acc: 0.888588791357191\n",
      "iteration 24 current loss: 0.000554762315005064 current acc: 0.8886075949367088\n",
      "iteration 25 current loss: 0.0005811969749629498 current acc: 0.8886263921700979\n",
      "iteration 26 current loss: 0.0006200853385962546 current acc: 0.8886451830605703\n",
      "iteration 27 current loss: 0.00033722363878041506 current acc: 0.888663967611336\n",
      "iteration 28 current loss: 0.0004864118527621031 current acc: 0.8886827458256029\n",
      "iteration 29 current loss: 0.0003798868856392801 current acc: 0.8887015177065767\n",
      "iteration 30 current loss: 0.0003422953886911273 current acc: 0.8887202832574608\n",
      "iteration 31 current loss: 0.00023736600996926427 current acc: 0.8887390424814565\n",
      "iteration 32 current loss: 0.00024023957666940987 current acc: 0.888757795381763\n",
      "iteration 33 current loss: 0.00027948172646574676 current acc: 0.8887765419615774\n",
      "iteration 34 current loss: 0.0008049531024880707 current acc: 0.8887952822240943\n",
      "iteration 35 current loss: 0.0003332446503918618 current acc: 0.8888140161725068\n",
      "iteration 36 current loss: 0.00029758390155620873 current acc: 0.8888327438100051\n",
      "iteration 37 current loss: 0.00028727605240419507 current acc: 0.8888514651397778\n",
      "iteration 38 current loss: 0.0003863250894937664 current acc: 0.888870180165011\n",
      "iteration 39 current loss: 0.00042947824113070965 current acc: 0.8888888888888888\n",
      "iteration 40 current loss: 0.00022517911565955728 current acc: 0.8889075913145935\n",
      "iteration 41 current loss: 0.00024982672766782343 current acc: 0.8889262874453047\n",
      "iteration 42 current loss: 0.0001864760124590248 current acc: 0.8889449772841999\n",
      "iteration 43 current loss: 0.0011121128918603063 current acc: 0.8889636608344549\n",
      "iteration 44 current loss: 0.00019740432617254555 current acc: 0.8889823380992431\n",
      "iteration 45 current loss: 0.0003270930901635438 current acc: 0.8890010090817356\n",
      "iteration 46 current loss: 0.0004877961764577776 current acc: 0.8890196737851017\n",
      "iteration 47 current loss: 0.00023455903283320367 current acc: 0.8890383322125084\n",
      "iteration 48 current loss: 0.0002857313957065344 current acc: 0.8890569843671206\n",
      "iteration 49 current loss: 0.00016727369802538306 current acc: 0.8890756302521008\n",
      "iteration 50 current loss: 0.0003123597998637706 current acc: 0.88909426987061\n",
      "iteration 51 current loss: 0.0002632167306728661 current acc: 0.8891129032258065\n",
      "iteration 52 current loss: 0.0001782445760909468 current acc: 0.8891315303208467\n",
      "iteration 53 current loss: 0.000534714141394943 current acc: 0.8891501511588847\n",
      "iteration 54 current loss: 0.00031971686985343695 current acc: 0.889168765743073\n",
      "iteration 55 current loss: 0.0003624823584686965 current acc: 0.8891873740765615\n",
      "iteration 56 current loss: 0.0004622862034011632 current acc: 0.8892059761624979\n",
      "iteration 57 current loss: 0.0003653774911072105 current acc: 0.8892245720040282\n",
      "iteration 58 current loss: 0.00014802358055021614 current acc: 0.8892431616042961\n",
      "iteration 59 current loss: 0.00023457332281395793 current acc: 0.889261744966443\n",
      "iteration 60 current loss: 0.0003485332417767495 current acc: 0.8892803220936084\n",
      "iteration 61 current loss: 0.00037973877624608576 current acc: 0.8892988929889298\n",
      "iteration 62 current loss: 0.00027892159414477646 current acc: 0.8893174576555425\n",
      "iteration 63 current loss: 0.0002792453742586076 current acc: 0.8893360160965795\n",
      "iteration 64 current loss: 0.0005483929417096078 current acc: 0.8893545683151718\n",
      "iteration 65 current loss: 0.000704888254404068 current acc: 0.8893731143144485\n",
      "iteration 66 current loss: 0.00033615980646573007 current acc: 0.8893916540975364\n",
      "iteration 67 current loss: 0.00029437587363645434 current acc: 0.8894101876675603\n",
      "iteration 68 current loss: 0.00013827306975144893 current acc: 0.8894287150276429\n",
      "iteration 69 current loss: 0.00026551572955213487 current acc: 0.8894472361809045\n",
      "iteration 70 current loss: 0.0003287428698968142 current acc: 0.8894657511304639\n",
      "iteration 71 current loss: 0.00036540103610605 current acc: 0.8894842598794374\n",
      "iteration 72 current loss: 0.0002676908334251493 current acc: 0.8895027624309392\n",
      "iteration 73 current loss: 0.0002565572503954172 current acc: 0.8895212587880816\n",
      "iteration 74 current loss: 0.0004190037143416703 current acc: 0.8895397489539749\n",
      "iteration 75 current loss: 0.0002291593700647354 current acc: 0.8895582329317269\n",
      "iteration 76 current loss: 0.0003405309980735183 current acc: 0.8895767107244437\n",
      "iteration 77 current loss: 0.0002477155940141529 current acc: 0.8895951823352292\n",
      "iteration 78 current loss: 0.00038107813452370465 current acc: 0.8896136477671851\n",
      "iteration 79 current loss: 0.00032687201746739447 current acc: 0.8896321070234113\n",
      "iteration 80 current loss: 0.0002777960035018623 current acc: 0.8896505601070055\n",
      "iteration 81 current loss: 0.00036173767875880003 current acc: 0.8896690070210632\n",
      "iteration 82 current loss: 0.0002576062688603997 current acc: 0.8896874477686779\n",
      "iteration 83 current loss: 0.00034307996975257993 current acc: 0.8897058823529411\n",
      "iteration 84 current loss: 0.0003713811165653169 current acc: 0.8897243107769424\n",
      "iteration 85 current loss: 0.0003166778478771448 current acc: 0.8897427330437688\n",
      "iteration 86 current loss: 0.0004821405455004424 current acc: 0.8897611491565057\n",
      "iteration 87 current loss: 0.0003650225989986211 current acc: 0.8897795591182365\n",
      "iteration 88 current loss: 0.0006383308209478855 current acc: 0.889797962932042\n",
      "iteration 89 current loss: 0.0003311559557914734 current acc: 0.8898163606010017\n",
      "iteration 90 current loss: 0.0002514958323445171 current acc: 0.8898347521281923\n",
      "iteration 91 current loss: 0.00029818015173077583 current acc: 0.8898531375166889\n",
      "iteration 92 current loss: 0.0002583945170044899 current acc: 0.8898715167695644\n",
      "iteration 93 current loss: 0.000540130480658263 current acc: 0.8898898898898899\n",
      "iteration 94 current loss: 0.00025449960958212614 current acc: 0.8899082568807339\n",
      "iteration 95 current loss: 0.00034535073791630566 current acc: 0.8899266177451635\n",
      "iteration 96 current loss: 0.00040962157072499394 current acc: 0.8899449724862432\n",
      "iteration 97 current loss: 0.0003114243154413998 current acc: 0.8899633211070357\n",
      "iteration 98 current loss: 0.0004877040919382125 current acc: 0.8899816636106017\n",
      "iteration 99 current loss: 0.000281312910374254 current acc: 0.89\n",
      "\t\tEpoch 59/100 complete. Epoch loss 0.0003761659673182294 Epoch accuracy 1.0\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 59, Validation Accuracy: 0.634375, Validation Loss: 1.9150293320417404\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.0004091304144822061 current acc: 0.8900183302782869\n",
      "iteration 1 current loss: 0.00023167188919615 current acc: 0.8900366544485172\n",
      "iteration 2 current loss: 0.000237014886806719 current acc: 0.8900549725137431\n",
      "iteration 3 current loss: 0.00015881485887803137 current acc: 0.8900732844770153\n",
      "iteration 4 current loss: 0.0002274361759191379 current acc: 0.8900915903413822\n",
      "iteration 5 current loss: 0.00018183836073148996 current acc: 0.8901098901098901\n",
      "iteration 6 current loss: 0.0003374601947143674 current acc: 0.8901281837855834\n",
      "iteration 7 current loss: 0.0003469941148068756 current acc: 0.8901464713715047\n",
      "iteration 8 current loss: 0.0003107509110122919 current acc: 0.890164752870694\n",
      "iteration 9 current loss: 0.0002659756864886731 current acc: 0.8901830282861897\n",
      "iteration 10 current loss: 0.00014932299382053316 current acc: 0.8902012976210281\n",
      "iteration 11 current loss: 0.00012820337724406272 current acc: 0.8902195608782435\n",
      "iteration 12 current loss: 0.00030004724976606667 current acc: 0.8902378180608681\n",
      "iteration 13 current loss: 0.00019121821969747543 current acc: 0.8902560691719321\n",
      "iteration 14 current loss: 0.0001824605860747397 current acc: 0.8902743142144638\n",
      "iteration 15 current loss: 0.00024291382578667253 current acc: 0.8902925531914894\n",
      "iteration 16 current loss: 0.00015976716531440616 current acc: 0.8903107861060329\n",
      "iteration 17 current loss: 0.0002362219092901796 current acc: 0.8903290129611167\n",
      "iteration 18 current loss: 0.0003030084480997175 current acc: 0.8903472337597608\n",
      "iteration 19 current loss: 0.00032420025672763586 current acc: 0.8903654485049833\n",
      "iteration 20 current loss: 0.00025529120466671884 current acc: 0.8903836571998007\n",
      "iteration 21 current loss: 0.0003069898812100291 current acc: 0.8904018598472269\n",
      "iteration 22 current loss: 0.00027399620739743114 current acc: 0.890420056450274\n",
      "iteration 23 current loss: 0.0003324858844280243 current acc: 0.8904382470119522\n",
      "iteration 24 current loss: 0.00028541096253320575 current acc: 0.8904564315352697\n",
      "iteration 25 current loss: 0.00021746588754467666 current acc: 0.8904746100232327\n",
      "iteration 26 current loss: 0.0002482594863977283 current acc: 0.8904927824788452\n",
      "iteration 27 current loss: 0.00032295947312377393 current acc: 0.8905109489051095\n",
      "iteration 28 current loss: 0.00030005117878317833 current acc: 0.8905291093050257\n",
      "iteration 29 current loss: 0.00025829978403635323 current acc: 0.8905472636815921\n",
      "iteration 30 current loss: 0.00018386862939223647 current acc: 0.8905654120378047\n",
      "iteration 31 current loss: 0.0004166672006249428 current acc: 0.8905835543766578\n",
      "iteration 32 current loss: 0.00019015803991351277 current acc: 0.8906016907011437\n",
      "iteration 33 current loss: 0.00020704264170490205 current acc: 0.8906198210142525\n",
      "iteration 34 current loss: 0.0002097875694744289 current acc: 0.8906379453189727\n",
      "iteration 35 current loss: 0.00027201406192034483 current acc: 0.8906560636182903\n",
      "iteration 36 current loss: 0.0002231550170108676 current acc: 0.8906741759151897\n",
      "iteration 37 current loss: 0.00016999858780764043 current acc: 0.8906922822126532\n",
      "iteration 38 current loss: 0.00023197729024104774 current acc: 0.8907103825136612\n",
      "iteration 39 current loss: 0.00013660959666594863 current acc: 0.890728476821192\n",
      "iteration 40 current loss: 0.00021577249572146684 current acc: 0.8907465651382221\n",
      "iteration 41 current loss: 0.0003452948876656592 current acc: 0.8907646474677259\n",
      "iteration 42 current loss: 0.00021243866649456322 current acc: 0.8907827238126759\n",
      "iteration 43 current loss: 0.000342447601724416 current acc: 0.8908007941760424\n",
      "iteration 44 current loss: 0.00020602726726792753 current acc: 0.890818858560794\n",
      "iteration 45 current loss: 0.00041988608427345753 current acc: 0.8908369169698974\n",
      "iteration 46 current loss: 0.0008833642932586372 current acc: 0.8908549694063171\n",
      "iteration 47 current loss: 0.00028917036252096295 current acc: 0.8908730158730159\n",
      "iteration 48 current loss: 0.00016427869559265673 current acc: 0.8908910563729542\n",
      "iteration 49 current loss: 0.00026230927323922515 current acc: 0.8909090909090909\n",
      "iteration 50 current loss: 0.00012721290113404393 current acc: 0.8909271194843827\n",
      "iteration 51 current loss: 0.00021770151215605438 current acc: 0.8909451421017845\n",
      "iteration 52 current loss: 0.00019588394206948578 current acc: 0.8909631587642491\n",
      "iteration 53 current loss: 0.000155651883687824 current acc: 0.8909811694747275\n",
      "iteration 54 current loss: 0.00030828738817945123 current acc: 0.8909991742361685\n",
      "iteration 55 current loss: 0.0001334948028670624 current acc: 0.8910171730515192\n",
      "iteration 56 current loss: 0.00016581377713009715 current acc: 0.8910351659237247\n",
      "iteration 57 current loss: 0.0003001746954396367 current acc: 0.891053152855728\n",
      "iteration 58 current loss: 0.00019577553030103445 current acc: 0.8910711338504703\n",
      "iteration 59 current loss: 0.0003156060993205756 current acc: 0.8910891089108911\n",
      "iteration 60 current loss: 0.0001736096164677292 current acc: 0.8911070780399274\n",
      "iteration 61 current loss: 0.0002437489602016285 current acc: 0.8911250412405147\n",
      "iteration 62 current loss: 0.00034306597081013024 current acc: 0.8911429985155863\n",
      "iteration 63 current loss: 0.0003899688890669495 current acc: 0.8911609498680739\n",
      "iteration 64 current loss: 0.0003643193922471255 current acc: 0.8911788953009069\n",
      "iteration 65 current loss: 0.00031545330421067774 current acc: 0.8911968348170128\n",
      "iteration 66 current loss: 0.00016763083112891763 current acc: 0.8912147684193176\n",
      "iteration 67 current loss: 0.00045614983537234366 current acc: 0.8912326961107448\n",
      "iteration 68 current loss: 0.00017444488184992224 current acc: 0.8912506178942166\n",
      "iteration 69 current loss: 0.00036043807631358504 current acc: 0.8912685337726524\n",
      "iteration 70 current loss: 0.0005136947147548199 current acc: 0.8912864437489705\n",
      "iteration 71 current loss: 0.00021425020531751215 current acc: 0.8913043478260869\n",
      "iteration 72 current loss: 0.000208746045245789 current acc: 0.8913222460069159\n",
      "iteration 73 current loss: 0.0003009013889823109 current acc: 0.8913401382943694\n",
      "iteration 74 current loss: 0.00037503131898120046 current acc: 0.891358024691358\n",
      "iteration 75 current loss: 0.00024566304637119174 current acc: 0.89137590520079\n",
      "iteration 76 current loss: 0.0001873088622232899 current acc: 0.8913937798255718\n",
      "iteration 77 current loss: 0.0002018998347921297 current acc: 0.8914116485686081\n",
      "iteration 78 current loss: 0.00028338172705844045 current acc: 0.8914295114328015\n",
      "iteration 79 current loss: 0.00038382201455533504 current acc: 0.8914473684210527\n",
      "iteration 80 current loss: 0.0002475342189427465 current acc: 0.8914652195362605\n",
      "iteration 81 current loss: 0.0003375962842255831 current acc: 0.8914830647813219\n",
      "iteration 82 current loss: 0.00017058252706192434 current acc: 0.891500904159132\n",
      "iteration 83 current loss: 0.00025784465833567083 current acc: 0.8915187376725838\n",
      "iteration 84 current loss: 0.00025654686032794416 current acc: 0.8915365653245686\n",
      "iteration 85 current loss: 0.00015550498210359365 current acc: 0.8915543871179756\n",
      "iteration 86 current loss: 0.0004097928758710623 current acc: 0.8915722030556925\n",
      "iteration 87 current loss: 0.0001875473972177133 current acc: 0.8915900131406045\n",
      "iteration 88 current loss: 0.00033183066989295185 current acc: 0.8916078173755954\n",
      "iteration 89 current loss: 0.00020021555246785283 current acc: 0.8916256157635468\n",
      "iteration 90 current loss: 0.00021712422312702984 current acc: 0.8916434083073387\n",
      "iteration 91 current loss: 0.0003194694872945547 current acc: 0.891661195009849\n",
      "iteration 92 current loss: 0.0001911211002152413 current acc: 0.8916789758739537\n",
      "iteration 93 current loss: 0.0003536618605721742 current acc: 0.8916967509025271\n",
      "iteration 94 current loss: 0.0002405296836514026 current acc: 0.8917145200984413\n",
      "iteration 95 current loss: 0.00027968932408839464 current acc: 0.8917322834645669\n",
      "iteration 96 current loss: 0.00028522268985398114 current acc: 0.8917500410037723\n",
      "iteration 97 current loss: 0.0003819000266958028 current acc: 0.8917677927189243\n",
      "iteration 98 current loss: 0.000314466655254364 current acc: 0.8917855386128873\n",
      "iteration 99 current loss: 0.0004037384351249784 current acc: 0.8918032786885246\n",
      "\t\tEpoch 60/100 complete. Epoch loss 0.0002686898069805466 Epoch accuracy 1.0\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 60, Validation Accuracy: 0.6345, Validation Loss: 1.9249828796833754\n",
      "best loss 0.0002686898069805466\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.00018050905782729387 current acc: 0.891821012948697\n",
      "iteration 1 current loss: 0.00015770617756061256 current acc: 0.8918387413962635\n",
      "iteration 2 current loss: 0.000230806675972417 current acc: 0.8918564640340816\n",
      "iteration 3 current loss: 0.0001430313423043117 current acc: 0.8918741808650066\n",
      "iteration 4 current loss: 0.0002915330114774406 current acc: 0.8918918918918919\n",
      "iteration 5 current loss: 0.000192390798474662 current acc: 0.8919095971175892\n",
      "iteration 6 current loss: 0.00018828058091457933 current acc: 0.8919272965449484\n",
      "iteration 7 current loss: 0.00025685413856990635 current acc: 0.8919449901768173\n",
      "iteration 8 current loss: 0.00015034993703011423 current acc: 0.8919626780160419\n",
      "iteration 9 current loss: 0.00022529622947331518 current acc: 0.8919803600654664\n",
      "iteration 10 current loss: 0.00035044667311012745 current acc: 0.8919980363279333\n",
      "iteration 11 current loss: 0.00020832050358876586 current acc: 0.8920157068062827\n",
      "iteration 12 current loss: 0.0002507923054508865 current acc: 0.8920333715033535\n",
      "iteration 13 current loss: 0.0002483316056896001 current acc: 0.8920510304219823\n",
      "iteration 14 current loss: 0.0003662882372736931 current acc: 0.8920686835650041\n",
      "iteration 15 current loss: 0.0001822585763875395 current acc: 0.8920863309352518\n",
      "iteration 16 current loss: 0.00022941670613363385 current acc: 0.8921039725355566\n",
      "iteration 17 current loss: 0.00018862033903133124 current acc: 0.892121608368748\n",
      "iteration 18 current loss: 0.0002547577314544469 current acc: 0.8921392384376532\n",
      "iteration 19 current loss: 0.00022972650185693055 current acc: 0.8921568627450981\n",
      "iteration 20 current loss: 0.00038358912570402026 current acc: 0.8921744812939062\n",
      "iteration 21 current loss: 0.0002371143054915592 current acc: 0.8921920940868997\n",
      "iteration 22 current loss: 0.0002518506080377847 current acc: 0.8922097011268986\n",
      "iteration 23 current loss: 0.00019294863159302622 current acc: 0.8922273024167211\n",
      "iteration 24 current loss: 0.0002002171822823584 current acc: 0.8922448979591837\n",
      "iteration 25 current loss: 0.00015377640374936163 current acc: 0.8922624877571009\n",
      "iteration 26 current loss: 0.0005213416879996657 current acc: 0.8922800718132855\n",
      "iteration 27 current loss: 0.0001607608428457752 current acc: 0.8922976501305483\n",
      "iteration 28 current loss: 0.00012848385085817426 current acc: 0.8923152227116985\n",
      "iteration 29 current loss: 0.00022096640896052122 current acc: 0.8923327895595432\n",
      "iteration 30 current loss: 0.0001259227137779817 current acc: 0.892350350676888\n",
      "iteration 31 current loss: 0.00015122618060559034 current acc: 0.8923679060665362\n",
      "iteration 32 current loss: 0.00029379193438217044 current acc: 0.8923854557312897\n",
      "iteration 33 current loss: 0.0002558448468334973 current acc: 0.8924029996739485\n",
      "iteration 34 current loss: 0.00020817472250200808 current acc: 0.8924205378973105\n",
      "iteration 35 current loss: 0.00022916872694622725 current acc: 0.8924380704041721\n",
      "iteration 36 current loss: 0.0002078353863907978 current acc: 0.8924555971973277\n",
      "iteration 37 current loss: 0.0001645329175516963 current acc: 0.8924731182795699\n",
      "iteration 38 current loss: 0.0002244885399704799 current acc: 0.8924906336536895\n",
      "iteration 39 current loss: 0.0002853195182979107 current acc: 0.8925081433224755\n",
      "iteration 40 current loss: 0.0002121811849065125 current acc: 0.8925256472887152\n",
      "iteration 41 current loss: 0.0001871807180577889 current acc: 0.8925431455551938\n",
      "iteration 42 current loss: 0.0003316491493023932 current acc: 0.8925606381246948\n",
      "iteration 43 current loss: 0.00021239776106085628 current acc: 0.892578125\n",
      "iteration 44 current loss: 0.0003413730883039534 current acc: 0.8925956061838893\n",
      "iteration 45 current loss: 0.00016547349514439702 current acc: 0.8926130816791409\n",
      "iteration 46 current loss: 0.00023510129540227354 current acc: 0.892630551488531\n",
      "iteration 47 current loss: 0.00013290080823935568 current acc: 0.8926480156148341\n",
      "iteration 48 current loss: 0.00040434664697386324 current acc: 0.8926654740608229\n",
      "iteration 49 current loss: 0.0003088609082624316 current acc: 0.8926829268292683\n",
      "iteration 50 current loss: 0.0004267619224265218 current acc: 0.8927003739229393\n",
      "iteration 51 current loss: 0.0002232501283288002 current acc: 0.8927178153446034\n",
      "iteration 52 current loss: 0.00024655135348439217 current acc: 0.8927352510970259\n",
      "iteration 53 current loss: 0.0002786436234600842 current acc: 0.8927526811829705\n",
      "iteration 54 current loss: 0.00017743239004630595 current acc: 0.892770105605199\n",
      "iteration 55 current loss: 0.00029414595337584615 current acc: 0.8927875243664717\n",
      "iteration 56 current loss: 0.00016941477952059358 current acc: 0.8928049374695468\n",
      "iteration 57 current loss: 0.0002870991884265095 current acc: 0.8928223449171809\n",
      "iteration 58 current loss: 0.0008564016316086054 current acc: 0.8928397467121286\n",
      "iteration 59 current loss: 0.00015838957915548235 current acc: 0.8928571428571429\n",
      "iteration 60 current loss: 0.00021751968597527593 current acc: 0.8928745333549748\n",
      "iteration 61 current loss: 0.00016814582340884954 current acc: 0.8928919182083739\n",
      "iteration 62 current loss: 0.0008813869790174067 current acc: 0.8929092974200876\n",
      "iteration 63 current loss: 0.0002831080637406558 current acc: 0.8929266709928618\n",
      "iteration 64 current loss: 0.00021636633027810603 current acc: 0.8929440389294404\n",
      "iteration 65 current loss: 0.00013509619748219848 current acc: 0.8929614012325657\n",
      "iteration 66 current loss: 0.00022402111790142953 current acc: 0.8929787579049782\n",
      "iteration 67 current loss: 0.0008752502617426217 current acc: 0.8929961089494164\n",
      "iteration 68 current loss: 0.00022946670651435852 current acc: 0.8930134543686172\n",
      "iteration 69 current loss: 0.00038189769838936627 current acc: 0.893030794165316\n",
      "iteration 70 current loss: 0.0002350691647734493 current acc: 0.893048128342246\n",
      "iteration 71 current loss: 0.0002060569095192477 current acc: 0.8930654569021387\n",
      "iteration 72 current loss: 0.0002337057958357036 current acc: 0.893082779847724\n",
      "iteration 73 current loss: 0.00022600460215471685 current acc: 0.8931000971817298\n",
      "iteration 74 current loss: 0.0003395408857613802 current acc: 0.8931174089068826\n",
      "iteration 75 current loss: 0.0005050521576777101 current acc: 0.8931347150259067\n",
      "iteration 76 current loss: 0.00020997734100092202 current acc: 0.893152015541525\n",
      "iteration 77 current loss: 0.0002317490434506908 current acc: 0.8931693104564584\n",
      "iteration 78 current loss: 0.0002850511227734387 current acc: 0.8931865997734261\n",
      "iteration 79 current loss: 0.0014162405859678984 current acc: 0.8932038834951457\n",
      "iteration 80 current loss: 0.00034716472146101296 current acc: 0.8932211616243326\n",
      "iteration 81 current loss: 0.0003705426352098584 current acc: 0.8932384341637011\n",
      "iteration 82 current loss: 0.0006950654205866158 current acc: 0.8932557011159631\n",
      "iteration 83 current loss: 0.00038365155342034996 current acc: 0.8932729624838293\n",
      "iteration 84 current loss: 0.0006966055952943861 current acc: 0.8932902182700081\n",
      "iteration 85 current loss: 0.0002327330003026873 current acc: 0.8933074684772065\n",
      "iteration 86 current loss: 0.0001452249416615814 current acc: 0.89332471310813\n",
      "iteration 87 current loss: 0.00018903924501501024 current acc: 0.8933419521654816\n",
      "iteration 88 current loss: 0.0005299050244502723 current acc: 0.8933591856519632\n",
      "iteration 89 current loss: 0.000663051672745496 current acc: 0.8933764135702746\n",
      "iteration 90 current loss: 0.00020891249005217105 current acc: 0.8933936359231142\n",
      "iteration 91 current loss: 0.0003472039825282991 current acc: 0.8934108527131783\n",
      "iteration 92 current loss: 0.00025675527285784483 current acc: 0.8934280639431617\n",
      "iteration 93 current loss: 0.0006202713120728731 current acc: 0.8934452696157572\n",
      "iteration 94 current loss: 0.00031762986327521503 current acc: 0.8934624697336562\n",
      "iteration 95 current loss: 0.0003805062151513994 current acc: 0.8934796642995481\n",
      "iteration 96 current loss: 0.0005553346127271652 current acc: 0.8934968533161207\n",
      "iteration 97 current loss: 0.0004970133886672556 current acc: 0.89351403678606\n",
      "iteration 98 current loss: 0.00028185461997054517 current acc: 0.8935312147120503\n",
      "iteration 99 current loss: 0.0004049885319545865 current acc: 0.8935483870967742\n",
      "\t\tEpoch 61/100 complete. Epoch loss 0.00030396787842619234 Epoch accuracy 1.0\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 61, Validation Accuracy: 0.633625, Validation Loss: 1.9514518894255162\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.0004491645086091012 current acc: 0.8935655539429125\n",
      "iteration 1 current loss: 0.00019361829617992043 current acc: 0.8935827152531441\n",
      "iteration 2 current loss: 0.00041931073064915836 current acc: 0.8935998710301467\n",
      "iteration 3 current loss: 0.00038126696017570794 current acc: 0.8936170212765957\n",
      "iteration 4 current loss: 0.00015473976964130998 current acc: 0.8936341659951652\n",
      "iteration 5 current loss: 0.00018935555999632925 current acc: 0.8936513051885272\n",
      "iteration 6 current loss: 0.0007296936237253249 current acc: 0.8936684388593523\n",
      "iteration 7 current loss: 0.00029450299916788936 current acc: 0.8936855670103093\n",
      "iteration 8 current loss: 0.00018252356676384807 current acc: 0.8937026896440651\n",
      "iteration 9 current loss: 0.0002022792905336246 current acc: 0.893719806763285\n",
      "iteration 10 current loss: 0.00032424877281300724 current acc: 0.8937369183706327\n",
      "iteration 11 current loss: 0.00021361040126066655 current acc: 0.8937540244687702\n",
      "iteration 12 current loss: 0.0002608583017718047 current acc: 0.8937711250603573\n",
      "iteration 13 current loss: 0.0002782101510092616 current acc: 0.8937882201480528\n",
      "iteration 14 current loss: 0.0003774169890675694 current acc: 0.8938053097345132\n",
      "iteration 15 current loss: 0.00024386661243624985 current acc: 0.8938223938223938\n",
      "iteration 16 current loss: 0.00018107425421476364 current acc: 0.8938394724143478\n",
      "iteration 17 current loss: 0.0002351606235606596 current acc: 0.8938565455130267\n",
      "iteration 18 current loss: 0.00029356934828683734 current acc: 0.8938736131210806\n",
      "iteration 19 current loss: 0.0002051234623650089 current acc: 0.8938906752411575\n",
      "iteration 20 current loss: 0.0002930248447228223 current acc: 0.8939077318759042\n",
      "iteration 21 current loss: 0.00041289039654657245 current acc: 0.8939247830279653\n",
      "iteration 22 current loss: 0.00022615886700805277 current acc: 0.893941828699984\n",
      "iteration 23 current loss: 0.0005972878425382078 current acc: 0.8939588688946015\n",
      "iteration 24 current loss: 0.0014345693634822965 current acc: 0.8939759036144578\n",
      "iteration 25 current loss: 0.0002555866085458547 current acc: 0.8939929328621908\n",
      "iteration 26 current loss: 0.00029768599779345095 current acc: 0.8940099566404368\n",
      "iteration 27 current loss: 0.00017732706328388304 current acc: 0.8940269749518305\n",
      "iteration 28 current loss: 0.0002868877782020718 current acc: 0.8940439877990046\n",
      "iteration 29 current loss: 0.00024277839111164212 current acc: 0.8940609951845907\n",
      "iteration 30 current loss: 0.0001531607413198799 current acc: 0.894077997111218\n",
      "iteration 31 current loss: 0.0002605920017231256 current acc: 0.8940949935815148\n",
      "iteration 32 current loss: 0.00028425658820196986 current acc: 0.8941119845981068\n",
      "iteration 33 current loss: 0.025520481169223785 current acc: 0.8941257619505936\n",
      "iteration 34 current loss: 0.0010954983299598098 current acc: 0.8941427425821973\n",
      "iteration 35 current loss: 0.004589811433106661 current acc: 0.8941597177677999\n",
      "iteration 36 current loss: 0.1460617035627365 current acc: 0.8941734808401475\n",
      "iteration 37 current loss: 0.2799185514450073 current acc: 0.8941840333440205\n",
      "iteration 38 current loss: 0.4187243580818176 current acc: 0.8941913768232088\n",
      "iteration 39 current loss: 1.1085190773010254 current acc: 0.8941698717948718\n",
      "iteration 40 current loss: 0.6529815793037415 current acc: 0.894161192116648\n",
      "iteration 41 current loss: 0.5243525505065918 current acc: 0.8941493111182314\n",
      "iteration 42 current loss: 0.9401599764823914 current acc: 0.8941278231619414\n",
      "iteration 43 current loss: 1.5925484895706177 current acc: 0.894096732863549\n",
      "iteration 44 current loss: 2.377911329269409 current acc: 0.8940560448358686\n",
      "iteration 45 current loss: 1.122420072555542 current acc: 0.8940217739353186\n",
      "iteration 46 current loss: 1.1434946060180664 current acc: 0.8939939170801985\n",
      "iteration 47 current loss: 1.809251070022583 current acc: 0.8939564660691421\n",
      "iteration 48 current loss: 1.4346426725387573 current acc: 0.8939158265322451\n",
      "iteration 49 current loss: 2.716021776199341 current acc: 0.8938528\n",
      "iteration 50 current loss: 1.5041981935501099 current acc: 0.8938185890257558\n",
      "iteration 51 current loss: 0.33680489659309387 current acc: 0.893809980806142\n",
      "iteration 52 current loss: 1.2451575994491577 current acc: 0.8937885814808891\n",
      "iteration 53 current loss: 1.0880366563796997 current acc: 0.8937639910457307\n",
      "iteration 54 current loss: 2.0352606773376465 current acc: 0.8937042366107114\n",
      "iteration 55 current loss: 1.6003035306930542 current acc: 0.8936572890025576\n",
      "iteration 56 current loss: 1.5765795707702637 current acc: 0.8936199456608598\n",
      "iteration 57 current loss: 1.961092233657837 current acc: 0.8935666347075744\n",
      "iteration 58 current loss: 0.9645413160324097 current acc: 0.8935357085796453\n",
      "iteration 59 current loss: 1.5776546001434326 current acc: 0.8934952076677316\n",
      "iteration 60 current loss: 1.3644752502441406 current acc: 0.8934547196933397\n",
      "iteration 61 current loss: 0.8757409453392029 current acc: 0.8934302139891408\n",
      "iteration 62 current loss: 1.5991517305374146 current acc: 0.8933961360370429\n",
      "iteration 63 current loss: 1.4033174514770508 current acc: 0.8933524904214559\n",
      "iteration 64 current loss: 1.3219975233078003 current acc: 0.8933216280925779\n",
      "iteration 65 current loss: 1.1583746671676636 current acc: 0.893290775614427\n",
      "iteration 66 current loss: 0.5687277317047119 current acc: 0.8932822722195628\n",
      "iteration 67 current loss: 0.6769095063209534 current acc: 0.8932578174856414\n",
      "iteration 68 current loss: 0.7416626214981079 current acc: 0.8932301802520338\n",
      "iteration 69 current loss: 0.6820841431617737 current acc: 0.8932057416267942\n",
      "iteration 70 current loss: 1.4772032499313354 current acc: 0.8931557965236804\n",
      "iteration 71 current loss: 1.2268726825714111 current acc: 0.893125\n",
      "iteration 72 current loss: 1.0451180934906006 current acc: 0.8930814602263669\n",
      "iteration 73 current loss: 1.0732425451278687 current acc: 0.8930602486452024\n",
      "iteration 74 current loss: 0.4830726981163025 current acc: 0.8930454183266933\n",
      "iteration 75 current loss: 0.986204981803894 current acc: 0.8930146590184831\n",
      "iteration 76 current loss: 0.9187537431716919 current acc: 0.8929902819818385\n",
      "iteration 77 current loss: 1.510609745979309 current acc: 0.8929436126154826\n",
      "iteration 78 current loss: 0.47160661220550537 current acc: 0.8929288103201146\n",
      "iteration 79 current loss: 0.4972672760486603 current acc: 0.8929140127388535\n",
      "iteration 80 current loss: 1.0339542627334595 current acc: 0.8928737462187549\n",
      "iteration 81 current loss: 0.7075579166412354 current acc: 0.8928589621139764\n",
      "iteration 82 current loss: 0.8589161038398743 current acc: 0.8928187171733248\n",
      "iteration 83 current loss: 0.8087660074234009 current acc: 0.8927975811584977\n",
      "iteration 84 current loss: 0.8445451855659485 current acc: 0.8927732696897375\n",
      "iteration 85 current loss: 0.5765004754066467 current acc: 0.8927553293032134\n",
      "iteration 86 current loss: 0.7043046355247498 current acc: 0.8927342134563385\n",
      "iteration 87 current loss: 0.5448186993598938 current acc: 0.892719465648855\n",
      "iteration 88 current loss: 0.652707040309906 current acc: 0.8926888217522658\n",
      "iteration 89 current loss: 0.5422020554542542 current acc: 0.8926740858505564\n",
      "iteration 90 current loss: 0.4309200346469879 current acc: 0.8926625337784136\n",
      "iteration 91 current loss: 0.3545200228691101 current acc: 0.8926605212968849\n",
      "iteration 92 current loss: 0.40664082765579224 current acc: 0.8926521531860798\n",
      "iteration 93 current loss: 0.5894671082496643 current acc: 0.8926374324753734\n",
      "iteration 94 current loss: 0.35413873195648193 current acc: 0.8926354249404289\n",
      "iteration 95 current loss: 0.437982439994812 current acc: 0.8926207115628971\n",
      "iteration 96 current loss: 0.6459432244300842 current acc: 0.8926028267428935\n",
      "iteration 97 current loss: 0.7773560285568237 current acc: 0.8925785963798031\n",
      "iteration 98 current loss: 0.7159356474876404 current acc: 0.8925638990315923\n",
      "iteration 99 current loss: 0.4793834984302521 current acc: 0.8925555555555555\n",
      "\t\tEpoch 62/100 complete. Epoch loss 0.6274752782484575 Epoch accuracy 0.831\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 62, Validation Accuracy: 0.53, Validation Loss: 1.9317345663905143\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.19901518523693085 current acc: 0.8925662593239169\n",
      "iteration 1 current loss: 0.32122233510017395 current acc: 0.892567438908283\n",
      "iteration 2 current loss: 0.4156026542186737 current acc: 0.8925622719339997\n",
      "iteration 3 current loss: 0.47603169083595276 current acc: 0.8925475888324873\n",
      "iteration 4 current loss: 0.5244017839431763 current acc: 0.8925360824742268\n",
      "iteration 5 current loss: 0.4741886556148529 current acc: 0.8925182366000635\n",
      "iteration 6 current loss: 0.4813153147697449 current acc: 0.8925067385444744\n",
      "iteration 7 current loss: 0.3707717955112457 current acc: 0.8925015852885225\n",
      "iteration 8 current loss: 0.516145646572113 current acc: 0.8924932635916945\n",
      "iteration 9 current loss: 0.3416145443916321 current acc: 0.8924944532488114\n",
      "iteration 10 current loss: 0.41855528950691223 current acc: 0.8924893043891617\n",
      "iteration 11 current loss: 0.15202228724956512 current acc: 0.8924968314321926\n",
      "iteration 12 current loss: 0.2668589651584625 current acc: 0.8924980199588152\n",
      "iteration 13 current loss: 0.2966707944869995 current acc: 0.8925055432372505\n",
      "iteration 14 current loss: 0.28149303793907166 current acc: 0.8925067300079177\n",
      "iteration 15 current loss: 0.4140895903110504 current acc: 0.8924984167194426\n",
      "iteration 16 current loss: 0.38759109377861023 current acc: 0.8924964381826817\n",
      "iteration 17 current loss: 0.138835147023201 current acc: 0.8925039569484013\n",
      "iteration 18 current loss: 0.24971534311771393 current acc: 0.8925083082766261\n",
      "iteration 19 current loss: 0.36882495880126953 current acc: 0.8925094936708861\n",
      "iteration 20 current loss: 0.347854346036911 current acc: 0.8925043505774403\n",
      "iteration 21 current loss: 0.2182636708021164 current acc: 0.8925086997785511\n",
      "iteration 22 current loss: 0.25916415452957153 current acc: 0.892516210659497\n",
      "iteration 23 current loss: 0.3879338800907135 current acc: 0.892517394054396\n",
      "iteration 24 current loss: 0.27140340209007263 current acc: 0.8925249011857708\n",
      "iteration 25 current loss: 0.245692178606987 current acc: 0.892529244388239\n",
      "iteration 26 current loss: 0.15157188475131989 current acc: 0.8925335862177968\n",
      "iteration 27 current loss: 0.22990670800209045 current acc: 0.8925410872313527\n",
      "iteration 28 current loss: 0.15096907317638397 current acc: 0.8925549059883078\n",
      "iteration 29 current loss: 0.2994498014450073 current acc: 0.8925497630331753\n",
      "iteration 30 current loss: 0.16724012792110443 current acc: 0.8925604169957353\n",
      "iteration 31 current loss: 0.2914429306983948 current acc: 0.8925615919140871\n",
      "iteration 32 current loss: 0.207382470369339 current acc: 0.8925722406442445\n",
      "iteration 33 current loss: 0.21733136475086212 current acc: 0.8925702557625513\n",
      "iteration 34 current loss: 0.19582071900367737 current acc: 0.8925714285714286\n",
      "iteration 35 current loss: 0.21685791015625 current acc: 0.8925789141414141\n",
      "iteration 36 current loss: 0.21895794570446014 current acc: 0.8925769291462837\n",
      "iteration 37 current loss: 0.20273596048355103 current acc: 0.892581255916693\n",
      "iteration 38 current loss: 0.31345081329345703 current acc: 0.8925792711784193\n",
      "iteration 39 current loss: 0.17601387202739716 current acc: 0.892589905362776\n",
      "iteration 40 current loss: 0.1284167766571045 current acc: 0.8926036902696736\n",
      "iteration 41 current loss: 0.2553848326206207 current acc: 0.8926017029328288\n",
      "iteration 42 current loss: 0.21392516791820526 current acc: 0.892609175469021\n",
      "iteration 43 current loss: 0.19357620179653168 current acc: 0.8926166456494325\n",
      "iteration 44 current loss: 0.23816652595996857 current acc: 0.8926241134751773\n",
      "iteration 45 current loss: 0.1683230847120285 current acc: 0.8926347305389222\n",
      "iteration 46 current loss: 0.19430404901504517 current acc: 0.8926327398771073\n",
      "iteration 47 current loss: 0.22073045372962952 current acc: 0.8926370510396976\n",
      "iteration 48 current loss: 0.1256934106349945 current acc: 0.8926476610489841\n",
      "iteration 49 current loss: 0.1661156415939331 current acc: 0.8926582677165354\n",
      "iteration 50 current loss: 0.16441930830478668 current acc: 0.8926657219335538\n",
      "iteration 51 current loss: 0.3294229507446289 current acc: 0.8926700251889169\n",
      "iteration 52 current loss: 0.18087123334407806 current acc: 0.8926774752085629\n",
      "iteration 53 current loss: 0.07123245298862457 current acc: 0.8926943657538559\n",
      "iteration 54 current loss: 0.1483292281627655 current acc: 0.8927018095987411\n",
      "iteration 55 current loss: 0.12671373784542084 current acc: 0.8927092511013216\n",
      "iteration 56 current loss: 0.1772485226392746 current acc: 0.8927166902627025\n",
      "iteration 57 current loss: 0.2933276295661926 current acc: 0.8927146901541365\n",
      "iteration 58 current loss: 0.20984458923339844 current acc: 0.8927189809718509\n",
      "iteration 59 current loss: 0.1575082540512085 current acc: 0.8927264150943396\n",
      "iteration 60 current loss: 0.15910491347312927 current acc: 0.8927307027196981\n",
      "iteration 61 current loss: 0.20870833098888397 current acc: 0.8927381326626846\n",
      "iteration 62 current loss: 0.10905349999666214 current acc: 0.8927487034417727\n",
      "iteration 63 current loss: 0.23012425005435944 current acc: 0.8927529855436832\n",
      "iteration 64 current loss: 0.09368816018104553 current acc: 0.8927666928515318\n",
      "iteration 65 current loss: 0.18646730482578278 current acc: 0.8927772541627396\n",
      "iteration 66 current loss: 0.259468674659729 current acc: 0.8927815297628396\n",
      "iteration 67 current loss: 0.07987882196903229 current acc: 0.8927952261306533\n",
      "iteration 68 current loss: 0.2647496461868286 current acc: 0.8927994975663369\n",
      "iteration 69 current loss: 0.18176551163196564 current acc: 0.8928100470957614\n",
      "iteration 70 current loss: 0.15353499352931976 current acc: 0.8928205933134515\n",
      "iteration 71 current loss: 0.09722177684307098 current acc: 0.8928311362209668\n",
      "iteration 72 current loss: 0.1469511091709137 current acc: 0.8928416758198651\n",
      "iteration 73 current loss: 0.16874681413173676 current acc: 0.8928490743646063\n",
      "iteration 74 current loss: 0.19275213778018951 current acc: 0.8928501960784314\n",
      "iteration 75 current loss: 0.1445516049861908 current acc: 0.8928607277289837\n",
      "iteration 76 current loss: 0.24091944098472595 current acc: 0.8928618472636036\n",
      "iteration 77 current loss: 0.23343372344970703 current acc: 0.8928661022264033\n",
      "iteration 78 current loss: 0.17026834189891815 current acc: 0.8928734911428123\n",
      "iteration 79 current loss: 0.0869370549917221 current acc: 0.8928902821316614\n",
      "iteration 80 current loss: 0.14704100787639618 current acc: 0.8929007992477668\n",
      "iteration 81 current loss: 0.2056536078453064 current acc: 0.8929081792541523\n",
      "iteration 82 current loss: 0.16937507688999176 current acc: 0.8929186902710324\n",
      "iteration 83 current loss: 0.08238391578197479 current acc: 0.8929323308270677\n",
      "iteration 84 current loss: 0.2563638687133789 current acc: 0.8929365700861394\n",
      "iteration 85 current loss: 0.18551254272460938 current acc: 0.8929408080175384\n",
      "iteration 86 current loss: 0.10918954759836197 current acc: 0.8929544387036167\n",
      "iteration 87 current loss: 0.12413790822029114 current acc: 0.892964934251722\n",
      "iteration 88 current loss: 0.05802859365940094 current acc: 0.8929816872750039\n",
      "iteration 89 current loss: 0.22826126217842102 current acc: 0.8929890453834116\n",
      "iteration 90 current loss: 0.14602845907211304 current acc: 0.8929964011891722\n",
      "iteration 91 current loss: 0.1476735770702362 current acc: 0.8930100125156446\n",
      "iteration 92 current loss: 0.09232789278030396 current acc: 0.89302361958392\n",
      "iteration 93 current loss: 0.09768214821815491 current acc: 0.8930340944635596\n",
      "iteration 94 current loss: 0.2092495858669281 current acc: 0.89304456606724\n",
      "iteration 95 current loss: 0.10023647546768188 current acc: 0.8930581613508443\n",
      "iteration 96 current loss: 0.10926257073879242 current acc: 0.89307175238393\n",
      "iteration 97 current loss: 0.1606728881597519 current acc: 0.8930853391684902\n",
      "iteration 98 current loss: 0.10756754875183105 current acc: 0.893095796218159\n",
      "iteration 99 current loss: 0.08888892084360123 current acc: 0.893109375\n",
      "\t\tEpoch 63/100 complete. Epoch loss 0.218618268892169 Epoch accuracy 0.928\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 63, Validation Accuracy: 0.5965, Validation Loss: 1.5875275444239378\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.0925549864768982 current acc: 0.8931198250273394\n",
      "iteration 1 current loss: 0.04594116285443306 current acc: 0.8931365198375507\n",
      "iteration 2 current loss: 0.042036671191453934 current acc: 0.8931532094330783\n",
      "iteration 3 current loss: 0.06857772171497345 current acc: 0.8931667707682698\n",
      "iteration 4 current loss: 0.06995730102062225 current acc: 0.8931834504293521\n",
      "iteration 5 current loss: 0.027502212673425674 current acc: 0.8932001248829222\n",
      "iteration 6 current loss: 0.0514531284570694 current acc: 0.8932167941314187\n",
      "iteration 7 current loss: 0.09302899986505508 current acc: 0.8932303370786517\n",
      "iteration 8 current loss: 0.05113816261291504 current acc: 0.8932469964112966\n",
      "iteration 9 current loss: 0.07772498577833176 current acc: 0.8932574102964118\n",
      "iteration 10 current loss: 0.04898092895746231 current acc: 0.8932709405708937\n",
      "iteration 11 current loss: 0.07358238101005554 current acc: 0.893284466625078\n",
      "iteration 12 current loss: 0.08621039241552353 current acc: 0.89330110712615\n",
      "iteration 13 current loss: 0.05132989585399628 current acc: 0.893317742438416\n",
      "iteration 14 current loss: 0.03710707277059555 current acc: 0.8933343725643024\n",
      "iteration 15 current loss: 0.057233963161706924 current acc: 0.8933478802992518\n",
      "iteration 16 current loss: 0.08449168503284454 current acc: 0.8933613838242169\n",
      "iteration 17 current loss: 0.08330681174993515 current acc: 0.8933748831411654\n",
      "iteration 18 current loss: 0.0378335602581501 current acc: 0.893391494002181\n",
      "iteration 19 current loss: 0.044612105935811996 current acc: 0.8934080996884736\n",
      "iteration 20 current loss: 0.038727983832359314 current acc: 0.8934247002024607\n",
      "iteration 21 current loss: 0.09596890211105347 current acc: 0.8934350669573342\n",
      "iteration 22 current loss: 0.03263438865542412 current acc: 0.8934516581036899\n",
      "iteration 23 current loss: 0.02332261949777603 current acc: 0.8934682440846824\n",
      "iteration 24 current loss: 0.04970068857073784 current acc: 0.8934848249027237\n",
      "iteration 25 current loss: 0.08566373586654663 current acc: 0.8934982882041705\n",
      "iteration 26 current loss: 0.026622848585247993 current acc: 0.8935148591878015\n",
      "iteration 27 current loss: 0.03796066716313362 current acc: 0.8935314250155569\n",
      "iteration 28 current loss: 0.03414110094308853 current acc: 0.893547985689843\n",
      "iteration 29 current loss: 0.043056849390268326 current acc: 0.8935645412130637\n",
      "iteration 30 current loss: 0.09215085953474045 current acc: 0.8935748717151298\n",
      "iteration 31 current loss: 0.04468369483947754 current acc: 0.8935914179104477\n",
      "iteration 32 current loss: 0.029587041586637497 current acc: 0.8936079589616043\n",
      "iteration 33 current loss: 0.034757327288389206 current acc: 0.8936244948709978\n",
      "iteration 34 current loss: 0.06001626327633858 current acc: 0.8936379176379177\n",
      "iteration 35 current loss: 0.07371670007705688 current acc: 0.8936482287134866\n",
      "iteration 36 current loss: 0.07637204229831696 current acc: 0.8936585365853659\n",
      "iteration 37 current loss: 0.048463936895132065 current acc: 0.8936750543647095\n",
      "iteration 38 current loss: 0.05332105606794357 current acc: 0.8936915670135114\n",
      "iteration 39 current loss: 0.059711869806051254 current acc: 0.8937049689440993\n",
      "iteration 40 current loss: 0.05197121948003769 current acc: 0.8937214718211458\n",
      "iteration 41 current loss: 0.06461690366268158 current acc: 0.8937379695746662\n",
      "iteration 42 current loss: 0.020884692668914795 current acc: 0.8937544622070464\n",
      "iteration 43 current loss: 0.046012140810489655 current acc: 0.8937709497206704\n",
      "iteration 44 current loss: 0.0313490554690361 current acc: 0.8937874321179209\n",
      "iteration 45 current loss: 0.028434274718165398 current acc: 0.893803909401179\n",
      "iteration 46 current loss: 0.02527664229273796 current acc: 0.8938203815728246\n",
      "iteration 47 current loss: 0.07324785739183426 current acc: 0.8938306451612903\n",
      "iteration 48 current loss: 0.018633194267749786 current acc: 0.8938471080787719\n",
      "iteration 49 current loss: 0.04810357093811035 current acc: 0.8938635658914729\n",
      "iteration 50 current loss: 0.05440117418766022 current acc: 0.8938769183072391\n",
      "iteration 51 current loss: 0.06494415551424026 current acc: 0.8938902665840049\n",
      "iteration 52 current loss: 0.05137796327471733 current acc: 0.8939036107236944\n",
      "iteration 53 current loss: 0.04125896468758583 current acc: 0.8939200495816548\n",
      "iteration 54 current loss: 0.0602201409637928 current acc: 0.8939364833462432\n",
      "iteration 55 current loss: 0.05003993213176727 current acc: 0.8939498141263941\n",
      "iteration 56 current loss: 0.04353000596165657 current acc: 0.8939662381911104\n",
      "iteration 57 current loss: 0.027973080053925514 current acc: 0.8939826571694023\n",
      "iteration 58 current loss: 0.07189299911260605 current acc: 0.8939959746090727\n",
      "iteration 59 current loss: 0.07331216335296631 current acc: 0.8940061919504644\n",
      "iteration 60 current loss: 0.09150171279907227 current acc: 0.8940164061290822\n",
      "iteration 61 current loss: 0.0518665611743927 current acc: 0.8940328071804395\n",
      "iteration 62 current loss: 0.022595243528485298 current acc: 0.8940492031564289\n",
      "iteration 63 current loss: 0.049436092376708984 current acc: 0.8940655940594059\n",
      "iteration 64 current loss: 0.03364982828497887 current acc: 0.8940819798917247\n",
      "iteration 65 current loss: 0.026204895228147507 current acc: 0.8940983606557377\n",
      "iteration 66 current loss: 0.03889809548854828 current acc: 0.8941147363537962\n",
      "iteration 67 current loss: 0.08215431869029999 current acc: 0.8941280148423005\n",
      "iteration 68 current loss: 0.03353862464427948 current acc: 0.894144380893492\n",
      "iteration 69 current loss: 0.02103574573993683 current acc: 0.894160741885626\n",
      "iteration 70 current loss: 0.0452042780816555 current acc: 0.8941770978210477\n",
      "iteration 71 current loss: 0.03924724832177162 current acc: 0.8941934487021014\n",
      "iteration 72 current loss: 0.031030792742967606 current acc: 0.8942097945311293\n",
      "iteration 73 current loss: 0.043202079832553864 current acc: 0.8942261353104727\n",
      "iteration 74 current loss: 0.022270366549491882 current acc: 0.894242471042471\n",
      "iteration 75 current loss: 0.029888363555073738 current acc: 0.8942588017294626\n",
      "iteration 76 current loss: 0.0652816966176033 current acc: 0.8942720395244712\n",
      "iteration 77 current loss: 0.029054174199700356 current acc: 0.8942883606051251\n",
      "iteration 78 current loss: 0.05318828672170639 current acc: 0.8943046766476308\n",
      "iteration 79 current loss: 0.04753347858786583 current acc: 0.894320987654321\n",
      "iteration 80 current loss: 0.053000420331954956 current acc: 0.8943372936275266\n",
      "iteration 81 current loss: 0.027595918625593185 current acc: 0.8943535945695773\n",
      "iteration 82 current loss: 0.05682075396180153 current acc: 0.8943698904828011\n",
      "iteration 83 current loss: 0.01886877603828907 current acc: 0.894386181369525\n",
      "iteration 84 current loss: 0.03119119256734848 current acc: 0.8944024672320741\n",
      "iteration 85 current loss: 0.024227626621723175 current acc: 0.8944187480727721\n",
      "iteration 86 current loss: 0.03256211429834366 current acc: 0.8944350238939417\n",
      "iteration 87 current loss: 0.03076246753334999 current acc: 0.8944512946979039\n",
      "iteration 88 current loss: 0.05179126188158989 current acc: 0.894467560486978\n",
      "iteration 89 current loss: 0.03720320761203766 current acc: 0.8944838212634822\n",
      "iteration 90 current loss: 0.04554092511534691 current acc: 0.8945000770297334\n",
      "iteration 91 current loss: 0.03894616290926933 current acc: 0.8945163277880468\n",
      "iteration 92 current loss: 0.06036128103733063 current acc: 0.8945294933004775\n",
      "iteration 93 current loss: 0.03951280936598778 current acc: 0.8945457345241762\n",
      "iteration 94 current loss: 0.021618561819195747 current acc: 0.8945619707467283\n",
      "iteration 95 current loss: 0.04518263041973114 current acc: 0.8945782019704434\n",
      "iteration 96 current loss: 0.030462494120001793 current acc: 0.8945944281976297\n",
      "iteration 97 current loss: 0.02420499548316002 current acc: 0.894610649430594\n",
      "iteration 98 current loss: 0.025511283427476883 current acc: 0.8946268656716417\n",
      "iteration 99 current loss: 0.05771205946803093 current acc: 0.89464\n",
      "\t\tEpoch 64/100 complete. Epoch loss 0.04822523662820458 Epoch accuracy 0.9926\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 64, Validation Accuracy: 0.611625, Validation Loss: 1.6076226372271776\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.03936190530657768 current acc: 0.894656206737425\n",
      "iteration 1 current loss: 0.03873699903488159 current acc: 0.8946724084896954\n",
      "iteration 2 current loss: 0.038426145911216736 current acc: 0.8946886052591112\n",
      "iteration 3 current loss: 0.0067048994824290276 current acc: 0.8947047970479705\n",
      "iteration 4 current loss: 0.016802646219730377 current acc: 0.8947209838585704\n",
      "iteration 5 current loss: 0.018260136246681213 current acc: 0.8947371656932063\n",
      "iteration 6 current loss: 0.015431070700287819 current acc: 0.8947533425541724\n",
      "iteration 7 current loss: 0.015937261283397675 current acc: 0.8947695144437615\n",
      "iteration 8 current loss: 0.026133110746741295 current acc: 0.8947856813642648\n",
      "iteration 9 current loss: 0.013662828132510185 current acc: 0.8948018433179723\n",
      "iteration 10 current loss: 0.021314766258001328 current acc: 0.8948180003071725\n",
      "iteration 11 current loss: 0.015688622370362282 current acc: 0.8948341523341523\n",
      "iteration 12 current loss: 0.020524822175502777 current acc: 0.8948502994011976\n",
      "iteration 13 current loss: 0.014104646630585194 current acc: 0.8948664415105926\n",
      "iteration 14 current loss: 0.0179031640291214 current acc: 0.8948825786646201\n",
      "iteration 15 current loss: 0.028463084250688553 current acc: 0.8948987108655617\n",
      "iteration 16 current loss: 0.03124132938683033 current acc: 0.8949148381156974\n",
      "iteration 17 current loss: 0.03565768152475357 current acc: 0.8949278919914084\n",
      "iteration 18 current loss: 0.01612192951142788 current acc: 0.8949440098174567\n",
      "iteration 19 current loss: 0.020192928612232208 current acc: 0.8949601226993865\n",
      "iteration 20 current loss: 0.01529203075915575 current acc: 0.8949762306394725\n",
      "iteration 21 current loss: 0.017831766977906227 current acc: 0.8949923336399878\n",
      "iteration 22 current loss: 0.013381162658333778 current acc: 0.8950084317032041\n",
      "iteration 23 current loss: 0.018809251487255096 current acc: 0.8950245248313918\n",
      "iteration 24 current loss: 0.008515250869095325 current acc: 0.8950406130268199\n",
      "iteration 25 current loss: 0.018549976870417595 current acc: 0.8950566962917561\n",
      "iteration 26 current loss: 0.014070047996938229 current acc: 0.8950727746284663\n",
      "iteration 27 current loss: 0.016884762793779373 current acc: 0.8950888480392157\n",
      "iteration 28 current loss: 0.03428595885634422 current acc: 0.8951049165262674\n",
      "iteration 29 current loss: 0.022406363859772682 current acc: 0.8951209800918836\n",
      "iteration 30 current loss: 0.01659052073955536 current acc: 0.895137038738325\n",
      "iteration 31 current loss: 0.01873784326016903 current acc: 0.8951530924678506\n",
      "iteration 32 current loss: 0.010738164186477661 current acc: 0.8951691412827185\n",
      "iteration 33 current loss: 0.015516966581344604 current acc: 0.8951851851851852\n",
      "iteration 34 current loss: 0.019190020859241486 current acc: 0.8952012241775057\n",
      "iteration 35 current loss: 0.013414997607469559 current acc: 0.8952172582619339\n",
      "iteration 36 current loss: 0.011793861165642738 current acc: 0.8952332874407221\n",
      "iteration 37 current loss: 0.011088414117693901 current acc: 0.8952493117161211\n",
      "iteration 38 current loss: 0.014541644603013992 current acc: 0.8952653310903808\n",
      "iteration 39 current loss: 0.018670672550797462 current acc: 0.8952813455657492\n",
      "iteration 40 current loss: 0.01225387305021286 current acc: 0.8952973551444733\n",
      "iteration 41 current loss: 0.012370795011520386 current acc: 0.8953133598287986\n",
      "iteration 42 current loss: 0.009312929585576057 current acc: 0.8953293596209689\n",
      "iteration 43 current loss: 0.008541248738765717 current acc: 0.8953453545232274\n",
      "iteration 44 current loss: 0.004852444399148226 current acc: 0.8953613445378151\n",
      "iteration 45 current loss: 0.01992686092853546 current acc: 0.8953773296669721\n",
      "iteration 46 current loss: 0.010945017449557781 current acc: 0.8953933099129372\n",
      "iteration 47 current loss: 0.016528259962797165 current acc: 0.8954092852779475\n",
      "iteration 48 current loss: 0.022330138832330704 current acc: 0.8954252557642388\n",
      "iteration 49 current loss: 0.01811377704143524 current acc: 0.8954412213740458\n",
      "iteration 50 current loss: 0.015499698929488659 current acc: 0.8954571821096016\n",
      "iteration 51 current loss: 0.014786740764975548 current acc: 0.895473137973138\n",
      "iteration 52 current loss: 0.03446550294756889 current acc: 0.8954890889668854\n",
      "iteration 53 current loss: 0.017330052331089973 current acc: 0.8955050350930729\n",
      "iteration 54 current loss: 0.019799234345555305 current acc: 0.8955209763539282\n",
      "iteration 55 current loss: 0.012863796204328537 current acc: 0.8955369127516779\n",
      "iteration 56 current loss: 0.01509781926870346 current acc: 0.8955528442885466\n",
      "iteration 57 current loss: 0.011386956088244915 current acc: 0.8955687709667581\n",
      "iteration 58 current loss: 0.01380456704646349 current acc: 0.8955846927885348\n",
      "iteration 59 current loss: 0.01459419820457697 current acc: 0.8956006097560976\n",
      "iteration 60 current loss: 0.020916998386383057 current acc: 0.8956165218716658\n",
      "iteration 61 current loss: 0.0156079838052392 current acc: 0.8956324291374581\n",
      "iteration 62 current loss: 0.014790368266403675 current acc: 0.895648331555691\n",
      "iteration 63 current loss: 0.01343065220862627 current acc: 0.8956642291285801\n",
      "iteration 64 current loss: 0.010372703894972801 current acc: 0.8956801218583397\n",
      "iteration 65 current loss: 0.010645401664078236 current acc: 0.8956960097471824\n",
      "iteration 66 current loss: 0.02343129552900791 current acc: 0.8957118927973199\n",
      "iteration 67 current loss: 0.010798624716699123 current acc: 0.8957277710109622\n",
      "iteration 68 current loss: 0.019238347187638283 current acc: 0.8957436443903182\n",
      "iteration 69 current loss: 0.0281363558024168 current acc: 0.8957595129375951\n",
      "iteration 70 current loss: 0.010180031880736351 current acc: 0.8957753766549993\n",
      "iteration 71 current loss: 0.02481948584318161 current acc: 0.8957912355447353\n",
      "iteration 72 current loss: 0.012484623119235039 current acc: 0.8958070896090066\n",
      "iteration 73 current loss: 0.013434195891022682 current acc: 0.8958229388500152\n",
      "iteration 74 current loss: 0.025918856263160706 current acc: 0.895838783269962\n",
      "iteration 75 current loss: 0.012279380112886429 current acc: 0.8958546228710462\n",
      "iteration 76 current loss: 0.010478205978870392 current acc: 0.895870457655466\n",
      "iteration 77 current loss: 0.009140166454017162 current acc: 0.8958862876254181\n",
      "iteration 78 current loss: 0.013698756694793701 current acc: 0.8959021127830977\n",
      "iteration 79 current loss: 0.011236519552767277 current acc: 0.8959179331306991\n",
      "iteration 80 current loss: 0.021839965134859085 current acc: 0.8959337486704149\n",
      "iteration 81 current loss: 0.018013902008533478 current acc: 0.8959495594044363\n",
      "iteration 82 current loss: 0.012491447851061821 current acc: 0.8959653653349536\n",
      "iteration 83 current loss: 0.01580614410340786 current acc: 0.8959811664641555\n",
      "iteration 84 current loss: 0.015367337502539158 current acc: 0.8959969627942294\n",
      "iteration 85 current loss: 0.007832582108676434 current acc: 0.896012754327361\n",
      "iteration 86 current loss: 0.009799860417842865 current acc: 0.8960285410657355\n",
      "iteration 87 current loss: 0.012594166211783886 current acc: 0.8960443230115361\n",
      "iteration 88 current loss: 0.013133523054420948 current acc: 0.8960601001669449\n",
      "iteration 89 current loss: 0.011906731873750687 current acc: 0.8960758725341427\n",
      "iteration 90 current loss: 0.014852476306259632 current acc: 0.8960916401153087\n",
      "iteration 91 current loss: 0.007064687553793192 current acc: 0.8961074029126214\n",
      "iteration 92 current loss: 0.01048645656555891 current acc: 0.8961231609282573\n",
      "iteration 93 current loss: 0.010282294824719429 current acc: 0.8961389141643918\n",
      "iteration 94 current loss: 0.010006648488342762 current acc: 0.8961546626231994\n",
      "iteration 95 current loss: 0.018834268674254417 current acc: 0.8961704063068526\n",
      "iteration 96 current loss: 0.010015318170189857 current acc: 0.8961861452175232\n",
      "iteration 97 current loss: 0.010980200953781605 current acc: 0.896201879357381\n",
      "iteration 98 current loss: 0.013555332086980343 current acc: 0.8962176087285952\n",
      "iteration 99 current loss: 0.021201424300670624 current acc: 0.8962333333333333\n",
      "\t\tEpoch 65/100 complete. Epoch loss 0.01662887289188802 Epoch accuracy 0.9998\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 65, Validation Accuracy: 0.618, Validation Loss: 1.6086651939898728\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.008054093457758427 current acc: 0.8962490531737616\n",
      "iteration 1 current loss: 0.0063407584093511105 current acc: 0.8962647682520448\n",
      "iteration 2 current loss: 0.005425159819424152 current acc: 0.8962804785703468\n",
      "iteration 3 current loss: 0.008941065520048141 current acc: 0.8962961841308298\n",
      "iteration 4 current loss: 0.009219993837177753 current acc: 0.8963118849356548\n",
      "iteration 5 current loss: 0.010543730109930038 current acc: 0.8963275809869815\n",
      "iteration 6 current loss: 0.004927173722535372 current acc: 0.8963432722869684\n",
      "iteration 7 current loss: 0.00847921334207058 current acc: 0.8963589588377724\n",
      "iteration 8 current loss: 0.007151263300329447 current acc: 0.8963746406415494\n",
      "iteration 9 current loss: 0.009063894860446453 current acc: 0.8963903177004539\n",
      "iteration 10 current loss: 0.010279910638928413 current acc: 0.8964059900166389\n",
      "iteration 11 current loss: 0.007836575619876385 current acc: 0.8964216575922564\n",
      "iteration 12 current loss: 0.006140376906841993 current acc: 0.8964373204294571\n",
      "iteration 13 current loss: 0.010386125184595585 current acc: 0.8964529785303901\n",
      "iteration 14 current loss: 0.008112587034702301 current acc: 0.8964686318972033\n",
      "iteration 15 current loss: 0.008861876092851162 current acc: 0.8964842805320435\n",
      "iteration 16 current loss: 0.0077003841288387775 current acc: 0.8964999244370561\n",
      "iteration 17 current loss: 0.012432086281478405 current acc: 0.896515563614385\n",
      "iteration 18 current loss: 0.011522612534463406 current acc: 0.8965311980661731\n",
      "iteration 19 current loss: 0.00543523533269763 current acc: 0.896546827794562\n",
      "iteration 20 current loss: 0.006113244686275721 current acc: 0.8965624528016916\n",
      "iteration 21 current loss: 0.0062828646041452885 current acc: 0.8965780730897009\n",
      "iteration 22 current loss: 0.01003766618669033 current acc: 0.8965936886607278\n",
      "iteration 23 current loss: 0.010742357932031155 current acc: 0.8966092995169083\n",
      "iteration 24 current loss: 0.008985728025436401 current acc: 0.8966249056603773\n",
      "iteration 25 current loss: 0.008856328204274178 current acc: 0.8966405070932689\n",
      "iteration 26 current loss: 0.012669564224779606 current acc: 0.8966561038177154\n",
      "iteration 27 current loss: 0.010742434300482273 current acc: 0.8966716958358479\n",
      "iteration 28 current loss: 0.007399401627480984 current acc: 0.8966872831497964\n",
      "iteration 29 current loss: 0.014232770539820194 current acc: 0.8967028657616893\n",
      "iteration 30 current loss: 0.007871508598327637 current acc: 0.896718443673654\n",
      "iteration 31 current loss: 0.006647493690252304 current acc: 0.8967340168878166\n",
      "iteration 32 current loss: 0.011352215893566608 current acc: 0.8967495854063018\n",
      "iteration 33 current loss: 0.005220956169068813 current acc: 0.8967651492312331\n",
      "iteration 34 current loss: 0.007710231002420187 current acc: 0.8967807083647324\n",
      "iteration 35 current loss: 0.007521859370172024 current acc: 0.896796262808921\n",
      "iteration 36 current loss: 0.007804925553500652 current acc: 0.8968118125659184\n",
      "iteration 37 current loss: 0.006482896860688925 current acc: 0.8968273576378427\n",
      "iteration 38 current loss: 0.009467439725995064 current acc: 0.8968428980268113\n",
      "iteration 39 current loss: 0.01125845406204462 current acc: 0.8968584337349398\n",
      "iteration 40 current loss: 0.00962692592293024 current acc: 0.8968739647643427\n",
      "iteration 41 current loss: 0.013620018027722836 current acc: 0.8968894911171333\n",
      "iteration 42 current loss: 0.014003456570208073 current acc: 0.8969050127954238\n",
      "iteration 43 current loss: 0.0067321439273655415 current acc: 0.8969205298013245\n",
      "iteration 44 current loss: 0.007874719798564911 current acc: 0.8969360421369451\n",
      "iteration 45 current loss: 0.010997984558343887 current acc: 0.8969515498043936\n",
      "iteration 46 current loss: 0.011750894598662853 current acc: 0.896967052805777\n",
      "iteration 47 current loss: 0.0035730814561247826 current acc: 0.8969825511432009\n",
      "iteration 48 current loss: 0.006768213119357824 current acc: 0.8969980448187698\n",
      "iteration 49 current loss: 0.006439536809921265 current acc: 0.8970135338345865\n",
      "iteration 50 current loss: 0.009368347004055977 current acc: 0.8970290181927529\n",
      "iteration 51 current loss: 0.009454209357500076 current acc: 0.8970444978953698\n",
      "iteration 52 current loss: 0.01091590616852045 current acc: 0.8970599729445363\n",
      "iteration 53 current loss: 0.006498044356703758 current acc: 0.8970754433423505\n",
      "iteration 54 current loss: 0.01052024494856596 current acc: 0.897090909090909\n",
      "iteration 55 current loss: 0.007967779412865639 current acc: 0.8971063701923077\n",
      "iteration 56 current loss: 0.006033811718225479 current acc: 0.8971218266486405\n",
      "iteration 57 current loss: 0.006378069519996643 current acc: 0.8971372784620006\n",
      "iteration 58 current loss: 0.007483434863388538 current acc: 0.8971527256344797\n",
      "iteration 59 current loss: 0.010102926753461361 current acc: 0.8971681681681681\n",
      "iteration 60 current loss: 0.007116041146218777 current acc: 0.8971836060651553\n",
      "iteration 61 current loss: 0.03463995084166527 current acc: 0.8971960372260582\n",
      "iteration 62 current loss: 0.009842388331890106 current acc: 0.8972114663064685\n",
      "iteration 63 current loss: 0.005582259502261877 current acc: 0.8972268907563026\n",
      "iteration 64 current loss: 0.009107560850679874 current acc: 0.8972423105776444\n",
      "iteration 65 current loss: 0.00933653861284256 current acc: 0.8972577257725772\n",
      "iteration 66 current loss: 0.004815009422600269 current acc: 0.8972731363431828\n",
      "iteration 67 current loss: 0.011036256328225136 current acc: 0.8972885422915416\n",
      "iteration 68 current loss: 0.00487350020557642 current acc: 0.8973039436197331\n",
      "iteration 69 current loss: 0.006909285672008991 current acc: 0.8973193403298351\n",
      "iteration 70 current loss: 0.007599287200719118 current acc: 0.8973347324239245\n",
      "iteration 71 current loss: 0.012182728387415409 current acc: 0.8973501199040768\n",
      "iteration 72 current loss: 0.009362163953483105 current acc: 0.8973655027723663\n",
      "iteration 73 current loss: 0.013572033494710922 current acc: 0.897380881030866\n",
      "iteration 74 current loss: 0.009932338260114193 current acc: 0.897396254681648\n",
      "iteration 75 current loss: 0.00687640905380249 current acc: 0.8974116237267825\n",
      "iteration 76 current loss: 0.012196412310004234 current acc: 0.8974269881683391\n",
      "iteration 77 current loss: 0.007130681071430445 current acc: 0.8974423480083857\n",
      "iteration 78 current loss: 0.004952653776854277 current acc: 0.8974577032489893\n",
      "iteration 79 current loss: 0.012705598026514053 current acc: 0.8974730538922155\n",
      "iteration 80 current loss: 0.008794920518994331 current acc: 0.8974883999401287\n",
      "iteration 81 current loss: 0.018029607832431793 current acc: 0.8975037413947919\n",
      "iteration 82 current loss: 0.010758817195892334 current acc: 0.8975190782582673\n",
      "iteration 83 current loss: 0.011793595738708973 current acc: 0.8975344105326152\n",
      "iteration 84 current loss: 0.005234524607658386 current acc: 0.8975497382198953\n",
      "iteration 85 current loss: 0.008166889660060406 current acc: 0.8975650613221657\n",
      "iteration 86 current loss: 0.004718733951449394 current acc: 0.8975803798414834\n",
      "iteration 87 current loss: 0.008657502010464668 current acc: 0.8975956937799043\n",
      "iteration 88 current loss: 0.007787878625094891 current acc: 0.8976110031394827\n",
      "iteration 89 current loss: 0.007939431816339493 current acc: 0.897626307922272\n",
      "iteration 90 current loss: 0.012466153129935265 current acc: 0.8976416081303243\n",
      "iteration 91 current loss: 0.008560055866837502 current acc: 0.8976569037656904\n",
      "iteration 92 current loss: 0.008542563766241074 current acc: 0.8976721948304198\n",
      "iteration 93 current loss: 0.008084985427558422 current acc: 0.8976874813265611\n",
      "iteration 94 current loss: 0.009469958953559399 current acc: 0.8977027632561613\n",
      "iteration 95 current loss: 0.009725769981741905 current acc: 0.8977180406212665\n",
      "iteration 96 current loss: 0.010513863526284695 current acc: 0.8977333134239212\n",
      "iteration 97 current loss: 0.010062485001981258 current acc: 0.897748581666169\n",
      "iteration 98 current loss: 0.005869277287274599 current acc: 0.8977638453500523\n",
      "iteration 99 current loss: 0.003940487280488014 current acc: 0.8977791044776119\n",
      "\t\tEpoch 66/100 complete. Epoch loss 0.008992508049122988 Epoch accuracy 0.9998\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 66, Validation Accuracy: 0.619, Validation Loss: 1.646278616040945\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.005615605041384697 current acc: 0.8977943590508879\n",
      "iteration 1 current loss: 0.005152860656380653 current acc: 0.8978096090719189\n",
      "iteration 2 current loss: 0.008379875682294369 current acc: 0.8978248545427421\n",
      "iteration 3 current loss: 0.005253300536423922 current acc: 0.8978400954653938\n",
      "iteration 4 current loss: 0.0042081838473677635 current acc: 0.897855331841909\n",
      "iteration 5 current loss: 0.009555406868457794 current acc: 0.8978705636743215\n",
      "iteration 6 current loss: 0.007033733185380697 current acc: 0.8978857909646638\n",
      "iteration 7 current loss: 0.004380505066365004 current acc: 0.8979010137149672\n",
      "iteration 8 current loss: 0.010292849503457546 current acc: 0.8979162319272619\n",
      "iteration 9 current loss: 0.009867638349533081 current acc: 0.8979314456035767\n",
      "iteration 10 current loss: 0.005451915320008993 current acc: 0.8979466547459395\n",
      "iteration 11 current loss: 0.005989608354866505 current acc: 0.8979618593563766\n",
      "iteration 12 current loss: 0.007655086927115917 current acc: 0.8979770594369134\n",
      "iteration 13 current loss: 0.004747555125504732 current acc: 0.897992254989574\n",
      "iteration 14 current loss: 0.006524267606437206 current acc: 0.8980074460163813\n",
      "iteration 15 current loss: 0.010726489126682281 current acc: 0.8980226325193568\n",
      "iteration 16 current loss: 0.0061631761491298676 current acc: 0.8980378145005211\n",
      "iteration 17 current loss: 0.006413137540221214 current acc: 0.8980529919618934\n",
      "iteration 18 current loss: 0.006125745829194784 current acc: 0.8980681649054919\n",
      "iteration 19 current loss: 0.0042620436288416386 current acc: 0.8980833333333333\n",
      "iteration 20 current loss: 0.004866728559136391 current acc: 0.8980984972474334\n",
      "iteration 21 current loss: 0.007763960864394903 current acc: 0.8981136566498066\n",
      "iteration 22 current loss: 0.004329188261181116 current acc: 0.8981288115424662\n",
      "iteration 23 current loss: 0.003077279543504119 current acc: 0.8981439619274242\n",
      "iteration 24 current loss: 0.005957756657153368 current acc: 0.8981591078066915\n",
      "iteration 25 current loss: 0.00523215951398015 current acc: 0.8981742491822777\n",
      "iteration 26 current loss: 0.007756471633911133 current acc: 0.8981893860561915\n",
      "iteration 27 current loss: 0.0059738303534686565 current acc: 0.8982045184304399\n",
      "iteration 28 current loss: 0.003967372700572014 current acc: 0.8982196463070293\n",
      "iteration 29 current loss: 0.00726575031876564 current acc: 0.8982347696879643\n",
      "iteration 30 current loss: 0.009090018458664417 current acc: 0.8982498885752489\n",
      "iteration 31 current loss: 0.005393518600612879 current acc: 0.8982650029708853\n",
      "iteration 32 current loss: 0.012957969680428505 current acc: 0.8982801128768751\n",
      "iteration 33 current loss: 0.0078047518618404865 current acc: 0.8982952182952183\n",
      "iteration 34 current loss: 0.005804887972772121 current acc: 0.8983103192279139\n",
      "iteration 35 current loss: 0.004514601081609726 current acc: 0.8983254156769597\n",
      "iteration 36 current loss: 0.005144445225596428 current acc: 0.8983405076443521\n",
      "iteration 37 current loss: 0.006238347850739956 current acc: 0.8983555951320866\n",
      "iteration 38 current loss: 0.003773682750761509 current acc: 0.8983706781421575\n",
      "iteration 39 current loss: 0.006786517333239317 current acc: 0.8983857566765578\n",
      "iteration 40 current loss: 0.004942887928336859 current acc: 0.8984008307372794\n",
      "iteration 41 current loss: 0.0056044249795377254 current acc: 0.8984159003263127\n",
      "iteration 42 current loss: 0.005360698327422142 current acc: 0.8984309654456474\n",
      "iteration 43 current loss: 0.00791475735604763 current acc: 0.8984460260972716\n",
      "iteration 44 current loss: 0.0039701880887150764 current acc: 0.8984610822831727\n",
      "iteration 45 current loss: 0.004268298856914043 current acc: 0.8984761340053365\n",
      "iteration 46 current loss: 0.004364930093288422 current acc: 0.8984911812657478\n",
      "iteration 47 current loss: 0.005419646389782429 current acc: 0.89850622406639\n",
      "iteration 48 current loss: 0.003640157636255026 current acc: 0.8985212624092458\n",
      "iteration 49 current loss: 0.0035747108049690723 current acc: 0.8985362962962963\n",
      "iteration 50 current loss: 0.010965228080749512 current acc: 0.8985513257295216\n",
      "iteration 51 current loss: 0.003915141802281141 current acc: 0.8985663507109005\n",
      "iteration 52 current loss: 0.0044016106985509396 current acc: 0.8985813712424108\n",
      "iteration 53 current loss: 0.005029028281569481 current acc: 0.898596387326029\n",
      "iteration 54 current loss: 0.008464466780424118 current acc: 0.8986113989637305\n",
      "iteration 55 current loss: 0.005603895988315344 current acc: 0.8986264061574897\n",
      "iteration 56 current loss: 0.005497521720826626 current acc: 0.8986414089092792\n",
      "iteration 57 current loss: 0.005871470086276531 current acc: 0.8986564072210713\n",
      "iteration 58 current loss: 0.006076146382838488 current acc: 0.8986714010948366\n",
      "iteration 59 current loss: 0.006274963729083538 current acc: 0.8986863905325444\n",
      "iteration 60 current loss: 0.004106148611754179 current acc: 0.8987013755361632\n",
      "iteration 61 current loss: 0.003822440281510353 current acc: 0.8987163561076604\n",
      "iteration 62 current loss: 0.005541483871638775 current acc: 0.8987313322490019\n",
      "iteration 63 current loss: 0.005151454359292984 current acc: 0.8987463039621526\n",
      "iteration 64 current loss: 0.0032571074552834034 current acc: 0.8987612712490761\n",
      "iteration 65 current loss: 0.01174384355545044 current acc: 0.8987762341117351\n",
      "iteration 66 current loss: 0.005899151787161827 current acc: 0.8987911925520911\n",
      "iteration 67 current loss: 0.005189154762774706 current acc: 0.898806146572104\n",
      "iteration 68 current loss: 0.0053759245201945305 current acc: 0.8988210961737332\n",
      "iteration 69 current loss: 0.007474891375750303 current acc: 0.8988360413589365\n",
      "iteration 70 current loss: 0.005497575271874666 current acc: 0.8988509821296706\n",
      "iteration 71 current loss: 0.004211895167827606 current acc: 0.8988659184878913\n",
      "iteration 72 current loss: 0.0038566142320632935 current acc: 0.8988808504355529\n",
      "iteration 73 current loss: 0.0074365995824337006 current acc: 0.8988957779746088\n",
      "iteration 74 current loss: 0.006089389324188232 current acc: 0.8989107011070111\n",
      "iteration 75 current loss: 0.005942723248153925 current acc: 0.8989256198347108\n",
      "iteration 76 current loss: 0.0039615025743842125 current acc: 0.8989405341596577\n",
      "iteration 77 current loss: 0.006226218771189451 current acc: 0.8989554440838006\n",
      "iteration 78 current loss: 0.007544573396444321 current acc: 0.8989703496090868\n",
      "iteration 79 current loss: 0.0031359954737126827 current acc: 0.8989852507374632\n",
      "iteration 80 current loss: 0.003226648084819317 current acc: 0.8990001474708745\n",
      "iteration 81 current loss: 0.0043623480014503 current acc: 0.8990150398112651\n",
      "iteration 82 current loss: 0.006035511381924152 current acc: 0.8990299277605779\n",
      "iteration 83 current loss: 0.004415729083120823 current acc: 0.8990448113207548\n",
      "iteration 84 current loss: 0.004145014565438032 current acc: 0.8990596904937361\n",
      "iteration 85 current loss: 0.004746228456497192 current acc: 0.8990745652814618\n",
      "iteration 86 current loss: 0.007665623910725117 current acc: 0.8990894356858701\n",
      "iteration 87 current loss: 0.004467099439352751 current acc: 0.8991043017088981\n",
      "iteration 88 current loss: 0.006559452041983604 current acc: 0.899119163352482\n",
      "iteration 89 current loss: 0.004480829928070307 current acc: 0.8991340206185567\n",
      "iteration 90 current loss: 0.005444021429866552 current acc: 0.8991488735090561\n",
      "iteration 91 current loss: 0.005838711746037006 current acc: 0.8991637220259129\n",
      "iteration 92 current loss: 0.010601893998682499 current acc: 0.8991785661710584\n",
      "iteration 93 current loss: 0.009720639325678349 current acc: 0.8991934059464233\n",
      "iteration 94 current loss: 0.0032870525028556585 current acc: 0.8992082413539367\n",
      "iteration 95 current loss: 0.007015603128820658 current acc: 0.8992230723955268\n",
      "iteration 96 current loss: 0.003353837411850691 current acc: 0.8992378990731205\n",
      "iteration 97 current loss: 0.00799635611474514 current acc: 0.8992527213886438\n",
      "iteration 98 current loss: 0.010137113742530346 current acc: 0.8992675393440211\n",
      "iteration 99 current loss: 0.006435627117753029 current acc: 0.8992823529411764\n",
      "\t\tEpoch 67/100 complete. Epoch loss 0.0060005842661485075 Epoch accuracy 1.0\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 67, Validation Accuracy: 0.620625, Validation Loss: 1.669631151854992\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.0039002420380711555 current acc: 0.899297162182032\n",
      "iteration 1 current loss: 0.005376303568482399 current acc: 0.8993119670685092\n",
      "iteration 2 current loss: 0.0026308270171284676 current acc: 0.8993267676025283\n",
      "iteration 3 current loss: 0.00450949277728796 current acc: 0.8993415637860083\n",
      "iteration 4 current loss: 0.004262059461325407 current acc: 0.899356355620867\n",
      "iteration 5 current loss: 0.008403807878494263 current acc: 0.8993711431090214\n",
      "iteration 6 current loss: 0.011870432645082474 current acc: 0.8993859262523872\n",
      "iteration 7 current loss: 0.006768638268113136 current acc: 0.899400705052879\n",
      "iteration 8 current loss: 0.00493110716342926 current acc: 0.8994154795124101\n",
      "iteration 9 current loss: 0.006027004215866327 current acc: 0.8994302496328928\n",
      "iteration 10 current loss: 0.005279804579913616 current acc: 0.8994450154162384\n",
      "iteration 11 current loss: 0.0037800604477524757 current acc: 0.899459776864357\n",
      "iteration 12 current loss: 0.006554147694259882 current acc: 0.8994745339791574\n",
      "iteration 13 current loss: 0.006574149709194899 current acc: 0.8994892867625477\n",
      "iteration 14 current loss: 0.00998687744140625 current acc: 0.8995040352164343\n",
      "iteration 15 current loss: 0.007061715703457594 current acc: 0.899518779342723\n",
      "iteration 16 current loss: 0.0037090927362442017 current acc: 0.8995335191433181\n",
      "iteration 17 current loss: 0.0034867683425545692 current acc: 0.8995482546201232\n",
      "iteration 18 current loss: 0.002310318872332573 current acc: 0.8995629857750403\n",
      "iteration 19 current loss: 0.00418304605409503 current acc: 0.8995777126099707\n",
      "iteration 20 current loss: 0.003847340354695916 current acc: 0.8995924351268143\n",
      "iteration 21 current loss: 0.013022304512560368 current acc: 0.89960715332747\n",
      "iteration 22 current loss: 0.00457302900031209 current acc: 0.8996218672138355\n",
      "iteration 23 current loss: 0.004279948305338621 current acc: 0.8996365767878077\n",
      "iteration 24 current loss: 0.003822893602773547 current acc: 0.8996512820512821\n",
      "iteration 25 current loss: 0.005815970711410046 current acc: 0.899665983006153\n",
      "iteration 26 current loss: 0.0059441435150802135 current acc: 0.8996806796543138\n",
      "iteration 27 current loss: 0.004510479513555765 current acc: 0.8996953719976567\n",
      "iteration 28 current loss: 0.004613238386809826 current acc: 0.899710060038073\n",
      "iteration 29 current loss: 0.0036134168040007353 current acc: 0.8997247437774524\n",
      "iteration 30 current loss: 0.007635951042175293 current acc: 0.8997394232176841\n",
      "iteration 31 current loss: 0.007596771232783794 current acc: 0.8997540983606558\n",
      "iteration 32 current loss: 0.00515320710837841 current acc: 0.8997687692082541\n",
      "iteration 33 current loss: 0.003550600493326783 current acc: 0.8997834357623646\n",
      "iteration 34 current loss: 0.007324595935642719 current acc: 0.899798098024872\n",
      "iteration 35 current loss: 0.0040558031760156155 current acc: 0.8998127559976594\n",
      "iteration 36 current loss: 0.0053950222209095955 current acc: 0.8998274096826093\n",
      "iteration 37 current loss: 0.00511147640645504 current acc: 0.8998420590816029\n",
      "iteration 38 current loss: 0.004405126441270113 current acc: 0.89985670419652\n",
      "iteration 39 current loss: 0.00431778421625495 current acc: 0.8998713450292397\n",
      "iteration 40 current loss: 0.004280820023268461 current acc: 0.8998859815816401\n",
      "iteration 41 current loss: 0.0042609344236552715 current acc: 0.8999006138555978\n",
      "iteration 42 current loss: 0.0038593944627791643 current acc: 0.8999152418529884\n",
      "iteration 43 current loss: 0.004912710282951593 current acc: 0.8999298655756868\n",
      "iteration 44 current loss: 0.0047276560217142105 current acc: 0.8999444850255661\n",
      "iteration 45 current loss: 0.0032537493389099836 current acc: 0.899959100204499\n",
      "iteration 46 current loss: 0.004123364575207233 current acc: 0.8999737111143566\n",
      "iteration 47 current loss: 0.004547250457108021 current acc: 0.8999883177570094\n",
      "iteration 48 current loss: 0.0076822699047625065 current acc: 0.9000029201343261\n",
      "iteration 49 current loss: 0.0033406196162104607 current acc: 0.9000175182481752\n",
      "iteration 50 current loss: 0.0075106434524059296 current acc: 0.9000321121004233\n",
      "iteration 51 current loss: 0.0031662527471780777 current acc: 0.9000467016929363\n",
      "iteration 52 current loss: 0.007039628457278013 current acc: 0.9000612870275791\n",
      "iteration 53 current loss: 0.004626973066478968 current acc: 0.9000758681062153\n",
      "iteration 54 current loss: 0.003227252746000886 current acc: 0.9000904449307076\n",
      "iteration 55 current loss: 0.007122921757400036 current acc: 0.9001050175029172\n",
      "iteration 56 current loss: 0.005238268990069628 current acc: 0.9001195858247046\n",
      "iteration 57 current loss: 0.003973557148128748 current acc: 0.9001341498979294\n",
      "iteration 58 current loss: 0.005775894038379192 current acc: 0.9001487097244496\n",
      "iteration 59 current loss: 0.005579305812716484 current acc: 0.9001632653061225\n",
      "iteration 60 current loss: 0.005086649674922228 current acc: 0.900177816644804\n",
      "iteration 61 current loss: 0.0037311722990125418 current acc: 0.9001923637423491\n",
      "iteration 62 current loss: 0.007483327761292458 current acc: 0.900206906600612\n",
      "iteration 63 current loss: 0.004562912508845329 current acc: 0.9002214452214452\n",
      "iteration 64 current loss: 0.0026738131418824196 current acc: 0.9002359796067007\n",
      "iteration 65 current loss: 0.004517196211963892 current acc: 0.900250509758229\n",
      "iteration 66 current loss: 0.002992223482578993 current acc: 0.9002650356778797\n",
      "iteration 67 current loss: 0.004190336912870407 current acc: 0.9002795573675014\n",
      "iteration 68 current loss: 0.0032160633709281683 current acc: 0.9002940748289416\n",
      "iteration 69 current loss: 0.0031563574448227882 current acc: 0.9003085880640466\n",
      "iteration 70 current loss: 0.004558505956083536 current acc: 0.9003230970746616\n",
      "iteration 71 current loss: 0.002557729836553335 current acc: 0.900337601862631\n",
      "iteration 72 current loss: 0.004212271422147751 current acc: 0.9003521024297978\n",
      "iteration 73 current loss: 0.005849333945661783 current acc: 0.9003665987780041\n",
      "iteration 74 current loss: 0.0066620660945773125 current acc: 0.9003810909090909\n",
      "iteration 75 current loss: 0.005245857406407595 current acc: 0.9003955788248982\n",
      "iteration 76 current loss: 0.003807661822065711 current acc: 0.9004100625272647\n",
      "iteration 77 current loss: 0.004739848896861076 current acc: 0.9004245420180285\n",
      "iteration 78 current loss: 0.003569468855857849 current acc: 0.900439017299026\n",
      "iteration 79 current loss: 0.004587383940815926 current acc: 0.900453488372093\n",
      "iteration 80 current loss: 0.0036061089485883713 current acc: 0.9004679552390641\n",
      "iteration 81 current loss: 0.005897171329706907 current acc: 0.9004824179017727\n",
      "iteration 82 current loss: 0.0030381837859749794 current acc: 0.9004968763620514\n",
      "iteration 83 current loss: 0.0034277993254363537 current acc: 0.9005113306217315\n",
      "iteration 84 current loss: 0.0033141947351396084 current acc: 0.9005257806826434\n",
      "iteration 85 current loss: 0.003941209055483341 current acc: 0.9005402265466164\n",
      "iteration 86 current loss: 0.004836249630898237 current acc: 0.9005546682154785\n",
      "iteration 87 current loss: 0.006008001510053873 current acc: 0.9005691056910569\n",
      "iteration 88 current loss: 0.005329434294253588 current acc: 0.9005835389751778\n",
      "iteration 89 current loss: 0.0055627659894526005 current acc: 0.9005979680696662\n",
      "iteration 90 current loss: 0.0034015788696706295 current acc: 0.900612392976346\n",
      "iteration 91 current loss: 0.005529842805117369 current acc: 0.90062681369704\n",
      "iteration 92 current loss: 0.004350693896412849 current acc: 0.9006412302335702\n",
      "iteration 93 current loss: 0.004339009523391724 current acc: 0.9006556425877574\n",
      "iteration 94 current loss: 0.005981302820146084 current acc: 0.9006700507614214\n",
      "iteration 95 current loss: 0.004670181777328253 current acc: 0.9006844547563805\n",
      "iteration 96 current loss: 0.003798641497269273 current acc: 0.9006988545744526\n",
      "iteration 97 current loss: 0.002240101108327508 current acc: 0.9007132502174543\n",
      "iteration 98 current loss: 0.0043554469011723995 current acc: 0.9007276416872011\n",
      "iteration 99 current loss: 0.003968402277678251 current acc: 0.9007420289855073\n",
      "\t\tEpoch 68/100 complete. Epoch loss 0.004934409679844975 Epoch accuracy 1.0\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 68, Validation Accuracy: 0.619875, Validation Loss: 1.6909752953797579\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.002909429371356964 current acc: 0.9007564121141863\n",
      "iteration 1 current loss: 0.0026946240104734898 current acc: 0.9007707910750508\n",
      "iteration 2 current loss: 0.004671901930123568 current acc: 0.9007851658699116\n",
      "iteration 3 current loss: 0.003559538396075368 current acc: 0.9007995365005794\n",
      "iteration 4 current loss: 0.004142323508858681 current acc: 0.9008139029688631\n",
      "iteration 5 current loss: 0.0042816875502467155 current acc: 0.9008282652765711\n",
      "iteration 6 current loss: 0.004875830374658108 current acc: 0.9008426234255104\n",
      "iteration 7 current loss: 0.005097170360386372 current acc: 0.9008569774174869\n",
      "iteration 8 current loss: 0.003397426102310419 current acc: 0.900871327254306\n",
      "iteration 9 current loss: 0.0021681899670511484 current acc: 0.9008856729377713\n",
      "iteration 10 current loss: 0.0028490847907960415 current acc: 0.900900014469686\n",
      "iteration 11 current loss: 0.0028747888281941414 current acc: 0.9009143518518519\n",
      "iteration 12 current loss: 0.0032032362651079893 current acc: 0.9009286850860697\n",
      "iteration 13 current loss: 0.005104065407067537 current acc: 0.9009430141741395\n",
      "iteration 14 current loss: 0.002824208466336131 current acc: 0.9009573391178597\n",
      "iteration 15 current loss: 0.002354806289076805 current acc: 0.9009716599190284\n",
      "iteration 16 current loss: 0.0026959972456097603 current acc: 0.9009859765794419\n",
      "iteration 17 current loss: 0.004085130523890257 current acc: 0.9010002891008962\n",
      "iteration 18 current loss: 0.003707922762259841 current acc: 0.9010145974851858\n",
      "iteration 19 current loss: 0.0036777725908905268 current acc: 0.901028901734104\n",
      "iteration 20 current loss: 0.002358050085604191 current acc: 0.9010432018494438\n",
      "iteration 21 current loss: 0.002718615345656872 current acc: 0.9010574978329963\n",
      "iteration 22 current loss: 0.004848324228078127 current acc: 0.9010717896865521\n",
      "iteration 23 current loss: 0.004211926367133856 current acc: 0.9010860774119006\n",
      "iteration 24 current loss: 0.00694490410387516 current acc: 0.9011003610108304\n",
      "iteration 25 current loss: 0.003977988846600056 current acc: 0.9011146404851285\n",
      "iteration 26 current loss: 0.004046325571835041 current acc: 0.9011289158365815\n",
      "iteration 27 current loss: 0.0038566640578210354 current acc: 0.9011431870669746\n",
      "iteration 28 current loss: 0.004498221445828676 current acc: 0.9011574541780921\n",
      "iteration 29 current loss: 0.0034808360505849123 current acc: 0.9011717171717172\n",
      "iteration 30 current loss: 0.0030433982610702515 current acc: 0.9011859760496321\n",
      "iteration 31 current loss: 0.003463359549641609 current acc: 0.901200230813618\n",
      "iteration 32 current loss: 0.002679280238226056 current acc: 0.901214481465455\n",
      "iteration 33 current loss: 0.0032724114134907722 current acc: 0.9012287280069224\n",
      "iteration 34 current loss: 0.0030975998379290104 current acc: 0.9012429704397982\n",
      "iteration 35 current loss: 0.005553069524466991 current acc: 0.9012572087658592\n",
      "iteration 36 current loss: 0.003683599177747965 current acc: 0.9012714429868819\n",
      "iteration 37 current loss: 0.003189139999449253 current acc: 0.9012856731046411\n",
      "iteration 38 current loss: 0.0037407446652650833 current acc: 0.9012998991209108\n",
      "iteration 39 current loss: 0.002934904070571065 current acc: 0.901314121037464\n",
      "iteration 40 current loss: 0.0026002107188105583 current acc: 0.9013283388560727\n",
      "iteration 41 current loss: 0.004749502055346966 current acc: 0.9013425525785076\n",
      "iteration 42 current loss: 0.005301502533257008 current acc: 0.9013567622065389\n",
      "iteration 43 current loss: 0.004250742960721254 current acc: 0.9013709677419355\n",
      "iteration 44 current loss: 0.0036783700343221426 current acc: 0.901385169186465\n",
      "iteration 45 current loss: 0.0057930173352360725 current acc: 0.9013993665418946\n",
      "iteration 46 current loss: 0.004227620083838701 current acc: 0.9014135598099899\n",
      "iteration 47 current loss: 0.0039688958786427975 current acc: 0.9014277489925159\n",
      "iteration 48 current loss: 0.005077343434095383 current acc: 0.9014419340912362\n",
      "iteration 49 current loss: 0.00623243348672986 current acc: 0.9014561151079137\n",
      "iteration 50 current loss: 0.0034534228034317493 current acc: 0.9014702920443102\n",
      "iteration 51 current loss: 0.0035180053673684597 current acc: 0.9014844649021864\n",
      "iteration 52 current loss: 0.00407633651047945 current acc: 0.9014986336833022\n",
      "iteration 53 current loss: 0.0034715565852820873 current acc: 0.9015127983894161\n",
      "iteration 54 current loss: 0.003383937291800976 current acc: 0.9015269590222861\n",
      "iteration 55 current loss: 0.004576963372528553 current acc: 0.9015411155836688\n",
      "iteration 56 current loss: 0.004850849974900484 current acc: 0.9015552680753198\n",
      "iteration 57 current loss: 0.004116186872124672 current acc: 0.9015694164989939\n",
      "iteration 58 current loss: 0.0071956319734454155 current acc: 0.9015835608564449\n",
      "iteration 59 current loss: 0.0049345712177455425 current acc: 0.9015977011494253\n",
      "iteration 60 current loss: 0.003212462645024061 current acc: 0.9016118373796869\n",
      "iteration 61 current loss: 0.003982056397944689 current acc: 0.9016259695489802\n",
      "iteration 62 current loss: 0.00417113583534956 current acc: 0.901640097659055\n",
      "iteration 63 current loss: 0.003695116611197591 current acc: 0.90165422171166\n",
      "iteration 64 current loss: 0.003225328866392374 current acc: 0.9016683417085427\n",
      "iteration 65 current loss: 0.003471683245152235 current acc: 0.9016824576514499\n",
      "iteration 66 current loss: 0.0031475438736379147 current acc: 0.9016965695421272\n",
      "iteration 67 current loss: 0.0033358975779265165 current acc: 0.9017106773823191\n",
      "iteration 68 current loss: 0.003611344378441572 current acc: 0.9017247811737695\n",
      "iteration 69 current loss: 0.002451899228617549 current acc: 0.901738880918221\n",
      "iteration 70 current loss: 0.0041475980542600155 current acc: 0.901752976617415\n",
      "iteration 71 current loss: 0.003961509559303522 current acc: 0.9017670682730924\n",
      "iteration 72 current loss: 0.00444421311840415 current acc: 0.9017811558869927\n",
      "iteration 73 current loss: 0.0058496431447565556 current acc: 0.9017952394608546\n",
      "iteration 74 current loss: 0.003229164518415928 current acc: 0.9018093189964158\n",
      "iteration 75 current loss: 0.0031084984075278044 current acc: 0.9018233944954128\n",
      "iteration 76 current loss: 0.001898454618640244 current acc: 0.9018374659595815\n",
      "iteration 77 current loss: 0.005587894935160875 current acc: 0.9018515333906564\n",
      "iteration 78 current loss: 0.002662066835910082 current acc: 0.9018655967903711\n",
      "iteration 79 current loss: 0.004352824296802282 current acc: 0.9018796561604584\n",
      "iteration 80 current loss: 0.005694800056517124 current acc: 0.9018937115026501\n",
      "iteration 81 current loss: 0.0040781800635159016 current acc: 0.9019077628186766\n",
      "iteration 82 current loss: 0.007082871161401272 current acc: 0.9019218101102678\n",
      "iteration 83 current loss: 0.0026887168642133474 current acc: 0.9019358533791524\n",
      "iteration 84 current loss: 0.003202555701136589 current acc: 0.9019498926270579\n",
      "iteration 85 current loss: 0.009687668643891811 current acc: 0.9019639278557114\n",
      "iteration 86 current loss: 0.002019554376602173 current acc: 0.9019779590668384\n",
      "iteration 87 current loss: 0.00301125249825418 current acc: 0.9019919862621637\n",
      "iteration 88 current loss: 0.0033208373934030533 current acc: 0.902006009443411\n",
      "iteration 89 current loss: 0.0037610644940286875 current acc: 0.9020200286123032\n",
      "iteration 90 current loss: 0.003918799571692944 current acc: 0.9020340437705622\n",
      "iteration 91 current loss: 0.003032463137060404 current acc: 0.9020480549199085\n",
      "iteration 92 current loss: 0.004903334658592939 current acc: 0.9020620620620621\n",
      "iteration 93 current loss: 0.0034988827537745237 current acc: 0.9020760651987417\n",
      "iteration 94 current loss: 0.00347281526774168 current acc: 0.9020900643316655\n",
      "iteration 95 current loss: 0.0056363316252827644 current acc: 0.90210405946255\n",
      "iteration 96 current loss: 0.0032671804074198008 current acc: 0.9021180505931113\n",
      "iteration 97 current loss: 0.0033713490702211857 current acc: 0.9021320377250643\n",
      "iteration 98 current loss: 0.012102381326258183 current acc: 0.9021460208601229\n",
      "iteration 99 current loss: 0.0038402837235480547 current acc: 0.90216\n",
      "\t\tEpoch 69/100 complete. Epoch loss 0.004013692570151761 Epoch accuracy 1.0\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 69, Validation Accuracy: 0.62125, Validation Loss: 1.7148254215717316\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.007816227152943611 current acc: 0.9021739751464076\n",
      "iteration 1 current loss: 0.002279979642480612 current acc: 0.9021879463010568\n",
      "iteration 2 current loss: 0.004224032163619995 current acc: 0.9022019134656576\n",
      "iteration 3 current loss: 0.006054795812815428 current acc: 0.9022158766419189\n",
      "iteration 4 current loss: 0.0032422624062746763 current acc: 0.9022298358315489\n",
      "iteration 5 current loss: 0.002027828013524413 current acc: 0.9022437910362546\n",
      "iteration 6 current loss: 0.003584635443985462 current acc: 0.9022577422577422\n",
      "iteration 7 current loss: 0.005324185360223055 current acc: 0.9022716894977169\n",
      "iteration 8 current loss: 0.002832509810104966 current acc: 0.9022856327578828\n",
      "iteration 9 current loss: 0.003311786102131009 current acc: 0.9022995720399429\n",
      "iteration 10 current loss: 0.002628820249810815 current acc: 0.9023135073455998\n",
      "iteration 11 current loss: 0.0026410988066345453 current acc: 0.9023274386765545\n",
      "iteration 12 current loss: 0.004240843933075666 current acc: 0.9023413660345073\n",
      "iteration 13 current loss: 0.0032809495460242033 current acc: 0.9023552894211577\n",
      "iteration 14 current loss: 0.0024428933393210173 current acc: 0.9023692088382038\n",
      "iteration 15 current loss: 0.0037550670094788074 current acc: 0.9023831242873432\n",
      "iteration 16 current loss: 0.002250425750389695 current acc: 0.9023970357702722\n",
      "iteration 17 current loss: 0.003145348746329546 current acc: 0.9024109432886862\n",
      "iteration 18 current loss: 0.0044999090023338795 current acc: 0.9024248468442798\n",
      "iteration 19 current loss: 0.0018577644368633628 current acc: 0.9024387464387464\n",
      "iteration 20 current loss: 0.004104092717170715 current acc: 0.9024526420737786\n",
      "iteration 21 current loss: 0.003217636374756694 current acc: 0.9024665337510681\n",
      "iteration 22 current loss: 0.0038276517298072577 current acc: 0.9024804214723052\n",
      "iteration 23 current loss: 0.004505156073719263 current acc: 0.90249430523918\n",
      "iteration 24 current loss: 0.0027189808897674084 current acc: 0.9025081850533808\n",
      "iteration 25 current loss: 0.0017711971886456013 current acc: 0.9025220609165955\n",
      "iteration 26 current loss: 0.002089528366923332 current acc: 0.9025359328305109\n",
      "iteration 27 current loss: 0.0042570834048092365 current acc: 0.9025498007968128\n",
      "iteration 28 current loss: 0.0030093735549598932 current acc: 0.902563664817186\n",
      "iteration 29 current loss: 0.002134065143764019 current acc: 0.9025775248933143\n",
      "iteration 30 current loss: 0.004893047735095024 current acc: 0.902591381026881\n",
      "iteration 31 current loss: 0.0034134944435209036 current acc: 0.9026052332195676\n",
      "iteration 32 current loss: 0.0023932503536343575 current acc: 0.9026190814730556\n",
      "iteration 33 current loss: 0.0028422612231224775 current acc: 0.9026329257890248\n",
      "iteration 34 current loss: 0.003143228590488434 current acc: 0.9026467661691542\n",
      "iteration 35 current loss: 0.0028759322594851255 current acc: 0.9026606026151223\n",
      "iteration 36 current loss: 0.005551099311560392 current acc: 0.9026744351286059\n",
      "iteration 37 current loss: 0.002510304097086191 current acc: 0.9026882637112816\n",
      "iteration 38 current loss: 0.0035132230259478092 current acc: 0.9027020883648246\n",
      "iteration 39 current loss: 0.0025961834471672773 current acc: 0.902715909090909\n",
      "iteration 40 current loss: 0.004350054077804089 current acc: 0.9027297258912086\n",
      "iteration 41 current loss: 0.0028008162043988705 current acc: 0.9027435387673957\n",
      "iteration 42 current loss: 0.002759731374680996 current acc: 0.9027573477211416\n",
      "iteration 43 current loss: 0.004696181043982506 current acc: 0.902771152754117\n",
      "iteration 44 current loss: 0.0027018196415156126 current acc: 0.9027849538679915\n",
      "iteration 45 current loss: 0.0016604475677013397 current acc: 0.9027987510644337\n",
      "iteration 46 current loss: 0.0036556306295096874 current acc: 0.9028125443451114\n",
      "iteration 47 current loss: 0.003039237577468157 current acc: 0.9028263337116913\n",
      "iteration 48 current loss: 0.0021851027850061655 current acc: 0.9028401191658392\n",
      "iteration 49 current loss: 0.004006749484688044 current acc: 0.9028539007092199\n",
      "iteration 50 current loss: 0.0033863335847854614 current acc: 0.9028676783434973\n",
      "iteration 51 current loss: 0.0045040990225970745 current acc: 0.9028814520703347\n",
      "iteration 52 current loss: 0.0028203907422721386 current acc: 0.9028952218913937\n",
      "iteration 53 current loss: 0.002919822931289673 current acc: 0.9029089878083357\n",
      "iteration 54 current loss: 0.0028237076476216316 current acc: 0.9029227498228207\n",
      "iteration 55 current loss: 0.0025697818491607904 current acc: 0.9029365079365079\n",
      "iteration 56 current loss: 0.003415732877328992 current acc: 0.9029502621510557\n",
      "iteration 57 current loss: 0.003191649913787842 current acc: 0.9029640124681213\n",
      "iteration 58 current loss: 0.0028487660456448793 current acc: 0.902977758889361\n",
      "iteration 59 current loss: 0.0035196852404624224 current acc: 0.9029915014164306\n",
      "iteration 60 current loss: 0.0048672594130039215 current acc: 0.9030052400509843\n",
      "iteration 61 current loss: 0.003426810260862112 current acc: 0.9030189747946757\n",
      "iteration 62 current loss: 0.0027394467033445835 current acc: 0.9030327056491576\n",
      "iteration 63 current loss: 0.0034135945606976748 current acc: 0.9030464326160815\n",
      "iteration 64 current loss: 0.0014770100824534893 current acc: 0.9030601556970984\n",
      "iteration 65 current loss: 0.003868924919515848 current acc: 0.9030738748938579\n",
      "iteration 66 current loss: 0.0029385886155068874 current acc: 0.9030875902080091\n",
      "iteration 67 current loss: 0.0022873394191265106 current acc: 0.9031013016411997\n",
      "iteration 68 current loss: 0.0028776396065950394 current acc: 0.9031150091950771\n",
      "iteration 69 current loss: 0.0022676857188344 current acc: 0.9031287128712872\n",
      "iteration 70 current loss: 0.003140667686238885 current acc: 0.903142412671475\n",
      "iteration 71 current loss: 0.003913985099643469 current acc: 0.9031561085972851\n",
      "iteration 72 current loss: 0.007624052930623293 current acc: 0.9031698006503606\n",
      "iteration 73 current loss: 0.0035082069225609303 current acc: 0.9031834888323438\n",
      "iteration 74 current loss: 0.0032438861671835184 current acc: 0.9031971731448764\n",
      "iteration 75 current loss: 0.0023389654234051704 current acc: 0.9032108535895986\n",
      "iteration 76 current loss: 0.006331568583846092 current acc: 0.9032245301681503\n",
      "iteration 77 current loss: 0.0029489940498024225 current acc: 0.90323820288217\n",
      "iteration 78 current loss: 0.0028150337748229504 current acc: 0.9032518717332957\n",
      "iteration 79 current loss: 0.0023313849233090878 current acc: 0.9032655367231638\n",
      "iteration 80 current loss: 0.0020624608732759953 current acc: 0.9032791978534105\n",
      "iteration 81 current loss: 0.010093966498970985 current acc: 0.9032928551256707\n",
      "iteration 82 current loss: 0.002591835567727685 current acc: 0.9033065085415785\n",
      "iteration 83 current loss: 0.0023983262944966555 current acc: 0.9033201581027668\n",
      "iteration 84 current loss: 0.0025888076052069664 current acc: 0.903333803810868\n",
      "iteration 85 current loss: 0.0031817692797631025 current acc: 0.9033474456675135\n",
      "iteration 86 current loss: 0.0019327072659507394 current acc: 0.9033610836743333\n",
      "iteration 87 current loss: 0.003011578693985939 current acc: 0.9033747178329571\n",
      "iteration 88 current loss: 0.0023761745542287827 current acc: 0.9033883481450135\n",
      "iteration 89 current loss: 0.002982997801154852 current acc: 0.9034019746121298\n",
      "iteration 90 current loss: 0.004718475509434938 current acc: 0.9034155972359329\n",
      "iteration 91 current loss: 0.0025221980176866055 current acc: 0.9034292160180485\n",
      "iteration 92 current loss: 0.0028836382552981377 current acc: 0.9034428309601015\n",
      "iteration 93 current loss: 0.004860443528741598 current acc: 0.9034564420637158\n",
      "iteration 94 current loss: 0.002751253778114915 current acc: 0.9034700493305144\n",
      "iteration 95 current loss: 0.0036099296994507313 current acc: 0.9034836527621195\n",
      "iteration 96 current loss: 0.0027012983337044716 current acc: 0.9034972523601522\n",
      "iteration 97 current loss: 0.0034630068112164736 current acc: 0.9035108481262327\n",
      "iteration 98 current loss: 0.003581226570531726 current acc: 0.9035244400619805\n",
      "iteration 99 current loss: 0.003045800607651472 current acc: 0.9035380281690141\n",
      "\t\tEpoch 70/100 complete. Epoch loss 0.003363828333094716 Epoch accuracy 1.0\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 70, Validation Accuracy: 0.621375, Validation Loss: 1.7283717393875122\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.002262483350932598 current acc: 0.9035516124489509\n",
      "iteration 1 current loss: 0.003922503907233477 current acc: 0.9035651929034075\n",
      "iteration 2 current loss: 0.0028390383813530207 current acc: 0.9035787695339997\n",
      "iteration 3 current loss: 0.0029077271465212107 current acc: 0.9035923423423423\n",
      "iteration 4 current loss: 0.0020312261767685413 current acc: 0.9036059113300493\n",
      "iteration 5 current loss: 0.002994805807247758 current acc: 0.9036194764987334\n",
      "iteration 6 current loss: 0.003110029036179185 current acc: 0.903633037850007\n",
      "iteration 7 current loss: 0.0025666302535682917 current acc: 0.9036465953854812\n",
      "iteration 8 current loss: 0.003905556397512555 current acc: 0.9036601491067661\n",
      "iteration 9 current loss: 0.002381635829806328 current acc: 0.9036736990154711\n",
      "iteration 10 current loss: 0.0013991525629535317 current acc: 0.9036872451132049\n",
      "iteration 11 current loss: 0.002109288237988949 current acc: 0.9037007874015748\n",
      "iteration 12 current loss: 0.002728149527683854 current acc: 0.9037143258821876\n",
      "iteration 13 current loss: 0.003914357163012028 current acc: 0.9037278605566489\n",
      "iteration 14 current loss: 0.002216414315626025 current acc: 0.9037413914265636\n",
      "iteration 15 current loss: 0.0031418446451425552 current acc: 0.9037549184935357\n",
      "iteration 16 current loss: 0.0014922310365363955 current acc: 0.9037684417591682\n",
      "iteration 17 current loss: 0.0022938745096325874 current acc: 0.9037819612250633\n",
      "iteration 18 current loss: 0.004407942295074463 current acc: 0.903795476892822\n",
      "iteration 19 current loss: 0.0023816917091608047 current acc: 0.9038089887640449\n",
      "iteration 20 current loss: 0.0018656938336789608 current acc: 0.9038224968403314\n",
      "iteration 21 current loss: 0.0019637274090200663 current acc: 0.90383600112328\n",
      "iteration 22 current loss: 0.0029730885289609432 current acc: 0.9038495016144883\n",
      "iteration 23 current loss: 0.0024156717117875814 current acc: 0.9038629983155531\n",
      "iteration 24 current loss: 0.0021190231200307608 current acc: 0.9038764912280701\n",
      "iteration 25 current loss: 0.004053465090692043 current acc: 0.9038899803536345\n",
      "iteration 26 current loss: 0.003455143189057708 current acc: 0.9039034656938403\n",
      "iteration 27 current loss: 0.007146058138459921 current acc: 0.9039169472502806\n",
      "iteration 28 current loss: 0.003301589284092188 current acc: 0.9039304250245476\n",
      "iteration 29 current loss: 0.0027600019238889217 current acc: 0.9039438990182328\n",
      "iteration 30 current loss: 0.0025803647004067898 current acc: 0.9039573692329267\n",
      "iteration 31 current loss: 0.0029851344879716635 current acc: 0.9039708356702187\n",
      "iteration 32 current loss: 0.0014701660256832838 current acc: 0.9039842983316977\n",
      "iteration 33 current loss: 0.002014362718909979 current acc: 0.9039977572189515\n",
      "iteration 34 current loss: 0.003370484337210655 current acc: 0.904011212333567\n",
      "iteration 35 current loss: 0.0016947485273703933 current acc: 0.9040246636771301\n",
      "iteration 36 current loss: 0.00220469874329865 current acc: 0.904038111251226\n",
      "iteration 37 current loss: 0.0019697868265211582 current acc: 0.9040515550574391\n",
      "iteration 38 current loss: 0.0024615887086838484 current acc: 0.9040649950973526\n",
      "iteration 39 current loss: 0.0026830267161130905 current acc: 0.9040784313725491\n",
      "iteration 40 current loss: 0.004291690420359373 current acc: 0.90409186388461\n",
      "iteration 41 current loss: 0.003931668121367693 current acc: 0.9041052926351162\n",
      "iteration 42 current loss: 0.0023592186626046896 current acc: 0.9041187176256474\n",
      "iteration 43 current loss: 0.0019370685331523418 current acc: 0.9041321388577828\n",
      "iteration 44 current loss: 0.00385624379850924 current acc: 0.9041455563331001\n",
      "iteration 45 current loss: 0.002407422987744212 current acc: 0.9041589700531766\n",
      "iteration 46 current loss: 0.002190564526244998 current acc: 0.9041723800195887\n",
      "iteration 47 current loss: 0.0018734043696895242 current acc: 0.9041857862339115\n",
      "iteration 48 current loss: 0.003728920128196478 current acc: 0.90419918869772\n",
      "iteration 49 current loss: 0.0036095029208809137 current acc: 0.9042125874125874\n",
      "iteration 50 current loss: 0.0026076072826981544 current acc: 0.9042259823800867\n",
      "iteration 51 current loss: 0.002844363683834672 current acc: 0.9042393736017897\n",
      "iteration 52 current loss: 0.0025961047504097223 current acc: 0.9042527610792674\n",
      "iteration 53 current loss: 0.0034955190494656563 current acc: 0.9042661448140901\n",
      "iteration 54 current loss: 0.002903285436332226 current acc: 0.9042795248078267\n",
      "iteration 55 current loss: 0.003143587615340948 current acc: 0.9042929010620459\n",
      "iteration 56 current loss: 0.0033373271580785513 current acc: 0.904306273578315\n",
      "iteration 57 current loss: 0.004449911881238222 current acc: 0.9043196423582006\n",
      "iteration 58 current loss: 0.003492511110380292 current acc: 0.9043330074032686\n",
      "iteration 59 current loss: 0.002408485161140561 current acc: 0.9043463687150838\n",
      "iteration 60 current loss: 0.0028734607622027397 current acc: 0.9043597262952102\n",
      "iteration 61 current loss: 0.003421071218326688 current acc: 0.9043730801452108\n",
      "iteration 62 current loss: 0.0024572699330747128 current acc: 0.904386430266648\n",
      "iteration 63 current loss: 0.002353704534471035 current acc: 0.9043997766610832\n",
      "iteration 64 current loss: 0.002022199332714081 current acc: 0.9044131193300767\n",
      "iteration 65 current loss: 0.003769821720197797 current acc: 0.9044264582751884\n",
      "iteration 66 current loss: 0.002219398273155093 current acc: 0.9044397934979769\n",
      "iteration 67 current loss: 0.002984421793371439 current acc: 0.904453125\n",
      "iteration 68 current loss: 0.0032875537872314453 current acc: 0.9044664527828149\n",
      "iteration 69 current loss: 0.00203738478012383 current acc: 0.9044797768479776\n",
      "iteration 70 current loss: 0.0022923676297068596 current acc: 0.9044930971970436\n",
      "iteration 71 current loss: 0.0034723321441560984 current acc: 0.9045064138315672\n",
      "iteration 72 current loss: 0.0031316629610955715 current acc: 0.9045197267531019\n",
      "iteration 73 current loss: 0.0022574043832719326 current acc: 0.9045330359632004\n",
      "iteration 74 current loss: 0.0023590484634041786 current acc: 0.9045463414634146\n",
      "iteration 75 current loss: 0.0019561988301575184 current acc: 0.9045596432552955\n",
      "iteration 76 current loss: 0.002070362213999033 current acc: 0.9045729413403929\n",
      "iteration 77 current loss: 0.0029584881849586964 current acc: 0.9045862357202563\n",
      "iteration 78 current loss: 0.0017525233561173081 current acc: 0.904599526396434\n",
      "iteration 79 current loss: 0.0033472012728452682 current acc: 0.9046128133704735\n",
      "iteration 80 current loss: 0.003413561498746276 current acc: 0.9046260966439215\n",
      "iteration 81 current loss: 0.0021995946299284697 current acc: 0.9046393762183236\n",
      "iteration 82 current loss: 0.001782923354767263 current acc: 0.9046526520952248\n",
      "iteration 83 current loss: 0.0023261914029717445 current acc: 0.9046659242761693\n",
      "iteration 84 current loss: 0.003931064158678055 current acc: 0.9046791927627\n",
      "iteration 85 current loss: 0.0030264086090028286 current acc: 0.9046924575563596\n",
      "iteration 86 current loss: 0.0032740735914558172 current acc: 0.9047057186586893\n",
      "iteration 87 current loss: 0.0016547197010368109 current acc: 0.9047189760712299\n",
      "iteration 88 current loss: 0.0017617222620174289 current acc: 0.9047322297955209\n",
      "iteration 89 current loss: 0.002281604567542672 current acc: 0.9047454798331015\n",
      "iteration 90 current loss: 0.0022424734197556973 current acc: 0.9047587261855097\n",
      "iteration 91 current loss: 0.00313836638815701 current acc: 0.9047719688542826\n",
      "iteration 92 current loss: 0.0014611589722335339 current acc: 0.9047852078409565\n",
      "iteration 93 current loss: 0.0019341969164088368 current acc: 0.904798443147067\n",
      "iteration 94 current loss: 0.0021389934699982405 current acc: 0.9048116747741487\n",
      "iteration 95 current loss: 0.0025967175606638193 current acc: 0.9048249027237354\n",
      "iteration 96 current loss: 0.0016160366358235478 current acc: 0.90483812699736\n",
      "iteration 97 current loss: 0.003284408478066325 current acc: 0.9048513475965546\n",
      "iteration 98 current loss: 0.004185607191175222 current acc: 0.9048645645228504\n",
      "iteration 99 current loss: 0.003460420062765479 current acc: 0.9048777777777778\n",
      "\t\tEpoch 71/100 complete. Epoch loss 0.0027669293235521764 Epoch accuracy 1.0\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 71, Validation Accuracy: 0.623, Validation Loss: 1.7428149085491895\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.002900069346651435 current acc: 0.9048909873628662\n",
      "iteration 1 current loss: 0.0036693094298243523 current acc: 0.9049041932796446\n",
      "iteration 2 current loss: 0.0023810514248907566 current acc: 0.9049173955296405\n",
      "iteration 3 current loss: 0.0014489017194136977 current acc: 0.9049305941143809\n",
      "iteration 4 current loss: 0.0019375340780243278 current acc: 0.9049437890353921\n",
      "iteration 5 current loss: 0.0027088148053735495 current acc: 0.9049569802941992\n",
      "iteration 6 current loss: 0.0013571659801527858 current acc: 0.9049701678923269\n",
      "iteration 7 current loss: 0.002266727387905121 current acc: 0.9049833518312985\n",
      "iteration 8 current loss: 0.001540611032396555 current acc: 0.904996532112637\n",
      "iteration 9 current loss: 0.0018202512292191386 current acc: 0.9050097087378641\n",
      "iteration 10 current loss: 0.0018470328068360686 current acc: 0.9050228817085009\n",
      "iteration 11 current loss: 0.002086490159854293 current acc: 0.9050360510260677\n",
      "iteration 12 current loss: 0.002347322413697839 current acc: 0.9050492166920837\n",
      "iteration 13 current loss: 0.002022736705839634 current acc: 0.9050623787080676\n",
      "iteration 14 current loss: 0.001586712896823883 current acc: 0.9050755370755371\n",
      "iteration 15 current loss: 0.0025106307584792376 current acc: 0.9050886917960088\n",
      "iteration 16 current loss: 0.003733192104846239 current acc: 0.905101842870999\n",
      "iteration 17 current loss: 0.004005352035164833 current acc: 0.9051149903020227\n",
      "iteration 18 current loss: 0.00198868615552783 current acc: 0.9051281340905942\n",
      "iteration 19 current loss: 0.0019257793901488185 current acc: 0.9051412742382271\n",
      "iteration 20 current loss: 0.002369315130636096 current acc: 0.905154410746434\n",
      "iteration 21 current loss: 0.0018344696145504713 current acc: 0.9051675436167267\n",
      "iteration 22 current loss: 0.002234583254903555 current acc: 0.9051806728506161\n",
      "iteration 23 current loss: 0.0019468365935608745 current acc: 0.9051937984496125\n",
      "iteration 24 current loss: 0.002867824165150523 current acc: 0.9052069204152249\n",
      "iteration 25 current loss: 0.0024355361238121986 current acc: 0.9052200387489621\n",
      "iteration 26 current loss: 0.0025070321280509233 current acc: 0.9052331534523316\n",
      "iteration 27 current loss: 0.002845549490302801 current acc: 0.9052462645268401\n",
      "iteration 28 current loss: 0.00185053376480937 current acc: 0.9052593719739936\n",
      "iteration 29 current loss: 0.003052889369428158 current acc: 0.9052724757952973\n",
      "iteration 30 current loss: 0.0022387602366507053 current acc: 0.9052855759922556\n",
      "iteration 31 current loss: 0.002637297147884965 current acc: 0.9052986725663716\n",
      "iteration 32 current loss: 0.0034920864272862673 current acc: 0.9053117655191484\n",
      "iteration 33 current loss: 0.002477381145581603 current acc: 0.9053248548520874\n",
      "iteration 34 current loss: 0.0018969443626701832 current acc: 0.9053379405666897\n",
      "iteration 35 current loss: 0.0033380594104528427 current acc: 0.9053510226644556\n",
      "iteration 36 current loss: 0.0018524661427363753 current acc: 0.905364101146884\n",
      "iteration 37 current loss: 0.0022121036890894175 current acc: 0.9053771760154739\n",
      "iteration 38 current loss: 0.00176907773129642 current acc: 0.9053902472717226\n",
      "iteration 39 current loss: 0.0020050792954862118 current acc: 0.9054033149171271\n",
      "iteration 40 current loss: 0.0024639414623379707 current acc: 0.9054163789531833\n",
      "iteration 41 current loss: 0.0023715796414762735 current acc: 0.9054294393813863\n",
      "iteration 42 current loss: 0.0024794081691652536 current acc: 0.9054424962032307\n",
      "iteration 43 current loss: 0.0021817951928824186 current acc: 0.9054555494202098\n",
      "iteration 44 current loss: 0.0015052887611091137 current acc: 0.9054685990338164\n",
      "iteration 45 current loss: 0.0022547163534909487 current acc: 0.9054816450455424\n",
      "iteration 46 current loss: 0.0019291825592517853 current acc: 0.9054946874568787\n",
      "iteration 47 current loss: 0.0023826858960092068 current acc: 0.9055077262693156\n",
      "iteration 48 current loss: 0.002491255756467581 current acc: 0.9055207614843427\n",
      "iteration 49 current loss: 0.0013319147983565927 current acc: 0.9055337931034483\n",
      "iteration 50 current loss: 0.002423869911581278 current acc: 0.9055468211281202\n",
      "iteration 51 current loss: 0.0028650653548538685 current acc: 0.9055598455598456\n",
      "iteration 52 current loss: 0.0018333145417273045 current acc: 0.9055728664001103\n",
      "iteration 53 current loss: 0.0035739592276513577 current acc: 0.9055858836503998\n",
      "iteration 54 current loss: 0.0020552342757582664 current acc: 0.9055988973121984\n",
      "iteration 55 current loss: 0.002246271586045623 current acc: 0.90561190738699\n",
      "iteration 56 current loss: 0.001364764990285039 current acc: 0.9056249138762574\n",
      "iteration 57 current loss: 0.001803080434910953 current acc: 0.9056379167814825\n",
      "iteration 58 current loss: 0.0021219600457698107 current acc: 0.9056509161041466\n",
      "iteration 59 current loss: 0.0022345467004925013 current acc: 0.90566391184573\n",
      "iteration 60 current loss: 0.0022152482997626066 current acc: 0.9056769040077124\n",
      "iteration 61 current loss: 0.001681025605648756 current acc: 0.9056898925915726\n",
      "iteration 62 current loss: 0.0019562742672860622 current acc: 0.9057028775987884\n",
      "iteration 63 current loss: 0.001879695220850408 current acc: 0.905715859030837\n",
      "iteration 64 current loss: 0.0018941140733659267 current acc: 0.9057288368891948\n",
      "iteration 65 current loss: 0.0017722680931910872 current acc: 0.9057418111753371\n",
      "iteration 66 current loss: 0.002223171992227435 current acc: 0.905754781890739\n",
      "iteration 67 current loss: 0.003192245727404952 current acc: 0.905767749036874\n",
      "iteration 68 current loss: 0.0012565042125061154 current acc: 0.9057807126152153\n",
      "iteration 69 current loss: 0.0021390144247561693 current acc: 0.9057936726272352\n",
      "iteration 70 current loss: 0.0023690389934927225 current acc: 0.9058066290744051\n",
      "iteration 71 current loss: 0.0019797964487224817 current acc: 0.9058195819581958\n",
      "iteration 72 current loss: 0.0031956203747540712 current acc: 0.905832531280077\n",
      "iteration 73 current loss: 0.0019627211149781942 current acc: 0.9058454770415177\n",
      "iteration 74 current loss: 0.0025754361413419247 current acc: 0.9058584192439862\n",
      "iteration 75 current loss: 0.002776498906314373 current acc: 0.90587135788895\n",
      "iteration 76 current loss: 0.002178990514948964 current acc: 0.9058842929778755\n",
      "iteration 77 current loss: 0.002511415397748351 current acc: 0.9058972245122286\n",
      "iteration 78 current loss: 0.0018215771997347474 current acc: 0.9059101524934744\n",
      "iteration 79 current loss: 0.0016945122042670846 current acc: 0.9059230769230769\n",
      "iteration 80 current loss: 0.0014685356291010976 current acc: 0.9059359978024997\n",
      "iteration 81 current loss: 0.0020083144772797823 current acc: 0.9059489151332052\n",
      "iteration 82 current loss: 0.002553609199821949 current acc: 0.9059618289166552\n",
      "iteration 83 current loss: 0.001726407092064619 current acc: 0.9059747391543108\n",
      "iteration 84 current loss: 0.0017912748735398054 current acc: 0.9059876458476321\n",
      "iteration 85 current loss: 0.002086983062326908 current acc: 0.9060005489980785\n",
      "iteration 86 current loss: 0.0030528635252267122 current acc: 0.9060134486071085\n",
      "iteration 87 current loss: 0.0021686821710318327 current acc: 0.90602634467618\n",
      "iteration 88 current loss: 0.0034705433063209057 current acc: 0.90603923720675\n",
      "iteration 89 current loss: 0.001329532591626048 current acc: 0.9060521262002743\n",
      "iteration 90 current loss: 0.0014974779915064573 current acc: 0.9060650116582087\n",
      "iteration 91 current loss: 0.002236494794487953 current acc: 0.9060778935820076\n",
      "iteration 92 current loss: 0.0019187164725735784 current acc: 0.9060907719731249\n",
      "iteration 93 current loss: 0.0018295907648280263 current acc: 0.9061036468330135\n",
      "iteration 94 current loss: 0.002895965473726392 current acc: 0.9061165181631254\n",
      "iteration 95 current loss: 0.003691930091008544 current acc: 0.9061293859649123\n",
      "iteration 96 current loss: 0.0027453291695564985 current acc: 0.9061422502398245\n",
      "iteration 97 current loss: 0.003740002866834402 current acc: 0.9061551109893121\n",
      "iteration 98 current loss: 0.0012126137735322118 current acc: 0.9061679682148239\n",
      "iteration 99 current loss: 0.002199647482484579 current acc: 0.9061808219178082\n",
      "\t\tEpoch 72/100 complete. Epoch loss 0.002267617167672142 Epoch accuracy 1.0\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 72, Validation Accuracy: 0.623625, Validation Loss: 1.7657652270048856\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.0015900779981166124 current acc: 0.9061936720997124\n",
      "iteration 1 current loss: 0.0021433501970022917 current acc: 0.906206518761983\n",
      "iteration 2 current loss: 0.002077622339129448 current acc: 0.906219361906066\n",
      "iteration 3 current loss: 0.0029364803340286016 current acc: 0.9062322015334063\n",
      "iteration 4 current loss: 0.0015234467573463917 current acc: 0.9062450376454483\n",
      "iteration 5 current loss: 0.0009895090479403734 current acc: 0.9062578702436354\n",
      "iteration 6 current loss: 0.0015843692235648632 current acc: 0.9062706993294102\n",
      "iteration 7 current loss: 0.002006536815315485 current acc: 0.9062835249042146\n",
      "iteration 8 current loss: 0.0020021446980535984 current acc: 0.9062963469694897\n",
      "iteration 9 current loss: 0.002535719657316804 current acc: 0.9063091655266758\n",
      "iteration 10 current loss: 0.0018291480373591185 current acc: 0.9063219805772125\n",
      "iteration 11 current loss: 0.001944700488820672 current acc: 0.9063347921225383\n",
      "iteration 12 current loss: 0.003179022343829274 current acc: 0.9063476001640913\n",
      "iteration 13 current loss: 0.0027861965354532003 current acc: 0.9063604047033087\n",
      "iteration 14 current loss: 0.0017654496477916837 current acc: 0.9063732057416268\n",
      "iteration 15 current loss: 0.0018736436031758785 current acc: 0.9063860032804811\n",
      "iteration 16 current loss: 0.0014945092843845487 current acc: 0.9063987973213066\n",
      "iteration 17 current loss: 0.001964832656085491 current acc: 0.906411587865537\n",
      "iteration 18 current loss: 0.0030686145182698965 current acc: 0.9064243749146058\n",
      "iteration 19 current loss: 0.0023284575436264277 current acc: 0.9064371584699453\n",
      "iteration 20 current loss: 0.0015504243783652782 current acc: 0.9064499385329873\n",
      "iteration 21 current loss: 0.0014604887692257762 current acc: 0.9064627151051625\n",
      "iteration 22 current loss: 0.0014611503574997187 current acc: 0.9064754881879011\n",
      "iteration 23 current loss: 0.0020808964036405087 current acc: 0.9064882577826324\n",
      "iteration 24 current loss: 0.002122245030477643 current acc: 0.906501023890785\n",
      "iteration 25 current loss: 0.001143772853538394 current acc: 0.9065137865137866\n",
      "iteration 26 current loss: 0.001676787855103612 current acc: 0.906526545653064\n",
      "iteration 27 current loss: 0.0030515282414853573 current acc: 0.9065393013100437\n",
      "iteration 28 current loss: 0.0014803903177380562 current acc: 0.9065520534861509\n",
      "iteration 29 current loss: 0.002549676224589348 current acc: 0.9065648021828103\n",
      "iteration 30 current loss: 0.0021973249968141317 current acc: 0.9065775474014459\n",
      "iteration 31 current loss: 0.0023194646928459406 current acc: 0.9065902891434806\n",
      "iteration 32 current loss: 0.0016306639881804585 current acc: 0.9066030274103368\n",
      "iteration 33 current loss: 0.0019827906507998705 current acc: 0.9066157622034361\n",
      "iteration 34 current loss: 0.002191899809986353 current acc: 0.906628493524199\n",
      "iteration 35 current loss: 0.002192653017118573 current acc: 0.9066412213740458\n",
      "iteration 36 current loss: 0.00119964184705168 current acc: 0.9066539457543955\n",
      "iteration 37 current loss: 0.007481600623577833 current acc: 0.9066666666666666\n",
      "iteration 38 current loss: 0.0016726456815376878 current acc: 0.9066793841122769\n",
      "iteration 39 current loss: 0.0016624212730675936 current acc: 0.9066920980926431\n",
      "iteration 40 current loss: 0.0013538694474846125 current acc: 0.9067048086091813\n",
      "iteration 41 current loss: 0.0013951213331893086 current acc: 0.906717515663307\n",
      "iteration 42 current loss: 0.0023976608645170927 current acc: 0.9067302192564347\n",
      "iteration 43 current loss: 0.0012692948803305626 current acc: 0.9067429193899782\n",
      "iteration 44 current loss: 0.001741770189255476 current acc: 0.9067556160653506\n",
      "iteration 45 current loss: 0.0024571926333010197 current acc: 0.906768309283964\n",
      "iteration 46 current loss: 0.0015189845580607653 current acc: 0.9067809990472302\n",
      "iteration 47 current loss: 0.0018687744159251451 current acc: 0.9067936853565596\n",
      "iteration 48 current loss: 0.00171079917345196 current acc: 0.9068063682133624\n",
      "iteration 49 current loss: 0.0020523264538496733 current acc: 0.9068190476190476\n",
      "iteration 50 current loss: 0.0015228037955239415 current acc: 0.9068317235750238\n",
      "iteration 51 current loss: 0.003546128049492836 current acc: 0.9068443960826986\n",
      "iteration 52 current loss: 0.0025386044289916754 current acc: 0.9068570651434789\n",
      "iteration 53 current loss: 0.0021847854368388653 current acc: 0.9068697307587708\n",
      "iteration 54 current loss: 0.002210154663771391 current acc: 0.9068823929299796\n",
      "iteration 55 current loss: 0.004578752443194389 current acc: 0.90689505165851\n",
      "iteration 56 current loss: 0.0025614630430936813 current acc: 0.906907706945766\n",
      "iteration 57 current loss: 0.002574904588982463 current acc: 0.9069203587931504\n",
      "iteration 58 current loss: 0.003802444087341428 current acc: 0.9069330072020655\n",
      "iteration 59 current loss: 0.001616692985408008 current acc: 0.9069456521739131\n",
      "iteration 60 current loss: 0.002409554086625576 current acc: 0.9069582937100937\n",
      "iteration 61 current loss: 0.0020771189592778683 current acc: 0.9069709318120076\n",
      "iteration 62 current loss: 0.0017644433537498116 current acc: 0.9069835664810539\n",
      "iteration 63 current loss: 0.002011003438383341 current acc: 0.9069961977186312\n",
      "iteration 64 current loss: 0.002559416927397251 current acc: 0.9070088255261372\n",
      "iteration 65 current loss: 0.0022239566314965487 current acc: 0.9070214499049688\n",
      "iteration 66 current loss: 0.001303090713918209 current acc: 0.9070340708565223\n",
      "iteration 67 current loss: 0.0018763248808681965 current acc: 0.9070466883821933\n",
      "iteration 68 current loss: 0.001991736004129052 current acc: 0.9070593024833763\n",
      "iteration 69 current loss: 0.0021670835558325052 current acc: 0.9070719131614654\n",
      "iteration 70 current loss: 0.0033071183133870363 current acc: 0.9070845204178537\n",
      "iteration 71 current loss: 0.0017594051314517856 current acc: 0.9070971242539339\n",
      "iteration 72 current loss: 0.0018308247672393918 current acc: 0.9071097246710973\n",
      "iteration 73 current loss: 0.0026257899589836597 current acc: 0.907122321670735\n",
      "iteration 74 current loss: 0.001915364759042859 current acc: 0.9071349152542373\n",
      "iteration 75 current loss: 0.0013455625157803297 current acc: 0.9071475054229935\n",
      "iteration 76 current loss: 0.0013677867827937007 current acc: 0.9071600921783923\n",
      "iteration 77 current loss: 0.0016286863246932626 current acc: 0.9071726755218217\n",
      "iteration 78 current loss: 0.0018332492327317595 current acc: 0.9071852554546687\n",
      "iteration 79 current loss: 0.0011669746600091457 current acc: 0.9071978319783198\n",
      "iteration 80 current loss: 0.002830218058079481 current acc: 0.9072104050941607\n",
      "iteration 81 current loss: 0.0020204412285238504 current acc: 0.9072229748035763\n",
      "iteration 82 current loss: 0.0019083231454715133 current acc: 0.9072355411079507\n",
      "iteration 83 current loss: 0.0018728806171566248 current acc: 0.9072481040086674\n",
      "iteration 84 current loss: 0.0013747282791882753 current acc: 0.907260663507109\n",
      "iteration 85 current loss: 0.0023290088865906 current acc: 0.9072732196046575\n",
      "iteration 86 current loss: 0.00137700408231467 current acc: 0.9072857723026939\n",
      "iteration 87 current loss: 0.002752118743956089 current acc: 0.9072983216025988\n",
      "iteration 88 current loss: 0.0013670623302459717 current acc: 0.9073108675057517\n",
      "iteration 89 current loss: 0.001409872667863965 current acc: 0.9073234100135318\n",
      "iteration 90 current loss: 0.001864358433522284 current acc: 0.907335949127317\n",
      "iteration 91 current loss: 0.0017473612679168582 current acc: 0.9073484848484848\n",
      "iteration 92 current loss: 0.0013895004522055387 current acc: 0.907361017178412\n",
      "iteration 93 current loss: 0.0017013095784932375 current acc: 0.9073735461184744\n",
      "iteration 94 current loss: 0.0016113874735310674 current acc: 0.9073860716700474\n",
      "iteration 95 current loss: 0.002168802311643958 current acc: 0.9073985938345052\n",
      "iteration 96 current loss: 0.0013781423913314939 current acc: 0.9074111126132216\n",
      "iteration 97 current loss: 0.0014208744978532195 current acc: 0.9074236280075696\n",
      "iteration 98 current loss: 0.001617813017219305 current acc: 0.9074361400189215\n",
      "iteration 99 current loss: 0.0022092037834227085 current acc: 0.9074486486486486\n",
      "\t\tEpoch 73/100 complete. Epoch loss 0.0020514972845558075 Epoch accuracy 1.0\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 73, Validation Accuracy: 0.620875, Validation Loss: 1.7696351800113916\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.0012310142628848553 current acc: 0.9074611538981219\n",
      "iteration 1 current loss: 0.0031791268847882748 current acc: 0.9074736557687112\n",
      "iteration 2 current loss: 0.004517962224781513 current acc: 0.9074861542617858\n",
      "iteration 3 current loss: 0.0013159242225810885 current acc: 0.9074986493787142\n",
      "iteration 4 current loss: 0.0024452530778944492 current acc: 0.9075111411208643\n",
      "iteration 5 current loss: 0.0013509036507457495 current acc: 0.9075236294896031\n",
      "iteration 6 current loss: 0.0010140965459868312 current acc: 0.9075361144862968\n",
      "iteration 7 current loss: 0.0009785647271201015 current acc: 0.907548596112311\n",
      "iteration 8 current loss: 0.001691230689175427 current acc: 0.9075610743690107\n",
      "iteration 9 current loss: 0.0020191017538309097 current acc: 0.9075735492577598\n",
      "iteration 10 current loss: 0.0017745211953297257 current acc: 0.9075860207799218\n",
      "iteration 11 current loss: 0.0009806066518649459 current acc: 0.9075984889368591\n",
      "iteration 12 current loss: 0.002436930313706398 current acc: 0.907610953729934\n",
      "iteration 13 current loss: 0.002094653435051441 current acc: 0.9076234151605072\n",
      "iteration 14 current loss: 0.0018108743242919445 current acc: 0.9076358732299393\n",
      "iteration 15 current loss: 0.00182810437399894 current acc: 0.90764832793959\n",
      "iteration 16 current loss: 0.0010398192098364234 current acc: 0.9076607792908183\n",
      "iteration 17 current loss: 0.0015284232795238495 current acc: 0.9076732272849825\n",
      "iteration 18 current loss: 0.0017398505005985498 current acc: 0.9076856719234399\n",
      "iteration 19 current loss: 0.0016482477076351643 current acc: 0.9076981132075471\n",
      "iteration 20 current loss: 0.001297365757636726 current acc: 0.9077105511386605\n",
      "iteration 21 current loss: 0.002611918142065406 current acc: 0.9077229857181353\n",
      "iteration 22 current loss: 0.0013183440314605832 current acc: 0.9077354169473258\n",
      "iteration 23 current loss: 0.001405061804689467 current acc: 0.9077478448275862\n",
      "iteration 24 current loss: 0.00214597606100142 current acc: 0.9077602693602693\n",
      "iteration 25 current loss: 0.0018945630872622132 current acc: 0.9077726905467277\n",
      "iteration 26 current loss: 0.0013508646516129375 current acc: 0.907785108388313\n",
      "iteration 27 current loss: 0.002980878110975027 current acc: 0.9077975228863758\n",
      "iteration 28 current loss: 0.0009663344826549292 current acc: 0.9078099340422668\n",
      "iteration 29 current loss: 0.0016241857083514333 current acc: 0.9078223418573351\n",
      "iteration 30 current loss: 0.0021196180023252964 current acc: 0.9078347463329296\n",
      "iteration 31 current loss: 0.0011595550458878279 current acc: 0.9078471474703983\n",
      "iteration 32 current loss: 0.0012318291701376438 current acc: 0.9078595452710884\n",
      "iteration 33 current loss: 0.0015271528391167521 current acc: 0.9078719397363465\n",
      "iteration 34 current loss: 0.0015799582470208406 current acc: 0.9078843308675185\n",
      "iteration 35 current loss: 0.002430552616715431 current acc: 0.9078967186659495\n",
      "iteration 36 current loss: 0.0019909124821424484 current acc: 0.9079091031329837\n",
      "iteration 37 current loss: 0.00138527387753129 current acc: 0.907921484269965\n",
      "iteration 38 current loss: 0.001789732719771564 current acc: 0.9079338620782363\n",
      "iteration 39 current loss: 0.0015492720995098352 current acc: 0.9079462365591398\n",
      "iteration 40 current loss: 0.0011811131844297051 current acc: 0.9079586077140169\n",
      "iteration 41 current loss: 0.001908923964947462 current acc: 0.9079709755442086\n",
      "iteration 42 current loss: 0.0012697051279246807 current acc: 0.9079833400510546\n",
      "iteration 43 current loss: 0.0020719331223517656 current acc: 0.9079957012358947\n",
      "iteration 44 current loss: 0.0016616454813629389 current acc: 0.9080080591000672\n",
      "iteration 45 current loss: 0.0021940350998193026 current acc: 0.90802041364491\n",
      "iteration 46 current loss: 0.0017498002853244543 current acc: 0.9080327648717604\n",
      "iteration 47 current loss: 0.00118232611566782 current acc: 0.9080451127819549\n",
      "iteration 48 current loss: 0.001996360020712018 current acc: 0.9080574573768291\n",
      "iteration 49 current loss: 0.001438806182704866 current acc: 0.9080697986577181\n",
      "iteration 50 current loss: 0.0014618741115555167 current acc: 0.9080821366259563\n",
      "iteration 51 current loss: 0.0010536682093515992 current acc: 0.9080944712828771\n",
      "iteration 52 current loss: 0.001894572633318603 current acc: 0.9081068026298135\n",
      "iteration 53 current loss: 0.0010082097724080086 current acc: 0.9081191306680977\n",
      "iteration 54 current loss: 0.0017241360619664192 current acc: 0.908131455399061\n",
      "iteration 55 current loss: 0.0014425519620999694 current acc: 0.9081437768240344\n",
      "iteration 56 current loss: 0.0013631279580295086 current acc: 0.9081560949443476\n",
      "iteration 57 current loss: 0.0019082257058471441 current acc: 0.9081684097613301\n",
      "iteration 58 current loss: 0.0018592196283861995 current acc: 0.9081807212763104\n",
      "iteration 59 current loss: 0.0016074877930805087 current acc: 0.9081930294906166\n",
      "iteration 60 current loss: 0.0014841073425486684 current acc: 0.9082053344055757\n",
      "iteration 61 current loss: 0.0015364921418949962 current acc: 0.908217636022514\n",
      "iteration 62 current loss: 0.0011099522234871984 current acc: 0.9082299343427576\n",
      "iteration 63 current loss: 0.0009975382126867771 current acc: 0.9082422293676313\n",
      "iteration 64 current loss: 0.002115140901878476 current acc: 0.9082545210984595\n",
      "iteration 65 current loss: 0.0016671167686581612 current acc: 0.9082668095365658\n",
      "iteration 66 current loss: 0.0021579742897301912 current acc: 0.9082790946832731\n",
      "iteration 67 current loss: 0.001533380476757884 current acc: 0.9082913765399035\n",
      "iteration 68 current loss: 0.0011687842197716236 current acc: 0.9083036551077788\n",
      "iteration 69 current loss: 0.001315760426223278 current acc: 0.9083159303882196\n",
      "iteration 70 current loss: 0.0010557974455878139 current acc: 0.9083282023825459\n",
      "iteration 71 current loss: 0.0022677353117614985 current acc: 0.908340471092077\n",
      "iteration 72 current loss: 0.001320984330959618 current acc: 0.908352736518132\n",
      "iteration 73 current loss: 0.0023048208095133305 current acc: 0.9083649986620284\n",
      "iteration 74 current loss: 0.0013123848475515842 current acc: 0.9083772575250836\n",
      "iteration 75 current loss: 0.001761475927196443 current acc: 0.9083895131086143\n",
      "iteration 76 current loss: 0.0014904539566487074 current acc: 0.908401765413936\n",
      "iteration 77 current loss: 0.003301373217254877 current acc: 0.9084140144423642\n",
      "iteration 78 current loss: 0.001354995765723288 current acc: 0.9084262601952132\n",
      "iteration 79 current loss: 0.0025766065809875727 current acc: 0.9084385026737968\n",
      "iteration 80 current loss: 0.0012524526100605726 current acc: 0.9084507418794279\n",
      "iteration 81 current loss: 0.0011555181117728353 current acc: 0.9084629778134189\n",
      "iteration 82 current loss: 0.0015091855311766267 current acc: 0.9084752104770814\n",
      "iteration 83 current loss: 0.0022467838134616613 current acc: 0.9084874398717263\n",
      "iteration 84 current loss: 0.0013017653254792094 current acc: 0.9084996659986639\n",
      "iteration 85 current loss: 0.0020309865940362215 current acc: 0.9085118888592039\n",
      "iteration 86 current loss: 0.002399028278887272 current acc: 0.9085241084546547\n",
      "iteration 87 current loss: 0.0018056790577247739 current acc: 0.9085363247863247\n",
      "iteration 88 current loss: 0.0015771022299304605 current acc: 0.9085485378555215\n",
      "iteration 89 current loss: 0.0017516958760097623 current acc: 0.9085607476635514\n",
      "iteration 90 current loss: 0.002448875457048416 current acc: 0.9085729542117207\n",
      "iteration 91 current loss: 0.0018674121238291264 current acc: 0.9085851575013347\n",
      "iteration 92 current loss: 0.001290059182792902 current acc: 0.9085973575336981\n",
      "iteration 93 current loss: 0.0023363481741398573 current acc: 0.9086095543101148\n",
      "iteration 94 current loss: 0.0014558969996869564 current acc: 0.9086217478318879\n",
      "iteration 95 current loss: 0.0013058033073320985 current acc: 0.9086339381003201\n",
      "iteration 96 current loss: 0.0011445814743638039 current acc: 0.9086461251167134\n",
      "iteration 97 current loss: 0.0020274510607123375 current acc: 0.9086583088823686\n",
      "iteration 98 current loss: 0.0014567006146535277 current acc: 0.9086704893985865\n",
      "iteration 99 current loss: 0.0014504402643069625 current acc: 0.9086826666666666\n",
      "\t\tEpoch 74/100 complete. Epoch loss 0.001706089216750115 Epoch accuracy 1.0\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 74, Validation Accuracy: 0.623125, Validation Loss: 1.7800485204905272\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.0010060727363452315 current acc: 0.9086948406879083\n",
      "iteration 1 current loss: 0.0015715130139142275 current acc: 0.9087070114636097\n",
      "iteration 2 current loss: 0.001660698908381164 current acc: 0.9087191789950686\n",
      "iteration 3 current loss: 0.0012430496281012893 current acc: 0.9087313432835821\n",
      "iteration 4 current loss: 0.0010980082442983985 current acc: 0.9087435043304464\n",
      "iteration 5 current loss: 0.0017130655469372869 current acc: 0.9087556621369571\n",
      "iteration 6 current loss: 0.001576656592078507 current acc: 0.9087678167044092\n",
      "iteration 7 current loss: 0.0008923821151256561 current acc: 0.908779968034097\n",
      "iteration 8 current loss: 0.0017814845778048038 current acc: 0.9087921161273139\n",
      "iteration 9 current loss: 0.001279445830732584 current acc: 0.9088042609853528\n",
      "iteration 10 current loss: 0.0019611746538430452 current acc: 0.908816402609506\n",
      "iteration 11 current loss: 0.002353282179683447 current acc: 0.9088285410010649\n",
      "iteration 12 current loss: 0.0011852887691929936 current acc: 0.9088406761613204\n",
      "iteration 13 current loss: 0.0012049094075337052 current acc: 0.9088528080915624\n",
      "iteration 14 current loss: 0.002365815918892622 current acc: 0.9088649367930806\n",
      "iteration 15 current loss: 0.0011457675136625767 current acc: 0.9088770622671634\n",
      "iteration 16 current loss: 0.0020778358448296785 current acc: 0.9088891845150991\n",
      "iteration 17 current loss: 0.0023098301608115435 current acc: 0.908901303538175\n",
      "iteration 18 current loss: 0.001917141256853938 current acc: 0.9089134193376779\n",
      "iteration 19 current loss: 0.0010789468651637435 current acc: 0.9089255319148937\n",
      "iteration 20 current loss: 0.0011384353274479508 current acc: 0.9089376412711075\n",
      "iteration 21 current loss: 0.0014089267933741212 current acc: 0.9089497474076044\n",
      "iteration 22 current loss: 0.0026176280807703733 current acc: 0.9089618503256679\n",
      "iteration 23 current loss: 0.0019876512233167887 current acc: 0.9089739500265817\n",
      "iteration 24 current loss: 0.0019028972601518035 current acc: 0.908986046511628\n",
      "iteration 25 current loss: 0.0016197818331420422 current acc: 0.9089981397820888\n",
      "iteration 26 current loss: 0.0017008355353027582 current acc: 0.9090102298392454\n",
      "iteration 27 current loss: 0.0016100305365398526 current acc: 0.9090223166843783\n",
      "iteration 28 current loss: 0.0012429544003680348 current acc: 0.9090344003187675\n",
      "iteration 29 current loss: 0.0021792135667055845 current acc: 0.9090464807436919\n",
      "iteration 30 current loss: 0.0012363727437332273 current acc: 0.9090585579604302\n",
      "iteration 31 current loss: 0.0010544739197939634 current acc: 0.9090706319702602\n",
      "iteration 32 current loss: 0.001471976051107049 current acc: 0.909082702774459\n",
      "iteration 33 current loss: 0.001084033166989684 current acc: 0.9090947703743032\n",
      "iteration 34 current loss: 0.00223534950055182 current acc: 0.9091068347710684\n",
      "iteration 35 current loss: 0.00134453980717808 current acc: 0.9091188959660297\n",
      "iteration 36 current loss: 0.001591709442436695 current acc: 0.9091309539604617\n",
      "iteration 37 current loss: 0.0009873181115835905 current acc: 0.9091430087556381\n",
      "iteration 38 current loss: 0.001186399138532579 current acc: 0.9091550603528319\n",
      "iteration 39 current loss: 0.0013311603106558323 current acc: 0.9091671087533156\n",
      "iteration 40 current loss: 0.0016678671818226576 current acc: 0.9091791539583609\n",
      "iteration 41 current loss: 0.0014796748291701078 current acc: 0.9091911959692389\n",
      "iteration 42 current loss: 0.0018358792876824737 current acc: 0.90920323478722\n",
      "iteration 43 current loss: 0.001494350377470255 current acc: 0.9092152704135738\n",
      "iteration 44 current loss: 0.001147091737948358 current acc: 0.9092273028495692\n",
      "iteration 45 current loss: 0.0009923967299982905 current acc: 0.909239332096475\n",
      "iteration 46 current loss: 0.0012941036839038134 current acc: 0.9092513581555585\n",
      "iteration 47 current loss: 0.001199688296765089 current acc: 0.9092633810280869\n",
      "iteration 48 current loss: 0.002109029097482562 current acc: 0.9092754007153265\n",
      "iteration 49 current loss: 0.002491358434781432 current acc: 0.909287417218543\n",
      "iteration 50 current loss: 0.0015869273338466883 current acc: 0.9092994305390014\n",
      "iteration 51 current loss: 0.0024660127237439156 current acc: 0.9093114406779661\n",
      "iteration 52 current loss: 0.0016414261190220714 current acc: 0.9093234476367007\n",
      "iteration 53 current loss: 0.0017656049458310008 current acc: 0.9093354514164681\n",
      "iteration 54 current loss: 0.001110640587285161 current acc: 0.9093474520185307\n",
      "iteration 55 current loss: 0.001795687829144299 current acc: 0.9093594494441504\n",
      "iteration 56 current loss: 0.0018686330877244473 current acc: 0.9093714436945878\n",
      "iteration 57 current loss: 0.0016742789885029197 current acc: 0.9093834347711035\n",
      "iteration 58 current loss: 0.0012342373374849558 current acc: 0.909395422674957\n",
      "iteration 59 current loss: 0.0013519449857994914 current acc: 0.9094074074074074\n",
      "iteration 60 current loss: 0.0019292496144771576 current acc: 0.909419388969713\n",
      "iteration 61 current loss: 0.000925663800444454 current acc: 0.9094313673631315\n",
      "iteration 62 current loss: 0.0014610834186896682 current acc: 0.9094433425889198\n",
      "iteration 63 current loss: 0.003625039476901293 current acc: 0.9094553146483342\n",
      "iteration 64 current loss: 0.001195029355585575 current acc: 0.9094672835426305\n",
      "iteration 65 current loss: 0.0019315349636599422 current acc: 0.9094792492730637\n",
      "iteration 66 current loss: 0.0009651761502027512 current acc: 0.9094912118408881\n",
      "iteration 67 current loss: 0.00111494364682585 current acc: 0.9095031712473572\n",
      "iteration 68 current loss: 0.001766952220350504 current acc: 0.9095151274937244\n",
      "iteration 69 current loss: 0.001341979019343853 current acc: 0.9095270805812418\n",
      "iteration 70 current loss: 0.0014036521315574646 current acc: 0.909539030511161\n",
      "iteration 71 current loss: 0.0012931674718856812 current acc: 0.9095509772847332\n",
      "iteration 72 current loss: 0.0008352377917617559 current acc: 0.9095629209032088\n",
      "iteration 73 current loss: 0.0015140649629756808 current acc: 0.9095748613678374\n",
      "iteration 74 current loss: 0.00151346146594733 current acc: 0.909586798679868\n",
      "iteration 75 current loss: 0.0011261773761361837 current acc: 0.9095987328405492\n",
      "iteration 76 current loss: 0.0021672151051461697 current acc: 0.9096106638511284\n",
      "iteration 77 current loss: 0.0019506989046931267 current acc: 0.909622591712853\n",
      "iteration 78 current loss: 0.0016624208074063063 current acc: 0.9096345164269692\n",
      "iteration 79 current loss: 0.0024261013604700565 current acc: 0.909646437994723\n",
      "iteration 80 current loss: 0.0014115965459495783 current acc: 0.9096583564173591\n",
      "iteration 81 current loss: 0.0018815285293385386 current acc: 0.9096702716961224\n",
      "iteration 82 current loss: 0.0015506567433476448 current acc: 0.9096821838322564\n",
      "iteration 83 current loss: 0.0011471532052382827 current acc: 0.9096940928270042\n",
      "iteration 84 current loss: 0.001009503728710115 current acc: 0.9097059986816084\n",
      "iteration 85 current loss: 0.0015292743919417262 current acc: 0.9097179013973108\n",
      "iteration 86 current loss: 0.0009523172047920525 current acc: 0.9097298009753526\n",
      "iteration 87 current loss: 0.0014599914429709315 current acc: 0.9097416974169742\n",
      "iteration 88 current loss: 0.0016550656873732805 current acc: 0.9097535907234154\n",
      "iteration 89 current loss: 0.001287846826016903 current acc: 0.9097654808959157\n",
      "iteration 90 current loss: 0.0013294762466102839 current acc: 0.9097773679357134\n",
      "iteration 91 current loss: 0.0026458734646439552 current acc: 0.9097892518440464\n",
      "iteration 92 current loss: 0.0015369816683232784 current acc: 0.909801132622152\n",
      "iteration 93 current loss: 0.0014831209555268288 current acc: 0.9098130102712668\n",
      "iteration 94 current loss: 0.00103733129799366 current acc: 0.9098248847926267\n",
      "iteration 95 current loss: 0.0012787787709385157 current acc: 0.9098367561874671\n",
      "iteration 96 current loss: 0.0014047999866306782 current acc: 0.9098486244570225\n",
      "iteration 97 current loss: 0.0024188982788473368 current acc: 0.909860489602527\n",
      "iteration 98 current loss: 0.0014282390475273132 current acc: 0.9098723516252138\n",
      "iteration 99 current loss: 0.0017679608426988125 current acc: 0.9098842105263157\n",
      "\t\tEpoch 75/100 complete. Epoch loss 0.0015692613401915878 Epoch accuracy 1.0\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 75, Validation Accuracy: 0.623, Validation Loss: 1.7894099559634924\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.0014963800786063075 current acc: 0.9098960663070649\n",
      "iteration 1 current loss: 0.0011820733780041337 current acc: 0.9099079189686925\n",
      "iteration 2 current loss: 0.0011977794347330928 current acc: 0.9099197685124293\n",
      "iteration 3 current loss: 0.0014856953639537096 current acc: 0.9099316149395055\n",
      "iteration 4 current loss: 0.0019415216520428658 current acc: 0.9099434582511505\n",
      "iteration 5 current loss: 0.00135498505551368 current acc: 0.9099552984485932\n",
      "iteration 6 current loss: 0.0010001789778470993 current acc: 0.9099671355330616\n",
      "iteration 7 current loss: 0.0011775889433920383 current acc: 0.9099789695057834\n",
      "iteration 8 current loss: 0.00301757687702775 current acc: 0.9099908003679853\n",
      "iteration 9 current loss: 0.002342951251193881 current acc: 0.9100026281208936\n",
      "iteration 10 current loss: 0.001100753783248365 current acc: 0.9100144527657338\n",
      "iteration 11 current loss: 0.001417664927430451 current acc: 0.910026274303731\n",
      "iteration 12 current loss: 0.0009887892520055175 current acc: 0.9100380927361092\n",
      "iteration 13 current loss: 0.0016579304356127977 current acc: 0.9100499080640925\n",
      "iteration 14 current loss: 0.0010586056159809232 current acc: 0.9100617202889035\n",
      "iteration 15 current loss: 0.001361998962238431 current acc: 0.9100735294117647\n",
      "iteration 16 current loss: 0.0017216266132891178 current acc: 0.9100853354338979\n",
      "iteration 17 current loss: 0.00136404181830585 current acc: 0.9100971383565241\n",
      "iteration 18 current loss: 0.0017625581240281463 current acc: 0.9101089381808636\n",
      "iteration 19 current loss: 0.0015681985532864928 current acc: 0.9101207349081365\n",
      "iteration 20 current loss: 0.0016237442614510655 current acc: 0.9101325285395617\n",
      "iteration 21 current loss: 0.001924567623063922 current acc: 0.910144319076358\n",
      "iteration 22 current loss: 0.0010260831331834197 current acc: 0.9101561065197429\n",
      "iteration 23 current loss: 0.0008083372376859188 current acc: 0.9101678908709339\n",
      "iteration 24 current loss: 0.00113153294660151 current acc: 0.9101796721311476\n",
      "iteration 25 current loss: 0.0015789377503097057 current acc: 0.9101914503015998\n",
      "iteration 26 current loss: 0.0014327935641631484 current acc: 0.910203225383506\n",
      "iteration 27 current loss: 0.0013435889268293977 current acc: 0.9102149973780808\n",
      "iteration 28 current loss: 0.0016949705313891172 current acc: 0.9102267662865382\n",
      "iteration 29 current loss: 0.003129035234451294 current acc: 0.9102385321100918\n",
      "iteration 30 current loss: 0.001456247759051621 current acc: 0.9102502948499541\n",
      "iteration 31 current loss: 0.0013039930490776896 current acc: 0.9102620545073375\n",
      "iteration 32 current loss: 0.0012414283119142056 current acc: 0.9102738110834534\n",
      "iteration 33 current loss: 0.0010014537256211042 current acc: 0.9102855645795127\n",
      "iteration 34 current loss: 0.0022668587043881416 current acc: 0.9102973149967256\n",
      "iteration 35 current loss: 0.0009507476352155209 current acc: 0.9103090623363017\n",
      "iteration 36 current loss: 0.0012514718109741807 current acc: 0.91032080659945\n",
      "iteration 37 current loss: 0.0011832895688712597 current acc: 0.9103325477873789\n",
      "iteration 38 current loss: 0.0008783587254583836 current acc: 0.910344285901296\n",
      "iteration 39 current loss: 0.0017122513381764293 current acc: 0.9103560209424084\n",
      "iteration 40 current loss: 0.00119403179269284 current acc: 0.9103677529119225\n",
      "iteration 41 current loss: 0.0011865142732858658 current acc: 0.9103794818110442\n",
      "iteration 42 current loss: 0.0014587506884709 current acc: 0.9103912076409787\n",
      "iteration 43 current loss: 0.001709884381853044 current acc: 0.9104029304029304\n",
      "iteration 44 current loss: 0.0017982119461521506 current acc: 0.9104146500981033\n",
      "iteration 45 current loss: 0.0010289634810760617 current acc: 0.9104263667277007\n",
      "iteration 46 current loss: 0.0018257289193570614 current acc: 0.9104380802929253\n",
      "iteration 47 current loss: 0.0017563430592417717 current acc: 0.9104497907949791\n",
      "iteration 48 current loss: 0.0008684466010890901 current acc: 0.9104614982350634\n",
      "iteration 49 current loss: 0.0010140998056158423 current acc: 0.9104732026143791\n",
      "iteration 50 current loss: 0.0013161151437088847 current acc: 0.9104849039341263\n",
      "iteration 51 current loss: 0.002728673629462719 current acc: 0.9104966021955044\n",
      "iteration 52 current loss: 0.0017468816367909312 current acc: 0.9105082973997125\n",
      "iteration 53 current loss: 0.0014326544478535652 current acc: 0.9105199895479488\n",
      "iteration 54 current loss: 0.0017444324912503362 current acc: 0.9105316786414108\n",
      "iteration 55 current loss: 0.0013505172682926059 current acc: 0.9105433646812957\n",
      "iteration 56 current loss: 0.001499892445281148 current acc: 0.9105550476687998\n",
      "iteration 57 current loss: 0.0016780996229499578 current acc: 0.9105667276051188\n",
      "iteration 58 current loss: 0.0008966628229245543 current acc: 0.910578404491448\n",
      "iteration 59 current loss: 0.001058786641806364 current acc: 0.9105900783289818\n",
      "iteration 60 current loss: 0.001253681955859065 current acc: 0.910601749118914\n",
      "iteration 61 current loss: 0.0013272874057292938 current acc: 0.910613416862438\n",
      "iteration 62 current loss: 0.0015979064628481865 current acc: 0.9106250815607464\n",
      "iteration 63 current loss: 0.001259937766008079 current acc: 0.9106367432150313\n",
      "iteration 64 current loss: 0.001743299188092351 current acc: 0.910648401826484\n",
      "iteration 65 current loss: 0.001335496548563242 current acc: 0.9106600573962953\n",
      "iteration 66 current loss: 0.0016801438760012388 current acc: 0.9106717099256554\n",
      "iteration 67 current loss: 0.0017621965380385518 current acc: 0.9106833594157537\n",
      "iteration 68 current loss: 0.0008671869873069227 current acc: 0.9106950058677794\n",
      "iteration 69 current loss: 0.0016569847939535975 current acc: 0.9107066492829204\n",
      "iteration 70 current loss: 0.0010578662622720003 current acc: 0.9107182896623648\n",
      "iteration 71 current loss: 0.0012589015532284975 current acc: 0.9107299270072993\n",
      "iteration 72 current loss: 0.0014418974751606584 current acc: 0.9107415613189105\n",
      "iteration 73 current loss: 0.0013326742919161916 current acc: 0.9107531925983842\n",
      "iteration 74 current loss: 0.0009689295548014343 current acc: 0.9107648208469056\n",
      "iteration 75 current loss: 0.000638732744846493 current acc: 0.9107764460656592\n",
      "iteration 76 current loss: 0.0012439631391316652 current acc: 0.9107880682558291\n",
      "iteration 77 current loss: 0.0013347603380680084 current acc: 0.9107996874185986\n",
      "iteration 78 current loss: 0.0018756396602839231 current acc: 0.9108113035551504\n",
      "iteration 79 current loss: 0.001097018364816904 current acc: 0.9108229166666667\n",
      "iteration 80 current loss: 0.0010497475741431117 current acc: 0.9108345267543289\n",
      "iteration 81 current loss: 0.0012555416906252503 current acc: 0.9108461338193179\n",
      "iteration 82 current loss: 0.001416183658875525 current acc: 0.910857737862814\n",
      "iteration 83 current loss: 0.0012090338859707117 current acc: 0.9108693388859969\n",
      "iteration 84 current loss: 0.0010539285140112042 current acc: 0.9108809368900456\n",
      "iteration 85 current loss: 0.001026320387609303 current acc: 0.9108925318761384\n",
      "iteration 86 current loss: 0.0010426099179312587 current acc: 0.9109041238454534\n",
      "iteration 87 current loss: 0.0013387512881308794 current acc: 0.9109157127991675\n",
      "iteration 88 current loss: 0.001416041748598218 current acc: 0.9109272987384576\n",
      "iteration 89 current loss: 0.0013009123504161835 current acc: 0.9109388816644993\n",
      "iteration 90 current loss: 0.0013193613849580288 current acc: 0.9109504615784684\n",
      "iteration 91 current loss: 0.0012935702688992023 current acc: 0.9109620384815392\n",
      "iteration 92 current loss: 0.0009572513517923653 current acc: 0.9109736123748863\n",
      "iteration 93 current loss: 0.0014125877059996128 current acc: 0.9109851832596829\n",
      "iteration 94 current loss: 0.0017036676872521639 current acc: 0.910996751137102\n",
      "iteration 95 current loss: 0.0013591204769909382 current acc: 0.911008316008316\n",
      "iteration 96 current loss: 0.001436227117665112 current acc: 0.9110198778744966\n",
      "iteration 97 current loss: 0.0013336603296920657 current acc: 0.9110314367368147\n",
      "iteration 98 current loss: 0.0009960285387933254 current acc: 0.9110429925964411\n",
      "iteration 99 current loss: 0.0020457671489566565 current acc: 0.9110545454545455\n",
      "\t\tEpoch 76/100 complete. Epoch loss 0.001411330999690108 Epoch accuracy 1.0\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 76, Validation Accuracy: 0.622, Validation Loss: 1.8144923996180296\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.0012490543304011226 current acc: 0.9110660953122971\n",
      "iteration 1 current loss: 0.0008912204066291451 current acc: 0.9110776421708647\n",
      "iteration 2 current loss: 0.0008975997334346175 current acc: 0.9110891860314163\n",
      "iteration 3 current loss: 0.001154617639258504 current acc: 0.9111007268951195\n",
      "iteration 4 current loss: 0.0014808732084929943 current acc: 0.9111122647631408\n",
      "iteration 5 current loss: 0.0010574396001175046 current acc: 0.9111237996366468\n",
      "iteration 6 current loss: 0.0009282981045544147 current acc: 0.9111353315168029\n",
      "iteration 7 current loss: 0.0007031373679637909 current acc: 0.9111468604047742\n",
      "iteration 8 current loss: 0.0014830322470515966 current acc: 0.9111583863017253\n",
      "iteration 9 current loss: 0.0018971371464431286 current acc: 0.9111699092088197\n",
      "iteration 10 current loss: 0.0005922266282141209 current acc: 0.9111814291272209\n",
      "iteration 11 current loss: 0.0010342812165617943 current acc: 0.9111929460580913\n",
      "iteration 12 current loss: 0.0031689940951764584 current acc: 0.911204460002593\n",
      "iteration 13 current loss: 0.0010552816092967987 current acc: 0.9112159709618874\n",
      "iteration 14 current loss: 0.0011858087964355946 current acc: 0.9112274789371354\n",
      "iteration 15 current loss: 0.0010921733919531107 current acc: 0.9112389839294972\n",
      "iteration 16 current loss: 0.001587343169376254 current acc: 0.9112504859401321\n",
      "iteration 17 current loss: 0.0011929828906431794 current acc: 0.9112619849701995\n",
      "iteration 18 current loss: 0.0010931654833257198 current acc: 0.9112734810208576\n",
      "iteration 19 current loss: 0.0009110443061217666 current acc: 0.9112849740932643\n",
      "iteration 20 current loss: 0.0019333385862410069 current acc: 0.9112964641885766\n",
      "iteration 21 current loss: 0.000951457186602056 current acc: 0.9113079513079513\n",
      "iteration 22 current loss: 0.0011121988063678145 current acc: 0.9113194354525443\n",
      "iteration 23 current loss: 0.0010806171922013164 current acc: 0.9113309166235112\n",
      "iteration 24 current loss: 0.0011223674518987536 current acc: 0.9113423948220065\n",
      "iteration 25 current loss: 0.001344119431450963 current acc: 0.9113538700491846\n",
      "iteration 26 current loss: 0.0012272619642317295 current acc: 0.911365342306199\n",
      "iteration 27 current loss: 0.0008715838193893433 current acc: 0.9113768115942029\n",
      "iteration 28 current loss: 0.0008978527039289474 current acc: 0.9113882779143485\n",
      "iteration 29 current loss: 0.001304797944612801 current acc: 0.9113997412677879\n",
      "iteration 30 current loss: 0.0010041160276159644 current acc: 0.911411201655672\n",
      "iteration 31 current loss: 0.0011217637220397592 current acc: 0.9114226590791515\n",
      "iteration 32 current loss: 0.0014676586724817753 current acc: 0.9114341135393766\n",
      "iteration 33 current loss: 0.0013745527248829603 current acc: 0.9114455650374967\n",
      "iteration 34 current loss: 0.0011672275140881538 current acc: 0.9114570135746606\n",
      "iteration 35 current loss: 0.001712485565803945 current acc: 0.9114684591520166\n",
      "iteration 36 current loss: 0.0007928945124149323 current acc: 0.9114799017707121\n",
      "iteration 37 current loss: 0.0008008930017240345 current acc: 0.9114913414318946\n",
      "iteration 38 current loss: 0.0007745735929347575 current acc: 0.9115027781367102\n",
      "iteration 39 current loss: 0.0010341638699173927 current acc: 0.9115142118863049\n",
      "iteration 40 current loss: 0.002173188840970397 current acc: 0.911525642681824\n",
      "iteration 41 current loss: 0.0013627440202981234 current acc: 0.9115370705244124\n",
      "iteration 42 current loss: 0.0009292715694755316 current acc: 0.9115484954152138\n",
      "iteration 43 current loss: 0.001696065184660256 current acc: 0.9115599173553719\n",
      "iteration 44 current loss: 0.001056155189871788 current acc: 0.9115713363460297\n",
      "iteration 45 current loss: 0.0010586726712062955 current acc: 0.9115827523883294\n",
      "iteration 46 current loss: 0.0009432791848666966 current acc: 0.911594165483413\n",
      "iteration 47 current loss: 0.0012527400394901633 current acc: 0.9116055756324213\n",
      "iteration 48 current loss: 0.0007549375877715647 current acc: 0.9116169828364951\n",
      "iteration 49 current loss: 0.0010879556648433208 current acc: 0.9116283870967742\n",
      "iteration 50 current loss: 0.0009904891485348344 current acc: 0.9116397884143982\n",
      "iteration 51 current loss: 0.001408010721206665 current acc: 0.9116511867905057\n",
      "iteration 52 current loss: 0.0010885727824643254 current acc: 0.911662582226235\n",
      "iteration 53 current loss: 0.0009285269770771265 current acc: 0.9116739747227237\n",
      "iteration 54 current loss: 0.0013401348842307925 current acc: 0.9116853642811089\n",
      "iteration 55 current loss: 0.0013361630262807012 current acc: 0.9116967509025271\n",
      "iteration 56 current loss: 0.0013036690652370453 current acc: 0.911708134588114\n",
      "iteration 57 current loss: 0.0011524059809744358 current acc: 0.9117195153390049\n",
      "iteration 58 current loss: 0.0009292701142840087 current acc: 0.9117308931563346\n",
      "iteration 59 current loss: 0.0011822503292933106 current acc: 0.9117422680412371\n",
      "iteration 60 current loss: 0.0011239718878641725 current acc: 0.911753639994846\n",
      "iteration 61 current loss: 0.001611746964044869 current acc: 0.9117650090182943\n",
      "iteration 62 current loss: 0.0008597856503911316 current acc: 0.9117763751127141\n",
      "iteration 63 current loss: 0.0009586032247170806 current acc: 0.9117877382792375\n",
      "iteration 64 current loss: 0.0011769389966502786 current acc: 0.9117990985189955\n",
      "iteration 65 current loss: 0.0022927653044462204 current acc: 0.9118104558331187\n",
      "iteration 66 current loss: 0.0008803003001958132 current acc: 0.9118218102227372\n",
      "iteration 67 current loss: 0.0015245445538312197 current acc: 0.9118331616889804\n",
      "iteration 68 current loss: 0.001116398023441434 current acc: 0.9118445102329772\n",
      "iteration 69 current loss: 0.0011474990751594305 current acc: 0.9118558558558558\n",
      "iteration 70 current loss: 0.0010715550743043423 current acc: 0.9118671985587441\n",
      "iteration 71 current loss: 0.0008953826036304235 current acc: 0.9118785383427689\n",
      "iteration 72 current loss: 0.0013423975324258208 current acc: 0.911889875209057\n",
      "iteration 73 current loss: 0.0015333719784393907 current acc: 0.9119012091587343\n",
      "iteration 74 current loss: 0.0008317618048749864 current acc: 0.911912540192926\n",
      "iteration 75 current loss: 0.0026221019215881824 current acc: 0.9119238683127572\n",
      "iteration 76 current loss: 0.0020174183882772923 current acc: 0.9119351935193519\n",
      "iteration 77 current loss: 0.0009764573187567294 current acc: 0.9119465158138339\n",
      "iteration 78 current loss: 0.0008899036329239607 current acc: 0.9119578351973261\n",
      "iteration 79 current loss: 0.000927638029679656 current acc: 0.9119691516709512\n",
      "iteration 80 current loss: 0.0020188207272440195 current acc: 0.9119804652358309\n",
      "iteration 81 current loss: 0.0009983227355405688 current acc: 0.9119917758930867\n",
      "iteration 82 current loss: 0.0014993292279541492 current acc: 0.9120030836438391\n",
      "iteration 83 current loss: 0.0010157324140891433 current acc: 0.9120143884892087\n",
      "iteration 84 current loss: 0.0011880992678925395 current acc: 0.9120256904303147\n",
      "iteration 85 current loss: 0.00251772697083652 current acc: 0.9120369894682764\n",
      "iteration 86 current loss: 0.0008458532975055277 current acc: 0.9120482856042121\n",
      "iteration 87 current loss: 0.0016090369317680597 current acc: 0.9120595788392398\n",
      "iteration 88 current loss: 0.0008241459145210683 current acc: 0.9120708691744768\n",
      "iteration 89 current loss: 0.000954188930336386 current acc: 0.9120821566110398\n",
      "iteration 90 current loss: 0.0012306352145969868 current acc: 0.9120934411500449\n",
      "iteration 91 current loss: 0.0011687814258038998 current acc: 0.9121047227926078\n",
      "iteration 92 current loss: 0.0009286158019676805 current acc: 0.9121160015398434\n",
      "iteration 93 current loss: 0.001215936616063118 current acc: 0.9121272773928663\n",
      "iteration 94 current loss: 0.0009225690155290067 current acc: 0.9121385503527902\n",
      "iteration 95 current loss: 0.0010579504305496812 current acc: 0.9121498204207286\n",
      "iteration 96 current loss: 0.0018096074927598238 current acc: 0.912161087597794\n",
      "iteration 97 current loss: 0.0014160905266180634 current acc: 0.9121723518850987\n",
      "iteration 98 current loss: 0.0010012219427153468 current acc: 0.9121836132837543\n",
      "iteration 99 current loss: 0.0012997360900044441 current acc: 0.9121948717948718\n",
      "\t\tEpoch 77/100 complete. Epoch loss 0.0012304908188525588 Epoch accuracy 1.0\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 77, Validation Accuracy: 0.621375, Validation Loss: 1.8169256690889597\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.0008805830148048699 current acc: 0.9122061274195616\n",
      "iteration 1 current loss: 0.0010080842766910791 current acc: 0.9122173801589336\n",
      "iteration 2 current loss: 0.0013006431981921196 current acc: 0.9122286300140972\n",
      "iteration 3 current loss: 0.0011855092598125339 current acc: 0.9122398769861609\n",
      "iteration 4 current loss: 0.0016188567969948053 current acc: 0.9122511210762332\n",
      "iteration 5 current loss: 0.0009084972552955151 current acc: 0.9122623622854215\n",
      "iteration 6 current loss: 0.0008582882583141327 current acc: 0.9122736006148329\n",
      "iteration 7 current loss: 0.0011480717221274972 current acc: 0.9122848360655738\n",
      "iteration 8 current loss: 0.0013158558867871761 current acc: 0.9122960686387501\n",
      "iteration 9 current loss: 0.0010415605502203107 current acc: 0.9123072983354673\n",
      "iteration 10 current loss: 0.000979450880549848 current acc: 0.9123185251568301\n",
      "iteration 11 current loss: 0.0011376927141100168 current acc: 0.9123297491039426\n",
      "iteration 12 current loss: 0.0014903765404596925 current acc: 0.9123409701779086\n",
      "iteration 13 current loss: 0.0012153837596997619 current acc: 0.9123521883798311\n",
      "iteration 14 current loss: 0.0006699157529510558 current acc: 0.9123634037108126\n",
      "iteration 15 current loss: 0.0010091726435348392 current acc: 0.9123746161719549\n",
      "iteration 16 current loss: 0.0008773512672632933 current acc: 0.9123858257643598\n",
      "iteration 17 current loss: 0.0009733866318129003 current acc: 0.9123970324891276\n",
      "iteration 18 current loss: 0.001155300298705697 current acc: 0.912408236347359\n",
      "iteration 19 current loss: 0.0009522748878225684 current acc: 0.9124194373401534\n",
      "iteration 20 current loss: 0.001237382646650076 current acc: 0.9124306354686101\n",
      "iteration 21 current loss: 0.001495205331593752 current acc: 0.9124418307338277\n",
      "iteration 22 current loss: 0.0012357294326648116 current acc: 0.912453023136904\n",
      "iteration 23 current loss: 0.0009706278215162456 current acc: 0.9124642126789366\n",
      "iteration 24 current loss: 0.0007655594381503761 current acc: 0.9124753993610224\n",
      "iteration 25 current loss: 0.0012660825159400702 current acc: 0.9124865831842576\n",
      "iteration 26 current loss: 0.0009693043539300561 current acc: 0.912497764149738\n",
      "iteration 27 current loss: 0.001089555793441832 current acc: 0.912508942258559\n",
      "iteration 28 current loss: 0.0012478301068767905 current acc: 0.912520117511815\n",
      "iteration 29 current loss: 0.0010679212864488363 current acc: 0.9125312899106003\n",
      "iteration 30 current loss: 0.001376267638988793 current acc: 0.9125424594560082\n",
      "iteration 31 current loss: 0.0009372838540002704 current acc: 0.9125536261491317\n",
      "iteration 32 current loss: 0.0009336571674793959 current acc: 0.9125647899910635\n",
      "iteration 33 current loss: 0.0011521322885528207 current acc: 0.9125759509828951\n",
      "iteration 34 current loss: 0.0010151222813874483 current acc: 0.9125871091257179\n",
      "iteration 35 current loss: 0.0010463942307978868 current acc: 0.9125982644206228\n",
      "iteration 36 current loss: 0.0007353940745815635 current acc: 0.9126094168686998\n",
      "iteration 37 current loss: 0.0011061474215239286 current acc: 0.9126205664710385\n",
      "iteration 38 current loss: 0.001243547536432743 current acc: 0.9126317132287282\n",
      "iteration 39 current loss: 0.000818494416307658 current acc: 0.9126428571428571\n",
      "iteration 40 current loss: 0.0008264746284112334 current acc: 0.9126539982145134\n",
      "iteration 41 current loss: 0.0010617509251460433 current acc: 0.9126651364447845\n",
      "iteration 42 current loss: 0.00120079074986279 current acc: 0.9126762718347571\n",
      "iteration 43 current loss: 0.001012333668768406 current acc: 0.9126874043855175\n",
      "iteration 44 current loss: 0.0014483166160061955 current acc: 0.9126985340981517\n",
      "iteration 45 current loss: 0.0008335880120284855 current acc: 0.9127096609737446\n",
      "iteration 46 current loss: 0.0009348340099677444 current acc: 0.9127207850133809\n",
      "iteration 47 current loss: 0.001253410242497921 current acc: 0.9127319062181447\n",
      "iteration 48 current loss: 0.0010649337200447917 current acc: 0.9127430245891196\n",
      "iteration 49 current loss: 0.0010056816972792149 current acc: 0.9127541401273885\n",
      "iteration 50 current loss: 0.000909308553673327 current acc: 0.9127652528340339\n",
      "iteration 51 current loss: 0.0008919218671508133 current acc: 0.9127763627101375\n",
      "iteration 52 current loss: 0.0013716289540752769 current acc: 0.9127874697567808\n",
      "iteration 53 current loss: 0.000940886908210814 current acc: 0.9127985739750446\n",
      "iteration 54 current loss: 0.0015147114172577858 current acc: 0.9128096753660089\n",
      "iteration 55 current loss: 0.0008435376803390682 current acc: 0.9128207739307536\n",
      "iteration 56 current loss: 0.0010753225069493055 current acc: 0.9128318696703577\n",
      "iteration 57 current loss: 0.0015294704353436828 current acc: 0.9128429625858997\n",
      "iteration 58 current loss: 0.001611741492524743 current acc: 0.9128540526784579\n",
      "iteration 59 current loss: 0.0012434772215783596 current acc: 0.9128651399491094\n",
      "iteration 60 current loss: 0.0010241300333291292 current acc: 0.9128762243989315\n",
      "iteration 61 current loss: 0.0008498754468746483 current acc: 0.9128873060290003\n",
      "iteration 62 current loss: 0.0010361603926867247 current acc: 0.9128983848403918\n",
      "iteration 63 current loss: 0.0012887908378615975 current acc: 0.9129094608341811\n",
      "iteration 64 current loss: 0.0008846374694257975 current acc: 0.9129205340114431\n",
      "iteration 65 current loss: 0.0006383808795362711 current acc: 0.9129316043732519\n",
      "iteration 66 current loss: 0.0009437061962671578 current acc: 0.9129426719206813\n",
      "iteration 67 current loss: 0.001106451964005828 current acc: 0.9129537366548043\n",
      "iteration 68 current loss: 0.0011296540033072233 current acc: 0.9129647985766933\n",
      "iteration 69 current loss: 0.0008544983575120568 current acc: 0.9129758576874206\n",
      "iteration 70 current loss: 0.0009326890576630831 current acc: 0.9129869139880574\n",
      "iteration 71 current loss: 0.0015728964935988188 current acc: 0.9129979674796748\n",
      "iteration 72 current loss: 0.0014598730485886335 current acc: 0.9130090181633431\n",
      "iteration 73 current loss: 0.0010462659411132336 current acc: 0.9130200660401321\n",
      "iteration 74 current loss: 0.0009502001921646297 current acc: 0.9130311111111111\n",
      "iteration 75 current loss: 0.0008128311601467431 current acc: 0.913042153377349\n",
      "iteration 76 current loss: 0.0010760477744042873 current acc: 0.9130531928399137\n",
      "iteration 77 current loss: 0.0015953362453728914 current acc: 0.9130642294998731\n",
      "iteration 78 current loss: 0.0014164765598252416 current acc: 0.9130752633582943\n",
      "iteration 79 current loss: 0.0011575479293242097 current acc: 0.9130862944162437\n",
      "iteration 80 current loss: 0.0009440527064725757 current acc: 0.9130973226747875\n",
      "iteration 81 current loss: 0.0015088244108483195 current acc: 0.9131083481349911\n",
      "iteration 82 current loss: 0.0012807059101760387 current acc: 0.9131193707979196\n",
      "iteration 83 current loss: 0.0010646808659657836 current acc: 0.9131303906646372\n",
      "iteration 84 current loss: 0.0011106115998700261 current acc: 0.913141407736208\n",
      "iteration 85 current loss: 0.000808479730039835 current acc: 0.9131524220136952\n",
      "iteration 86 current loss: 0.000520643254276365 current acc: 0.9131634334981615\n",
      "iteration 87 current loss: 0.0012402806896716356 current acc: 0.9131744421906693\n",
      "iteration 88 current loss: 0.0009558211895637214 current acc: 0.9131854480922804\n",
      "iteration 89 current loss: 0.0012280778028070927 current acc: 0.9131964512040558\n",
      "iteration 90 current loss: 0.0010019869077950716 current acc: 0.9132074515270562\n",
      "iteration 91 current loss: 0.0006698365323245525 current acc: 0.9132184490623416\n",
      "iteration 92 current loss: 0.0012146359076723456 current acc: 0.9132294438109717\n",
      "iteration 93 current loss: 0.0012919239234179258 current acc: 0.9132404357740056\n",
      "iteration 94 current loss: 0.0009150627302005887 current acc: 0.9132514249525016\n",
      "iteration 95 current loss: 0.0005332290893420577 current acc: 0.9132624113475177\n",
      "iteration 96 current loss: 0.0012018383713439107 current acc: 0.9132733949601114\n",
      "iteration 97 current loss: 0.0016315046232193708 current acc: 0.9132843757913396\n",
      "iteration 98 current loss: 0.0011149115161970258 current acc: 0.9132953538422585\n",
      "iteration 99 current loss: 0.001109727774746716 current acc: 0.9133063291139241\n",
      "\t\tEpoch 78/100 complete. Epoch loss 0.0010913330205949024 Epoch accuracy 1.0\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 78, Validation Accuracy: 0.623, Validation Loss: 1.8373210672289133\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.0008902921690605581 current acc: 0.9133173016073914\n",
      "iteration 1 current loss: 0.001093335566110909 current acc: 0.9133282713237155\n",
      "iteration 2 current loss: 0.0007622927660122514 current acc: 0.9133392382639504\n",
      "iteration 3 current loss: 0.0010903043439611793 current acc: 0.9133502024291498\n",
      "iteration 4 current loss: 0.0008640575688332319 current acc: 0.9133611638203668\n",
      "iteration 5 current loss: 0.0014657943975180387 current acc: 0.9133721224386542\n",
      "iteration 6 current loss: 0.0026949455495923758 current acc: 0.9133830782850638\n",
      "iteration 7 current loss: 0.001055138767696917 current acc: 0.9133940313606475\n",
      "iteration 8 current loss: 0.0011972395004704595 current acc: 0.9134049816664559\n",
      "iteration 9 current loss: 0.0009506559581495821 current acc: 0.9134159292035399\n",
      "iteration 10 current loss: 0.0009430294740013778 current acc: 0.9134268739729491\n",
      "iteration 11 current loss: 0.0010301960865035653 current acc: 0.913437815975733\n",
      "iteration 12 current loss: 0.0008272361592389643 current acc: 0.9134487552129408\n",
      "iteration 13 current loss: 0.0009565409272909164 current acc: 0.9134596916856205\n",
      "iteration 14 current loss: 0.0006212525768205523 current acc: 0.9134706253948199\n",
      "iteration 15 current loss: 0.0006117380689829588 current acc: 0.9134815563415867\n",
      "iteration 16 current loss: 0.001419474952854216 current acc: 0.9134924845269673\n",
      "iteration 17 current loss: 0.0007512021693401039 current acc: 0.9135034099520081\n",
      "iteration 18 current loss: 0.002300158841535449 current acc: 0.9135143326177547\n",
      "iteration 19 current loss: 0.000778631423600018 current acc: 0.9135252525252525\n",
      "iteration 20 current loss: 0.0010467468528077006 current acc: 0.913536169675546\n",
      "iteration 21 current loss: 0.0010529770515859127 current acc: 0.9135470840696793\n",
      "iteration 22 current loss: 0.0009460041183046997 current acc: 0.9135579957086962\n",
      "iteration 23 current loss: 0.0012436900287866592 current acc: 0.9135689045936396\n",
      "iteration 24 current loss: 0.0009844166925176978 current acc: 0.913579810725552\n",
      "iteration 25 current loss: 0.0012050654040649533 current acc: 0.9135907141054757\n",
      "iteration 26 current loss: 0.001539753400720656 current acc: 0.9136016147344519\n",
      "iteration 27 current loss: 0.0016098028281703591 current acc: 0.9136125126135217\n",
      "iteration 28 current loss: 0.0008475745562463999 current acc: 0.9136234077437255\n",
      "iteration 29 current loss: 0.001490494585596025 current acc: 0.9136343001261034\n",
      "iteration 30 current loss: 0.0012502755271270871 current acc: 0.9136451897616946\n",
      "iteration 31 current loss: 0.0006166883395053446 current acc: 0.913656076651538\n",
      "iteration 32 current loss: 0.0010828770464286208 current acc: 0.9136669607966721\n",
      "iteration 33 current loss: 0.0009756832732819021 current acc: 0.9136778421981346\n",
      "iteration 34 current loss: 0.0010271252831444144 current acc: 0.9136887208569628\n",
      "iteration 35 current loss: 0.0010096360929310322 current acc: 0.9136995967741935\n",
      "iteration 36 current loss: 0.0008531351923011243 current acc: 0.913710469950863\n",
      "iteration 37 current loss: 0.0007727740448899567 current acc: 0.913721340388007\n",
      "iteration 38 current loss: 0.0010040531633421779 current acc: 0.9137322080866608\n",
      "iteration 39 current loss: 0.000709164422005415 current acc: 0.913743073047859\n",
      "iteration 40 current loss: 0.0012282395036891103 current acc: 0.9137539352726357\n",
      "iteration 41 current loss: 0.0007207564194686711 current acc: 0.9137647947620247\n",
      "iteration 42 current loss: 0.0005906288861297071 current acc: 0.9137756515170591\n",
      "iteration 43 current loss: 0.0012534019770100713 current acc: 0.9137865055387714\n",
      "iteration 44 current loss: 0.0006117560551501811 current acc: 0.9137973568281939\n",
      "iteration 45 current loss: 0.0009517932776361704 current acc: 0.9138082053863579\n",
      "iteration 46 current loss: 0.0012816700618714094 current acc: 0.9138190512142947\n",
      "iteration 47 current loss: 0.0006285125855356455 current acc: 0.9138298943130347\n",
      "iteration 48 current loss: 0.000993809662759304 current acc: 0.913840734683608\n",
      "iteration 49 current loss: 0.0018985129427164793 current acc: 0.9138515723270441\n",
      "iteration 50 current loss: 0.0011458765948191285 current acc: 0.9138624072443717\n",
      "iteration 51 current loss: 0.0010974481701850891 current acc: 0.9138732394366197\n",
      "iteration 52 current loss: 0.000791882339399308 current acc: 0.9138840689048158\n",
      "iteration 53 current loss: 0.0009554102434776723 current acc: 0.9138948956499874\n",
      "iteration 54 current loss: 0.0009611881687305868 current acc: 0.9139057196731616\n",
      "iteration 55 current loss: 0.0009147844975814223 current acc: 0.9139165409753645\n",
      "iteration 56 current loss: 0.0009323717677034438 current acc: 0.9139273595576222\n",
      "iteration 57 current loss: 0.0010104394750669599 current acc: 0.91393817542096\n",
      "iteration 58 current loss: 0.0010316894622519612 current acc: 0.9139489885664028\n",
      "iteration 59 current loss: 0.00139330152887851 current acc: 0.9139597989949749\n",
      "iteration 60 current loss: 0.0006518872105516493 current acc: 0.9139706067077\n",
      "iteration 61 current loss: 0.0007351600215770304 current acc: 0.9139814117056017\n",
      "iteration 62 current loss: 0.0005919396644458175 current acc: 0.9139922139897024\n",
      "iteration 63 current loss: 0.0014232880203053355 current acc: 0.9140030135610246\n",
      "iteration 64 current loss: 0.0010857202578336 current acc: 0.91401381042059\n",
      "iteration 65 current loss: 0.001066077034920454 current acc: 0.91402460456942\n",
      "iteration 66 current loss: 0.0007738840067759156 current acc: 0.9140353960085352\n",
      "iteration 67 current loss: 0.0010125595144927502 current acc: 0.9140461847389558\n",
      "iteration 68 current loss: 0.0012453431263566017 current acc: 0.9140569707617016\n",
      "iteration 69 current loss: 0.0007938231574371457 current acc: 0.9140677540777917\n",
      "iteration 70 current loss: 0.0007684232550673187 current acc: 0.9140785346882448\n",
      "iteration 71 current loss: 0.0008438572403974831 current acc: 0.9140893125940793\n",
      "iteration 72 current loss: 0.0007510143332183361 current acc: 0.9141000877963126\n",
      "iteration 73 current loss: 0.000746391131542623 current acc: 0.9141108602959619\n",
      "iteration 74 current loss: 0.0010018012253567576 current acc: 0.9141216300940439\n",
      "iteration 75 current loss: 0.0013212113408371806 current acc: 0.9141323971915747\n",
      "iteration 76 current loss: 0.0009302007965743542 current acc: 0.91414316158957\n",
      "iteration 77 current loss: 0.0017556275706738234 current acc: 0.9141539232890449\n",
      "iteration 78 current loss: 0.0007669071201235056 current acc: 0.9141646822910139\n",
      "iteration 79 current loss: 0.0008886683499440551 current acc: 0.9141754385964912\n",
      "iteration 80 current loss: 0.0009426199831068516 current acc: 0.9141861922064904\n",
      "iteration 81 current loss: 0.0010701337596401572 current acc: 0.9141969431220246\n",
      "iteration 82 current loss: 0.0010238998802378774 current acc: 0.9142076913441062\n",
      "iteration 83 current loss: 0.0006325620342977345 current acc: 0.9142184368737475\n",
      "iteration 84 current loss: 0.0017239968292415142 current acc: 0.9142291797119599\n",
      "iteration 85 current loss: 0.001160569372586906 current acc: 0.9142399198597546\n",
      "iteration 86 current loss: 0.0009228808921761811 current acc: 0.914250657318142\n",
      "iteration 87 current loss: 0.001254904898814857 current acc: 0.9142613920881322\n",
      "iteration 88 current loss: 0.0011603591265156865 current acc: 0.9142721241707348\n",
      "iteration 89 current loss: 0.000662488688249141 current acc: 0.9142828535669587\n",
      "iteration 90 current loss: 0.0007458452600985765 current acc: 0.9142935802778125\n",
      "iteration 91 current loss: 0.0007978521753102541 current acc: 0.9143043043043043\n",
      "iteration 92 current loss: 0.0008425659034401178 current acc: 0.9143150256474415\n",
      "iteration 93 current loss: 0.0009827354224398732 current acc: 0.9143257443082312\n",
      "iteration 94 current loss: 0.0008045472204685211 current acc: 0.9143364602876798\n",
      "iteration 95 current loss: 0.0008876344654709101 current acc: 0.9143471735867934\n",
      "iteration 96 current loss: 0.0007115543121472001 current acc: 0.9143578842065775\n",
      "iteration 97 current loss: 0.0007495171739719808 current acc: 0.914368592148037\n",
      "iteration 98 current loss: 0.0014712458942085505 current acc: 0.9143792974121765\n",
      "iteration 99 current loss: 0.0006072921096347272 current acc: 0.91439\n",
      "\t\tEpoch 79/100 complete. Epoch loss 0.0010227391857188196 Epoch accuracy 1.0\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 79, Validation Accuracy: 0.619875, Validation Loss: 1.8547681525349617\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.0006494583212770522 current acc: 0.9144006999125109\n",
      "iteration 1 current loss: 0.0006116321310400963 current acc: 0.9144113971507123\n",
      "iteration 2 current loss: 0.0008242743206210434 current acc: 0.9144220917156066\n",
      "iteration 3 current loss: 0.00045575437252409756 current acc: 0.9144327836081959\n",
      "iteration 4 current loss: 0.0015812413766980171 current acc: 0.9144434728294816\n",
      "iteration 5 current loss: 0.0007949997670948505 current acc: 0.9144541593804647\n",
      "iteration 6 current loss: 0.0009107403457164764 current acc: 0.9144648432621456\n",
      "iteration 7 current loss: 0.0013330149231478572 current acc: 0.9144755244755245\n",
      "iteration 8 current loss: 0.0008374525350518525 current acc: 0.9144862030216007\n",
      "iteration 9 current loss: 0.0008618842693977058 current acc: 0.9144968789013733\n",
      "iteration 10 current loss: 0.0007589839515276253 current acc: 0.9145075521158407\n",
      "iteration 11 current loss: 0.0013700851704925299 current acc: 0.914518222666001\n",
      "iteration 12 current loss: 0.00074216746725142 current acc: 0.9145288905528516\n",
      "iteration 13 current loss: 0.0009151689009740949 current acc: 0.9145395557773895\n",
      "iteration 14 current loss: 0.0010447322856634855 current acc: 0.9145502183406113\n",
      "iteration 15 current loss: 0.0006463441532105207 current acc: 0.914560878243513\n",
      "iteration 16 current loss: 0.0007780148298479617 current acc: 0.9145715354870899\n",
      "iteration 17 current loss: 0.0010807152139022946 current acc: 0.9145821900723372\n",
      "iteration 18 current loss: 0.0006825100863352418 current acc: 0.9145928420002494\n",
      "iteration 19 current loss: 0.0010303739691153169 current acc: 0.9146034912718205\n",
      "iteration 20 current loss: 0.0007820589817129076 current acc: 0.9146141378880439\n",
      "iteration 21 current loss: 0.0007854878203943372 current acc: 0.9146247818499127\n",
      "iteration 22 current loss: 0.0007641216507181525 current acc: 0.9146354231584195\n",
      "iteration 23 current loss: 0.000985284335911274 current acc: 0.9146460618145563\n",
      "iteration 24 current loss: 0.0005888150190003216 current acc: 0.9146566978193147\n",
      "iteration 25 current loss: 0.001253725728020072 current acc: 0.9146673311736855\n",
      "iteration 26 current loss: 0.0009195486200042069 current acc: 0.9146779618786596\n",
      "iteration 27 current loss: 0.0009838907280936837 current acc: 0.9146885899352267\n",
      "iteration 28 current loss: 0.0014624070608988404 current acc: 0.9146992153443766\n",
      "iteration 29 current loss: 0.0007466687238775194 current acc: 0.9147098381070984\n",
      "iteration 30 current loss: 0.0006872941739857197 current acc: 0.9147204582243805\n",
      "iteration 31 current loss: 0.001679970184341073 current acc: 0.9147310756972111\n",
      "iteration 32 current loss: 0.0007300116121768951 current acc: 0.9147416905265778\n",
      "iteration 33 current loss: 0.0008487225859425962 current acc: 0.9147523027134677\n",
      "iteration 34 current loss: 0.0008998984121717513 current acc: 0.9147629122588674\n",
      "iteration 35 current loss: 0.00047180018736980855 current acc: 0.914773519163763\n",
      "iteration 36 current loss: 0.0006262753158807755 current acc: 0.9147841234291402\n",
      "iteration 37 current loss: 0.0008056365768425167 current acc: 0.914794725055984\n",
      "iteration 38 current loss: 0.0009203072404488921 current acc: 0.9148053240452793\n",
      "iteration 39 current loss: 0.0005589084466919303 current acc: 0.91481592039801\n",
      "iteration 40 current loss: 0.0008314329315908253 current acc: 0.9148265141151598\n",
      "iteration 41 current loss: 0.0007707216427661479 current acc: 0.914837105197712\n",
      "iteration 42 current loss: 0.0009703154792077839 current acc: 0.9148476936466493\n",
      "iteration 43 current loss: 0.0006949227536097169 current acc: 0.9148582794629537\n",
      "iteration 44 current loss: 0.0009853704832494259 current acc: 0.9148688626476073\n",
      "iteration 45 current loss: 0.0010350393131375313 current acc: 0.9148794432015909\n",
      "iteration 46 current loss: 0.0007683985168114305 current acc: 0.9148900211258855\n",
      "iteration 47 current loss: 0.0009245286928489804 current acc: 0.9149005964214711\n",
      "iteration 48 current loss: 0.000899541424587369 current acc: 0.9149111690893279\n",
      "iteration 49 current loss: 0.0011119423434138298 current acc: 0.9149217391304347\n",
      "iteration 50 current loss: 0.0007089449209161103 current acc: 0.9149323065457707\n",
      "iteration 51 current loss: 0.0010118128266185522 current acc: 0.9149428713363139\n",
      "iteration 52 current loss: 0.0009299250668846071 current acc: 0.9149534335030424\n",
      "iteration 53 current loss: 0.0009863441810011864 current acc: 0.9149639930469332\n",
      "iteration 54 current loss: 0.0013159381924197078 current acc: 0.9149745499689633\n",
      "iteration 55 current loss: 0.0009412973886355758 current acc: 0.9149851042701093\n",
      "iteration 56 current loss: 0.0009480818989686668 current acc: 0.9149956559513467\n",
      "iteration 57 current loss: 0.0009176643216051161 current acc: 0.915006205013651\n",
      "iteration 58 current loss: 0.0010218904353678226 current acc: 0.9150167514579973\n",
      "iteration 59 current loss: 0.0007306686020456254 current acc: 0.9150272952853598\n",
      "iteration 60 current loss: 0.0013968652347102761 current acc: 0.9150378364967126\n",
      "iteration 61 current loss: 0.0008404093096032739 current acc: 0.915048375093029\n",
      "iteration 62 current loss: 0.0010015075094997883 current acc: 0.9150589110752821\n",
      "iteration 63 current loss: 0.0007265149033628404 current acc: 0.9150694444444445\n",
      "iteration 64 current loss: 0.000932339287828654 current acc: 0.9150799752014879\n",
      "iteration 65 current loss: 0.0009890309302136302 current acc: 0.9150905033473841\n",
      "iteration 66 current loss: 0.0012816872913390398 current acc: 0.915101028883104\n",
      "iteration 67 current loss: 0.0007819689344614744 current acc: 0.9151115518096182\n",
      "iteration 68 current loss: 0.0009698099456727505 current acc: 0.9151220721278969\n",
      "iteration 69 current loss: 0.000957400887273252 current acc: 0.9151325898389095\n",
      "iteration 70 current loss: 0.0007046661339700222 current acc: 0.9151431049436253\n",
      "iteration 71 current loss: 0.000823605980258435 current acc: 0.9151536174430129\n",
      "iteration 72 current loss: 0.0006998538738116622 current acc: 0.9151641273380404\n",
      "iteration 73 current loss: 0.0008134261588566005 current acc: 0.9151746346296755\n",
      "iteration 74 current loss: 0.0006579245673492551 current acc: 0.9151851393188855\n",
      "iteration 75 current loss: 0.0007520877406932414 current acc: 0.9151956414066369\n",
      "iteration 76 current loss: 0.0010288002667948604 current acc: 0.9152061408938963\n",
      "iteration 77 current loss: 0.0007395802531391382 current acc: 0.9152166377816291\n",
      "iteration 78 current loss: 0.0011811687145382166 current acc: 0.9152271320708009\n",
      "iteration 79 current loss: 0.0013742416631430387 current acc: 0.9152376237623763\n",
      "iteration 80 current loss: 0.0006753919296897948 current acc: 0.9152481128573197\n",
      "iteration 81 current loss: 0.0013585491105914116 current acc: 0.9152585993565949\n",
      "iteration 82 current loss: 0.0008618794963695109 current acc: 0.9152690832611654\n",
      "iteration 83 current loss: 0.0011854947078973055 current acc: 0.9152795645719941\n",
      "iteration 84 current loss: 0.0012516905553638935 current acc: 0.9152900432900433\n",
      "iteration 85 current loss: 0.0007143889088183641 current acc: 0.915300519416275\n",
      "iteration 86 current loss: 0.0008026749710552394 current acc: 0.9153109929516507\n",
      "iteration 87 current loss: 0.0010560456430539489 current acc: 0.9153214638971315\n",
      "iteration 88 current loss: 0.0009714615880511701 current acc: 0.9153319322536778\n",
      "iteration 89 current loss: 0.000579424318857491 current acc: 0.9153423980222497\n",
      "iteration 90 current loss: 0.0008475620998069644 current acc: 0.9153528612038067\n",
      "iteration 91 current loss: 0.001341283437795937 current acc: 0.9153633217993079\n",
      "iteration 92 current loss: 0.0010103670647367835 current acc: 0.9153737798097121\n",
      "iteration 93 current loss: 0.0008704087231308222 current acc: 0.9153842352359772\n",
      "iteration 94 current loss: 0.0008234165143221617 current acc: 0.9153946880790611\n",
      "iteration 95 current loss: 0.0006039702566340566 current acc: 0.915405138339921\n",
      "iteration 96 current loss: 0.0009622971992939711 current acc: 0.9154155860195134\n",
      "iteration 97 current loss: 0.0006482009775936604 current acc: 0.9154260311187947\n",
      "iteration 98 current loss: 0.0007465603412128985 current acc: 0.9154364736387208\n",
      "iteration 99 current loss: 0.0013157215435057878 current acc: 0.9154469135802469\n",
      "\t\tEpoch 80/100 complete. Epoch loss 0.0009119484218535945 Epoch accuracy 1.0\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 80, Validation Accuracy: 0.6225, Validation Loss: 1.8641476150602103\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.0007940890500321984 current acc: 0.9154573509443279\n",
      "iteration 1 current loss: 0.000680542376358062 current acc: 0.915467785731918\n",
      "iteration 2 current loss: 0.0016087456606328487 current acc: 0.9154782179439713\n",
      "iteration 3 current loss: 0.0007404908537864685 current acc: 0.9154886475814412\n",
      "iteration 4 current loss: 0.0006919106817804277 current acc: 0.9154990746452807\n",
      "iteration 5 current loss: 0.0017281277105212212 current acc: 0.9155094991364422\n",
      "iteration 6 current loss: 0.0005197622231207788 current acc: 0.9155199210558777\n",
      "iteration 7 current loss: 0.0016823464538902044 current acc: 0.9155303404045387\n",
      "iteration 8 current loss: 0.0004997773212380707 current acc: 0.9155407571833765\n",
      "iteration 9 current loss: 0.0013036977034062147 current acc: 0.9155511713933415\n",
      "iteration 10 current loss: 0.0011704014614224434 current acc: 0.915561583035384\n",
      "iteration 11 current loss: 0.0007906205719336867 current acc: 0.9155719921104537\n",
      "iteration 12 current loss: 0.0007192899938672781 current acc: 0.9155823986194995\n",
      "iteration 13 current loss: 0.000640464189928025 current acc: 0.9155928025634705\n",
      "iteration 14 current loss: 0.0007284893654286861 current acc: 0.9156032039433148\n",
      "iteration 15 current loss: 0.0005879881791770458 current acc: 0.9156136027599803\n",
      "iteration 16 current loss: 0.0007399159949272871 current acc: 0.9156239990144142\n",
      "iteration 17 current loss: 0.0011454680934548378 current acc: 0.9156343927075634\n",
      "iteration 18 current loss: 0.0007202299893833697 current acc: 0.9156447838403744\n",
      "iteration 19 current loss: 0.000736404734198004 current acc: 0.9156551724137931\n",
      "iteration 20 current loss: 0.0007650221232324839 current acc: 0.915665558428765\n",
      "iteration 21 current loss: 0.0006503944168798625 current acc: 0.9156759418862349\n",
      "iteration 22 current loss: 0.0005704258219338953 current acc: 0.9156863227871476\n",
      "iteration 23 current loss: 0.0010007444070652127 current acc: 0.915696701132447\n",
      "iteration 24 current loss: 0.0006886429619044065 current acc: 0.915707076923077\n",
      "iteration 25 current loss: 0.0008229344384744763 current acc: 0.9157174501599803\n",
      "iteration 26 current loss: 0.0009894350077956915 current acc: 0.9157278208440999\n",
      "iteration 27 current loss: 0.0008620233274996281 current acc: 0.9157381889763779\n",
      "iteration 28 current loss: 0.0008254130952991545 current acc: 0.9157485545577562\n",
      "iteration 29 current loss: 0.0010199081152677536 current acc: 0.9157589175891759\n",
      "iteration 30 current loss: 0.0010248578619211912 current acc: 0.9157692780715779\n",
      "iteration 31 current loss: 0.0006072311080060899 current acc: 0.9157796360059026\n",
      "iteration 32 current loss: 0.000991730485111475 current acc: 0.9157899913930899\n",
      "iteration 33 current loss: 0.0007101214141584933 current acc: 0.9158003442340792\n",
      "iteration 34 current loss: 0.0006780773401260376 current acc: 0.9158106945298095\n",
      "iteration 35 current loss: 0.0006792491767555475 current acc: 0.9158210422812193\n",
      "iteration 36 current loss: 0.0007400974864140153 current acc: 0.9158313874892466\n",
      "iteration 37 current loss: 0.0010216519003733993 current acc: 0.9158417301548292\n",
      "iteration 38 current loss: 0.0008266451768577099 current acc: 0.915852070278904\n",
      "iteration 39 current loss: 0.0009512159158475697 current acc: 0.9158624078624079\n",
      "iteration 40 current loss: 0.0008363164379261434 current acc: 0.9158727429062768\n",
      "iteration 41 current loss: 0.0009063487523235381 current acc: 0.9158830754114469\n",
      "iteration 42 current loss: 0.0011996192624792457 current acc: 0.915893405378853\n",
      "iteration 43 current loss: 0.001143986009992659 current acc: 0.9159037328094303\n",
      "iteration 44 current loss: 0.0008310768171213567 current acc: 0.9159140577041129\n",
      "iteration 45 current loss: 0.0008644590270705521 current acc: 0.915924380063835\n",
      "iteration 46 current loss: 0.0006210708525031805 current acc: 0.9159346998895299\n",
      "iteration 47 current loss: 0.0007644597208127379 current acc: 0.9159450171821306\n",
      "iteration 48 current loss: 0.0008899735403247178 current acc: 0.9159553319425696\n",
      "iteration 49 current loss: 0.000621414219494909 current acc: 0.9159656441717792\n",
      "iteration 50 current loss: 0.0006710216403007507 current acc: 0.9159759538706908\n",
      "iteration 51 current loss: 0.0010255960514768958 current acc: 0.9159862610402355\n",
      "iteration 52 current loss: 0.000784978037700057 current acc: 0.9159965656813442\n",
      "iteration 53 current loss: 0.0008769237902015448 current acc: 0.9160068677949472\n",
      "iteration 54 current loss: 0.0007756783743388951 current acc: 0.9160171673819743\n",
      "iteration 55 current loss: 0.000789022829849273 current acc: 0.9160274644433546\n",
      "iteration 56 current loss: 0.0006956184515729547 current acc: 0.9160377589800172\n",
      "iteration 57 current loss: 0.0009734241757541895 current acc: 0.9160480509928904\n",
      "iteration 58 current loss: 0.0007874172297306359 current acc: 0.9160583404829024\n",
      "iteration 59 current loss: 0.007634143345057964 current acc: 0.9160686274509804\n",
      "iteration 60 current loss: 0.0011267312802374363 current acc: 0.9160789118980517\n",
      "iteration 61 current loss: 0.0008177710114978254 current acc: 0.9160891938250428\n",
      "iteration 62 current loss: 0.0010554402833804488 current acc: 0.9160994732328801\n",
      "iteration 63 current loss: 0.001281369011849165 current acc: 0.916109750122489\n",
      "iteration 64 current loss: 0.004168656188994646 current acc: 0.9161200244947949\n",
      "iteration 65 current loss: 0.0026551587507128716 current acc: 0.9161302963507225\n",
      "iteration 66 current loss: 0.0012192083522677422 current acc: 0.9161405656911963\n",
      "iteration 67 current loss: 0.0012822612188756466 current acc: 0.91615083251714\n",
      "iteration 68 current loss: 0.000799319357611239 current acc: 0.9161610968294773\n",
      "iteration 69 current loss: 0.0006359420949593186 current acc: 0.916171358629131\n",
      "iteration 70 current loss: 0.0010328592034056783 current acc: 0.9161816179170236\n",
      "iteration 71 current loss: 0.0014183636521920562 current acc: 0.9161918746940774\n",
      "iteration 72 current loss: 0.0012316369684413075 current acc: 0.9162021289612138\n",
      "iteration 73 current loss: 0.001164587796665728 current acc: 0.9162123807193541\n",
      "iteration 74 current loss: 0.0013653557980433106 current acc: 0.916222629969419\n",
      "iteration 75 current loss: 0.0007283474551513791 current acc: 0.9162328767123288\n",
      "iteration 76 current loss: 0.0010818465379998088 current acc: 0.9162431209490033\n",
      "iteration 77 current loss: 0.001517962315119803 current acc: 0.916253362680362\n",
      "iteration 78 current loss: 0.0007024165824986994 current acc: 0.9162636019073236\n",
      "iteration 79 current loss: 0.0008273567073047161 current acc: 0.9162738386308068\n",
      "iteration 80 current loss: 0.0005427678697742522 current acc: 0.9162840728517296\n",
      "iteration 81 current loss: 0.0005420976667664945 current acc: 0.9162943045710096\n",
      "iteration 82 current loss: 0.0008232808904722333 current acc: 0.9163045337895638\n",
      "iteration 83 current loss: 0.0007114118197932839 current acc: 0.9163147605083088\n",
      "iteration 84 current loss: 0.0009737221989780664 current acc: 0.9163249847281613\n",
      "iteration 85 current loss: 0.0011119565460830927 current acc: 0.9163352064500366\n",
      "iteration 86 current loss: 0.0026721174363046885 current acc: 0.9163454256748503\n",
      "iteration 87 current loss: 0.0012921117013320327 current acc: 0.9163556424035173\n",
      "iteration 88 current loss: 0.0005817178171128035 current acc: 0.916365856636952\n",
      "iteration 89 current loss: 0.001574366120621562 current acc: 0.9163760683760683\n",
      "iteration 90 current loss: 0.0008029373711906374 current acc: 0.91638627762178\n",
      "iteration 91 current loss: 0.001170099712908268 current acc: 0.916396484375\n",
      "iteration 92 current loss: 0.0009102391195483506 current acc: 0.916406688636641\n",
      "iteration 93 current loss: 0.0011727727251127362 current acc: 0.9164168904076153\n",
      "iteration 94 current loss: 0.00100333068985492 current acc: 0.9164270896888347\n",
      "iteration 95 current loss: 0.000836461316794157 current acc: 0.9164372864812104\n",
      "iteration 96 current loss: 0.0005802239757031202 current acc: 0.9164474807856533\n",
      "iteration 97 current loss: 0.0007145850104279816 current acc: 0.9164576726030739\n",
      "iteration 98 current loss: 0.0008432836038991809 current acc: 0.9164678619343822\n",
      "iteration 99 current loss: 0.0008125986787490547 current acc: 0.9164780487804878\n",
      "\t\tEpoch 81/100 complete. Epoch loss 0.001041318055940792 Epoch accuracy 1.0\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 81, Validation Accuracy: 0.6245, Validation Loss: 1.8792901154607535\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.0005143433809280396 current acc: 0.9164882331422998\n",
      "iteration 1 current loss: 0.00065903045469895 current acc: 0.9164984150207266\n",
      "iteration 2 current loss: 0.001020029536448419 current acc: 0.9165085944166769\n",
      "iteration 3 current loss: 0.000833121535833925 current acc: 0.916518771331058\n",
      "iteration 4 current loss: 0.0009138700552284718 current acc: 0.9165289457647776\n",
      "iteration 5 current loss: 0.0017298795282840729 current acc: 0.9165391177187424\n",
      "iteration 6 current loss: 0.0007357140420936048 current acc: 0.9165492871938589\n",
      "iteration 7 current loss: 0.0008753114379942417 current acc: 0.9165594541910331\n",
      "iteration 8 current loss: 0.0005423130351118743 current acc: 0.9165696187111707\n",
      "iteration 9 current loss: 0.0006866766489110887 current acc: 0.9165797807551767\n",
      "iteration 10 current loss: 0.0013358289143070579 current acc: 0.9165899403239557\n",
      "iteration 11 current loss: 0.0008647619397379458 current acc: 0.9166000974184121\n",
      "iteration 12 current loss: 0.0011992378858849406 current acc: 0.9166102520394497\n",
      "iteration 13 current loss: 0.0006089535891078413 current acc: 0.9166204041879717\n",
      "iteration 14 current loss: 0.0011694770073518157 current acc: 0.9166305538648813\n",
      "iteration 15 current loss: 0.001071895007044077 current acc: 0.9166407010710809\n",
      "iteration 16 current loss: 0.0005539893172681332 current acc: 0.9166508458074724\n",
      "iteration 17 current loss: 0.0010140110971406102 current acc: 0.9166609880749574\n",
      "iteration 18 current loss: 0.0006963283522054553 current acc: 0.9166711278744373\n",
      "iteration 19 current loss: 0.001061702729202807 current acc: 0.9166812652068127\n",
      "iteration 20 current loss: 0.0007369962404482067 current acc: 0.9166914000729838\n",
      "iteration 21 current loss: 0.0009943030308932066 current acc: 0.9167015324738507\n",
      "iteration 22 current loss: 0.0005604375037364662 current acc: 0.9167116624103125\n",
      "iteration 23 current loss: 0.0007080927607603371 current acc: 0.9167217898832685\n",
      "iteration 24 current loss: 0.0007708360790275037 current acc: 0.916731914893617\n",
      "iteration 25 current loss: 0.0008771167485974729 current acc: 0.9167420374422562\n",
      "iteration 26 current loss: 0.0006840100395493209 current acc: 0.9167521575300839\n",
      "iteration 27 current loss: 0.000774149433709681 current acc: 0.9167622751579971\n",
      "iteration 28 current loss: 0.000978421070612967 current acc: 0.9167723903268927\n",
      "iteration 29 current loss: 0.0011127900797873735 current acc: 0.9167825030376671\n",
      "iteration 30 current loss: 0.0006633161101490259 current acc: 0.9167926132912161\n",
      "iteration 31 current loss: 0.0004474801244214177 current acc: 0.9168027210884354\n",
      "iteration 32 current loss: 0.0008366407710127532 current acc: 0.9168128264302199\n",
      "iteration 33 current loss: 0.00048138972488231957 current acc: 0.9168229293174641\n",
      "iteration 34 current loss: 0.0009672146406956017 current acc: 0.9168330297510625\n",
      "iteration 35 current loss: 0.0010018014581874013 current acc: 0.9168431277319087\n",
      "iteration 36 current loss: 0.0010548396967351437 current acc: 0.9168532232608959\n",
      "iteration 37 current loss: 0.0012563974596560001 current acc: 0.9168633163389173\n",
      "iteration 38 current loss: 0.0007830139365978539 current acc: 0.9168734069668649\n",
      "iteration 39 current loss: 0.0015765625284984708 current acc: 0.9168834951456311\n",
      "iteration 40 current loss: 0.0007526305853389204 current acc: 0.9168935808761073\n",
      "iteration 41 current loss: 0.0009584200452081859 current acc: 0.9169036641591847\n",
      "iteration 42 current loss: 0.0008528160396963358 current acc: 0.916913744995754\n",
      "iteration 43 current loss: 0.001022657030262053 current acc: 0.9169238233867055\n",
      "iteration 44 current loss: 0.0007835068390704691 current acc: 0.916933899332929\n",
      "iteration 45 current loss: 0.0021417445968836546 current acc: 0.916943972835314\n",
      "iteration 46 current loss: 0.0007250679773278534 current acc: 0.9169540438947497\n",
      "iteration 47 current loss: 0.0005345392855815589 current acc: 0.9169641125121242\n",
      "iteration 48 current loss: 0.0009181845816783607 current acc: 0.9169741786883259\n",
      "iteration 49 current loss: 0.0010593447368592024 current acc: 0.9169842424242425\n",
      "iteration 50 current loss: 0.0006743231206201017 current acc: 0.9169943037207611\n",
      "iteration 51 current loss: 0.0008370438008569181 current acc: 0.9170043625787688\n",
      "iteration 52 current loss: 0.0007830947870388627 current acc: 0.9170144189991518\n",
      "iteration 53 current loss: 0.0008557771798223257 current acc: 0.9170244729827962\n",
      "iteration 54 current loss: 0.0005186318885535002 current acc: 0.9170345245305875\n",
      "iteration 55 current loss: 0.000952957896515727 current acc: 0.9170445736434109\n",
      "iteration 56 current loss: 0.0009107044315896928 current acc: 0.9170546203221509\n",
      "iteration 57 current loss: 0.0008609279175288975 current acc: 0.917064664567692\n",
      "iteration 58 current loss: 0.0008222555625252426 current acc: 0.9170747063809178\n",
      "iteration 59 current loss: 0.00047923176316544414 current acc: 0.9170847457627118\n",
      "iteration 60 current loss: 0.001407212927006185 current acc: 0.9170947827139572\n",
      "iteration 61 current loss: 0.0008983665611594915 current acc: 0.9171048172355362\n",
      "iteration 62 current loss: 0.0006173551664687693 current acc: 0.9171148493283311\n",
      "iteration 63 current loss: 0.0008866541902534664 current acc: 0.9171248789932236\n",
      "iteration 64 current loss: 0.003091013990342617 current acc: 0.917134906231095\n",
      "iteration 65 current loss: 0.0006041262531653047 current acc: 0.917144931042826\n",
      "iteration 66 current loss: 0.0006728573353029788 current acc: 0.9171549534292972\n",
      "iteration 67 current loss: 0.0005752104916609824 current acc: 0.9171649733913885\n",
      "iteration 68 current loss: 0.0011332947760820389 current acc: 0.9171749909299795\n",
      "iteration 69 current loss: 0.0010611622128635645 current acc: 0.9171850060459492\n",
      "iteration 70 current loss: 0.0006887633935548365 current acc: 0.9171950187401765\n",
      "iteration 71 current loss: 0.0007764632464386523 current acc: 0.9172050290135396\n",
      "iteration 72 current loss: 0.0010834202403202653 current acc: 0.9172150368669165\n",
      "iteration 73 current loss: 0.000799009867478162 current acc: 0.9172250423011844\n",
      "iteration 74 current loss: 0.003228231333196163 current acc: 0.9172350453172206\n",
      "iteration 75 current loss: 0.0006336297374218702 current acc: 0.9172450459159014\n",
      "iteration 76 current loss: 0.0010703728767111897 current acc: 0.9172550440981032\n",
      "iteration 77 current loss: 0.0005047193844802678 current acc: 0.9172650398647016\n",
      "iteration 78 current loss: 0.0005442197434604168 current acc: 0.917275033216572\n",
      "iteration 79 current loss: 0.0010558163048699498 current acc: 0.9172850241545893\n",
      "iteration 80 current loss: 0.000825293071102351 current acc: 0.9172950126796281\n",
      "iteration 81 current loss: 0.0006339550600387156 current acc: 0.9173049987925622\n",
      "iteration 82 current loss: 0.0006314339698292315 current acc: 0.9173149824942654\n",
      "iteration 83 current loss: 0.0010106239933520555 current acc: 0.9173249637856108\n",
      "iteration 84 current loss: 0.0006327247829176486 current acc: 0.9173349426674713\n",
      "iteration 85 current loss: 0.0009564282954670489 current acc: 0.9173449191407193\n",
      "iteration 86 current loss: 0.0005334900924935937 current acc: 0.9173548932062267\n",
      "iteration 87 current loss: 0.0008748283726163208 current acc: 0.9173648648648649\n",
      "iteration 88 current loss: 0.003964317962527275 current acc: 0.9173748341175051\n",
      "iteration 89 current loss: 0.0006298369844444096 current acc: 0.9173848009650181\n",
      "iteration 90 current loss: 0.0012990179238840938 current acc: 0.917394765408274\n",
      "iteration 91 current loss: 0.000646193977445364 current acc: 0.9174047274481428\n",
      "iteration 92 current loss: 0.001022455282509327 current acc: 0.9174146870854938\n",
      "iteration 93 current loss: 0.0009012562804855406 current acc: 0.9174246443211961\n",
      "iteration 94 current loss: 0.000736933434382081 current acc: 0.9174345991561181\n",
      "iteration 95 current loss: 0.000645547523163259 current acc: 0.9174445515911283\n",
      "iteration 96 current loss: 0.0006728784064762294 current acc: 0.9174545016270941\n",
      "iteration 97 current loss: 0.0012069891672581434 current acc: 0.9174644492648831\n",
      "iteration 98 current loss: 0.000682479003444314 current acc: 0.9174743945053621\n",
      "iteration 99 current loss: 0.0005073752836324275 current acc: 0.9174843373493976\n",
      "\t\tEpoch 82/100 complete. Epoch loss 0.0009314210389857181 Epoch accuracy 1.0\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 82, Validation Accuracy: 0.621625, Validation Loss: 1.8824432224035264\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.0010715245734900236 current acc: 0.9174942777978556\n",
      "iteration 1 current loss: 0.0015898779965937138 current acc: 0.917504215851602\n",
      "iteration 2 current loss: 0.000820977846160531 current acc: 0.9175141515115018\n",
      "iteration 3 current loss: 0.0010526056867092848 current acc: 0.9175240847784201\n",
      "iteration 4 current loss: 0.0006178727489896119 current acc: 0.9175340156532209\n",
      "iteration 5 current loss: 0.000977661577053368 current acc: 0.9175439441367687\n",
      "iteration 6 current loss: 0.0005332447472028434 current acc: 0.9175538702299265\n",
      "iteration 7 current loss: 0.0006308757001534104 current acc: 0.917563793933558\n",
      "iteration 8 current loss: 0.0005068427417427301 current acc: 0.9175737152485257\n",
      "iteration 9 current loss: 0.001305192825384438 current acc: 0.9175836341756919\n",
      "iteration 10 current loss: 0.0006190190324559808 current acc: 0.9175935507159186\n",
      "iteration 11 current loss: 0.0008209132356569171 current acc: 0.9176034648700674\n",
      "iteration 12 current loss: 0.0006155837327241898 current acc: 0.9176133766389991\n",
      "iteration 13 current loss: 0.0005370154394768178 current acc: 0.9176232860235747\n",
      "iteration 14 current loss: 0.000701087701600045 current acc: 0.9176331930246543\n",
      "iteration 15 current loss: 0.0008514289511367679 current acc: 0.9176430976430976\n",
      "iteration 16 current loss: 0.0007291287183761597 current acc: 0.9176529998797643\n",
      "iteration 17 current loss: 0.0006910297088325024 current acc: 0.9176628997355133\n",
      "iteration 18 current loss: 0.0013823549961671233 current acc: 0.9176727972112033\n",
      "iteration 19 current loss: 0.0008384949760511518 current acc: 0.9176826923076923\n",
      "iteration 20 current loss: 0.00044364616042003036 current acc: 0.9176925850258383\n",
      "iteration 21 current loss: 0.000803475035354495 current acc: 0.9177024753664984\n",
      "iteration 22 current loss: 0.0008011667523533106 current acc: 0.9177123633305299\n",
      "iteration 23 current loss: 0.0008219292503781617 current acc: 0.9177222489187891\n",
      "iteration 24 current loss: 0.0010077428305521607 current acc: 0.9177321321321321\n",
      "iteration 25 current loss: 0.000762224430218339 current acc: 0.9177420129714149\n",
      "iteration 26 current loss: 0.0008901887340471148 current acc: 0.9177518914374925\n",
      "iteration 27 current loss: 0.00048324602539651096 current acc: 0.91776176753122\n",
      "iteration 28 current loss: 0.0010021984344348311 current acc: 0.9177716412534518\n",
      "iteration 29 current loss: 0.0006436489638872445 current acc: 0.917781512605042\n",
      "iteration 30 current loss: 0.0005363584496080875 current acc: 0.9177913815868444\n",
      "iteration 31 current loss: 0.0006654884782619774 current acc: 0.917801248199712\n",
      "iteration 32 current loss: 0.001047502737492323 current acc: 0.9178111124444978\n",
      "iteration 33 current loss: 0.0007475682068616152 current acc: 0.9178209743220542\n",
      "iteration 34 current loss: 0.0004978677025064826 current acc: 0.9178308338332334\n",
      "iteration 35 current loss: 0.0006899327854625881 current acc: 0.9178406909788868\n",
      "iteration 36 current loss: 0.000662390433717519 current acc: 0.9178505457598657\n",
      "iteration 37 current loss: 0.0007416041335090995 current acc: 0.9178603981770209\n",
      "iteration 38 current loss: 0.000753816741053015 current acc: 0.9178702482312028\n",
      "iteration 39 current loss: 0.0009988577803596854 current acc: 0.9178800959232614\n",
      "iteration 40 current loss: 0.0005627298378385603 current acc: 0.9178899412540463\n",
      "iteration 41 current loss: 0.0006268650759011507 current acc: 0.9178997842244067\n",
      "iteration 42 current loss: 0.0005259758909232914 current acc: 0.9179096248351912\n",
      "iteration 43 current loss: 0.0011731357080861926 current acc: 0.9179194630872484\n",
      "iteration 44 current loss: 0.0010988533031195402 current acc: 0.9179292989814261\n",
      "iteration 45 current loss: 0.0016523130470886827 current acc: 0.9179391325185717\n",
      "iteration 46 current loss: 0.0008363814558833838 current acc: 0.9179489636995327\n",
      "iteration 47 current loss: 0.000603624212089926 current acc: 0.9179587925251558\n",
      "iteration 48 current loss: 0.0006548592355102301 current acc: 0.917968618996287\n",
      "iteration 49 current loss: 0.0004287717747502029 current acc: 0.9179784431137724\n",
      "iteration 50 current loss: 0.0008205194608308375 current acc: 0.9179882648784576\n",
      "iteration 51 current loss: 0.0007503014639951289 current acc: 0.9179980842911878\n",
      "iteration 52 current loss: 0.0006762094562873244 current acc: 0.9180079013528074\n",
      "iteration 53 current loss: 0.0007544105756096542 current acc: 0.9180177160641608\n",
      "iteration 54 current loss: 0.0007237970712594688 current acc: 0.9180275284260921\n",
      "iteration 55 current loss: 0.000685530889313668 current acc: 0.9180373384394447\n",
      "iteration 56 current loss: 0.0004635583609342575 current acc: 0.9180471461050617\n",
      "iteration 57 current loss: 0.0010637551313266158 current acc: 0.9180569514237856\n",
      "iteration 58 current loss: 0.0007019518525339663 current acc: 0.9180667543964589\n",
      "iteration 59 current loss: 0.000828930118586868 current acc: 0.9180765550239235\n",
      "iteration 60 current loss: 0.00064667291007936 current acc: 0.9180863533070207\n",
      "iteration 61 current loss: 0.0007813681149855256 current acc: 0.9180961492465918\n",
      "iteration 62 current loss: 0.0005930504412390292 current acc: 0.9181059428434772\n",
      "iteration 63 current loss: 0.00046292509068734944 current acc: 0.9181157340985174\n",
      "iteration 64 current loss: 0.0003446074260864407 current acc: 0.9181255230125523\n",
      "iteration 65 current loss: 0.0006034689140506089 current acc: 0.9181353095864212\n",
      "iteration 66 current loss: 0.0006439462886191905 current acc: 0.9181450938209633\n",
      "iteration 67 current loss: 0.000616859586443752 current acc: 0.9181548757170173\n",
      "iteration 68 current loss: 0.000686895102262497 current acc: 0.9181646552754212\n",
      "iteration 69 current loss: 0.0006195190944708884 current acc: 0.9181744324970131\n",
      "iteration 70 current loss: 0.0004694979579653591 current acc: 0.9181842073826305\n",
      "iteration 71 current loss: 0.000736145069822669 current acc: 0.9181939799331104\n",
      "iteration 72 current loss: 0.0009649202111177146 current acc: 0.9182037501492893\n",
      "iteration 73 current loss: 0.0007473092409782112 current acc: 0.9182135180320038\n",
      "iteration 74 current loss: 0.0005453586927615106 current acc: 0.9182232835820896\n",
      "iteration 75 current loss: 0.0008495167712680995 current acc: 0.9182330468003821\n",
      "iteration 76 current loss: 0.0012771725887432694 current acc: 0.9182428076877164\n",
      "iteration 77 current loss: 0.0008765631355345249 current acc: 0.9182525662449272\n",
      "iteration 78 current loss: 0.0007161838584579527 current acc: 0.9182623224728488\n",
      "iteration 79 current loss: 0.0007633689674548805 current acc: 0.918272076372315\n",
      "iteration 80 current loss: 0.0007014711736701429 current acc: 0.9182818279441594\n",
      "iteration 81 current loss: 0.0008405555272474885 current acc: 0.918291577189215\n",
      "iteration 82 current loss: 0.0005203731125220656 current acc: 0.9183013241083144\n",
      "iteration 83 current loss: 0.0010405195644125342 current acc: 0.9183110687022901\n",
      "iteration 84 current loss: 0.000779768917709589 current acc: 0.9183208109719737\n",
      "iteration 85 current loss: 0.0009610552224330604 current acc: 0.918330550918197\n",
      "iteration 86 current loss: 0.0007797692669555545 current acc: 0.9183402885417908\n",
      "iteration 87 current loss: 0.0004886107635684311 current acc: 0.9183500238435861\n",
      "iteration 88 current loss: 0.0008760514901950955 current acc: 0.918359756824413\n",
      "iteration 89 current loss: 0.0006450903601944447 current acc: 0.9183694874851013\n",
      "iteration 90 current loss: 0.0006183214718475938 current acc: 0.9183792158264807\n",
      "iteration 91 current loss: 0.000617676880210638 current acc: 0.9183889418493804\n",
      "iteration 92 current loss: 0.0010601370595395565 current acc: 0.9183986655546289\n",
      "iteration 93 current loss: 0.0005445127026177943 current acc: 0.9184083869430546\n",
      "iteration 94 current loss: 0.0006985221989452839 current acc: 0.9184181060154855\n",
      "iteration 95 current loss: 0.0008279149187728763 current acc: 0.9184278227727489\n",
      "iteration 96 current loss: 0.0006834203959442675 current acc: 0.9184375372156722\n",
      "iteration 97 current loss: 0.0008655451820231974 current acc: 0.9184472493450822\n",
      "iteration 98 current loss: 0.0005812032613903284 current acc: 0.918456959161805\n",
      "iteration 99 current loss: 0.0005615492700599134 current acc: 0.9184666666666667\n",
      "\t\tEpoch 83/100 complete. Epoch loss 0.0007616347912698984 Epoch accuracy 1.0\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 83, Validation Accuracy: 0.622125, Validation Loss: 1.8755328256636858\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.0006907717906869948 current acc: 0.9184763718604928\n",
      "iteration 1 current loss: 0.0005139096174389124 current acc: 0.9184860747441086\n",
      "iteration 2 current loss: 0.0004380251339171082 current acc: 0.9184957753183387\n",
      "iteration 3 current loss: 0.0006193025619722903 current acc: 0.9185054735840076\n",
      "iteration 4 current loss: 0.0010436405427753925 current acc: 0.9185151695419393\n",
      "iteration 5 current loss: 0.0005836336058564484 current acc: 0.9185248631929575\n",
      "iteration 6 current loss: 0.0006223087548278272 current acc: 0.9185345545378851\n",
      "iteration 7 current loss: 0.0006224006065167487 current acc: 0.9185442435775452\n",
      "iteration 8 current loss: 0.000869335257448256 current acc: 0.9185539303127601\n",
      "iteration 9 current loss: 0.00047371984692290425 current acc: 0.918563614744352\n",
      "iteration 10 current loss: 0.0007178244995884597 current acc: 0.9185732968731423\n",
      "iteration 11 current loss: 0.0005682662595063448 current acc: 0.9185829766999525\n",
      "iteration 12 current loss: 0.0006244073156267405 current acc: 0.9185926542256032\n",
      "iteration 13 current loss: 0.0006203930242918432 current acc: 0.9186023294509151\n",
      "iteration 14 current loss: 0.0013158575166016817 current acc: 0.9186120023767083\n",
      "iteration 15 current loss: 0.0009623845107853413 current acc: 0.9186216730038023\n",
      "iteration 16 current loss: 0.0008707054075784981 current acc: 0.9186313413330165\n",
      "iteration 17 current loss: 0.0008063189452514052 current acc: 0.9186410073651698\n",
      "iteration 18 current loss: 0.0006964071071706712 current acc: 0.9186506711010809\n",
      "iteration 19 current loss: 0.0006873101810924709 current acc: 0.9186603325415676\n",
      "iteration 20 current loss: 0.001238116528838873 current acc: 0.918669991687448\n",
      "iteration 21 current loss: 0.0009683780372142792 current acc: 0.9186796485395393\n",
      "iteration 22 current loss: 0.0006311418255791068 current acc: 0.9186893030986585\n",
      "iteration 23 current loss: 0.0009237050544470549 current acc: 0.9186989553656221\n",
      "iteration 24 current loss: 0.0007366423960775137 current acc: 0.9187086053412463\n",
      "iteration 25 current loss: 0.00048172296374104917 current acc: 0.918718253026347\n",
      "iteration 26 current loss: 0.0012355399085208774 current acc: 0.9187278984217396\n",
      "iteration 27 current loss: 0.0007376527646556497 current acc: 0.9187375415282392\n",
      "iteration 28 current loss: 0.0008823386742733419 current acc: 0.9187471823466603\n",
      "iteration 29 current loss: 0.0006649125134572387 current acc: 0.9187568208778173\n",
      "iteration 30 current loss: 0.0006287545547820628 current acc: 0.918766457122524\n",
      "iteration 31 current loss: 0.000752733729314059 current acc: 0.918776091081594\n",
      "iteration 32 current loss: 0.0005716985324397683 current acc: 0.9187857227558401\n",
      "iteration 33 current loss: 0.0009828145848587155 current acc: 0.9187953521460754\n",
      "iteration 34 current loss: 0.00045502078137360513 current acc: 0.918804979253112\n",
      "iteration 35 current loss: 0.0008254167623817921 current acc: 0.9188146040777619\n",
      "iteration 36 current loss: 0.00036671466659754515 current acc: 0.9188242266208367\n",
      "iteration 37 current loss: 0.0004804185009561479 current acc: 0.9188338468831476\n",
      "iteration 38 current loss: 0.0004560619709081948 current acc: 0.9188434648655054\n",
      "iteration 39 current loss: 0.0004799184971489012 current acc: 0.9188530805687204\n",
      "iteration 40 current loss: 0.000694829213898629 current acc: 0.9188626939936027\n",
      "iteration 41 current loss: 0.0008071331540122628 current acc: 0.9188723051409619\n",
      "iteration 42 current loss: 0.0005309488042257726 current acc: 0.9188819140116072\n",
      "iteration 43 current loss: 0.0005192617536522448 current acc: 0.9188915206063477\n",
      "iteration 44 current loss: 0.00046261571696959436 current acc: 0.9189011249259917\n",
      "iteration 45 current loss: 0.0005171935772523284 current acc: 0.9189107269713473\n",
      "iteration 46 current loss: 0.0004934082389809191 current acc: 0.9189203267432224\n",
      "iteration 47 current loss: 0.0004482059448491782 current acc: 0.9189299242424243\n",
      "iteration 48 current loss: 0.000761900853831321 current acc: 0.9189395194697597\n",
      "iteration 49 current loss: 0.001056155888363719 current acc: 0.9189491124260355\n",
      "iteration 50 current loss: 0.0003749342286027968 current acc: 0.9189587031120577\n",
      "iteration 51 current loss: 0.0006627771072089672 current acc: 0.9189682915286322\n",
      "iteration 52 current loss: 0.0005608923966065049 current acc: 0.9189778776765646\n",
      "iteration 53 current loss: 0.0009068633080460131 current acc: 0.9189874615566596\n",
      "iteration 54 current loss: 0.0006050894153304398 current acc: 0.9189970431697221\n",
      "iteration 55 current loss: 0.0006185310194268823 current acc: 0.9190066225165563\n",
      "iteration 56 current loss: 0.0005057359812781215 current acc: 0.9190161995979662\n",
      "iteration 57 current loss: 0.0007236104574985802 current acc: 0.9190257744147553\n",
      "iteration 58 current loss: 0.00045733596198260784 current acc: 0.9190353469677267\n",
      "iteration 59 current loss: 0.0007722215377725661 current acc: 0.9190449172576832\n",
      "iteration 60 current loss: 0.00045995667460374534 current acc: 0.9190544852854272\n",
      "iteration 61 current loss: 0.0008230675593949854 current acc: 0.9190640510517608\n",
      "iteration 62 current loss: 0.0005115254898555577 current acc: 0.9190736145574855\n",
      "iteration 63 current loss: 0.0005173409590497613 current acc: 0.9190831758034026\n",
      "iteration 64 current loss: 0.0009143131319433451 current acc: 0.9190927347903131\n",
      "iteration 65 current loss: 0.0008761739009059966 current acc: 0.9191022915190172\n",
      "iteration 66 current loss: 0.0006470965454354882 current acc: 0.9191118459903154\n",
      "iteration 67 current loss: 0.0005307307001203299 current acc: 0.919121398205007\n",
      "iteration 68 current loss: 0.00046045705676078796 current acc: 0.9191309481638918\n",
      "iteration 69 current loss: 0.0009145537042059004 current acc: 0.9191404958677686\n",
      "iteration 70 current loss: 0.0005504945293068886 current acc: 0.9191500413174359\n",
      "iteration 71 current loss: 0.0007516244659200311 current acc: 0.9191595845136922\n",
      "iteration 72 current loss: 0.0004382906190585345 current acc: 0.9191691254573351\n",
      "iteration 73 current loss: 0.000555751146748662 current acc: 0.9191786641491622\n",
      "iteration 74 current loss: 0.0009099809103645384 current acc: 0.9191882005899705\n",
      "iteration 75 current loss: 0.0010165837593376637 current acc: 0.9191977347805569\n",
      "iteration 76 current loss: 0.0005059387185610831 current acc: 0.9192072667217176\n",
      "iteration 77 current loss: 0.0005119823617860675 current acc: 0.9192167964142487\n",
      "iteration 78 current loss: 0.0004790173552464694 current acc: 0.9192263238589456\n",
      "iteration 79 current loss: 0.0008396629127673805 current acc: 0.9192358490566038\n",
      "iteration 80 current loss: 0.0010055303573608398 current acc: 0.9192453720080179\n",
      "iteration 81 current loss: 0.0008910897304303944 current acc: 0.9192548927139825\n",
      "iteration 82 current loss: 0.0005434383638203144 current acc: 0.9192644111752918\n",
      "iteration 83 current loss: 0.0004816311120521277 current acc: 0.9192739273927393\n",
      "iteration 84 current loss: 0.0006008119089528918 current acc: 0.9192834413671185\n",
      "iteration 85 current loss: 0.0007057022303342819 current acc: 0.9192929530992222\n",
      "iteration 86 current loss: 0.0007004700601100922 current acc: 0.9193024625898433\n",
      "iteration 87 current loss: 0.0005883682169951499 current acc: 0.9193119698397738\n",
      "iteration 88 current loss: 0.0007674274384044111 current acc: 0.9193214748498056\n",
      "iteration 89 current loss: 0.00041611716733314097 current acc: 0.9193309776207302\n",
      "iteration 90 current loss: 0.0006717607611790299 current acc: 0.9193404781533389\n",
      "iteration 91 current loss: 0.0005215654382482171 current acc: 0.9193499764484221\n",
      "iteration 92 current loss: 0.0008705007494427264 current acc: 0.9193594725067703\n",
      "iteration 93 current loss: 0.0006639055791310966 current acc: 0.9193689663291735\n",
      "iteration 94 current loss: 0.0006805331795476377 current acc: 0.9193784579164215\n",
      "iteration 95 current loss: 0.0005641268799081445 current acc: 0.9193879472693032\n",
      "iteration 96 current loss: 0.0006334308418445289 current acc: 0.9193974343886078\n",
      "iteration 97 current loss: 0.0004918213235214353 current acc: 0.9194069192751235\n",
      "iteration 98 current loss: 0.00040800179704092443 current acc: 0.9194164019296388\n",
      "iteration 99 current loss: 0.0006834313389845192 current acc: 0.9194258823529412\n",
      "\t\tEpoch 84/100 complete. Epoch loss 0.0006743047726922669 Epoch accuracy 1.0\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 84, Validation Accuracy: 0.623625, Validation Loss: 1.8892865795642138\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.0006545374053530395 current acc: 0.9194353605458181\n",
      "iteration 1 current loss: 0.0004596803046297282 current acc: 0.9194448365090567\n",
      "iteration 2 current loss: 0.0004215530934743583 current acc: 0.9194543102434435\n",
      "iteration 3 current loss: 0.0007130415760912001 current acc: 0.9194637817497648\n",
      "iteration 4 current loss: 0.0008537740213796496 current acc: 0.9194732510288066\n",
      "iteration 5 current loss: 0.0005855096387676895 current acc: 0.9194827180813543\n",
      "iteration 6 current loss: 0.000584831926971674 current acc: 0.9194921829081932\n",
      "iteration 7 current loss: 0.0007200322579592466 current acc: 0.9195016455101082\n",
      "iteration 8 current loss: 0.0006035528494976461 current acc: 0.9195111058878834\n",
      "iteration 9 current loss: 0.0020296594593673944 current acc: 0.9195205640423032\n",
      "iteration 10 current loss: 0.0004427068051882088 current acc: 0.9195300199741511\n",
      "iteration 11 current loss: 0.0009925254853442311 current acc: 0.9195394736842105\n",
      "iteration 12 current loss: 0.0006244317046366632 current acc: 0.9195489251732644\n",
      "iteration 13 current loss: 0.00044787611113861203 current acc: 0.9195583744420954\n",
      "iteration 14 current loss: 0.0006582333007827401 current acc: 0.9195678214914856\n",
      "iteration 15 current loss: 0.0005203266628086567 current acc: 0.919577266322217\n",
      "iteration 16 current loss: 0.0005524154985323548 current acc: 0.919586708935071\n",
      "iteration 17 current loss: 0.0005718453321605921 current acc: 0.9195961493308288\n",
      "iteration 18 current loss: 0.0007127687567844987 current acc: 0.9196055875102711\n",
      "iteration 19 current loss: 0.000578580773435533 current acc: 0.9196150234741785\n",
      "iteration 20 current loss: 0.0005360188079066575 current acc: 0.9196244572233306\n",
      "iteration 21 current loss: 0.0006226508994586766 current acc: 0.9196338887585074\n",
      "iteration 22 current loss: 0.000779397611040622 current acc: 0.919643318080488\n",
      "iteration 23 current loss: 0.0007407211232930422 current acc: 0.9196527451900516\n",
      "iteration 24 current loss: 0.0006334701320156455 current acc: 0.9196621700879766\n",
      "iteration 25 current loss: 0.001134751015342772 current acc: 0.9196715927750411\n",
      "iteration 26 current loss: 0.0003422982699703425 current acc: 0.919681013252023\n",
      "iteration 27 current loss: 0.0006342142587527633 current acc: 0.9196904315196998\n",
      "iteration 28 current loss: 0.0005052587948739529 current acc: 0.9196998475788486\n",
      "iteration 29 current loss: 0.0004917075857520103 current acc: 0.9197092614302462\n",
      "iteration 30 current loss: 0.00044857049942947924 current acc: 0.9197186730746688\n",
      "iteration 31 current loss: 0.001792416675016284 current acc: 0.9197280825128926\n",
      "iteration 32 current loss: 0.0007435403531417251 current acc: 0.9197374897456931\n",
      "iteration 33 current loss: 0.0005157081177458167 current acc: 0.9197468947738457\n",
      "iteration 34 current loss: 0.00047731303493492305 current acc: 0.9197562975981254\n",
      "iteration 35 current loss: 0.0010595846688374877 current acc: 0.9197656982193064\n",
      "iteration 36 current loss: 0.0005901087424717844 current acc: 0.9197750966381633\n",
      "iteration 37 current loss: 0.0008051998447626829 current acc: 0.9197844928554697\n",
      "iteration 38 current loss: 0.0008429973386228085 current acc: 0.919793886871999\n",
      "iteration 39 current loss: 0.000558557512704283 current acc: 0.9198032786885246\n",
      "iteration 40 current loss: 0.0009342856938019395 current acc: 0.919812668305819\n",
      "iteration 41 current loss: 0.0011771763674914837 current acc: 0.9198220557246547\n",
      "iteration 42 current loss: 0.0009275528718717396 current acc: 0.9198314409458036\n",
      "iteration 43 current loss: 0.0006235403707250953 current acc: 0.9198408239700374\n",
      "iteration 44 current loss: 0.0005158676067367196 current acc: 0.9198502047981275\n",
      "iteration 45 current loss: 0.0010387200163677335 current acc: 0.9198595834308448\n",
      "iteration 46 current loss: 0.00046212441520765424 current acc: 0.9198689598689599\n",
      "iteration 47 current loss: 0.00064998515881598 current acc: 0.9198783341132428\n",
      "iteration 48 current loss: 0.0004937751800753176 current acc: 0.9198877061644637\n",
      "iteration 49 current loss: 0.0005005180719308555 current acc: 0.9198970760233918\n",
      "iteration 50 current loss: 0.00047677737893536687 current acc: 0.9199064436907964\n",
      "iteration 51 current loss: 0.0009315552306361496 current acc: 0.9199158091674462\n",
      "iteration 52 current loss: 0.00027922962908633053 current acc: 0.9199251724541097\n",
      "iteration 53 current loss: 0.000591157702729106 current acc: 0.9199345335515549\n",
      "iteration 54 current loss: 0.0006283575785346329 current acc: 0.9199438924605494\n",
      "iteration 55 current loss: 0.0009970043320208788 current acc: 0.9199532491818607\n",
      "iteration 56 current loss: 0.00038350652903318405 current acc: 0.9199626037162557\n",
      "iteration 57 current loss: 0.0006891717202961445 current acc: 0.9199719560645011\n",
      "iteration 58 current loss: 0.0005109760677441955 current acc: 0.919981306227363\n",
      "iteration 59 current loss: 0.0007347287028096616 current acc: 0.9199906542056074\n",
      "iteration 60 current loss: 0.0008291670819744468 current acc: 0.92\n",
      "iteration 61 current loss: 0.0009260858641937375 current acc: 0.9200093436113058\n",
      "iteration 62 current loss: 0.0008061306434683502 current acc: 0.9200186850402896\n",
      "iteration 63 current loss: 0.0014279469614848495 current acc: 0.920028024287716\n",
      "iteration 64 current loss: 0.0003866049519274384 current acc: 0.9200373613543491\n",
      "iteration 65 current loss: 0.0006697236676700413 current acc: 0.9200466962409526\n",
      "iteration 66 current loss: 0.0006684527616016567 current acc: 0.92005602894829\n",
      "iteration 67 current loss: 0.0004777258145622909 current acc: 0.9200653594771242\n",
      "iteration 68 current loss: 0.0004310056974645704 current acc: 0.920074687828218\n",
      "iteration 69 current loss: 0.0008689559763297439 current acc: 0.9200840140023337\n",
      "iteration 70 current loss: 0.0007076452020555735 current acc: 0.9200933380002333\n",
      "iteration 71 current loss: 0.0004952930030412972 current acc: 0.9201026598226785\n",
      "iteration 72 current loss: 0.0006208113045431674 current acc: 0.9201119794704304\n",
      "iteration 73 current loss: 0.0005754022859036922 current acc: 0.92012129694425\n",
      "iteration 74 current loss: 0.0006827075267210603 current acc: 0.920130612244898\n",
      "iteration 75 current loss: 0.00045723255607299507 current acc: 0.9201399253731344\n",
      "iteration 76 current loss: 0.0004467297112569213 current acc: 0.920149236329719\n",
      "iteration 77 current loss: 0.0006906281923875213 current acc: 0.9201585451154115\n",
      "iteration 78 current loss: 0.0006669850554317236 current acc: 0.9201678517309709\n",
      "iteration 79 current loss: 0.0007367823272943497 current acc: 0.9201771561771562\n",
      "iteration 80 current loss: 0.0005937641835771501 current acc: 0.9201864584547256\n",
      "iteration 81 current loss: 0.0009911463130265474 current acc: 0.9201957585644372\n",
      "iteration 82 current loss: 0.0012098237639293075 current acc: 0.9202050565070489\n",
      "iteration 83 current loss: 0.0003666783741209656 current acc: 0.9202143522833178\n",
      "iteration 84 current loss: 0.0005009204032830894 current acc: 0.9202236458940012\n",
      "iteration 85 current loss: 0.0005058193346485496 current acc: 0.9202329373398556\n",
      "iteration 86 current loss: 0.0007863775826990604 current acc: 0.9202422266216373\n",
      "iteration 87 current loss: 0.0005131277721375227 current acc: 0.9202515137401025\n",
      "iteration 88 current loss: 0.0004952356102876365 current acc: 0.9202607986960065\n",
      "iteration 89 current loss: 0.0006052652606740594 current acc: 0.9202700814901048\n",
      "iteration 90 current loss: 0.0004054257879033685 current acc: 0.9202793621231521\n",
      "iteration 91 current loss: 0.0007375552668236196 current acc: 0.9202886405959032\n",
      "iteration 92 current loss: 0.0004814287240151316 current acc: 0.9202979169091121\n",
      "iteration 93 current loss: 0.00038494187174364924 current acc: 0.9203071910635326\n",
      "iteration 94 current loss: 0.00039508641930297017 current acc: 0.9203164630599185\n",
      "iteration 95 current loss: 0.0006073393160477281 current acc: 0.9203257328990228\n",
      "iteration 96 current loss: 0.0004162922268733382 current acc: 0.9203350005815982\n",
      "iteration 97 current loss: 0.0006419930723495781 current acc: 0.9203442661083973\n",
      "iteration 98 current loss: 0.00040445529157295823 current acc: 0.9203535294801721\n",
      "iteration 99 current loss: 0.00047810139949433506 current acc: 0.9203627906976745\n",
      "\t\tEpoch 85/100 complete. Epoch loss 0.0006662117343512364 Epoch accuracy 1.0\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 85, Validation Accuracy: 0.6245, Validation Loss: 1.9073768012225627\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.0008872028556652367 current acc: 0.9203720497616557\n",
      "iteration 1 current loss: 0.0005279945326037705 current acc: 0.9203813066728668\n",
      "iteration 2 current loss: 0.0004646100860554725 current acc: 0.9203905614320586\n",
      "iteration 3 current loss: 0.0006966613000258803 current acc: 0.9203998140399814\n",
      "iteration 4 current loss: 0.0005075304070487618 current acc: 0.9204090644973852\n",
      "iteration 5 current loss: 0.0003306384023744613 current acc: 0.9204183128050197\n",
      "iteration 6 current loss: 0.0005982333677820861 current acc: 0.9204275589636343\n",
      "iteration 7 current loss: 0.0005444615962915123 current acc: 0.9204368029739777\n",
      "iteration 8 current loss: 0.0005413235630840063 current acc: 0.9204460448367987\n",
      "iteration 9 current loss: 0.0005050646723248065 current acc: 0.9204552845528455\n",
      "iteration 10 current loss: 0.0009042291203513741 current acc: 0.9204645221228661\n",
      "iteration 11 current loss: 0.0008658877923153341 current acc: 0.920473757547608\n",
      "iteration 12 current loss: 0.0008059886749833822 current acc: 0.9204829908278184\n",
      "iteration 13 current loss: 0.00032407775870524347 current acc: 0.9204922219642443\n",
      "iteration 14 current loss: 0.0031233453191816807 current acc: 0.920501450957632\n",
      "iteration 15 current loss: 0.0009615690796636045 current acc: 0.920510677808728\n",
      "iteration 16 current loss: 0.0006375600351020694 current acc: 0.9205199025182779\n",
      "iteration 17 current loss: 0.0009932041866704822 current acc: 0.9205291250870271\n",
      "iteration 18 current loss: 0.0012828661128878593 current acc: 0.9205383455157211\n",
      "iteration 19 current loss: 0.0014362360816448927 current acc: 0.9205475638051044\n",
      "iteration 20 current loss: 0.0008404882391914725 current acc: 0.9205567799559216\n",
      "iteration 21 current loss: 0.0007372339023277164 current acc: 0.9205659939689167\n",
      "iteration 22 current loss: 0.0004652476345654577 current acc: 0.9205752058448335\n",
      "iteration 23 current loss: 0.000723633449524641 current acc: 0.9205844155844156\n",
      "iteration 24 current loss: 0.0006239092908799648 current acc: 0.9205936231884058\n",
      "iteration 25 current loss: 0.002160829957574606 current acc: 0.920602828657547\n",
      "iteration 26 current loss: 0.0005016299546696246 current acc: 0.9206120319925815\n",
      "iteration 27 current loss: 0.0009357903036288917 current acc: 0.9206212331942513\n",
      "iteration 28 current loss: 0.0006986313965171576 current acc: 0.9206304322632982\n",
      "iteration 29 current loss: 0.0006591757410205901 current acc: 0.9206396292004635\n",
      "iteration 30 current loss: 0.0006410335772670805 current acc: 0.9206488240064883\n",
      "iteration 31 current loss: 0.00037427281495183706 current acc: 0.9206580166821131\n",
      "iteration 32 current loss: 0.00045182311441749334 current acc: 0.9206672072280783\n",
      "iteration 33 current loss: 0.0012578064342960715 current acc: 0.9206763956451239\n",
      "iteration 34 current loss: 0.0012674673926085234 current acc: 0.9206855819339895\n",
      "iteration 35 current loss: 0.001126087037846446 current acc: 0.9206947660954146\n",
      "iteration 36 current loss: 0.0005559263518080115 current acc: 0.9207039481301378\n",
      "iteration 37 current loss: 0.0007226045709103346 current acc: 0.9207131280388979\n",
      "iteration 38 current loss: 0.0006657601916231215 current acc: 0.9207223058224332\n",
      "iteration 39 current loss: 0.0004131619934923947 current acc: 0.9207314814814814\n",
      "iteration 40 current loss: 0.0007176075014285743 current acc: 0.9207406550167805\n",
      "iteration 41 current loss: 0.0005361537914723158 current acc: 0.9207498264290673\n",
      "iteration 42 current loss: 0.0003834815579466522 current acc: 0.920758995719079\n",
      "iteration 43 current loss: 0.0008715840522199869 current acc: 0.920768162887552\n",
      "iteration 44 current loss: 0.0005642514443024993 current acc: 0.9207773279352227\n",
      "iteration 45 current loss: 0.0007356886053457856 current acc: 0.9207864908628267\n",
      "iteration 46 current loss: 0.000574426376260817 current acc: 0.9207956516710998\n",
      "iteration 47 current loss: 0.0007454436272382736 current acc: 0.9208048103607771\n",
      "iteration 48 current loss: 0.00047969367005862296 current acc: 0.9208139669325933\n",
      "iteration 49 current loss: 0.00040082645136862993 current acc: 0.9208231213872833\n",
      "iteration 50 current loss: 0.0005241132457740605 current acc: 0.9208322737255809\n",
      "iteration 51 current loss: 0.0005901889526285231 current acc: 0.9208414239482201\n",
      "iteration 52 current loss: 0.0004220558039378375 current acc: 0.9208505720559343\n",
      "iteration 53 current loss: 0.0006453873356804252 current acc: 0.9208597180494569\n",
      "iteration 54 current loss: 0.0007765828631818295 current acc: 0.9208688619295206\n",
      "iteration 55 current loss: 0.0009782093111425638 current acc: 0.9208780036968577\n",
      "iteration 56 current loss: 0.000582858978305012 current acc: 0.9208871433522006\n",
      "iteration 57 current loss: 0.0006037615821696818 current acc: 0.920896280896281\n",
      "iteration 58 current loss: 0.0006650045979768038 current acc: 0.9209054163298303\n",
      "iteration 59 current loss: 0.0006121373153291643 current acc: 0.9209145496535797\n",
      "iteration 60 current loss: 0.0004437075986061245 current acc: 0.92092368086826\n",
      "iteration 61 current loss: 0.00047429860569536686 current acc: 0.9209328099746017\n",
      "iteration 62 current loss: 0.0005391158629208803 current acc: 0.9209419369733348\n",
      "iteration 63 current loss: 0.0007002289639785886 current acc: 0.9209510618651893\n",
      "iteration 64 current loss: 0.0005833191680721939 current acc: 0.9209601846508944\n",
      "iteration 65 current loss: 0.0006823400617577136 current acc: 0.9209693053311793\n",
      "iteration 66 current loss: 0.0003476036945357919 current acc: 0.9209784239067729\n",
      "iteration 67 current loss: 0.0008299228502437472 current acc: 0.9209875403784034\n",
      "iteration 68 current loss: 0.00036924739833921194 current acc: 0.9209966547467989\n",
      "iteration 69 current loss: 0.000593138684052974 current acc: 0.9210057670126874\n",
      "iteration 70 current loss: 0.0008998659322969615 current acc: 0.9210148771767962\n",
      "iteration 71 current loss: 0.0006946308421902359 current acc: 0.9210239852398524\n",
      "iteration 72 current loss: 0.0008218316943384707 current acc: 0.9210330912025827\n",
      "iteration 73 current loss: 0.0004888399271294475 current acc: 0.9210421950657136\n",
      "iteration 74 current loss: 0.0003340329567436129 current acc: 0.9210512968299712\n",
      "iteration 75 current loss: 0.0007501429063268006 current acc: 0.9210603964960812\n",
      "iteration 76 current loss: 0.000548672629520297 current acc: 0.9210694940647689\n",
      "iteration 77 current loss: 0.00202216231264174 current acc: 0.9210785895367596\n",
      "iteration 78 current loss: 0.0007517588092014194 current acc: 0.921087682912778\n",
      "iteration 79 current loss: 0.0006505817291326821 current acc: 0.9210967741935484\n",
      "iteration 80 current loss: 0.0004553355392999947 current acc: 0.9211058633797949\n",
      "iteration 81 current loss: 0.00042514712549746037 current acc: 0.9211149504722415\n",
      "iteration 82 current loss: 0.0005820441292598844 current acc: 0.9211240354716111\n",
      "iteration 83 current loss: 0.0006930778035894036 current acc: 0.9211331183786273\n",
      "iteration 84 current loss: 0.0007219479302875698 current acc: 0.9211421991940126\n",
      "iteration 85 current loss: 0.0006026654737070203 current acc: 0.9211512779184895\n",
      "iteration 86 current loss: 0.0005148821510374546 current acc: 0.92116035455278\n",
      "iteration 87 current loss: 0.0005429559387266636 current acc: 0.9211694290976059\n",
      "iteration 88 current loss: 0.00035458008642308414 current acc: 0.9211785015536885\n",
      "iteration 89 current loss: 0.0007711920188739896 current acc: 0.9211875719217492\n",
      "iteration 90 current loss: 0.00041950689046643674 current acc: 0.9211966402025084\n",
      "iteration 91 current loss: 0.00029031391022726893 current acc: 0.9212057063966866\n",
      "iteration 92 current loss: 0.00044760521268472075 current acc: 0.921214770505004\n",
      "iteration 93 current loss: 0.0008283923380076885 current acc: 0.9212238325281804\n",
      "iteration 94 current loss: 0.0009018281125463545 current acc: 0.9212328924669351\n",
      "iteration 95 current loss: 0.001028155442327261 current acc: 0.9212419503219871\n",
      "iteration 96 current loss: 0.00053496501641348 current acc: 0.9212510060940554\n",
      "iteration 97 current loss: 0.000543536152690649 current acc: 0.9212600597838584\n",
      "iteration 98 current loss: 0.0005426620482467115 current acc: 0.921269111392114\n",
      "iteration 99 current loss: 0.0006248198333196342 current acc: 0.9212781609195402\n",
      "\t\tEpoch 86/100 complete. Epoch loss 0.0007052171113900841 Epoch accuracy 1.0\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 86, Validation Accuracy: 0.624625, Validation Loss: 1.9010260537266732\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.0005349338171072304 current acc: 0.9212872083668544\n",
      "iteration 1 current loss: 0.000539844564627856 current acc: 0.9212962537347736\n",
      "iteration 2 current loss: 0.0005271509289741516 current acc: 0.9213052970240148\n",
      "iteration 3 current loss: 0.0007066023536026478 current acc: 0.9213143382352941\n",
      "iteration 4 current loss: 0.0003615851455833763 current acc: 0.921323377369328\n",
      "iteration 5 current loss: 0.0006339251413010061 current acc: 0.9213324144268321\n",
      "iteration 6 current loss: 0.00048564118333160877 current acc: 0.9213414494085219\n",
      "iteration 7 current loss: 0.0008100105915218592 current acc: 0.9213504823151125\n",
      "iteration 8 current loss: 0.0006402360158972442 current acc: 0.9213595131473189\n",
      "iteration 9 current loss: 0.0009577667224220932 current acc: 0.9213685419058554\n",
      "iteration 10 current loss: 0.0003772833151742816 current acc: 0.9213775685914362\n",
      "iteration 11 current loss: 0.0006269472651183605 current acc: 0.921386593204775\n",
      "iteration 12 current loss: 0.00041173581848852336 current acc: 0.9213956157465856\n",
      "iteration 13 current loss: 0.0005607125931419432 current acc: 0.921404636217581\n",
      "iteration 14 current loss: 0.00033992226235568523 current acc: 0.9214136546184739\n",
      "iteration 15 current loss: 0.0007191119948402047 current acc: 0.9214226709499771\n",
      "iteration 16 current loss: 0.0004482197400648147 current acc: 0.9214316852128026\n",
      "iteration 17 current loss: 0.0004824268107768148 current acc: 0.9214406974076623\n",
      "iteration 18 current loss: 0.00111480918712914 current acc: 0.9214497075352678\n",
      "iteration 19 current loss: 0.0005626749480143189 current acc: 0.9214587155963303\n",
      "iteration 20 current loss: 0.0004725354665424675 current acc: 0.9214677215915607\n",
      "iteration 21 current loss: 0.0009725496056489646 current acc: 0.9214767255216694\n",
      "iteration 22 current loss: 0.000614988908637315 current acc: 0.9214857273873668\n",
      "iteration 23 current loss: 0.0005286040832288563 current acc: 0.9214947271893627\n",
      "iteration 24 current loss: 0.00037547587999142706 current acc: 0.9215037249283667\n",
      "iteration 25 current loss: 0.0004295116232242435 current acc: 0.9215127206050883\n",
      "iteration 26 current loss: 0.0004909403505735099 current acc: 0.9215217142202361\n",
      "iteration 27 current loss: 0.0004416879382915795 current acc: 0.9215307057745188\n",
      "iteration 28 current loss: 0.0003848940832540393 current acc: 0.9215396952686448\n",
      "iteration 29 current loss: 0.0004594107740558684 current acc: 0.9215486827033219\n",
      "iteration 30 current loss: 0.000697607989422977 current acc: 0.9215576680792578\n",
      "iteration 31 current loss: 0.00038028263952583075 current acc: 0.9215666513971599\n",
      "iteration 32 current loss: 0.000623902422375977 current acc: 0.921575632657735\n",
      "iteration 33 current loss: 0.0005999881541356444 current acc: 0.9215846118616899\n",
      "iteration 34 current loss: 0.0006002073641866446 current acc: 0.921593589009731\n",
      "iteration 35 current loss: 0.0008494715439155698 current acc: 0.9216025641025641\n",
      "iteration 36 current loss: 0.0005930186598561704 current acc: 0.921611537140895\n",
      "iteration 37 current loss: 0.0005088988109491765 current acc: 0.9216205081254292\n",
      "iteration 38 current loss: 0.0007095095352269709 current acc: 0.9216294770568715\n",
      "iteration 39 current loss: 0.00040236616041511297 current acc: 0.9216384439359268\n",
      "iteration 40 current loss: 0.0005968011682853103 current acc: 0.9216474087632994\n",
      "iteration 41 current loss: 0.00034057642915286124 current acc: 0.9216563715396935\n",
      "iteration 42 current loss: 0.0003664920513983816 current acc: 0.9216653322658126\n",
      "iteration 43 current loss: 0.0017008823342621326 current acc: 0.9216742909423605\n",
      "iteration 44 current loss: 0.0006466691847890615 current acc: 0.92168324757004\n",
      "iteration 45 current loss: 0.0003869605716317892 current acc: 0.921692202149554\n",
      "iteration 46 current loss: 0.0006595498416572809 current acc: 0.9217011546816051\n",
      "iteration 47 current loss: 0.00042453297646716237 current acc: 0.9217101051668953\n",
      "iteration 48 current loss: 0.00034823030000552535 current acc: 0.9217190536061264\n",
      "iteration 49 current loss: 0.00042864258284680545 current acc: 0.921728\n",
      "iteration 50 current loss: 0.0005179483559913933 current acc: 0.9217369443492173\n",
      "iteration 51 current loss: 0.0007777858409099281 current acc: 0.921745886654479\n",
      "iteration 52 current loss: 0.0005768528208136559 current acc: 0.9217548269164858\n",
      "iteration 53 current loss: 0.000429684529080987 current acc: 0.9217637651359378\n",
      "iteration 54 current loss: 0.000383967359084636 current acc: 0.9217727013135352\n",
      "iteration 55 current loss: 0.0007051449501886964 current acc: 0.9217816354499772\n",
      "iteration 56 current loss: 0.0007394863641820848 current acc: 0.9217905675459632\n",
      "iteration 57 current loss: 0.00048311043065041304 current acc: 0.9217994976021923\n",
      "iteration 58 current loss: 0.0011913226917386055 current acc: 0.9218084256193629\n",
      "iteration 59 current loss: 0.0004072138690389693 current acc: 0.9218173515981735\n",
      "iteration 60 current loss: 0.0006729886517859995 current acc: 0.921826275539322\n",
      "iteration 61 current loss: 0.0004643714346457273 current acc: 0.9218351974435061\n",
      "iteration 62 current loss: 0.00048467848682776093 current acc: 0.921844117311423\n",
      "iteration 63 current loss: 0.0009019323624670506 current acc: 0.9218530351437699\n",
      "iteration 64 current loss: 0.0006207950646057725 current acc: 0.9218619509412436\n",
      "iteration 65 current loss: 0.0004971110029146075 current acc: 0.9218708647045403\n",
      "iteration 66 current loss: 0.0009819696424528956 current acc: 0.9218797764343561\n",
      "iteration 67 current loss: 0.000663021404761821 current acc: 0.9218886861313869\n",
      "iteration 68 current loss: 0.0006392054492607713 current acc: 0.921897593796328\n",
      "iteration 69 current loss: 0.0003481815801933408 current acc: 0.9219064994298746\n",
      "iteration 70 current loss: 0.0003518985176924616 current acc: 0.9219154030327215\n",
      "iteration 71 current loss: 0.00041772882104851305 current acc: 0.9219243046055632\n",
      "iteration 72 current loss: 0.0005060508265160024 current acc: 0.9219332041490939\n",
      "iteration 73 current loss: 0.000612060131970793 current acc: 0.9219421016640073\n",
      "iteration 74 current loss: 0.0006501004681922495 current acc: 0.9219509971509972\n",
      "iteration 75 current loss: 0.0006812150240875781 current acc: 0.9219598906107566\n",
      "iteration 76 current loss: 0.00041291379602625966 current acc: 0.9219687820439786\n",
      "iteration 77 current loss: 0.0006625675596296787 current acc: 0.9219776714513557\n",
      "iteration 78 current loss: 0.000961463840212673 current acc: 0.9219865588335802\n",
      "iteration 79 current loss: 0.0004122603568248451 current acc: 0.9219954441913439\n",
      "iteration 80 current loss: 0.0003803557774517685 current acc: 0.9220043275253388\n",
      "iteration 81 current loss: 0.0003934803535230458 current acc: 0.9220132088362559\n",
      "iteration 82 current loss: 0.0007863747305236757 current acc: 0.9220220881247865\n",
      "iteration 83 current loss: 0.0007701668655499816 current acc: 0.9220309653916211\n",
      "iteration 84 current loss: 0.0005871964967809618 current acc: 0.9220398406374501\n",
      "iteration 85 current loss: 0.0004111271118745208 current acc: 0.9220487138629638\n",
      "iteration 86 current loss: 0.00041672118823044 current acc: 0.9220575850688517\n",
      "iteration 87 current loss: 0.0005089424084872007 current acc: 0.9220664542558034\n",
      "iteration 88 current loss: 0.0006070748786441982 current acc: 0.9220753214245079\n",
      "iteration 89 current loss: 0.000594191369600594 current acc: 0.9220841865756542\n",
      "iteration 90 current loss: 0.000854055630043149 current acc: 0.9220930497099306\n",
      "iteration 91 current loss: 0.0006779743707738817 current acc: 0.9221019108280255\n",
      "iteration 92 current loss: 0.0004013811412733048 current acc: 0.9221107699306267\n",
      "iteration 93 current loss: 0.000360555830411613 current acc: 0.9221196270184217\n",
      "iteration 94 current loss: 0.0006586462259292603 current acc: 0.9221284820920977\n",
      "iteration 95 current loss: 0.0004476238100323826 current acc: 0.922137335152342\n",
      "iteration 96 current loss: 0.0005535089876502752 current acc: 0.9221461861998409\n",
      "iteration 97 current loss: 0.0005767179536633193 current acc: 0.9221550352352808\n",
      "iteration 98 current loss: 0.0003937419969588518 current acc: 0.9221638822593476\n",
      "iteration 99 current loss: 0.00042055698577314615 current acc: 0.9221727272727273\n",
      "\t\tEpoch 87/100 complete. Epoch loss 0.0005783215558039956 Epoch accuracy 1.0\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 87, Validation Accuracy: 0.623625, Validation Loss: 1.9192744024097919\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.0003610494313761592 current acc: 0.922181570276105\n",
      "iteration 1 current loss: 0.00047079642536118627 current acc: 0.9221904112701659\n",
      "iteration 2 current loss: 0.0004084196698386222 current acc: 0.9221992502555947\n",
      "iteration 3 current loss: 0.0003689471632242203 current acc: 0.9222080872330759\n",
      "iteration 4 current loss: 0.000529496930539608 current acc: 0.9222169222032935\n",
      "iteration 5 current loss: 0.0003625185345299542 current acc: 0.9222257551669316\n",
      "iteration 6 current loss: 0.00045890684123151004 current acc: 0.9222345861246736\n",
      "iteration 7 current loss: 0.0005978826666250825 current acc: 0.9222434150772025\n",
      "iteration 8 current loss: 0.0005976647371426225 current acc: 0.9222522420252015\n",
      "iteration 9 current loss: 0.00044913351302966475 current acc: 0.9222610669693531\n",
      "iteration 10 current loss: 0.000285729649476707 current acc: 0.9222698899103393\n",
      "iteration 11 current loss: 0.00038205913733690977 current acc: 0.9222787108488425\n",
      "iteration 12 current loss: 0.00044954390614293516 current acc: 0.9222875297855441\n",
      "iteration 13 current loss: 0.0005408807774074376 current acc: 0.9222963467211255\n",
      "iteration 14 current loss: 0.0009593742433935404 current acc: 0.9223051616562677\n",
      "iteration 15 current loss: 0.0005997264524921775 current acc: 0.9223139745916515\n",
      "iteration 16 current loss: 0.0003589786356315017 current acc: 0.9223227855279573\n",
      "iteration 17 current loss: 0.0005005409475415945 current acc: 0.9223315944658653\n",
      "iteration 18 current loss: 0.0002636938588693738 current acc: 0.9223404014060551\n",
      "iteration 19 current loss: 0.00025117944460362196 current acc: 0.9223492063492064\n",
      "iteration 20 current loss: 0.00044988785521127284 current acc: 0.9223580092959982\n",
      "iteration 21 current loss: 0.0005037579103372991 current acc: 0.9223668102471095\n",
      "iteration 22 current loss: 0.0005321471253409982 current acc: 0.9223756092032188\n",
      "iteration 23 current loss: 0.000675262650474906 current acc: 0.9223844061650045\n",
      "iteration 24 current loss: 0.0005242754705250263 current acc: 0.9223932011331445\n",
      "iteration 25 current loss: 0.00040519374306313694 current acc: 0.9224019941083164\n",
      "iteration 26 current loss: 0.0003407016338314861 current acc: 0.9224107850911974\n",
      "iteration 27 current loss: 0.0006725828279741108 current acc: 0.9224195740824649\n",
      "iteration 28 current loss: 0.00034494727151468396 current acc: 0.9224283610827954\n",
      "iteration 29 current loss: 0.0004173534398432821 current acc: 0.9224371460928652\n",
      "iteration 30 current loss: 0.00043113864376209676 current acc: 0.9224459291133507\n",
      "iteration 31 current loss: 0.0005975668900646269 current acc: 0.9224547101449275\n",
      "iteration 32 current loss: 0.0002768115955404937 current acc: 0.9224634891882713\n",
      "iteration 33 current loss: 0.0004717850242741406 current acc: 0.9224722662440571\n",
      "iteration 34 current loss: 0.0004584557900670916 current acc: 0.9224810413129598\n",
      "iteration 35 current loss: 0.0003938064619433135 current acc: 0.9224898143956541\n",
      "iteration 36 current loss: 0.0007090261788107455 current acc: 0.9224985854928143\n",
      "iteration 37 current loss: 0.0003457144775893539 current acc: 0.9225073546051142\n",
      "iteration 38 current loss: 0.0006108339875936508 current acc: 0.9225161217332277\n",
      "iteration 39 current loss: 0.0006271932506933808 current acc: 0.9225248868778281\n",
      "iteration 40 current loss: 0.0003274788032285869 current acc: 0.9225336500395883\n",
      "iteration 41 current loss: 0.0003144352522213012 current acc: 0.9225424112191812\n",
      "iteration 42 current loss: 0.00046192278387025 current acc: 0.9225511704172792\n",
      "iteration 43 current loss: 0.00032452985760755837 current acc: 0.9225599276345545\n",
      "iteration 44 current loss: 0.000325840461300686 current acc: 0.9225686828716789\n",
      "iteration 45 current loss: 0.0007375430432148278 current acc: 0.922577436129324\n",
      "iteration 46 current loss: 0.0005455524660646915 current acc: 0.922586187408161\n",
      "iteration 47 current loss: 0.00047200865810737014 current acc: 0.9225949367088607\n",
      "iteration 48 current loss: 0.0005585768376477063 current acc: 0.922603684032094\n",
      "iteration 49 current loss: 0.00033551969681866467 current acc: 0.922612429378531\n",
      "iteration 50 current loss: 0.0005193995893932879 current acc: 0.9226211727488419\n",
      "iteration 51 current loss: 0.00034849048824980855 current acc: 0.9226299141436963\n",
      "iteration 52 current loss: 0.0003908181970473379 current acc: 0.9226386535637637\n",
      "iteration 53 current loss: 0.0003772501659113914 current acc: 0.9226473910097132\n",
      "iteration 54 current loss: 0.0008255165885202587 current acc: 0.9226561264822134\n",
      "iteration 55 current loss: 0.0006619656924158335 current acc: 0.9226648599819331\n",
      "iteration 56 current loss: 0.0006088926456868649 current acc: 0.9226735915095404\n",
      "iteration 57 current loss: 0.0004229400074109435 current acc: 0.9226823210657034\n",
      "iteration 58 current loss: 0.0003208325942978263 current acc: 0.9226910486510893\n",
      "iteration 59 current loss: 0.0007044071098789573 current acc: 0.9226997742663657\n",
      "iteration 60 current loss: 0.0006169532425701618 current acc: 0.9227084979121996\n",
      "iteration 61 current loss: 0.0007695323438383639 current acc: 0.9227172195892575\n",
      "iteration 62 current loss: 0.00027619607863016427 current acc: 0.922725939298206\n",
      "iteration 63 current loss: 0.0009887891355901957 current acc: 0.9227346570397111\n",
      "iteration 64 current loss: 0.0006595124141313136 current acc: 0.9227433728144389\n",
      "iteration 65 current loss: 0.00038778374437242746 current acc: 0.9227520866230544\n",
      "iteration 66 current loss: 0.00042040616972371936 current acc: 0.922760798466223\n",
      "iteration 67 current loss: 0.00048384119872935116 current acc: 0.9227695083446098\n",
      "iteration 68 current loss: 0.0004630909243132919 current acc: 0.9227782162588792\n",
      "iteration 69 current loss: 0.0004690605856012553 current acc: 0.9227869222096956\n",
      "iteration 70 current loss: 0.0004408629611134529 current acc: 0.9227956261977229\n",
      "iteration 71 current loss: 0.00046573381405323744 current acc: 0.9228043282236249\n",
      "iteration 72 current loss: 0.0005611923988908529 current acc: 0.9228130282880649\n",
      "iteration 73 current loss: 0.0006443539168685675 current acc: 0.9228217263917061\n",
      "iteration 74 current loss: 0.0007174212951213121 current acc: 0.9228304225352113\n",
      "iteration 75 current loss: 0.0004893610603176057 current acc: 0.9228391167192429\n",
      "iteration 76 current loss: 0.0004491944273468107 current acc: 0.9228478089444632\n",
      "iteration 77 current loss: 0.00027423753635957837 current acc: 0.9228564992115341\n",
      "iteration 78 current loss: 0.0003932779945898801 current acc: 0.9228651875211172\n",
      "iteration 79 current loss: 0.0005390477017499506 current acc: 0.9228738738738739\n",
      "iteration 80 current loss: 0.0003250004374422133 current acc: 0.9228825582704651\n",
      "iteration 81 current loss: 0.0006180237978696823 current acc: 0.9228912407115515\n",
      "iteration 82 current loss: 0.0008860337547957897 current acc: 0.9228999211977935\n",
      "iteration 83 current loss: 0.0005019196541979909 current acc: 0.9229085997298514\n",
      "iteration 84 current loss: 0.0006277889478951693 current acc: 0.9229172763083849\n",
      "iteration 85 current loss: 0.0004054750897921622 current acc: 0.9229259509340536\n",
      "iteration 86 current loss: 0.00047740229638293386 current acc: 0.9229346236075165\n",
      "iteration 87 current loss: 0.000476106652058661 current acc: 0.9229432943294329\n",
      "iteration 88 current loss: 0.00034103012876585126 current acc: 0.9229519631004612\n",
      "iteration 89 current loss: 0.0004621654807124287 current acc: 0.9229606299212598\n",
      "iteration 90 current loss: 0.00037331777275539935 current acc: 0.9229692947924868\n",
      "iteration 91 current loss: 0.0005605762708000839 current acc: 0.9229779577147998\n",
      "iteration 92 current loss: 0.0004999752854928374 current acc: 0.9229866186888565\n",
      "iteration 93 current loss: 0.0004966268897987902 current acc: 0.9229952777153136\n",
      "iteration 94 current loss: 0.00043077164446003735 current acc: 0.9230039347948286\n",
      "iteration 95 current loss: 0.00035847266553901136 current acc: 0.9230125899280576\n",
      "iteration 96 current loss: 0.0004024654335808009 current acc: 0.9230212431156569\n",
      "iteration 97 current loss: 0.0005804747343063354 current acc: 0.9230298943582828\n",
      "iteration 98 current loss: 0.0005709425895474851 current acc: 0.9230385436565907\n",
      "iteration 99 current loss: 0.0005915295332670212 current acc: 0.923047191011236\n",
      "\t\tEpoch 88/100 complete. Epoch loss 0.0004906483413651586 Epoch accuracy 1.0\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 88, Validation Accuracy: 0.62275, Validation Loss: 1.9214074540883304\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.0004081536899320781 current acc: 0.9230558364228738\n",
      "iteration 1 current loss: 0.00047831711708568037 current acc: 0.9230644798921591\n",
      "iteration 2 current loss: 0.00042495160596445203 current acc: 0.9230731214197462\n",
      "iteration 3 current loss: 0.0009679526556283236 current acc: 0.9230817610062894\n",
      "iteration 4 current loss: 0.0002820214140228927 current acc: 0.9230903986524425\n",
      "iteration 5 current loss: 0.0006116906297393143 current acc: 0.9230990343588592\n",
      "iteration 6 current loss: 0.0002999568823724985 current acc: 0.9231076681261929\n",
      "iteration 7 current loss: 0.0004198873066343367 current acc: 0.9231162999550966\n",
      "iteration 8 current loss: 0.0006220498471520841 current acc: 0.923124929846223\n",
      "iteration 9 current loss: 0.0003702518006321043 current acc: 0.9231335578002244\n",
      "iteration 10 current loss: 0.0005096200620755553 current acc: 0.9231421838177534\n",
      "iteration 11 current loss: 0.00073792360490188 current acc: 0.9231508078994614\n",
      "iteration 12 current loss: 0.00045489834155887365 current acc: 0.9231594300460002\n",
      "iteration 13 current loss: 0.00037116423482075334 current acc: 0.9231680502580211\n",
      "iteration 14 current loss: 0.0006678325589746237 current acc: 0.923176668536175\n",
      "iteration 15 current loss: 0.0003231842420063913 current acc: 0.9231852848811126\n",
      "iteration 16 current loss: 0.00043695536442101 current acc: 0.9231938992934844\n",
      "iteration 17 current loss: 0.00046568133984692395 current acc: 0.9232025117739403\n",
      "iteration 18 current loss: 0.000676944328006357 current acc: 0.9232111223231304\n",
      "iteration 19 current loss: 0.0005257706507109106 current acc: 0.9232197309417041\n",
      "iteration 20 current loss: 0.0003640561189968139 current acc: 0.9232283376303105\n",
      "iteration 21 current loss: 0.0007161003304645419 current acc: 0.9232369423895987\n",
      "iteration 22 current loss: 0.0003200508654117584 current acc: 0.9232455452202174\n",
      "iteration 23 current loss: 0.00037996075116097927 current acc: 0.9232541461228149\n",
      "iteration 24 current loss: 0.0005589014035649598 current acc: 0.9232627450980392\n",
      "iteration 25 current loss: 0.0005212030373513699 current acc: 0.9232713421465382\n",
      "iteration 26 current loss: 0.0003805736778303981 current acc: 0.9232799372689593\n",
      "iteration 27 current loss: 0.00035568579914979637 current acc: 0.9232885304659498\n",
      "iteration 28 current loss: 0.0006660432554781437 current acc: 0.9232971217381566\n",
      "iteration 29 current loss: 0.0005671581020578742 current acc: 0.9233057110862262\n",
      "iteration 30 current loss: 0.0003491834504529834 current acc: 0.923314298510805\n",
      "iteration 31 current loss: 0.0005763028748333454 current acc: 0.9233228840125391\n",
      "iteration 32 current loss: 0.0005475443322211504 current acc: 0.9233314675920743\n",
      "iteration 33 current loss: 0.00029879820067435503 current acc: 0.923340049250056\n",
      "iteration 34 current loss: 0.0007122494280338287 current acc: 0.9233486289871292\n",
      "iteration 35 current loss: 0.0003318835806567222 current acc: 0.9233572068039391\n",
      "iteration 36 current loss: 0.0002983380982186645 current acc: 0.9233657827011301\n",
      "iteration 37 current loss: 0.0005992183578200638 current acc: 0.9233743566793466\n",
      "iteration 38 current loss: 0.000672830268740654 current acc: 0.9233829287392326\n",
      "iteration 39 current loss: 0.000426863698521629 current acc: 0.9233914988814318\n",
      "iteration 40 current loss: 0.0004080283979419619 current acc: 0.9234000671065876\n",
      "iteration 41 current loss: 0.00046539102913811803 current acc: 0.9234086334153433\n",
      "iteration 42 current loss: 0.0004975534975528717 current acc: 0.9234171978083417\n",
      "iteration 43 current loss: 0.0006609841948375106 current acc: 0.9234257602862254\n",
      "iteration 44 current loss: 0.000414312060456723 current acc: 0.9234343208496366\n",
      "iteration 45 current loss: 0.0003520736354403198 current acc: 0.9234428794992176\n",
      "iteration 46 current loss: 0.000385157618438825 current acc: 0.9234514362356097\n",
      "iteration 47 current loss: 0.00044288169010542333 current acc: 0.9234599910594546\n",
      "iteration 48 current loss: 0.0005675459979102015 current acc: 0.9234685439713934\n",
      "iteration 49 current loss: 0.0004832023987546563 current acc: 0.9234770949720671\n",
      "iteration 50 current loss: 0.0005014156340621412 current acc: 0.923485644062116\n",
      "iteration 51 current loss: 0.00045324547681957483 current acc: 0.9234941912421806\n",
      "iteration 52 current loss: 0.00040014542173594236 current acc: 0.9235027365129007\n",
      "iteration 53 current loss: 0.0006835748790763319 current acc: 0.9235112798749162\n",
      "iteration 54 current loss: 0.00028927368111908436 current acc: 0.9235198213288666\n",
      "iteration 55 current loss: 0.0003816923126578331 current acc: 0.9235283608753908\n",
      "iteration 56 current loss: 0.0004201757546979934 current acc: 0.9235368985151279\n",
      "iteration 57 current loss: 0.0005361789371818304 current acc: 0.9235454342487163\n",
      "iteration 58 current loss: 0.000391077104723081 current acc: 0.9235539680767942\n",
      "iteration 59 current loss: 0.0004823594936169684 current acc: 0.9235625\n",
      "iteration 60 current loss: 0.00030595407588407397 current acc: 0.9235710300189711\n",
      "iteration 61 current loss: 0.00044343926128931344 current acc: 0.923579558134345\n",
      "iteration 62 current loss: 0.000546990311704576 current acc: 0.9235880843467589\n",
      "iteration 63 current loss: 0.0003744309360627085 current acc: 0.9235966086568497\n",
      "iteration 64 current loss: 0.0004867382231168449 current acc: 0.9236051310652538\n",
      "iteration 65 current loss: 0.00032692551030777395 current acc: 0.9236136515726077\n",
      "iteration 66 current loss: 0.00028172816382721066 current acc: 0.9236221701795472\n",
      "iteration 67 current loss: 0.0004635005025193095 current acc: 0.9236306868867082\n",
      "iteration 68 current loss: 0.0004807843652088195 current acc: 0.9236392016947262\n",
      "iteration 69 current loss: 0.000524896546266973 current acc: 0.9236477146042363\n",
      "iteration 70 current loss: 0.00043032917892560363 current acc: 0.9236562256158733\n",
      "iteration 71 current loss: 0.0006346348091028631 current acc: 0.923664734730272\n",
      "iteration 72 current loss: 0.0003165027010254562 current acc: 0.9236732419480664\n",
      "iteration 73 current loss: 0.0003701159730553627 current acc: 0.9236817472698908\n",
      "iteration 74 current loss: 0.0006035869009792805 current acc: 0.9236902506963788\n",
      "iteration 75 current loss: 0.00045211095130071044 current acc: 0.923698752228164\n",
      "iteration 76 current loss: 0.0005379553767852485 current acc: 0.9237072518658794\n",
      "iteration 77 current loss: 0.0006113375420682132 current acc: 0.9237157496101581\n",
      "iteration 78 current loss: 0.0003612609871197492 current acc: 0.9237242454616327\n",
      "iteration 79 current loss: 0.00048289692495018244 current acc: 0.9237327394209354\n",
      "iteration 80 current loss: 0.0006761032273061574 current acc: 0.9237412314886984\n",
      "iteration 81 current loss: 0.00044566235737875104 current acc: 0.9237497216655534\n",
      "iteration 82 current loss: 0.0007863230421207845 current acc: 0.9237582099521318\n",
      "iteration 83 current loss: 0.0006092459079809487 current acc: 0.923766696349065\n",
      "iteration 84 current loss: 0.0010918297339230776 current acc: 0.9237751808569838\n",
      "iteration 85 current loss: 0.0007995928172022104 current acc: 0.9237836634765191\n",
      "iteration 86 current loss: 0.0002556182735133916 current acc: 0.9237921442083009\n",
      "iteration 87 current loss: 0.0007997392676770687 current acc: 0.9238006230529595\n",
      "iteration 88 current loss: 0.0003861681616399437 current acc: 0.9238091000111247\n",
      "iteration 89 current loss: 0.0004662281717173755 current acc: 0.923817575083426\n",
      "iteration 90 current loss: 0.00045050217886455357 current acc: 0.9238260482704928\n",
      "iteration 91 current loss: 0.0004374995769467205 current acc: 0.9238345195729537\n",
      "iteration 92 current loss: 0.00036878191167488694 current acc: 0.9238429889914378\n",
      "iteration 93 current loss: 0.0002838211366906762 current acc: 0.9238514565265733\n",
      "iteration 94 current loss: 0.0003105691575910896 current acc: 0.9238599221789884\n",
      "iteration 95 current loss: 0.0007489997078664601 current acc: 0.9238683859493108\n",
      "iteration 96 current loss: 0.00027427671011537313 current acc: 0.9238768478381683\n",
      "iteration 97 current loss: 0.0003477731079328805 current acc: 0.923885307846188\n",
      "iteration 98 current loss: 0.00043144598021171987 current acc: 0.9238937659739971\n",
      "iteration 99 current loss: 0.0005302352365106344 current acc: 0.9239022222222222\n",
      "\t\tEpoch 89/100 complete. Epoch loss 0.0004845095743075945 Epoch accuracy 1.0\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 89, Validation Accuracy: 0.6235, Validation Loss: 1.9397990107536316\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.0004407390661071986 current acc: 0.9239106765914898\n",
      "iteration 1 current loss: 0.00033233192516490817 current acc: 0.9239191290824261\n",
      "iteration 2 current loss: 0.0003627002006396651 current acc: 0.923927579695657\n",
      "iteration 3 current loss: 0.0005675965221598744 current acc: 0.9239360284318081\n",
      "iteration 4 current loss: 0.0008611322264187038 current acc: 0.9239444752915047\n",
      "iteration 5 current loss: 0.00048139746650122106 current acc: 0.923952920275372\n",
      "iteration 6 current loss: 0.0005220931489020586 current acc: 0.9239613633840347\n",
      "iteration 7 current loss: 0.0006508295773528516 current acc: 0.9239698046181172\n",
      "iteration 8 current loss: 0.0002808353747241199 current acc: 0.923978243978244\n",
      "iteration 9 current loss: 0.00035861789365299046 current acc: 0.9239866814650388\n",
      "iteration 10 current loss: 0.0006427672342397273 current acc: 0.9239951170791255\n",
      "iteration 11 current loss: 0.0006900065345689654 current acc: 0.9240035508211274\n",
      "iteration 12 current loss: 0.00030148669611662626 current acc: 0.9240119826916676\n",
      "iteration 13 current loss: 0.0004891625139862299 current acc: 0.924020412691369\n",
      "iteration 14 current loss: 0.0004328067589085549 current acc: 0.9240288408208541\n",
      "iteration 15 current loss: 0.0004529549041762948 current acc: 0.9240372670807453\n",
      "iteration 16 current loss: 0.0003318274102639407 current acc: 0.9240456914716646\n",
      "iteration 17 current loss: 0.000343035877449438 current acc: 0.9240541139942338\n",
      "iteration 18 current loss: 0.0003753701166715473 current acc: 0.9240625346490742\n",
      "iteration 19 current loss: 0.00031266597216017544 current acc: 0.9240709534368071\n",
      "iteration 20 current loss: 0.0004354824195615947 current acc: 0.9240793703580534\n",
      "iteration 21 current loss: 0.0003055813431274146 current acc: 0.9240877854134338\n",
      "iteration 22 current loss: 0.0006230841390788555 current acc: 0.9240961986035686\n",
      "iteration 23 current loss: 0.0004052897565998137 current acc: 0.9241046099290781\n",
      "iteration 24 current loss: 0.0003990605764556676 current acc: 0.9241130193905818\n",
      "iteration 25 current loss: 0.00026753346901386976 current acc: 0.9241214269886994\n",
      "iteration 26 current loss: 0.004774364642798901 current acc: 0.92412983272405\n",
      "iteration 27 current loss: 0.00030065723694860935 current acc: 0.924138236597253\n",
      "iteration 28 current loss: 0.0006491751410067081 current acc: 0.9241466386089268\n",
      "iteration 29 current loss: 0.0008507631719112396 current acc: 0.92415503875969\n",
      "iteration 30 current loss: 0.00040255062049254775 current acc: 0.9241634370501606\n",
      "iteration 31 current loss: 0.0003235863405279815 current acc: 0.9241718334809566\n",
      "iteration 32 current loss: 0.0004617614613380283 current acc: 0.9241802280526956\n",
      "iteration 33 current loss: 0.0003723434347193688 current acc: 0.9241886207659952\n",
      "iteration 34 current loss: 0.0004414564464241266 current acc: 0.924197011621472\n",
      "iteration 35 current loss: 0.0006198559422045946 current acc: 0.9242054006197432\n",
      "iteration 36 current loss: 0.00034118775511160493 current acc: 0.9242137877614253\n",
      "iteration 37 current loss: 0.0009737820946611464 current acc: 0.9242221730471343\n",
      "iteration 38 current loss: 0.00036748030106537044 current acc: 0.9242305564774864\n",
      "iteration 39 current loss: 0.0006386134773492813 current acc: 0.9242389380530973\n",
      "iteration 40 current loss: 0.0004926884430460632 current acc: 0.9242473177745825\n",
      "iteration 41 current loss: 0.00032820136402733624 current acc: 0.9242556956425569\n",
      "iteration 42 current loss: 0.0005192968528717756 current acc: 0.9242640716576357\n",
      "iteration 43 current loss: 0.000477494701044634 current acc: 0.9242724458204334\n",
      "iteration 44 current loss: 0.00037685324787162244 current acc: 0.9242808181315644\n",
      "iteration 45 current loss: 0.0007520834915339947 current acc: 0.9242891885916427\n",
      "iteration 46 current loss: 0.00035559770185500383 current acc: 0.9242975572012821\n",
      "iteration 47 current loss: 0.0002664113708306104 current acc: 0.9243059239610963\n",
      "iteration 48 current loss: 0.00050004618242383 current acc: 0.9243142888716985\n",
      "iteration 49 current loss: 0.000670142937451601 current acc: 0.9243226519337017\n",
      "iteration 50 current loss: 0.0005478865932673216 current acc: 0.9243310131477185\n",
      "iteration 51 current loss: 0.0006161613273434341 current acc: 0.9243393725143615\n",
      "iteration 52 current loss: 0.0004619825922418386 current acc: 0.9243477300342428\n",
      "iteration 53 current loss: 0.0004937989288009703 current acc: 0.9243560857079743\n",
      "iteration 54 current loss: 0.0004019621410407126 current acc: 0.9243644395361679\n",
      "iteration 55 current loss: 0.0004684892774093896 current acc: 0.9243727915194346\n",
      "iteration 56 current loss: 0.00029965321300551295 current acc: 0.9243811416583858\n",
      "iteration 57 current loss: 0.0004550652811303735 current acc: 0.9243894899536321\n",
      "iteration 58 current loss: 0.000550129625480622 current acc: 0.9243978364057843\n",
      "iteration 59 current loss: 0.00029741637990809977 current acc: 0.9244061810154526\n",
      "iteration 60 current loss: 0.0005191618110984564 current acc: 0.9244145237832468\n",
      "iteration 61 current loss: 0.000434093177318573 current acc: 0.9244228647097771\n",
      "iteration 62 current loss: 0.0004312752280384302 current acc: 0.9244312037956527\n",
      "iteration 63 current loss: 0.0005295951850712299 current acc: 0.9244395410414827\n",
      "iteration 64 current loss: 0.00024753736215643585 current acc: 0.9244478764478764\n",
      "iteration 65 current loss: 0.0003065174678340554 current acc: 0.9244562100154423\n",
      "iteration 66 current loss: 0.0007625840371474624 current acc: 0.9244645417447888\n",
      "iteration 67 current loss: 0.00037397354026325047 current acc: 0.924472871636524\n",
      "iteration 68 current loss: 0.0004147384606767446 current acc: 0.924481199691256\n",
      "iteration 69 current loss: 0.0003682704409584403 current acc: 0.9244895259095921\n",
      "iteration 70 current loss: 0.0004375079879537225 current acc: 0.9244978502921398\n",
      "iteration 71 current loss: 0.0005225370987318456 current acc: 0.9245061728395062\n",
      "iteration 72 current loss: 0.0003655709442682564 current acc: 0.924514493552298\n",
      "iteration 73 current loss: 0.00046657706843689084 current acc: 0.9245228124311219\n",
      "iteration 74 current loss: 0.00041052873712033033 current acc: 0.924531129476584\n",
      "iteration 75 current loss: 0.00034690258326008916 current acc: 0.9245394446892904\n",
      "iteration 76 current loss: 0.0007190702599473298 current acc: 0.9245477580698469\n",
      "iteration 77 current loss: 0.0003894655383192003 current acc: 0.9245560696188588\n",
      "iteration 78 current loss: 0.00024965417105704546 current acc: 0.9245643793369314\n",
      "iteration 79 current loss: 0.000863411114551127 current acc: 0.9245726872246696\n",
      "iteration 80 current loss: 0.0007392469560727477 current acc: 0.9245809932826781\n",
      "iteration 81 current loss: 0.0004187368613202125 current acc: 0.9245892975115614\n",
      "iteration 82 current loss: 0.0003598667972255498 current acc: 0.9245975999119234\n",
      "iteration 83 current loss: 0.0003501257160678506 current acc: 0.9246059004843681\n",
      "iteration 84 current loss: 0.0006135505391284823 current acc: 0.9246141992294992\n",
      "iteration 85 current loss: 0.00041570543544366956 current acc: 0.9246224961479199\n",
      "iteration 86 current loss: 0.00029442179948091507 current acc: 0.9246307912402333\n",
      "iteration 87 current loss: 0.0012191731948405504 current acc: 0.9246390845070422\n",
      "iteration 88 current loss: 0.0003009283682331443 current acc: 0.9246473759489493\n",
      "iteration 89 current loss: 0.0003730404714588076 current acc: 0.9246556655665567\n",
      "iteration 90 current loss: 0.00030651511042378843 current acc: 0.9246639533604664\n",
      "iteration 91 current loss: 0.0006164206424728036 current acc: 0.9246722393312803\n",
      "iteration 92 current loss: 0.0002772430016193539 current acc: 0.9246805234795997\n",
      "iteration 93 current loss: 0.00027125494671054184 current acc: 0.924688805806026\n",
      "iteration 94 current loss: 0.0004301420121919364 current acc: 0.92469708631116\n",
      "iteration 95 current loss: 0.00044394200085662305 current acc: 0.9247053649956024\n",
      "iteration 96 current loss: 0.00038899717037566006 current acc: 0.9247136418599539\n",
      "iteration 97 current loss: 0.00044569559395313263 current acc: 0.9247219169048142\n",
      "iteration 98 current loss: 0.00041160776163451374 current acc: 0.9247301901307836\n",
      "iteration 99 current loss: 0.00036189774982631207 current acc: 0.9247384615384615\n",
      "\t\tEpoch 90/100 complete. Epoch loss 0.0005071281318669207 Epoch accuracy 1.0\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 90, Validation Accuracy: 0.62575, Validation Loss: 1.9504940059036016\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.0002379474026383832 current acc: 0.9247467311284474\n",
      "iteration 1 current loss: 0.000303692533634603 current acc: 0.9247549989013404\n",
      "iteration 2 current loss: 0.0005469064926728606 current acc: 0.9247632648577392\n",
      "iteration 3 current loss: 0.00022353502572514117 current acc: 0.9247715289982426\n",
      "iteration 4 current loss: 0.0004543276154436171 current acc: 0.9247797913234487\n",
      "iteration 5 current loss: 0.0003733382618520409 current acc: 0.9247880518339556\n",
      "iteration 6 current loss: 0.0003562497440725565 current acc: 0.9247963105303613\n",
      "iteration 7 current loss: 0.0004963711835443974 current acc: 0.9248045674132631\n",
      "iteration 8 current loss: 0.0005098750116303563 current acc: 0.9248128224832584\n",
      "iteration 9 current loss: 0.0003308262676000595 current acc: 0.924821075740944\n",
      "iteration 10 current loss: 0.000368507084203884 current acc: 0.9248293271869169\n",
      "iteration 11 current loss: 0.000347393739502877 current acc: 0.9248375768217735\n",
      "iteration 12 current loss: 0.00029658625135198236 current acc: 0.92484582464611\n",
      "iteration 13 current loss: 0.0009592315764166415 current acc: 0.9248540706605223\n",
      "iteration 14 current loss: 0.0005491970223374665 current acc: 0.9248623148656061\n",
      "iteration 15 current loss: 0.00022793981770519167 current acc: 0.924870557261957\n",
      "iteration 16 current loss: 0.0006407967885024846 current acc: 0.92487879785017\n",
      "iteration 17 current loss: 0.0004331497475504875 current acc: 0.92488703663084\n",
      "iteration 18 current loss: 0.00030525625334121287 current acc: 0.9248952736045619\n",
      "iteration 19 current loss: 0.000343418272677809 current acc: 0.9249035087719298\n",
      "iteration 20 current loss: 0.0005370259168557823 current acc: 0.924911742133538\n",
      "iteration 21 current loss: 0.0003898097202181816 current acc: 0.9249199736899802\n",
      "iteration 22 current loss: 0.0004810522368643433 current acc: 0.9249282034418502\n",
      "iteration 23 current loss: 0.00037278563831932843 current acc: 0.9249364313897414\n",
      "iteration 24 current loss: 0.00034844581387005746 current acc: 0.9249446575342466\n",
      "iteration 25 current loss: 0.0004714652895927429 current acc: 0.9249528818759588\n",
      "iteration 26 current loss: 0.00037764012813568115 current acc: 0.9249611044154706\n",
      "iteration 27 current loss: 0.00045116458204574883 current acc: 0.9249693251533743\n",
      "iteration 28 current loss: 0.00044136386713944376 current acc: 0.9249775440902618\n",
      "iteration 29 current loss: 0.000370396941434592 current acc: 0.924985761226725\n",
      "iteration 30 current loss: 0.00024571569520048797 current acc: 0.9249939765633556\n",
      "iteration 31 current loss: 0.00043049201485700905 current acc: 0.9250021901007446\n",
      "iteration 32 current loss: 0.00038283210597001016 current acc: 0.9250104018394832\n",
      "iteration 33 current loss: 0.0007058625924400985 current acc: 0.925018611780162\n",
      "iteration 34 current loss: 0.00020700256573036313 current acc: 0.9250268199233717\n",
      "iteration 35 current loss: 0.0006090954411774874 current acc: 0.9250350262697022\n",
      "iteration 36 current loss: 0.0003881577868014574 current acc: 0.9250432308197439\n",
      "iteration 37 current loss: 0.0003451091470196843 current acc: 0.9250514335740863\n",
      "iteration 38 current loss: 0.00031361408764496446 current acc: 0.9250596345333187\n",
      "iteration 39 current loss: 0.0003077899164054543 current acc: 0.9250678336980306\n",
      "iteration 40 current loss: 0.0005775142344646156 current acc: 0.9250760310688109\n",
      "iteration 41 current loss: 0.0004321412998251617 current acc: 0.9250842266462481\n",
      "iteration 42 current loss: 0.0002957686083391309 current acc: 0.9250924204309308\n",
      "iteration 43 current loss: 0.00039494133670814335 current acc: 0.9251006124234471\n",
      "iteration 44 current loss: 0.00029006635304540396 current acc: 0.9251088026243849\n",
      "iteration 45 current loss: 0.0003733987978193909 current acc: 0.925116991034332\n",
      "iteration 46 current loss: 0.0006173800211399794 current acc: 0.9251251776538756\n",
      "iteration 47 current loss: 0.0004881364875473082 current acc: 0.925133362483603\n",
      "iteration 48 current loss: 0.0005291716661304235 current acc: 0.925141545524101\n",
      "iteration 49 current loss: 0.0003825135063380003 current acc: 0.9251497267759563\n",
      "iteration 50 current loss: 0.0003878766729030758 current acc: 0.9251579062397552\n",
      "iteration 51 current loss: 0.00045173964463174343 current acc: 0.9251660839160839\n",
      "iteration 52 current loss: 0.0006127679953351617 current acc: 0.9251742598055283\n",
      "iteration 53 current loss: 0.00037217893986962736 current acc: 0.9251824339086738\n",
      "iteration 54 current loss: 0.00032180960988625884 current acc: 0.9251906062261059\n",
      "iteration 55 current loss: 0.00048724538646638393 current acc: 0.9251987767584098\n",
      "iteration 56 current loss: 0.0002223690098617226 current acc: 0.9252069455061701\n",
      "iteration 57 current loss: 0.0003459598810877651 current acc: 0.9252151124699716\n",
      "iteration 58 current loss: 0.0003184325178153813 current acc: 0.9252232776503985\n",
      "iteration 59 current loss: 0.0002080014382954687 current acc: 0.9252314410480349\n",
      "iteration 60 current loss: 0.0002396089257672429 current acc: 0.9252396026634647\n",
      "iteration 61 current loss: 0.00025112219736911356 current acc: 0.9252477624972714\n",
      "iteration 62 current loss: 0.00042552888044156134 current acc: 0.9252559205500382\n",
      "iteration 63 current loss: 0.0004396817530505359 current acc: 0.9252640768223483\n",
      "iteration 64 current loss: 0.0003052715619560331 current acc: 0.9252722313147845\n",
      "iteration 65 current loss: 0.0003243857645429671 current acc: 0.9252803840279293\n",
      "iteration 66 current loss: 0.0002613407850731164 current acc: 0.925288534962365\n",
      "iteration 67 current loss: 0.00015040021389722824 current acc: 0.9252966841186736\n",
      "iteration 68 current loss: 0.0004399300378281623 current acc: 0.925304831497437\n",
      "iteration 69 current loss: 0.00040885177440941334 current acc: 0.9253129770992367\n",
      "iteration 70 current loss: 0.000616922159679234 current acc: 0.9253211209246538\n",
      "iteration 71 current loss: 0.0003220515791326761 current acc: 0.9253292629742695\n",
      "iteration 72 current loss: 0.0010365619091317058 current acc: 0.9253374032486645\n",
      "iteration 73 current loss: 0.0003446848422754556 current acc: 0.9253455417484194\n",
      "iteration 74 current loss: 0.0003332403430249542 current acc: 0.9253536784741144\n",
      "iteration 75 current loss: 0.000264297443209216 current acc: 0.9253618134263295\n",
      "iteration 76 current loss: 0.00027130835223942995 current acc: 0.9253699466056445\n",
      "iteration 77 current loss: 0.00036422625998966396 current acc: 0.9253780780126389\n",
      "iteration 78 current loss: 0.00029470439767464995 current acc: 0.9253862076478919\n",
      "iteration 79 current loss: 0.0005133554805070162 current acc: 0.9253943355119826\n",
      "iteration 80 current loss: 0.004468882922083139 current acc: 0.9254024616054896\n",
      "iteration 81 current loss: 0.0002484722062945366 current acc: 0.9254105859289915\n",
      "iteration 82 current loss: 0.00022895455185789615 current acc: 0.9254187084830665\n",
      "iteration 83 current loss: 0.00035211205249652267 current acc: 0.9254268292682927\n",
      "iteration 84 current loss: 0.0006803377182222903 current acc: 0.9254349482852476\n",
      "iteration 85 current loss: 0.00044748690561391413 current acc: 0.9254430655345091\n",
      "iteration 86 current loss: 0.0005732951103709638 current acc: 0.9254511810166539\n",
      "iteration 87 current loss: 0.0004471399588510394 current acc: 0.9254592947322595\n",
      "iteration 88 current loss: 0.00031438746373169124 current acc: 0.9254674066819023\n",
      "iteration 89 current loss: 0.0004116830532439053 current acc: 0.9254755168661589\n",
      "iteration 90 current loss: 0.000574616075027734 current acc: 0.9254836252856055\n",
      "iteration 91 current loss: 0.0003247549757361412 current acc: 0.9254917319408181\n",
      "iteration 92 current loss: 0.0003712678444571793 current acc: 0.9254998368323725\n",
      "iteration 93 current loss: 0.0004924723762087524 current acc: 0.925507939960844\n",
      "iteration 94 current loss: 0.00048818052164278924 current acc: 0.9255160413268081\n",
      "iteration 95 current loss: 0.0005197043064981699 current acc: 0.9255241409308395\n",
      "iteration 96 current loss: 0.00046880971058271825 current acc: 0.9255322387735131\n",
      "iteration 97 current loss: 0.0004258441913407296 current acc: 0.9255403348554033\n",
      "iteration 98 current loss: 0.0008148226188495755 current acc: 0.9255484291770845\n",
      "iteration 99 current loss: 0.000427419989136979 current acc: 0.9255565217391304\n",
      "\t\tEpoch 91/100 complete. Epoch loss 0.0004527890020108316 Epoch accuracy 1.0\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 91, Validation Accuracy: 0.62025, Validation Loss: 1.9730074420571326\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.000805256946478039 current acc: 0.925564612542115\n",
      "iteration 1 current loss: 0.0010045006638392806 current acc: 0.9255727015866116\n",
      "iteration 2 current loss: 0.0004898464540019631 current acc: 0.9255807888731935\n",
      "iteration 3 current loss: 0.00041026092367246747 current acc: 0.9255888744024338\n",
      "iteration 4 current loss: 0.0006909712683409452 current acc: 0.925596958174905\n",
      "iteration 5 current loss: 0.0004886125680059195 current acc: 0.9256050401911796\n",
      "iteration 6 current loss: 0.0004481251526158303 current acc: 0.9256131204518301\n",
      "iteration 7 current loss: 0.0005524366861209273 current acc: 0.9256211989574283\n",
      "iteration 8 current loss: 0.0007324203033931553 current acc: 0.925629275708546\n",
      "iteration 9 current loss: 0.0003749698807951063 current acc: 0.9256373507057546\n",
      "iteration 10 current loss: 0.0008905433351173997 current acc: 0.9256454239496255\n",
      "iteration 11 current loss: 0.0004956547636538744 current acc: 0.9256534954407295\n",
      "iteration 12 current loss: 0.00036588695365935564 current acc: 0.9256615651796375\n",
      "iteration 13 current loss: 0.0002074668591376394 current acc: 0.9256696331669199\n",
      "iteration 14 current loss: 0.0002955723612103611 current acc: 0.925677699403147\n",
      "iteration 15 current loss: 0.00046335533261299133 current acc: 0.9256857638888889\n",
      "iteration 16 current loss: 0.0006216618348844349 current acc: 0.9256938266247152\n",
      "iteration 17 current loss: 0.0004335905541665852 current acc: 0.9257018876111955\n",
      "iteration 18 current loss: 0.00043929542880505323 current acc: 0.9257099468488991\n",
      "iteration 19 current loss: 0.0003041040326934308 current acc: 0.9257180043383948\n",
      "iteration 20 current loss: 0.0006634617457166314 current acc: 0.9257260600802516\n",
      "iteration 21 current loss: 0.0005362645606510341 current acc: 0.9257341140750379\n",
      "iteration 22 current loss: 0.00031055239378474653 current acc: 0.9257421663233222\n",
      "iteration 23 current loss: 0.000459744333056733 current acc: 0.9257502168256722\n",
      "iteration 24 current loss: 0.0003109484969172627 current acc: 0.9257582655826558\n",
      "iteration 25 current loss: 0.0004580870154313743 current acc: 0.9257663125948407\n",
      "iteration 26 current loss: 0.0005991390789858997 current acc: 0.9257743578627939\n",
      "iteration 27 current loss: 0.00021372262563090771 current acc: 0.9257824013870828\n",
      "iteration 28 current loss: 0.0004293898236937821 current acc: 0.925790443168274\n",
      "iteration 29 current loss: 0.0004869126423727721 current acc: 0.9257984832069339\n",
      "iteration 30 current loss: 0.0003035113913938403 current acc: 0.9258065215036291\n",
      "iteration 31 current loss: 0.0003977700835093856 current acc: 0.9258145580589254\n",
      "iteration 32 current loss: 0.0003141651686746627 current acc: 0.925822592873389\n",
      "iteration 33 current loss: 0.000720432901289314 current acc: 0.925830625947585\n",
      "iteration 34 current loss: 0.0003280988603364676 current acc: 0.925838657282079\n",
      "iteration 35 current loss: 0.0017262616893276572 current acc: 0.9258466868774361\n",
      "iteration 36 current loss: 0.0003361614653840661 current acc: 0.9258547147342211\n",
      "iteration 37 current loss: 0.00034799505374394357 current acc: 0.9258627408529985\n",
      "iteration 38 current loss: 0.00034948968095704913 current acc: 0.9258707652343328\n",
      "iteration 39 current loss: 0.00048751794383861125 current acc: 0.9258787878787879\n",
      "iteration 40 current loss: 0.0005085199954919517 current acc: 0.9258868087869279\n",
      "iteration 41 current loss: 0.0008858131477609277 current acc: 0.9258948279593162\n",
      "iteration 42 current loss: 0.0005472494522109628 current acc: 0.9259028453965162\n",
      "iteration 43 current loss: 0.003642500378191471 current acc: 0.9259108610990913\n",
      "iteration 44 current loss: 0.000286168564343825 current acc: 0.9259188750676041\n",
      "iteration 45 current loss: 0.0003525311185512692 current acc: 0.9259268873026173\n",
      "iteration 46 current loss: 0.000593349861446768 current acc: 0.9259348978046934\n",
      "iteration 47 current loss: 0.0003741932159755379 current acc: 0.9259429065743945\n",
      "iteration 48 current loss: 0.0006371331983245909 current acc: 0.9259509136122824\n",
      "iteration 49 current loss: 0.00022900637122802436 current acc: 0.9259589189189189\n",
      "iteration 50 current loss: 0.0007279313285835087 current acc: 0.9259669224948655\n",
      "iteration 51 current loss: 0.0032945803832262754 current acc: 0.9259749243406831\n",
      "iteration 52 current loss: 0.0011186563642695546 current acc: 0.9259829244569329\n",
      "iteration 53 current loss: 0.0006793120410293341 current acc: 0.9259909228441755\n",
      "iteration 54 current loss: 0.00034010838135145605 current acc: 0.9259989195029714\n",
      "iteration 55 current loss: 0.0015535523416474462 current acc: 0.9260069144338807\n",
      "iteration 56 current loss: 0.0006150863482616842 current acc: 0.9260149076374635\n",
      "iteration 57 current loss: 0.0004961482482030988 current acc: 0.9260228991142796\n",
      "iteration 58 current loss: 0.0008044871501624584 current acc: 0.9260308888648883\n",
      "iteration 59 current loss: 0.000698045187164098 current acc: 0.9260388768898488\n",
      "iteration 60 current loss: 0.0004005897499155253 current acc: 0.9260468631897203\n",
      "iteration 61 current loss: 0.0018874986562877893 current acc: 0.9260548477650615\n",
      "iteration 62 current loss: 0.0004628121096175164 current acc: 0.9260628306164309\n",
      "iteration 63 current loss: 0.0006094433483667672 current acc: 0.9260708117443869\n",
      "iteration 64 current loss: 0.0011295913718640804 current acc: 0.9260787911494873\n",
      "iteration 65 current loss: 0.0004539237997960299 current acc: 0.9260867688322901\n",
      "iteration 66 current loss: 0.0009389800834469497 current acc: 0.9260947447933527\n",
      "iteration 67 current loss: 0.0006806435412727296 current acc: 0.9261027190332326\n",
      "iteration 68 current loss: 0.0007596694049425423 current acc: 0.9261106915524868\n",
      "iteration 69 current loss: 0.0003688292927108705 current acc: 0.9261186623516721\n",
      "iteration 70 current loss: 0.0004930880386382341 current acc: 0.9261266314313451\n",
      "iteration 71 current loss: 0.0006599157932214439 current acc: 0.9261345987920622\n",
      "iteration 72 current loss: 0.0003444358007982373 current acc: 0.9261425644343794\n",
      "iteration 73 current loss: 0.0006999451434239745 current acc: 0.9261505283588527\n",
      "iteration 74 current loss: 0.0004900940111838281 current acc: 0.9261584905660377\n",
      "iteration 75 current loss: 0.0005877934745512903 current acc: 0.9261664510564899\n",
      "iteration 76 current loss: 0.0004531742015387863 current acc: 0.9261744098307643\n",
      "iteration 77 current loss: 0.0005088060279376805 current acc: 0.9261823668894158\n",
      "iteration 78 current loss: 0.0005714779254049063 current acc: 0.9261903222329992\n",
      "iteration 79 current loss: 0.0004891676944680512 current acc: 0.9261982758620689\n",
      "iteration 80 current loss: 0.0006988988607190549 current acc: 0.9262062277771792\n",
      "iteration 81 current loss: 0.0003204625390935689 current acc: 0.9262141779788838\n",
      "iteration 82 current loss: 0.00036702395300380886 current acc: 0.9262221264677367\n",
      "iteration 83 current loss: 0.0005968624027445912 current acc: 0.9262300732442913\n",
      "iteration 84 current loss: 0.0003990445693489164 current acc: 0.9262380183091007\n",
      "iteration 85 current loss: 0.0003869845240842551 current acc: 0.9262459616627181\n",
      "iteration 86 current loss: 0.00042735564056783915 current acc: 0.9262539033056961\n",
      "iteration 87 current loss: 0.00040070581599138677 current acc: 0.9262618432385874\n",
      "iteration 88 current loss: 0.0003501608152873814 current acc: 0.9262697814619443\n",
      "iteration 89 current loss: 0.00048800953663885593 current acc: 0.9262777179763186\n",
      "iteration 90 current loss: 0.00037879933370277286 current acc: 0.9262856527822624\n",
      "iteration 91 current loss: 0.0002725528902374208 current acc: 0.9262935858803272\n",
      "iteration 92 current loss: 0.00027721477090381086 current acc: 0.9263015172710642\n",
      "iteration 93 current loss: 0.0003742205735761672 current acc: 0.9263094469550247\n",
      "iteration 94 current loss: 0.00024444592418149114 current acc: 0.9263173749327596\n",
      "iteration 95 current loss: 0.0006985536892898381 current acc: 0.9263253012048193\n",
      "iteration 96 current loss: 0.0004950565635226667 current acc: 0.9263332257717544\n",
      "iteration 97 current loss: 0.0005931880441494286 current acc: 0.9263411486341149\n",
      "iteration 98 current loss: 0.0007247552857734263 current acc: 0.9263490697924508\n",
      "iteration 99 current loss: 0.0007671580533497036 current acc: 0.9263569892473118\n",
      "\t\tEpoch 92/100 complete. Epoch loss 0.0006092986559087877 Epoch accuracy 1.0\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 92, Validation Accuracy: 0.622375, Validation Loss: 1.9809793952852488\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.0012712003663182259 current acc: 0.9263649069992473\n",
      "iteration 1 current loss: 0.0002818657667376101 current acc: 0.9263728230488067\n",
      "iteration 2 current loss: 0.0003816749667748809 current acc: 0.9263807373965387\n",
      "iteration 3 current loss: 0.00035297253634780645 current acc: 0.9263886500429923\n",
      "iteration 4 current loss: 0.00025864862254820764 current acc: 0.9263965609887157\n",
      "iteration 5 current loss: 0.00033258995972573757 current acc: 0.9264044702342574\n",
      "iteration 6 current loss: 0.00035388339892961085 current acc: 0.9264123777801655\n",
      "iteration 7 current loss: 0.0005935390945523977 current acc: 0.9264202836269876\n",
      "iteration 8 current loss: 0.000300171464914456 current acc: 0.9264281877752713\n",
      "iteration 9 current loss: 0.0005259158206172287 current acc: 0.926436090225564\n",
      "iteration 10 current loss: 0.0005108646582812071 current acc: 0.9264439909784127\n",
      "iteration 11 current loss: 0.00039228241075761616 current acc: 0.9264518900343642\n",
      "iteration 12 current loss: 0.00046898340224288404 current acc: 0.9264597873939654\n",
      "iteration 13 current loss: 0.0003173939185217023 current acc: 0.9264676830577625\n",
      "iteration 14 current loss: 0.00028875848511233926 current acc: 0.9264755770263017\n",
      "iteration 15 current loss: 0.00025636900682002306 current acc: 0.9264834693001288\n",
      "iteration 16 current loss: 0.0003621430369094014 current acc: 0.9264913598797896\n",
      "iteration 17 current loss: 0.0005193761317059398 current acc: 0.9264992487658296\n",
      "iteration 18 current loss: 0.000411541317589581 current acc: 0.9265071359587939\n",
      "iteration 19 current loss: 0.00022293382789939642 current acc: 0.9265150214592275\n",
      "iteration 20 current loss: 0.0002730174455791712 current acc: 0.9265229052676751\n",
      "iteration 21 current loss: 0.00024217236205004156 current acc: 0.9265307873846814\n",
      "iteration 22 current loss: 0.00035040953662246466 current acc: 0.9265386678107905\n",
      "iteration 23 current loss: 0.00019831840472761542 current acc: 0.9265465465465466\n",
      "iteration 24 current loss: 0.0003089758101850748 current acc: 0.9265544235924933\n",
      "iteration 25 current loss: 0.00030757623608224094 current acc: 0.9265622989491743\n",
      "iteration 26 current loss: 0.00039167821523733437 current acc: 0.926570172617133\n",
      "iteration 27 current loss: 0.00022833480034023523 current acc: 0.9265780445969125\n",
      "iteration 28 current loss: 0.0003382085124030709 current acc: 0.9265859148890556\n",
      "iteration 29 current loss: 0.00043023269972763956 current acc: 0.926593783494105\n",
      "iteration 30 current loss: 0.0004457364266272634 current acc: 0.9266016504126031\n",
      "iteration 31 current loss: 0.0004955785116180778 current acc: 0.9266095156450922\n",
      "iteration 32 current loss: 0.00028022221522405744 current acc: 0.926617379192114\n",
      "iteration 33 current loss: 0.0003987523668911308 current acc: 0.9266252410542104\n",
      "iteration 34 current loss: 0.00027561691240407526 current acc: 0.9266331012319229\n",
      "iteration 35 current loss: 0.0001511130394646898 current acc: 0.9266409597257926\n",
      "iteration 36 current loss: 0.00021712180750910193 current acc: 0.9266488165363607\n",
      "iteration 37 current loss: 0.0003859283169731498 current acc: 0.9266566716641679\n",
      "iteration 38 current loss: 0.0003400203713681549 current acc: 0.9266645251097548\n",
      "iteration 39 current loss: 0.00033936850377358496 current acc: 0.9266723768736617\n",
      "iteration 40 current loss: 0.0003186682879459113 current acc: 0.9266802269564286\n",
      "iteration 41 current loss: 0.0007655990193597972 current acc: 0.9266880753585955\n",
      "iteration 42 current loss: 0.0005267939995974302 current acc: 0.9266959220807022\n",
      "iteration 43 current loss: 0.0006567758973687887 current acc: 0.9267037671232877\n",
      "iteration 44 current loss: 0.0011925945291295648 current acc: 0.9267116104868914\n",
      "iteration 45 current loss: 0.0007884802762418985 current acc: 0.9267194521720522\n",
      "iteration 46 current loss: 0.00033747166162356734 current acc: 0.9267272921793088\n",
      "iteration 47 current loss: 0.0004070280119776726 current acc: 0.9267351305091999\n",
      "iteration 48 current loss: 0.00036482143332250416 current acc: 0.9267429671622633\n",
      "iteration 49 current loss: 0.00043004858889617026 current acc: 0.9267508021390375\n",
      "iteration 50 current loss: 0.0005022347322665155 current acc: 0.9267586354400599\n",
      "iteration 51 current loss: 0.00023973705538082868 current acc: 0.9267664670658683\n",
      "iteration 52 current loss: 0.0001850198459578678 current acc: 0.9267742970169999\n",
      "iteration 53 current loss: 0.00042556849075481296 current acc: 0.9267821252939918\n",
      "iteration 54 current loss: 0.00044619172695092857 current acc: 0.9267899518973811\n",
      "iteration 55 current loss: 0.0003583039215300232 current acc: 0.9267977768277041\n",
      "iteration 56 current loss: 0.0002395358169451356 current acc: 0.9268056000854975\n",
      "iteration 57 current loss: 0.0006610809359699488 current acc: 0.9268134216712973\n",
      "iteration 58 current loss: 0.0002885927679017186 current acc: 0.9268212415856395\n",
      "iteration 59 current loss: 0.0004064893291797489 current acc: 0.9268290598290598\n",
      "iteration 60 current loss: 0.0004530831065494567 current acc: 0.9268368764020938\n",
      "iteration 61 current loss: 0.0007468392723239958 current acc: 0.9268446913052767\n",
      "iteration 62 current loss: 0.00031436156132258475 current acc: 0.9268525045391435\n",
      "iteration 63 current loss: 0.0005639028968289495 current acc: 0.9268603161042289\n",
      "iteration 64 current loss: 0.0002891776675824076 current acc: 0.9268681260010678\n",
      "iteration 65 current loss: 0.0004616107908077538 current acc: 0.9268759342301943\n",
      "iteration 66 current loss: 0.0003151121491100639 current acc: 0.9268837407921426\n",
      "iteration 67 current loss: 0.00029615979292429984 current acc: 0.9268915456874466\n",
      "iteration 68 current loss: 0.0002645397325977683 current acc: 0.9268993489166399\n",
      "iteration 69 current loss: 0.00028070059488527477 current acc: 0.9269071504802562\n",
      "iteration 70 current loss: 0.0003940299211535603 current acc: 0.9269149503788283\n",
      "iteration 71 current loss: 0.0009837107500061393 current acc: 0.9269227486128895\n",
      "iteration 72 current loss: 0.0001950930745806545 current acc: 0.9269305451829724\n",
      "iteration 73 current loss: 0.0006202359218150377 current acc: 0.9269383400896095\n",
      "iteration 74 current loss: 0.00026015876210294664 current acc: 0.9269461333333333\n",
      "iteration 75 current loss: 0.00026405445532873273 current acc: 0.9269539249146758\n",
      "iteration 76 current loss: 0.0003143363865092397 current acc: 0.9269617148341687\n",
      "iteration 77 current loss: 0.00044990115566179156 current acc: 0.9269695030923438\n",
      "iteration 78 current loss: 0.0006101479521021247 current acc: 0.9269772896897324\n",
      "iteration 79 current loss: 0.00025993192684836686 current acc: 0.9269850746268656\n",
      "iteration 80 current loss: 0.0002541109861340374 current acc: 0.9269928579042745\n",
      "iteration 81 current loss: 0.0003776959201786667 current acc: 0.9270006395224899\n",
      "iteration 82 current loss: 0.0004161554970778525 current acc: 0.927008419482042\n",
      "iteration 83 current loss: 0.00034942443016916513 current acc: 0.9270161977834612\n",
      "iteration 84 current loss: 0.0004499928909353912 current acc: 0.9270239744272776\n",
      "iteration 85 current loss: 0.000328322610585019 current acc: 0.9270317494140209\n",
      "iteration 86 current loss: 0.0004595394420903176 current acc: 0.9270395227442207\n",
      "iteration 87 current loss: 0.00040093495044857264 current acc: 0.9270472944184065\n",
      "iteration 88 current loss: 0.00037366861943155527 current acc: 0.9270550644371073\n",
      "iteration 89 current loss: 0.00024682574439793825 current acc: 0.927062832800852\n",
      "iteration 90 current loss: 0.0004398690944071859 current acc: 0.9270705995101693\n",
      "iteration 91 current loss: 0.0003491087118163705 current acc: 0.9270783645655878\n",
      "iteration 92 current loss: 0.0006291422178037465 current acc: 0.9270861279676355\n",
      "iteration 93 current loss: 0.0003917452704627067 current acc: 0.9270938897168406\n",
      "iteration 94 current loss: 0.000549445569049567 current acc: 0.9271016498137307\n",
      "iteration 95 current loss: 0.00027810054598376155 current acc: 0.9271094082588336\n",
      "iteration 96 current loss: 0.00029356940649449825 current acc: 0.9271171650526764\n",
      "iteration 97 current loss: 0.000298851344268769 current acc: 0.9271249201957863\n",
      "iteration 98 current loss: 0.00033553942921571434 current acc: 0.9271326736886902\n",
      "iteration 99 current loss: 0.000634320022072643 current acc: 0.9271404255319149\n",
      "\t\tEpoch 93/100 complete. Epoch loss 0.0004053597759047989 Epoch accuracy 1.0\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 93, Validation Accuracy: 0.6235, Validation Loss: 1.9779310643672943\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.0002210026723332703 current acc: 0.9271481757259866\n",
      "iteration 1 current loss: 0.0003085874195676297 current acc: 0.9271559242714316\n",
      "iteration 2 current loss: 0.00032827589893713593 current acc: 0.927163671168776\n",
      "iteration 3 current loss: 0.00031209675944410264 current acc: 0.9271714164185453\n",
      "iteration 4 current loss: 0.0002630901290103793 current acc: 0.9271791600212653\n",
      "iteration 5 current loss: 0.0005875216447748244 current acc: 0.9271869019774612\n",
      "iteration 6 current loss: 0.0003037368878722191 current acc: 0.9271946422876581\n",
      "iteration 7 current loss: 0.0003181109204888344 current acc: 0.927202380952381\n",
      "iteration 8 current loss: 0.0005873016198165715 current acc: 0.9272101179721544\n",
      "iteration 9 current loss: 0.00028062646742910147 current acc: 0.9272178533475026\n",
      "iteration 10 current loss: 0.0002771011786535382 current acc: 0.9272255870789502\n",
      "iteration 11 current loss: 0.00034337546094320714 current acc: 0.9272333191670208\n",
      "iteration 12 current loss: 0.0002568837662693113 current acc: 0.9272410496122384\n",
      "iteration 13 current loss: 0.0003113734710495919 current acc: 0.9272487784151264\n",
      "iteration 14 current loss: 0.0004430804110597819 current acc: 0.9272565055762082\n",
      "iteration 15 current loss: 0.000308783957734704 current acc: 0.9272642310960068\n",
      "iteration 16 current loss: 0.00016066730313468724 current acc: 0.9272719549750451\n",
      "iteration 17 current loss: 0.00042597882566042244 current acc: 0.9272796772138459\n",
      "iteration 18 current loss: 0.000405552564188838 current acc: 0.9272873978129313\n",
      "iteration 19 current loss: 0.0003128205717075616 current acc: 0.9272951167728237\n",
      "iteration 20 current loss: 0.000269368349108845 current acc: 0.9273028340940452\n",
      "iteration 21 current loss: 0.00043492010445334017 current acc: 0.9273105497771174\n",
      "iteration 22 current loss: 0.00014365366951096803 current acc: 0.9273182638225618\n",
      "iteration 23 current loss: 0.0007855453877709806 current acc: 0.9273259762308999\n",
      "iteration 24 current loss: 0.00034129247069358826 current acc: 0.9273336870026525\n",
      "iteration 25 current loss: 0.00046053537516854703 current acc: 0.9273413961383408\n",
      "iteration 26 current loss: 0.00032314707641489804 current acc: 0.9273491036384852\n",
      "iteration 27 current loss: 0.0002805186668410897 current acc: 0.9273568095036063\n",
      "iteration 28 current loss: 0.0002563014277257025 current acc: 0.9273645137342242\n",
      "iteration 29 current loss: 0.00026554791838862 current acc: 0.927372216330859\n",
      "iteration 30 current loss: 0.00020281103206798434 current acc: 0.9273799172940304\n",
      "iteration 31 current loss: 0.00043117403401993215 current acc: 0.9273876166242578\n",
      "iteration 32 current loss: 0.00043525610817596316 current acc: 0.9273953143220609\n",
      "iteration 33 current loss: 0.00038230500649660826 current acc: 0.9274030103879585\n",
      "iteration 34 current loss: 0.00027078366838395596 current acc: 0.9274107048224696\n",
      "iteration 35 current loss: 0.00023832758597563952 current acc: 0.9274183976261128\n",
      "iteration 36 current loss: 0.00026702837203629315 current acc: 0.9274260887994066\n",
      "iteration 37 current loss: 0.0003137935418635607 current acc: 0.9274337783428692\n",
      "iteration 38 current loss: 0.0003586472594179213 current acc: 0.9274414662570187\n",
      "iteration 39 current loss: 0.00031216477509588003 current acc: 0.9274491525423729\n",
      "iteration 40 current loss: 0.00025208969600498676 current acc: 0.9274568371994492\n",
      "iteration 41 current loss: 0.0002426446444587782 current acc: 0.9274645202287651\n",
      "iteration 42 current loss: 0.00048158341087400913 current acc: 0.9274722016308377\n",
      "iteration 43 current loss: 0.0006221870426088572 current acc: 0.9274798814061839\n",
      "iteration 44 current loss: 0.000275766768027097 current acc: 0.9274875595553203\n",
      "iteration 45 current loss: 0.000276686972938478 current acc: 0.9274952360787635\n",
      "iteration 46 current loss: 0.0004002716741524637 current acc: 0.9275029109770297\n",
      "iteration 47 current loss: 0.00016508031694684178 current acc: 0.927510584250635\n",
      "iteration 48 current loss: 0.00037390802754089236 current acc: 0.9275182559000953\n",
      "iteration 49 current loss: 0.0002893995260819793 current acc: 0.927525925925926\n",
      "iteration 50 current loss: 0.0006183998775668442 current acc: 0.9275335943286425\n",
      "iteration 51 current loss: 0.00019425286154728383 current acc: 0.92754126110876\n",
      "iteration 52 current loss: 0.0002594872785266489 current acc: 0.9275489262667936\n",
      "iteration 53 current loss: 0.00042780544026754797 current acc: 0.9275565898032578\n",
      "iteration 54 current loss: 0.0004377728037070483 current acc: 0.9275642517186674\n",
      "iteration 55 current loss: 0.0003864140890073031 current acc: 0.9275719120135364\n",
      "iteration 56 current loss: 0.0002294319710927084 current acc: 0.9275795706883789\n",
      "iteration 57 current loss: 0.00037804123712703586 current acc: 0.927587227743709\n",
      "iteration 58 current loss: 0.00043416573316790164 current acc: 0.9275948831800401\n",
      "iteration 59 current loss: 0.00021323602413758636 current acc: 0.9276025369978859\n",
      "iteration 60 current loss: 0.00036413248744793236 current acc: 0.9276101891977592\n",
      "iteration 61 current loss: 0.0002539288252592087 current acc: 0.9276178397801733\n",
      "iteration 62 current loss: 0.0002256750303786248 current acc: 0.9276254887456409\n",
      "iteration 63 current loss: 0.00026083129341714084 current acc: 0.9276331360946746\n",
      "iteration 64 current loss: 0.0006612252327613533 current acc: 0.9276407818277865\n",
      "iteration 65 current loss: 0.00036074192030355334 current acc: 0.9276484259454891\n",
      "iteration 66 current loss: 0.00033215605071745813 current acc: 0.9276560684482941\n",
      "iteration 67 current loss: 0.0006184703670442104 current acc: 0.9276637093367132\n",
      "iteration 68 current loss: 0.0006881069857627153 current acc: 0.9276713486112578\n",
      "iteration 69 current loss: 0.0004589780292008072 current acc: 0.9276789862724393\n",
      "iteration 70 current loss: 0.0002007472503464669 current acc: 0.9276866223207687\n",
      "iteration 71 current loss: 0.0002668375673238188 current acc: 0.9276942567567568\n",
      "iteration 72 current loss: 0.0002503679133951664 current acc: 0.9277018895809142\n",
      "iteration 73 current loss: 0.0002509147161617875 current acc: 0.9277095207937514\n",
      "iteration 74 current loss: 0.0002467849990352988 current acc: 0.9277171503957784\n",
      "iteration 75 current loss: 0.0001665772288106382 current acc: 0.9277247783875052\n",
      "iteration 76 current loss: 0.0003491090319585055 current acc: 0.9277324047694419\n",
      "iteration 77 current loss: 0.00023157788382377476 current acc: 0.9277400295420974\n",
      "iteration 78 current loss: 0.00026575144147500396 current acc: 0.9277476527059817\n",
      "iteration 79 current loss: 0.00028921477496623993 current acc: 0.9277552742616034\n",
      "iteration 80 current loss: 0.0002364847605349496 current acc: 0.9277628942094716\n",
      "iteration 81 current loss: 0.00019362481543794274 current acc: 0.9277705125500949\n",
      "iteration 82 current loss: 0.0004081025836057961 current acc: 0.9277781292839818\n",
      "iteration 83 current loss: 0.00025220992392860353 current acc: 0.9277857444116406\n",
      "iteration 84 current loss: 0.00016266533930320293 current acc: 0.9277933579335793\n",
      "iteration 85 current loss: 0.001132693258114159 current acc: 0.9278009698503057\n",
      "iteration 86 current loss: 0.0008576158434152603 current acc: 0.9278085801623274\n",
      "iteration 87 current loss: 0.00031252860208041966 current acc: 0.9278161888701518\n",
      "iteration 88 current loss: 0.00026806307141669095 current acc: 0.927823795974286\n",
      "iteration 89 current loss: 0.0003874738176818937 current acc: 0.9278314014752371\n",
      "iteration 90 current loss: 0.0002791564038489014 current acc: 0.9278390053735117\n",
      "iteration 91 current loss: 0.0002738343901000917 current acc: 0.9278466076696166\n",
      "iteration 92 current loss: 0.00026505489950068295 current acc: 0.9278542083640577\n",
      "iteration 93 current loss: 0.00015166515368036926 current acc: 0.9278618074573415\n",
      "iteration 94 current loss: 0.00027666709502227604 current acc: 0.9278694049499737\n",
      "iteration 95 current loss: 0.00028421886963769794 current acc: 0.9278770008424599\n",
      "iteration 96 current loss: 0.00019743606389965862 current acc: 0.9278845951353059\n",
      "iteration 97 current loss: 0.00034956150921061635 current acc: 0.9278921878290166\n",
      "iteration 98 current loss: 0.0003362345742061734 current acc: 0.9278997789240973\n",
      "iteration 99 current loss: 0.0004371271934360266 current acc: 0.9279073684210526\n",
      "\t\tEpoch 94/100 complete. Epoch loss 0.0003429226235311944 Epoch accuracy 1.0\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 94, Validation Accuracy: 0.622625, Validation Loss: 1.9962419077754021\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.0003219001227989793 current acc: 0.9279149563203873\n",
      "iteration 1 current loss: 0.0002611934905871749 current acc: 0.9279225426226058\n",
      "iteration 2 current loss: 0.00035566347651183605 current acc: 0.9279301273282121\n",
      "iteration 3 current loss: 0.0003311309264972806 current acc: 0.9279377104377105\n",
      "iteration 4 current loss: 0.0002638666774146259 current acc: 0.9279452919516045\n",
      "iteration 5 current loss: 0.00026934032212011516 current acc: 0.9279528718703977\n",
      "iteration 6 current loss: 0.0003478017169982195 current acc: 0.9279604501945935\n",
      "iteration 7 current loss: 0.00016503116057720035 current acc: 0.927968026924695\n",
      "iteration 8 current loss: 0.000236339692492038 current acc: 0.9279756020612052\n",
      "iteration 9 current loss: 0.0002994225069414824 current acc: 0.9279831756046267\n",
      "iteration 10 current loss: 0.00019345506734680384 current acc: 0.9279907475554621\n",
      "iteration 11 current loss: 0.0002635775599628687 current acc: 0.9279983179142136\n",
      "iteration 12 current loss: 0.0008656936697661877 current acc: 0.9280058866813834\n",
      "iteration 13 current loss: 0.00021851442579645663 current acc: 0.9280134538574732\n",
      "iteration 14 current loss: 0.0002969816268887371 current acc: 0.9280210194429848\n",
      "iteration 15 current loss: 0.0003364338481333107 current acc: 0.9280285834384195\n",
      "iteration 16 current loss: 0.00020635365217458457 current acc: 0.9280361458442786\n",
      "iteration 17 current loss: 0.000382958009140566 current acc: 0.9280437066610633\n",
      "iteration 18 current loss: 0.00024177700106520206 current acc: 0.9280512658892741\n",
      "iteration 19 current loss: 0.0002714145230129361 current acc: 0.9280588235294117\n",
      "iteration 20 current loss: 0.00034317109384573996 current acc: 0.9280663795819767\n",
      "iteration 21 current loss: 0.00020969151228200644 current acc: 0.9280739340474691\n",
      "iteration 22 current loss: 0.00021670880960300565 current acc: 0.9280814869263887\n",
      "iteration 23 current loss: 0.00014803820522502065 current acc: 0.9280890382192356\n",
      "iteration 24 current loss: 0.00031167600536718965 current acc: 0.9280965879265092\n",
      "iteration 25 current loss: 0.00022864952916279435 current acc: 0.9281041360487088\n",
      "iteration 26 current loss: 0.000252238183747977 current acc: 0.9281116825863336\n",
      "iteration 27 current loss: 0.00024130729434546083 current acc: 0.9281192275398824\n",
      "iteration 28 current loss: 0.00017505732830613852 current acc: 0.9281267709098542\n",
      "iteration 29 current loss: 0.0002718631294555962 current acc: 0.9281343126967471\n",
      "iteration 30 current loss: 0.0003943192132283002 current acc: 0.9281418529010597\n",
      "iteration 31 current loss: 0.00034178729401901364 current acc: 0.92814939152329\n",
      "iteration 32 current loss: 0.00027306340052746236 current acc: 0.9281569285639358\n",
      "iteration 33 current loss: 0.0003709769225679338 current acc: 0.9281644640234948\n",
      "iteration 34 current loss: 0.00036709170672111213 current acc: 0.9281719979024646\n",
      "iteration 35 current loss: 0.00024421076523140073 current acc: 0.9281795302013423\n",
      "iteration 36 current loss: 0.0003511573013383895 current acc: 0.928187060920625\n",
      "iteration 37 current loss: 0.00031156561453826725 current acc: 0.9281945900608094\n",
      "iteration 38 current loss: 0.0004326881899032742 current acc: 0.9282021176223922\n",
      "iteration 39 current loss: 0.00029217576957307756 current acc: 0.92820964360587\n",
      "iteration 40 current loss: 0.00045517689432017505 current acc: 0.9282171680117388\n",
      "iteration 41 current loss: 0.0001635430962778628 current acc: 0.9282246908404946\n",
      "iteration 42 current loss: 0.00043131932034157217 current acc: 0.9282322120926333\n",
      "iteration 43 current loss: 0.00027917136321775615 current acc: 0.9282397317686505\n",
      "iteration 44 current loss: 0.00028198902145959437 current acc: 0.9282472498690414\n",
      "iteration 45 current loss: 0.0003872500965371728 current acc: 0.9282547663943013\n",
      "iteration 46 current loss: 0.0002913989592343569 current acc: 0.928262281344925\n",
      "iteration 47 current loss: 0.00031283427961170673 current acc: 0.9282697947214076\n",
      "iteration 48 current loss: 0.00023561010311823338 current acc: 0.9282773065242433\n",
      "iteration 49 current loss: 0.00036258966429159045 current acc: 0.9282848167539267\n",
      "iteration 50 current loss: 0.00025060889311134815 current acc: 0.9282923254109517\n",
      "iteration 51 current loss: 0.0003907487553078681 current acc: 0.9282998324958124\n",
      "iteration 52 current loss: 0.00026775029255077243 current acc: 0.9283073380090024\n",
      "iteration 53 current loss: 0.0003923392796423286 current acc: 0.9283148419510153\n",
      "iteration 54 current loss: 0.00033416575752198696 current acc: 0.9283223443223443\n",
      "iteration 55 current loss: 0.0008280971669591963 current acc: 0.9283298451234826\n",
      "iteration 56 current loss: 0.0003958726010750979 current acc: 0.9283373443549231\n",
      "iteration 57 current loss: 0.00026632356457412243 current acc: 0.9283448420171584\n",
      "iteration 58 current loss: 0.000591206131502986 current acc: 0.9283523381106811\n",
      "iteration 59 current loss: 0.0003947336517740041 current acc: 0.9283598326359833\n",
      "iteration 60 current loss: 0.00020927736477460712 current acc: 0.9283673255935572\n",
      "iteration 61 current loss: 0.000261449720710516 current acc: 0.9283748169838946\n",
      "iteration 62 current loss: 0.00019976438488811255 current acc: 0.9283823068074872\n",
      "iteration 63 current loss: 0.00024462054716423154 current acc: 0.9283897950648264\n",
      "iteration 64 current loss: 0.0002837474748957902 current acc: 0.9283972817564036\n",
      "iteration 65 current loss: 0.0002530982019379735 current acc: 0.9284047668827096\n",
      "iteration 66 current loss: 0.00023860599321778864 current acc: 0.9284122504442354\n",
      "iteration 67 current loss: 0.0003673205792438239 current acc: 0.9284197324414716\n",
      "iteration 68 current loss: 0.0002562448207754642 current acc: 0.9284272128749086\n",
      "iteration 69 current loss: 0.00020878150826320052 current acc: 0.9284346917450366\n",
      "iteration 70 current loss: 0.00017218402354046702 current acc: 0.9284421690523457\n",
      "iteration 71 current loss: 0.00019658682867884636 current acc: 0.9284496447973255\n",
      "iteration 72 current loss: 0.00019736848480533808 current acc: 0.9284571189804659\n",
      "iteration 73 current loss: 0.0004968513385392725 current acc: 0.9284645916022561\n",
      "iteration 74 current loss: 0.0002763660450000316 current acc: 0.9284720626631854\n",
      "iteration 75 current loss: 0.000255322054727003 current acc: 0.9284795321637427\n",
      "iteration 76 current loss: 0.0003667913260869682 current acc: 0.9284870001044169\n",
      "iteration 77 current loss: 0.00018182871281169355 current acc: 0.9284944664856963\n",
      "iteration 78 current loss: 0.00020550012413877994 current acc: 0.9285019313080697\n",
      "iteration 79 current loss: 0.0002480324765201658 current acc: 0.9285093945720251\n",
      "iteration 80 current loss: 0.00028899734024889767 current acc: 0.9285168562780504\n",
      "iteration 81 current loss: 0.0002232064725831151 current acc: 0.9285243164266332\n",
      "iteration 82 current loss: 0.0005297211464494467 current acc: 0.9285317750182615\n",
      "iteration 83 current loss: 0.0003085417556576431 current acc: 0.9285392320534224\n",
      "iteration 84 current loss: 0.0002679484023246914 current acc: 0.928546687532603\n",
      "iteration 85 current loss: 0.0001242915604962036 current acc: 0.9285541414562904\n",
      "iteration 86 current loss: 0.0006001002620905638 current acc: 0.9285615938249713\n",
      "iteration 87 current loss: 0.000229369179578498 current acc: 0.9285690446391323\n",
      "iteration 88 current loss: 0.00030229229014366865 current acc: 0.9285764938992596\n",
      "iteration 89 current loss: 0.00043427557102404535 current acc: 0.9285839416058395\n",
      "iteration 90 current loss: 0.0002667113731149584 current acc: 0.9285913877593577\n",
      "iteration 91 current loss: 0.0003687783027999103 current acc: 0.9285988323603003\n",
      "iteration 92 current loss: 0.0006389769841916859 current acc: 0.9286062754091525\n",
      "iteration 93 current loss: 0.0003662644303403795 current acc: 0.9286137169063998\n",
      "iteration 94 current loss: 0.00018583956989459693 current acc: 0.9286211568525273\n",
      "iteration 95 current loss: 0.00022961363720241934 current acc: 0.92862859524802\n",
      "iteration 96 current loss: 0.00024639119510538876 current acc: 0.9286360320933625\n",
      "iteration 97 current loss: 0.0003003018209710717 current acc: 0.9286434673890394\n",
      "iteration 98 current loss: 0.0002750710700638592 current acc: 0.928650901135535\n",
      "iteration 99 current loss: 0.00018956423446070403 current acc: 0.9286583333333334\n",
      "\t\tEpoch 95/100 complete. Epoch loss 0.0003071501424710732 Epoch accuracy 1.0\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 95, Validation Accuracy: 0.624125, Validation Loss: 1.9973779313266278\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.0002367156121181324 current acc: 0.9286657639829184\n",
      "iteration 1 current loss: 0.00035612244391813874 current acc: 0.928673193084774\n",
      "iteration 2 current loss: 0.0003148741670884192 current acc: 0.9286806206393835\n",
      "iteration 3 current loss: 0.00027127028442919254 current acc: 0.9286880466472304\n",
      "iteration 4 current loss: 0.00020916573703289032 current acc: 0.9286954711087975\n",
      "iteration 5 current loss: 0.00022037113376427442 current acc: 0.928702894024568\n",
      "iteration 6 current loss: 0.0002609063812997192 current acc: 0.9287103153950245\n",
      "iteration 7 current loss: 0.00022000934404786676 current acc: 0.9287177352206495\n",
      "iteration 8 current loss: 0.0002362740196986124 current acc: 0.9287251535019253\n",
      "iteration 9 current loss: 0.00020258183940313756 current acc: 0.928732570239334\n",
      "iteration 10 current loss: 0.00030045921448618174 current acc: 0.9287399854333576\n",
      "iteration 11 current loss: 0.0003789695620071143 current acc: 0.9287473990844778\n",
      "iteration 12 current loss: 0.00022696917585562915 current acc: 0.9287548111931759\n",
      "iteration 13 current loss: 0.00032247958006337285 current acc: 0.9287622217599334\n",
      "iteration 14 current loss: 0.0004477686306927353 current acc: 0.9287696307852314\n",
      "iteration 15 current loss: 0.00036028012982569635 current acc: 0.9287770382695507\n",
      "iteration 16 current loss: 0.0002570384240243584 current acc: 0.9287844442133721\n",
      "iteration 17 current loss: 0.0002004537673201412 current acc: 0.9287918486171761\n",
      "iteration 18 current loss: 0.0003219540521968156 current acc: 0.9287992514814429\n",
      "iteration 19 current loss: 0.000190013088285923 current acc: 0.9288066528066528\n",
      "iteration 20 current loss: 0.0002305670641362667 current acc: 0.9288140525932855\n",
      "iteration 21 current loss: 0.0002595421392470598 current acc: 0.9288214508418208\n",
      "iteration 22 current loss: 0.0002134814130840823 current acc: 0.9288288475527382\n",
      "iteration 23 current loss: 0.0002371693990426138 current acc: 0.928836242726517\n",
      "iteration 24 current loss: 0.0003952504775952548 current acc: 0.9288436363636363\n",
      "iteration 25 current loss: 0.0025899920146912336 current acc: 0.9288510284645751\n",
      "iteration 26 current loss: 0.0009021386504173279 current acc: 0.928858419029812\n",
      "iteration 27 current loss: 0.00035883401869796216 current acc: 0.9288658080598255\n",
      "iteration 28 current loss: 0.00029285880737006664 current acc: 0.928873195555094\n",
      "iteration 29 current loss: 0.000345429900335148 current acc: 0.9288805815160955\n",
      "iteration 30 current loss: 0.0005458792438730597 current acc: 0.928887965943308\n",
      "iteration 31 current loss: 0.0002055343211395666 current acc: 0.9288953488372093\n",
      "iteration 32 current loss: 0.0003348479513078928 current acc: 0.9289027301982767\n",
      "iteration 33 current loss: 0.00027445779414847493 current acc: 0.9289101100269878\n",
      "iteration 34 current loss: 0.00036901573184877634 current acc: 0.9289174883238194\n",
      "iteration 35 current loss: 0.00028082073549740016 current acc: 0.9289248650892487\n",
      "iteration 36 current loss: 0.00044551375322043896 current acc: 0.9289322403237522\n",
      "iteration 37 current loss: 0.00019845450879074633 current acc: 0.9289396140278066\n",
      "iteration 38 current loss: 0.0002858800580725074 current acc: 0.9289469862018882\n",
      "iteration 39 current loss: 0.00016137050988618284 current acc: 0.928954356846473\n",
      "iteration 40 current loss: 0.00025501471827737987 current acc: 0.9289617259620372\n",
      "iteration 41 current loss: 0.0001814851420931518 current acc: 0.9289690935490562\n",
      "iteration 42 current loss: 0.00031716719968244433 current acc: 0.9289764596080058\n",
      "iteration 43 current loss: 0.00027550655067898333 current acc: 0.9289838241393613\n",
      "iteration 44 current loss: 0.00029137334786355495 current acc: 0.9289911871435977\n",
      "iteration 45 current loss: 0.00032529039890505373 current acc: 0.9289985486211901\n",
      "iteration 46 current loss: 0.00022247132437769324 current acc: 0.9290059085726132\n",
      "iteration 47 current loss: 0.0005237199366092682 current acc: 0.9290132669983416\n",
      "iteration 48 current loss: 0.000330714596202597 current acc: 0.9290206238988497\n",
      "iteration 49 current loss: 0.00021417843527160585 current acc: 0.9290279792746114\n",
      "iteration 50 current loss: 0.0002588213828857988 current acc: 0.9290353331261009\n",
      "iteration 51 current loss: 0.0004267626500222832 current acc: 0.929042685453792\n",
      "iteration 52 current loss: 0.00017368685803376138 current acc: 0.9290500362581581\n",
      "iteration 53 current loss: 0.0002858065126929432 current acc: 0.9290573855396727\n",
      "iteration 54 current loss: 0.0002124795864801854 current acc: 0.9290647332988089\n",
      "iteration 55 current loss: 0.0001763016771292314 current acc: 0.9290720795360398\n",
      "iteration 56 current loss: 0.0004869667754974216 current acc: 0.9290794242518381\n",
      "iteration 57 current loss: 0.0003552164416760206 current acc: 0.9290867674466763\n",
      "iteration 58 current loss: 0.00026385311502963305 current acc: 0.929094109121027\n",
      "iteration 59 current loss: 0.00031938121537677944 current acc: 0.9291014492753623\n",
      "iteration 60 current loss: 0.0003058174334000796 current acc: 0.9291087879101543\n",
      "iteration 61 current loss: 0.0003107975935563445 current acc: 0.9291161250258746\n",
      "iteration 62 current loss: 0.0003472394892014563 current acc: 0.9291234606229949\n",
      "iteration 63 current loss: 0.0002698478347156197 current acc: 0.9291307947019868\n",
      "iteration 64 current loss: 0.0001738743158057332 current acc: 0.9291381272633212\n",
      "iteration 65 current loss: 0.0002615915145725012 current acc: 0.9291454583074695\n",
      "iteration 66 current loss: 0.0001907908881548792 current acc: 0.9291527878349023\n",
      "iteration 67 current loss: 0.0003454222169239074 current acc: 0.9291601158460902\n",
      "iteration 68 current loss: 0.0002862859982997179 current acc: 0.9291674423415037\n",
      "iteration 69 current loss: 0.00017764337826520205 current acc: 0.9291747673216132\n",
      "iteration 70 current loss: 0.000375147646991536 current acc: 0.9291820907868886\n",
      "iteration 71 current loss: 0.000331456889398396 current acc: 0.9291894127377999\n",
      "iteration 72 current loss: 0.0003342987329233438 current acc: 0.9291967331748165\n",
      "iteration 73 current loss: 0.00020389063865877688 current acc: 0.9292040520984081\n",
      "iteration 74 current loss: 0.00042682955972850323 current acc: 0.9292113695090439\n",
      "iteration 75 current loss: 0.00032708869548514485 current acc: 0.9292186854071931\n",
      "iteration 76 current loss: 0.0002545046154409647 current acc: 0.9292259997933244\n",
      "iteration 77 current loss: 0.00026087320293299854 current acc: 0.9292333126679065\n",
      "iteration 78 current loss: 0.0003018163552042097 current acc: 0.9292406240314082\n",
      "iteration 79 current loss: 0.00032659078715369105 current acc: 0.9292479338842975\n",
      "iteration 80 current loss: 0.00023125894949771464 current acc: 0.9292552422270427\n",
      "iteration 81 current loss: 0.0003483989567030221 current acc: 0.9292625490601115\n",
      "iteration 82 current loss: 0.00024607384693808854 current acc: 0.9292698543839719\n",
      "iteration 83 current loss: 0.0003634954337030649 current acc: 0.9292771581990913\n",
      "iteration 84 current loss: 0.000267882103798911 current acc: 0.929284460505937\n",
      "iteration 85 current loss: 0.00018105996423400939 current acc: 0.9292917613049763\n",
      "iteration 86 current loss: 0.00028161078807897866 current acc: 0.929299060596676\n",
      "iteration 87 current loss: 0.0002058771497104317 current acc: 0.9293063583815029\n",
      "iteration 88 current loss: 0.0002948101609945297 current acc: 0.9293136546599237\n",
      "iteration 89 current loss: 0.00022967264521867037 current acc: 0.9293209494324045\n",
      "iteration 90 current loss: 0.0002647451183293015 current acc: 0.9293282426994118\n",
      "iteration 91 current loss: 0.0003367545723449439 current acc: 0.9293355344614115\n",
      "iteration 92 current loss: 0.00025418854784220457 current acc: 0.9293428247188693\n",
      "iteration 93 current loss: 0.00030238128965720534 current acc: 0.9293501134722508\n",
      "iteration 94 current loss: 0.000273937766905874 current acc: 0.9293574007220217\n",
      "iteration 95 current loss: 0.0005313543369993567 current acc: 0.9293646864686469\n",
      "iteration 96 current loss: 0.0003338310052640736 current acc: 0.9293719707125915\n",
      "iteration 97 current loss: 0.0001840779441408813 current acc: 0.9293792534543205\n",
      "iteration 98 current loss: 0.00018831515626516193 current acc: 0.9293865346942983\n",
      "iteration 99 current loss: 0.00024051174113992602 current acc: 0.9293938144329897\n",
      "\t\tEpoch 96/100 complete. Epoch loss 0.0003172995820932556 Epoch accuracy 1.0\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 96, Validation Accuracy: 0.623625, Validation Loss: 1.9952937848865986\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.000351499009411782 current acc: 0.9294010926708587\n",
      "iteration 1 current loss: 0.0003729489108081907 current acc: 0.9294083694083695\n",
      "iteration 2 current loss: 0.00017858354840427637 current acc: 0.9294156446459858\n",
      "iteration 3 current loss: 0.00018153765995521098 current acc: 0.9294229183841715\n",
      "iteration 4 current loss: 0.00019123444508295506 current acc: 0.92943019062339\n",
      "iteration 5 current loss: 0.000194744803593494 current acc: 0.9294374613641047\n",
      "iteration 6 current loss: 0.00022108670964371413 current acc: 0.9294447306067786\n",
      "iteration 7 current loss: 0.0001592823537066579 current acc: 0.9294519983518748\n",
      "iteration 8 current loss: 0.00022546062245965004 current acc: 0.9294592645998558\n",
      "iteration 9 current loss: 0.0001441278145648539 current acc: 0.9294665293511843\n",
      "iteration 10 current loss: 0.00035316296271048486 current acc: 0.9294737926063227\n",
      "iteration 11 current loss: 0.0001940360525622964 current acc: 0.9294810543657331\n",
      "iteration 12 current loss: 0.00032186112366616726 current acc: 0.9294883146298775\n",
      "iteration 13 current loss: 0.00022343015007209033 current acc: 0.9294955733992176\n",
      "iteration 14 current loss: 0.00015542333130724728 current acc: 0.9295028306742151\n",
      "iteration 15 current loss: 0.00022365366748999804 current acc: 0.9295100864553314\n",
      "iteration 16 current loss: 0.000297548744129017 current acc: 0.9295173407430277\n",
      "iteration 17 current loss: 0.00028376514092087746 current acc: 0.929524593537765\n",
      "iteration 18 current loss: 0.00014953066420275718 current acc: 0.9295318448400041\n",
      "iteration 19 current loss: 0.00015482035814784467 current acc: 0.9295390946502058\n",
      "iteration 20 current loss: 0.00016241833509411663 current acc: 0.9295463429688303\n",
      "iteration 21 current loss: 0.0004763828474096954 current acc: 0.9295535897963382\n",
      "iteration 22 current loss: 0.00026188281481154263 current acc: 0.9295608351331893\n",
      "iteration 23 current loss: 0.00034668220905587077 current acc: 0.9295680789798437\n",
      "iteration 24 current loss: 0.00017849303549155593 current acc: 0.9295753213367609\n",
      "iteration 25 current loss: 0.00025229909806512296 current acc: 0.9295825622044006\n",
      "iteration 26 current loss: 0.00027064193272963166 current acc: 0.9295898015832219\n",
      "iteration 27 current loss: 0.00017727464728523046 current acc: 0.9295970394736842\n",
      "iteration 28 current loss: 0.00021638408361468464 current acc: 0.9296042758762463\n",
      "iteration 29 current loss: 0.0002264724171254784 current acc: 0.929611510791367\n",
      "iteration 30 current loss: 0.00023970520123839378 current acc: 0.9296187442195046\n",
      "iteration 31 current loss: 0.0001760682207532227 current acc: 0.929625976161118\n",
      "iteration 32 current loss: 0.00016593006148468703 current acc: 0.9296332066166649\n",
      "iteration 33 current loss: 0.00034279286046512425 current acc: 0.9296404355866037\n",
      "iteration 34 current loss: 0.0002049210452241823 current acc: 0.9296476630713919\n",
      "iteration 35 current loss: 0.0001772074174368754 current acc: 0.9296548890714873\n",
      "iteration 36 current loss: 0.00027367391157895327 current acc: 0.9296621135873472\n",
      "iteration 37 current loss: 0.00040557869942858815 current acc: 0.9296693366194291\n",
      "iteration 38 current loss: 0.00018404313595965505 current acc: 0.9296765581681897\n",
      "iteration 39 current loss: 0.00022945918317418545 current acc: 0.9296837782340862\n",
      "iteration 40 current loss: 0.0002067613968392834 current acc: 0.9296909968175752\n",
      "iteration 41 current loss: 0.000288711569737643 current acc: 0.9296982139191131\n",
      "iteration 42 current loss: 0.0002523254370316863 current acc: 0.9297054295391564\n",
      "iteration 43 current loss: 0.0007965327822603285 current acc: 0.9297126436781609\n",
      "iteration 44 current loss: 0.00024378385569434613 current acc: 0.9297198563365828\n",
      "iteration 45 current loss: 0.00027838838286697865 current acc: 0.9297270675148779\n",
      "iteration 46 current loss: 0.0002053680073004216 current acc: 0.9297342772135015\n",
      "iteration 47 current loss: 0.0001402698107995093 current acc: 0.9297414854329094\n",
      "iteration 48 current loss: 0.0002074037620332092 current acc: 0.9297486921735563\n",
      "iteration 49 current loss: 0.00037368733319453895 current acc: 0.9297558974358975\n",
      "iteration 50 current loss: 0.00016116896586026996 current acc: 0.9297631012203876\n",
      "iteration 51 current loss: 0.00022481301857624203 current acc: 0.9297703035274816\n",
      "iteration 52 current loss: 0.00027245207456871867 current acc: 0.9297775043576335\n",
      "iteration 53 current loss: 0.00022078660549595952 current acc: 0.929784703711298\n",
      "iteration 54 current loss: 0.00025581696536391973 current acc: 0.9297919015889288\n",
      "iteration 55 current loss: 0.00018735614139586687 current acc: 0.9297990979909799\n",
      "iteration 56 current loss: 0.00019621598767116666 current acc: 0.9298062929179051\n",
      "iteration 57 current loss: 0.0002169880026485771 current acc: 0.9298134863701578\n",
      "iteration 58 current loss: 0.00020611949730664492 current acc: 0.9298206783481914\n",
      "iteration 59 current loss: 0.00022157514467835426 current acc: 0.929827868852459\n",
      "iteration 60 current loss: 0.00022134243045002222 current acc: 0.9298350578834136\n",
      "iteration 61 current loss: 0.0005231898394413292 current acc: 0.9298422454415078\n",
      "iteration 62 current loss: 0.00017478608060628176 current acc: 0.9298494315271945\n",
      "iteration 63 current loss: 0.0002464176213834435 current acc: 0.9298566161409259\n",
      "iteration 64 current loss: 0.00017730194667819887 current acc: 0.9298637992831541\n",
      "iteration 65 current loss: 0.0002014222409343347 current acc: 0.9298709809543314\n",
      "iteration 66 current loss: 0.0006212375592440367 current acc: 0.9298781611549094\n",
      "iteration 67 current loss: 0.00024291418958455324 current acc: 0.9298853398853398\n",
      "iteration 68 current loss: 0.0002743201912380755 current acc: 0.9298925171460743\n",
      "iteration 69 current loss: 0.00023549982870463282 current acc: 0.929899692937564\n",
      "iteration 70 current loss: 0.00020253975526429713 current acc: 0.92990686726026\n",
      "iteration 71 current loss: 0.00024249863054137677 current acc: 0.9299140401146132\n",
      "iteration 72 current loss: 0.00021922461746726185 current acc: 0.9299212115010744\n",
      "iteration 73 current loss: 0.00035468724672682583 current acc: 0.9299283814200942\n",
      "iteration 74 current loss: 0.0002693849091883749 current acc: 0.9299355498721228\n",
      "iteration 75 current loss: 0.0001790128881111741 current acc: 0.9299427168576104\n",
      "iteration 76 current loss: 0.00016723111912142485 current acc: 0.9299498823770073\n",
      "iteration 77 current loss: 0.0001483539235778153 current acc: 0.9299570464307629\n",
      "iteration 78 current loss: 0.0002525121672078967 current acc: 0.9299642090193271\n",
      "iteration 79 current loss: 0.000361248996341601 current acc: 0.9299713701431492\n",
      "iteration 80 current loss: 0.00024428017786704004 current acc: 0.9299785298026787\n",
      "iteration 81 current loss: 0.00032710906816646457 current acc: 0.9299856879983643\n",
      "iteration 82 current loss: 0.00027428800240159035 current acc: 0.9299928447306552\n",
      "iteration 83 current loss: 0.00013585409033112228 current acc: 0.93\n",
      "iteration 84 current loss: 0.00016494962619617581 current acc: 0.9300071538068472\n",
      "iteration 85 current loss: 0.00018125739006791264 current acc: 0.9300143061516452\n",
      "iteration 86 current loss: 0.0002778930065687746 current acc: 0.9300214570348422\n",
      "iteration 87 current loss: 0.00036446016747504473 current acc: 0.930028606456886\n",
      "iteration 88 current loss: 0.0002962660219054669 current acc: 0.9300357544182245\n",
      "iteration 89 current loss: 0.00018641560745891184 current acc: 0.9300429009193054\n",
      "iteration 90 current loss: 0.0003110682882834226 current acc: 0.930050045960576\n",
      "iteration 91 current loss: 0.00019914018048439175 current acc: 0.9300571895424836\n",
      "iteration 92 current loss: 0.00023132641217671335 current acc: 0.9300643316654753\n",
      "iteration 93 current loss: 0.0001979563239729032 current acc: 0.930071472329998\n",
      "iteration 94 current loss: 0.00021749205188825727 current acc: 0.9300786115364982\n",
      "iteration 95 current loss: 0.0001961781963473186 current acc: 0.9300857492854226\n",
      "iteration 96 current loss: 0.00023202459851745516 current acc: 0.9300928855772175\n",
      "iteration 97 current loss: 0.0002172190579585731 current acc: 0.9301000204123291\n",
      "iteration 98 current loss: 0.00024481970467604697 current acc: 0.9301071537912032\n",
      "iteration 99 current loss: 0.0002727473038248718 current acc: 0.9301142857142857\n",
      "\t\tEpoch 97/100 complete. Epoch loss 0.0002479044542997144 Epoch accuracy 1.0\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 97, Validation Accuracy: 0.624875, Validation Loss: 2.006846297159791\n",
      "best loss 0.0002479044542997144\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.00020380242494866252 current acc: 0.9301214161820223\n",
      "iteration 1 current loss: 0.000257503823377192 current acc: 0.9301285451948582\n",
      "iteration 2 current loss: 0.0001625698059797287 current acc: 0.9301356727532388\n",
      "iteration 3 current loss: 0.0002674468560144305 current acc: 0.9301427988576091\n",
      "iteration 4 current loss: 0.00022624654229730368 current acc: 0.9301499235084141\n",
      "iteration 5 current loss: 0.00024619296891614795 current acc: 0.9301570467060983\n",
      "iteration 6 current loss: 0.0002441309334244579 current acc: 0.9301641684511064\n",
      "iteration 7 current loss: 0.0002351940202061087 current acc: 0.9301712887438826\n",
      "iteration 8 current loss: 0.00021524472685996443 current acc: 0.930178407584871\n",
      "iteration 9 current loss: 0.00018950813682749867 current acc: 0.9301855249745158\n",
      "iteration 10 current loss: 0.00022236401855479926 current acc: 0.9301926409132606\n",
      "iteration 11 current loss: 0.0002095673407893628 current acc: 0.9301997554015491\n",
      "iteration 12 current loss: 0.00026617792900651693 current acc: 0.9302068684398247\n",
      "iteration 13 current loss: 0.0002762112417258322 current acc: 0.9302139800285306\n",
      "iteration 14 current loss: 0.00021275899780448526 current acc: 0.93022109016811\n",
      "iteration 15 current loss: 0.00020050903549417853 current acc: 0.9302281988590057\n",
      "iteration 16 current loss: 0.0004218512331135571 current acc: 0.9302353061016604\n",
      "iteration 17 current loss: 0.0002715050068218261 current acc: 0.9302424118965166\n",
      "iteration 18 current loss: 0.00027607494848780334 current acc: 0.9302495162440167\n",
      "iteration 19 current loss: 0.0002041810075752437 current acc: 0.9302566191446029\n",
      "iteration 20 current loss: 0.0003953010600525886 current acc: 0.930263720598717\n",
      "iteration 21 current loss: 0.0001699529093457386 current acc: 0.9302708206068011\n",
      "iteration 22 current loss: 0.00020136413513682783 current acc: 0.9302779191692966\n",
      "iteration 23 current loss: 0.00040782339056022465 current acc: 0.930285016286645\n",
      "iteration 24 current loss: 0.00026754505233839154 current acc: 0.9302921119592875\n",
      "iteration 25 current loss: 0.0003283900732640177 current acc: 0.9302992061876654\n",
      "iteration 26 current loss: 0.0002284236397827044 current acc: 0.9303062989722194\n",
      "iteration 27 current loss: 0.00028562918305397034 current acc: 0.9303133903133903\n",
      "iteration 28 current loss: 0.00017585787281859666 current acc: 0.9303204802116187\n",
      "iteration 29 current loss: 0.0001517503842478618 current acc: 0.9303275686673449\n",
      "iteration 30 current loss: 0.00033404308487661183 current acc: 0.930334655681009\n",
      "iteration 31 current loss: 0.00046572633436881006 current acc: 0.9303417412530512\n",
      "iteration 32 current loss: 0.0003374495718162507 current acc: 0.9303488253839113\n",
      "iteration 33 current loss: 0.00038006313843652606 current acc: 0.9303559080740289\n",
      "iteration 34 current loss: 0.00016946274263318628 current acc: 0.9303629893238434\n",
      "iteration 35 current loss: 0.000311604468151927 current acc: 0.9303700691337943\n",
      "iteration 36 current loss: 0.00023938989033922553 current acc: 0.9303771475043204\n",
      "iteration 37 current loss: 0.0001752526732161641 current acc: 0.930384224435861\n",
      "iteration 38 current loss: 0.00022469612304121256 current acc: 0.9303912999288546\n",
      "iteration 39 current loss: 0.00026271448587067425 current acc: 0.9303983739837398\n",
      "iteration 40 current loss: 0.00018449263006914407 current acc: 0.9304054466009551\n",
      "iteration 41 current loss: 0.00022278385586105287 current acc: 0.9304125177809388\n",
      "iteration 42 current loss: 0.0002842581016011536 current acc: 0.9304195875241288\n",
      "iteration 43 current loss: 0.00024143041810020804 current acc: 0.9304266558309631\n",
      "iteration 44 current loss: 0.00020980041881557554 current acc: 0.9304337227018791\n",
      "iteration 45 current loss: 0.00020549618056975305 current acc: 0.9304407881373147\n",
      "iteration 46 current loss: 0.00013004953507333994 current acc: 0.9304478521377069\n",
      "iteration 47 current loss: 0.0003176383615937084 current acc: 0.9304549147034931\n",
      "iteration 48 current loss: 0.0002042030537268147 current acc: 0.9304619758351101\n",
      "iteration 49 current loss: 0.00015090264787431806 current acc: 0.9304690355329949\n",
      "iteration 50 current loss: 0.00016538611089345068 current acc: 0.930476093797584\n",
      "iteration 51 current loss: 0.00013381658936850727 current acc: 0.9304831506293139\n",
      "iteration 52 current loss: 0.00016056305321399122 current acc: 0.9304902060286208\n",
      "iteration 53 current loss: 0.0003087841614615172 current acc: 0.9304972599959407\n",
      "iteration 54 current loss: 0.0002159087307518348 current acc: 0.9305043125317098\n",
      "iteration 55 current loss: 0.00021726726845372468 current acc: 0.9305113636363637\n",
      "iteration 56 current loss: 0.00022269357577897608 current acc: 0.9305184133103378\n",
      "iteration 57 current loss: 0.00017113039211835712 current acc: 0.9305254615540678\n",
      "iteration 58 current loss: 0.00022461311891674995 current acc: 0.9305325083679886\n",
      "iteration 59 current loss: 0.00024402880808338523 current acc: 0.9305395537525355\n",
      "iteration 60 current loss: 0.00020842113008257002 current acc: 0.9305465977081432\n",
      "iteration 61 current loss: 0.00022981746587902308 current acc: 0.9305536402352464\n",
      "iteration 62 current loss: 0.0002257414598716423 current acc: 0.9305606813342796\n",
      "iteration 63 current loss: 0.00017560500418767333 current acc: 0.9305677210056772\n",
      "iteration 64 current loss: 0.0002636120771057904 current acc: 0.9305747592498733\n",
      "iteration 65 current loss: 0.0003355267981532961 current acc: 0.9305817960673018\n",
      "iteration 66 current loss: 0.0001927031553350389 current acc: 0.9305888314583967\n",
      "iteration 67 current loss: 0.0002300882915733382 current acc: 0.9305958654235914\n",
      "iteration 68 current loss: 0.00021866755560040474 current acc: 0.9306028979633195\n",
      "iteration 69 current loss: 0.00018626701785251498 current acc: 0.9306099290780142\n",
      "iteration 70 current loss: 0.00018816173542290926 current acc: 0.9306169587681086\n",
      "iteration 71 current loss: 0.00019043838256038725 current acc: 0.9306239870340357\n",
      "iteration 72 current loss: 0.00026140836416743696 current acc: 0.930631013876228\n",
      "iteration 73 current loss: 0.0002749093691818416 current acc: 0.9306380392951185\n",
      "iteration 74 current loss: 0.00037278031231835485 current acc: 0.9306450632911393\n",
      "iteration 75 current loss: 0.0002893535711336881 current acc: 0.9306520858647226\n",
      "iteration 76 current loss: 0.00022449943935498595 current acc: 0.9306591070163005\n",
      "iteration 77 current loss: 0.00016959579079411924 current acc: 0.9306661267463049\n",
      "iteration 78 current loss: 0.00015875333338044584 current acc: 0.9306731450551675\n",
      "iteration 79 current loss: 0.00019602729298640043 current acc: 0.9306801619433198\n",
      "iteration 80 current loss: 0.0003195478639099747 current acc: 0.9306871774111932\n",
      "iteration 81 current loss: 0.00020096226944588125 current acc: 0.9306941914592188\n",
      "iteration 82 current loss: 0.0003722293768078089 current acc: 0.9307012040878276\n",
      "iteration 83 current loss: 0.00036628206726163626 current acc: 0.9307082152974504\n",
      "iteration 84 current loss: 0.00018531436217017472 current acc: 0.930715225088518\n",
      "iteration 85 current loss: 0.00018597987946122885 current acc: 0.9307222334614607\n",
      "iteration 86 current loss: 0.00023380348284263164 current acc: 0.9307292404167088\n",
      "iteration 87 current loss: 0.00019871944095939398 current acc: 0.9307362459546925\n",
      "iteration 88 current loss: 0.00015686088590882719 current acc: 0.9307432500758418\n",
      "iteration 89 current loss: 0.00027178737218491733 current acc: 0.9307502527805864\n",
      "iteration 90 current loss: 0.00020645683980546892 current acc: 0.9307572540693559\n",
      "iteration 91 current loss: 0.00030008700559847057 current acc: 0.9307642539425799\n",
      "iteration 92 current loss: 0.00026902559329755604 current acc: 0.9307712524006874\n",
      "iteration 93 current loss: 0.0002227004588348791 current acc: 0.9307782494441076\n",
      "iteration 94 current loss: 0.00026692156097851694 current acc: 0.9307852450732693\n",
      "iteration 95 current loss: 0.00022506674577016383 current acc: 0.9307922392886014\n",
      "iteration 96 current loss: 0.00023090356262400746 current acc: 0.9307992320905325\n",
      "iteration 97 current loss: 0.00035845019738189876 current acc: 0.9308062234794908\n",
      "iteration 98 current loss: 0.00026940065436065197 current acc: 0.9308132134559046\n",
      "iteration 99 current loss: 0.0002071802446153015 current acc: 0.930820202020202\n",
      "\t\tEpoch 98/100 complete. Epoch loss 0.00024176785635063425 Epoch accuracy 1.0\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 98, Validation Accuracy: 0.623875, Validation Loss: 2.020633730664849\n",
      "best loss 0.00024176785635063425\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 0.00016091902216430753 current acc: 0.9308271891728108\n",
      "iteration 1 current loss: 0.0003217754128854722 current acc: 0.9308341749141588\n",
      "iteration 2 current loss: 0.00017338022007606924 current acc: 0.9308411592446734\n",
      "iteration 3 current loss: 0.0002981501747854054 current acc: 0.930848142164782\n",
      "iteration 4 current loss: 0.00016688255709595978 current acc: 0.9308551236749116\n",
      "iteration 5 current loss: 0.00022468862880486995 current acc: 0.9308621037754896\n",
      "iteration 6 current loss: 0.0002454125788062811 current acc: 0.9308690824669426\n",
      "iteration 7 current loss: 0.00016910239355638623 current acc: 0.9308760597496972\n",
      "iteration 8 current loss: 0.00015602460189256817 current acc: 0.9308830356241801\n",
      "iteration 9 current loss: 0.00022932482534088194 current acc: 0.9308900100908174\n",
      "iteration 10 current loss: 0.000378476019250229 current acc: 0.9308969831500353\n",
      "iteration 11 current loss: 0.00014909599849488586 current acc: 0.9309039548022598\n",
      "iteration 12 current loss: 0.00032099446980282664 current acc: 0.9309109250479168\n",
      "iteration 13 current loss: 0.00017644443141762167 current acc: 0.930917893887432\n",
      "iteration 14 current loss: 0.0001806192158255726 current acc: 0.9309248613212304\n",
      "iteration 15 current loss: 0.0001645558513700962 current acc: 0.9309318273497378\n",
      "iteration 16 current loss: 0.00033056337269954383 current acc: 0.930938791973379\n",
      "iteration 17 current loss: 0.00017813155136536807 current acc: 0.9309457551925792\n",
      "iteration 18 current loss: 0.00024760307860560715 current acc: 0.9309527170077628\n",
      "iteration 19 current loss: 0.00033687325776554644 current acc: 0.9309596774193548\n",
      "iteration 20 current loss: 0.0001559174561407417 current acc: 0.9309666364277794\n",
      "iteration 21 current loss: 0.00017630221555009484 current acc: 0.930973594033461\n",
      "iteration 22 current loss: 0.00028646018472500145 current acc: 0.9309805502368236\n",
      "iteration 23 current loss: 0.00014725129585713148 current acc: 0.930987505038291\n",
      "iteration 24 current loss: 0.0002594551770016551 current acc: 0.9309944584382872\n",
      "iteration 25 current loss: 0.00014151303912512958 current acc: 0.9310014104372355\n",
      "iteration 26 current loss: 0.00024955341359600425 current acc: 0.9310083610355596\n",
      "iteration 27 current loss: 0.0004451987915672362 current acc: 0.9310153102336826\n",
      "iteration 28 current loss: 0.00018519730656407773 current acc: 0.9310222580320274\n",
      "iteration 29 current loss: 0.00014739297330379486 current acc: 0.9310292044310171\n",
      "iteration 30 current loss: 0.00019273153156973422 current acc: 0.9310361494310744\n",
      "iteration 31 current loss: 0.00013185896386858076 current acc: 0.9310430930326218\n",
      "iteration 32 current loss: 0.0001930735306814313 current acc: 0.9310500352360818\n",
      "iteration 33 current loss: 0.0002026143338298425 current acc: 0.9310569760418764\n",
      "iteration 34 current loss: 0.0001615132059669122 current acc: 0.9310639154504278\n",
      "iteration 35 current loss: 0.0002909741597250104 current acc: 0.9310708534621578\n",
      "iteration 36 current loss: 0.0001432602439308539 current acc: 0.9310777900774881\n",
      "iteration 37 current loss: 0.00018595183792058378 current acc: 0.9310847252968404\n",
      "iteration 38 current loss: 0.00018692819867283106 current acc: 0.9310916591206359\n",
      "iteration 39 current loss: 0.00025024378555826843 current acc: 0.9310985915492958\n",
      "iteration 40 current loss: 0.00015787439770065248 current acc: 0.9311055225832411\n",
      "iteration 41 current loss: 0.00020866532577201724 current acc: 0.9311124522228927\n",
      "iteration 42 current loss: 0.00027802668046206236 current acc: 0.9311193804686714\n",
      "iteration 43 current loss: 0.0002004236594075337 current acc: 0.9311263073209975\n",
      "iteration 44 current loss: 0.00027692143339663744 current acc: 0.9311332327802916\n",
      "iteration 45 current loss: 0.00031492114067077637 current acc: 0.9311401568469737\n",
      "iteration 46 current loss: 0.00016846439393702894 current acc: 0.9311470795214638\n",
      "iteration 47 current loss: 0.0002795152831822634 current acc: 0.9311540008041818\n",
      "iteration 48 current loss: 0.00017609665519557893 current acc: 0.9311609206955473\n",
      "iteration 49 current loss: 0.0002341990766581148 current acc: 0.9311678391959799\n",
      "iteration 50 current loss: 0.00018604991782922298 current acc: 0.9311747563058989\n",
      "iteration 51 current loss: 0.00013836818106938154 current acc: 0.9311816720257234\n",
      "iteration 52 current loss: 0.00017633401148486882 current acc: 0.9311885863558726\n",
      "iteration 53 current loss: 0.00033254773006774485 current acc: 0.9311954992967652\n",
      "iteration 54 current loss: 0.0008270181133411825 current acc: 0.9312024108488197\n",
      "iteration 55 current loss: 0.000265733600826934 current acc: 0.9312093210124548\n",
      "iteration 56 current loss: 0.0001963861723197624 current acc: 0.9312162297880888\n",
      "iteration 57 current loss: 0.0001790711539797485 current acc: 0.9312231371761398\n",
      "iteration 58 current loss: 0.00019015956786461174 current acc: 0.9312300431770258\n",
      "iteration 59 current loss: 0.0001951115409610793 current acc: 0.9312369477911646\n",
      "iteration 60 current loss: 0.00016379191947635263 current acc: 0.931243851018974\n",
      "iteration 61 current loss: 0.0001722174638416618 current acc: 0.9312507528608713\n",
      "iteration 62 current loss: 0.00017271662363782525 current acc: 0.931257653317274\n",
      "iteration 63 current loss: 0.00021470418141689152 current acc: 0.931264552388599\n",
      "iteration 64 current loss: 0.00022687111049890518 current acc: 0.9312714500752635\n",
      "iteration 65 current loss: 0.0001366453943774104 current acc: 0.9312783463776841\n",
      "iteration 66 current loss: 0.0001927698904182762 current acc: 0.9312852412962778\n",
      "iteration 67 current loss: 0.000362979102646932 current acc: 0.9312921348314607\n",
      "iteration 68 current loss: 0.00021306857524905354 current acc: 0.9312990269836493\n",
      "iteration 69 current loss: 0.0002192511601606384 current acc: 0.9313059177532598\n",
      "iteration 70 current loss: 0.00032327871304005384 current acc: 0.9313128071407081\n",
      "iteration 71 current loss: 0.0001360423630103469 current acc: 0.93131969514641\n",
      "iteration 72 current loss: 0.00023491811589337885 current acc: 0.9313265817707811\n",
      "iteration 73 current loss: 0.00035503305844031274 current acc: 0.931333467014237\n",
      "iteration 74 current loss: 0.00018405601440463215 current acc: 0.931340350877193\n",
      "iteration 75 current loss: 0.0002464751887600869 current acc: 0.9313472333600642\n",
      "iteration 76 current loss: 0.00013763309107162058 current acc: 0.9313541144632655\n",
      "iteration 77 current loss: 0.00021440230193547904 current acc: 0.9313609941872119\n",
      "iteration 78 current loss: 0.00012145028449594975 current acc: 0.9313678725323179\n",
      "iteration 79 current loss: 0.00034136761678382754 current acc: 0.931374749498998\n",
      "iteration 80 current loss: 0.00022263127902988344 current acc: 0.9313816250876665\n",
      "iteration 81 current loss: 0.0002252331905765459 current acc: 0.9313884992987377\n",
      "iteration 82 current loss: 0.00017423767712898552 current acc: 0.9313953721326255\n",
      "iteration 83 current loss: 0.0001889906416181475 current acc: 0.9314022435897436\n",
      "iteration 84 current loss: 0.00023624040477443486 current acc: 0.9314091136705057\n",
      "iteration 85 current loss: 0.0002352377341594547 current acc: 0.9314159823753254\n",
      "iteration 86 current loss: 0.00012616976164281368 current acc: 0.931422849704616\n",
      "iteration 87 current loss: 0.00014799613563809544 current acc: 0.9314297156587905\n",
      "iteration 88 current loss: 0.00023342088388744742 current acc: 0.931436580238262\n",
      "iteration 89 current loss: 0.0003598468319978565 current acc: 0.9314434434434434\n",
      "iteration 90 current loss: 0.0002591561060398817 current acc: 0.9314503052747473\n",
      "iteration 91 current loss: 0.00012794797657988966 current acc: 0.9314571657325861\n",
      "iteration 92 current loss: 0.0001763556501828134 current acc: 0.9314640248173721\n",
      "iteration 93 current loss: 0.00015884551976341754 current acc: 0.9314708825295177\n",
      "iteration 94 current loss: 0.00014986272435635328 current acc: 0.9314777388694347\n",
      "iteration 95 current loss: 0.00015189814439509064 current acc: 0.931484593837535\n",
      "iteration 96 current loss: 0.0001940702786669135 current acc: 0.9314914474342303\n",
      "iteration 97 current loss: 0.0002727238752413541 current acc: 0.931498299659932\n",
      "iteration 98 current loss: 0.00024721803492866457 current acc: 0.9315051505150514\n",
      "iteration 99 current loss: 0.00020265887724235654 current acc: 0.931512\n",
      "\t\tEpoch 99/100 complete. Epoch loss 0.00022184711240697653 Epoch accuracy 1.0\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 99, Validation Accuracy: 0.624625, Validation Loss: 2.030552374944091\n",
      "best loss 0.00022184711240697653\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'learning_rate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 167\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# Save the best model\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m best_model_state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 167\u001b[0m     PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../models/finetuned_encoder_weights_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[43mlearning_rate\u001b[49m, batch_size, weight_decay)\n\u001b[1;32m    168\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(best_model_state, PATH)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'learning_rate' is not defined"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "best_loss = float('inf')\n",
    "best_model_state = None\n",
    "num_train = len(train_dataset)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Set up total loss/acc trackers\n",
    "all_loss = []\n",
    "all_acc = []\n",
    "all_correct = 0\n",
    "train_running_total = 0\n",
    "\n",
    "\n",
    "\n",
    "# Set up epochal loss/acc trackers\n",
    "epoch_loss = []\n",
    "epoch_acc = []\n",
    "\n",
    "\n",
    "# Set up validation loss/acc trackers\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "val_running_total = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "\n",
    "# For each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    # Refresh Epoch Statistics\n",
    "    print('reset epoch statistics')\n",
    "    epoch_correct = 0\n",
    "    epoch_loss_val = 0\n",
    "\n",
    "\n",
    "    # Set Network to Train Mode\n",
    "    encoder.train()\n",
    "\n",
    "\n",
    "    # For each batch in the dataloader\n",
    "    for i, (data, labels) in enumerate(train_loader, 0):\n",
    "        # Put train data to device (CPU, GPU, or TPU)\n",
    "        data_real = data.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        #  what does this do? why is this needed here?\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass batch through D\n",
    "        output = encoder(data_real)\n",
    "\n",
    "\n",
    "        # Calculate loss on batch\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "        # Compute Predicted Labels for a Batch in Training Dataset\n",
    "        predicted = torch.argmax(output.data, dim=1).to(device)\n",
    "\n",
    "\n",
    "\n",
    "        correct = (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Update All Data\n",
    "        all_loss.append(loss.item())\n",
    "\n",
    "        all_correct += correct\n",
    "        train_running_total += labels.size(0)\n",
    "\n",
    "\n",
    "        # Compute All Loss/Acc at each datapoint\n",
    "        all_accuracy = all_correct / train_running_total\n",
    "        all_acc.append(all_accuracy)\n",
    "\n",
    "        print(f'iteration {i} current loss: {loss.item()} current acc: {all_accuracy}')\n",
    "\n",
    "\n",
    "        # Update Epoch Data\n",
    "        epoch_correct += correct\n",
    "        epoch_loss_val += loss.item()\n",
    "\n",
    "\n",
    "    wandb.log({\"TrainLoss\": loss.item()})\n",
    "    wandb.log({\"TrainAccuracy\": all_accuracy})\n",
    "    \n",
    "    # Compute Epoch Loss/Acc at end of Epoch\n",
    "    epoch_accuracy = epoch_correct / num_train\n",
    "    epoch_acc.append(epoch_accuracy)\n",
    "\n",
    "    avg_epoch_loss = epoch_loss_val / len(train_loader)\n",
    "    epoch_loss.append(avg_epoch_loss)\n",
    "\n",
    "    print(f'\\t\\tEpoch {epoch}/{num_epochs} complete. Epoch loss {avg_epoch_loss} Epoch accuracy {epoch_accuracy}')\n",
    "\n",
    "    # Validation Step\n",
    "    print('Starting Validation Loop...')\n",
    "\n",
    "\n",
    "\n",
    "    # Refresh Validation Statistics\n",
    "    print('reset Validation statistics')\n",
    "    val_correct = 0\n",
    "    val_loss_value = 0\n",
    "\n",
    "\n",
    "    # Set the model to valuation mode\n",
    "    encoder.eval()\n",
    "\n",
    "\n",
    "    # Iterate over the validation dataset in batches\n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_loader:\n",
    "\n",
    "            # Put val data to device (CPU, GPU, or TPU)\n",
    "            data_real = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "\n",
    "            # Forward pass batch through D\n",
    "            output = encoder(data_real)\n",
    "\n",
    "            # Calculate loss on validation batch\n",
    "            v_loss = criterion(output, labels)\n",
    "\n",
    "            # Compute Predicted Labels for a Batch in Validation Dataset\n",
    "            predicted = torch.argmax(output.data, dim=1).to(device)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Update Val Data\n",
    "            val_loss_value += v_loss.item()\n",
    "\n",
    "\n",
    "\n",
    "    val_accuracy = val_correct / len(test_dataset)\n",
    "    val_acc.append(val_accuracy)\n",
    "    wandb.log({\"ValidationAccuracy\": val_accuracy})\n",
    "\n",
    "    \n",
    "    avg_val_loss = val_loss_value / len(test_loader)\n",
    "    val_loss.append(avg_val_loss)\n",
    "\n",
    "    print(f\"\\t\\tValidation Epoch {epoch}, Validation Accuracy: {val_accuracy}, Validation Loss: {avg_val_loss}\")\n",
    "    wandb.log({\"ValidationLoss\": val_loss_value})\n",
    "\n",
    "    # Update best model if this epoch had the higest accuracy so far\n",
    "    if avg_epoch_loss < best_loss:\n",
    "        best_loss = avg_epoch_loss\n",
    "        print(f'best loss {best_loss}')\n",
    "        best_model_state = encoder.main.state_dict()\n",
    "\n",
    "\n",
    "\n",
    "# Save the best model\n",
    "if best_model_state is not None:\n",
    "    PATH = '../models/finetuned_encoder_weights_{}_{}_{}.pth'.format(learning_rate, batch_size, weight_decay)\n",
    "    torch.save(best_model_state, PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2074f4-c49a-4768-aaab-ad9a5156ecc6",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "490f4bda-2de3-42ea-a3ae-416c1768473e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff32b7d16d0>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(all_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8286e5bb-b167-42e0-b223-24f907f17eda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff35d5df750>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(val_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3796927b-585b-4ab1-880e-a0a68d50dff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff2c4a5b890>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGdCAYAAADAAnMpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAABS5UlEQVR4nO3dd3iUVfo+8Ht6+qSQXkjoPQkgiKiA4CIiirpiXbGuBVYQV1d0bWvB1bXr6s9dhfWriFhARSwICKJ0CC20kEB6L5M67T2/PyYzJEAgZWbeKffnuua6NlMyjy/ZzJ1znnOOQgghQERERCQTpdwFEBERkX9jGCEiIiJZMYwQERGRrBhGiIiISFYMI0RERCQrhhEiIiKSFcMIERERyYphhIiIiGSllruAzpAkCcXFxQgNDYVCoZC7HCIiIuoEIQTq6+uRkJAApbLj8Q+vCCPFxcVITk6WuwwiIiLqhoKCAiQlJXX4uFeEkdDQUAC2/5iwsDCZqyEiIqLOMBgMSE5OdnyOd8Qrwoh9aiYsLIxhhIiIyMucq8WCDaxEREQkK4YRIiIikhXDCBEREcmKYYSIiIhkxTBCREREsmIYISIiIlkxjBAREZGsGEaIiIhIVgwjREREJCuGESIiIpIVwwgRERHJimGEiIiIZMUwQkRE5Me25FbhTx9sRZPJIlsNDCNERER+yCoJvP7zEdz0ny349Wgl3lmfI1statnemYiIiGRRWteCect2Y2teNQDgj6OScP/EfrLVwzBCRETkR9YdKsNDy/egpsmMIK0Kz189DFdnJslaE8MIERGRn1i6NR+PrdgHABiaEIa3bsxEn+gQmatiGCEiIvILy3cUOILITWNT8NSMIdCpVTJXZcMwQkRE5ONW7C7E377cCwC47YJUPDVjCBQKhcxVncTVNERERD7smz3FeGj5HggB3HJ+iscFEaCLYWTRokU477zzEBoaipiYGMycOROHDx8+5+s+//xzDBo0CAEBARg+fDhWr17d7YKJiIioc1bvK8GDn2VBEsD1o5PxjyuHeVwQAboYRjZs2IA5c+Zgy5YtWLNmDcxmM/7whz+gsbGxw9f8/vvvuPHGG3HnnXdi9+7dmDlzJmbOnIn9+/f3uHgiIiI6nRAC//01F3OW7oJVErhmZCIWXTMcSqXnBREAUAghRHdfXFFRgZiYGGzYsAEXX3zxGZ9z/fXXo7GxEatWrXLcd/755yMjIwPvvfdep97HYDBAr9ejrq4OYWFh3S2XiIjI51msEp7+9gA+3pIPwNas+uxVw6CSIYh09vO7Rw2sdXV1AIDIyMgOn7N582YsWLCg3X1Tp07FypUrO3yN0WiE0Wh0fG0wGHpSJhERkdcTQsDQbEFVoxFVjSZUNZigVAB9okPQOyoIGpUS9S1mzF26GxuOVEChAB6bNhh3XZTmkVMzbXU7jEiShPnz52P8+PEYNmxYh88rLS1FbGxsu/tiY2NRWlra4WsWLVqEZ555prulERER+ZQfD5TikS/2oq7ZfMbHVUoFekcGwWSVUFjTjACNEm/ckImpQ+PcXGn3dDuMzJkzB/v378emTZucWQ8AYOHChe1GUwwGA5KTk53+PkRERJ7ueGUjFnyWhUaTFQAQqlMjMkSLyGAtzFYJuRWNaDJZkVtp69+MDtXhg9mjMSIpXMaqu6ZbYWTu3LlYtWoVNm7ciKSks28hGxcXh7Kysnb3lZWVIS6u47Sm0+mg0+m6UxoREZHPMFqsmPvpLjSarBiTFomP7hiDAE37jcqEECg1tCC3ohHl9S24sF80okO96zO0S6tphBCYO3cuVqxYgXXr1iEtLe2crxk3bhzWrl3b7r41a9Zg3LhxXauUiIjIz7z4/SHsLzIgIkiDN2/IPC2IAIBCoUC8PhDj+/XC1ZlJXhdEgC6OjMyZMwdLly7F119/jdDQUEffh16vR2BgIADg1ltvRWJiIhYtWgQAmDdvHiZMmIBXXnkF06dPx7Jly7Bjxw68//77Tv5PISIi8h1rssuw+LfjAIBXZqUjTh8gb0Eu1KUw8u677wIAJk6c2O7+xYsX47bbbgMA5OfnQ6k8OeBywQUXYOnSpfj73/+Oxx57DP3798fKlSvP2vRKRETkD8xWCf/7/Th+yi5Dn17BGNk7AiNTIhCoVeHhL/YAAO68MA2XDIo9x3fybj3aZ8RduM8IEbnbzhPVuPujnfjHVUNxxYgEucshH/Tr0Qo8/c0BHKs4feNQlVIBqyQwPFGPL++7AFq1d57e4pZ9RoiIfNWqvSWobjRh2bYChhFyqoLqJjz/3UH8cMDW6hAVrMU9E/qgutGMXSdqsKewFkaLhFCdGm/dmOm1QaQrGEaIiM7geOsyyT2FtZAk4bHbaJN3OVHViCve3IR6owUqpQK3juuN+VMGQB+ocTzHZJFwuLQeUSFaJIQHylit+zCMEBGdwfGqJgBAfYsFeVWN6BsdInNF5O2EEHj6mwOoN1owNCEMr87KwMC40NOep1UrMTxJL0OF8vH9sR8ioi6yWCUUVDc5vs7Kr5WvGPIZPx8sx/rDFdCoFHjzxswzBhF/xTBCRHSKotpmWKSTvf17CmvlK4Z8QovZime+PQAAuOuiPhxpOwXDCBHRKfIq269u2FNQK08h5DPe/eUYCmuaEa8PwF8u6Sd3OR6HYYSI6BQnWvtFBsfbliJmlxjQYrbKWRJ5sRNVjXh3wzEAwBNXDEGQlu2ap2IYISI6hX1k5KL+vVoPIxM4WGKQuSryVv/4Nhsmi4QL+/XCtGHecYquuzGMEBGd4niVLYykRgUjIzkcQOenaj7YlIdbP9yG8voWF1VH3uSnA6VYe6gcGpUCT185FAoFl4ifCcMIEdEp7HuMpPYKQnrrMex7CuvO+bqC6iYsWn0QG49UYOGX++AFG1yTCy3fXoC5n+4GANx5YR/0i2HTakcYRoiI2jBbJRTWNAOwjYykJ9v2e8jqxMjIW+uOOlbhrD1Ujs93FLqsTvJcJouEx1fswyNf7oXJImHK4FjMn9Jf7rI8GsMIEVEbRTW2Zb06tRJxYQGOaZq8ykbUNpk6fN3xykZ8uasIAHBlum37+H+sym63Xwn5vjJDC254fzM+2ZoPhQJ46NIBeP9PoxCgUcldmkdjGCEiaiOvTb+IUqlAeJAWqVFBAIC9Z5mqeXPtUVglgUkDo/Ha9RkY3TsCDUYLHv5iDySJ0zX+4ERVI654axN25dciLECND2efh79M7s+jBDqBYYSIqI0TbfpF7NJbR0c6mqrJKW/AyizbqMiCSwdCpVTgX9elI1Cjwpbcaiz+/bgrSyYPYLJI+Munu1FRb8SA2BB8M/dCTBoUI3dZXoNhhIioDfuZNKlRwY77HE2sHYSRN9YehSSAS4fEOs4USe0VjMemDwYAvPTDIeSUN7iuaJLdyz8ewt7COugDNVhy+xik9go+94vIgWGEiKiNPMfIyMkPk4yUcAC2beFPXSFzuLQeq/YWAwAenDKg3WO3jE3BRf17wWiR8MLqgy6smuS0/lA5/vNrHgDg5T+O8JuTdp2JYYSIqI0TbXpG7IbEh0GtVKCywYSi2uZ2z39j7REIAVw+PA5DEsLaPaZQKHDfxL4ATu5dQr6lzNCChz7fAwC47YJU/GEoNzXrDoYRIqJWZquEAvuy3jY9IwEalWNr+LZ9Iz/sL8HqfaVQKID5p4yK2IXqNACAFhO3k/c1Vkngwc+yUN1owpD4MDw6bZDcJXktbpBPRNSqsKYZVkkgQKNEbGhAu8fSk/XYV1SHPQW1uHRILBatPoQlrY2p12QmYUDsmY+DD9Ta/uZr4tk2Xs8qCRTVNCOnoh455Q3YlleD349VIUirwls3ZXL5bg8wjBARtXLsvNq6rLetjOQIfLwlHxuOVGBzbhX2F9nOqrnrwjQ8clnHfxHbP6CaOTLi1bblVeO+j3eiqvH0vWb+cdUw9I3m7qo9wTBCRNTK3tfROyrotMcyWndiPVJmWxUTEaTBK7PSccmg2LN+z8DWMGK0SJAkwT0nvFBlgxFzl+5CVaMJWrUSfXoFo29MCPpGh+D8PpG4oG8vuUv0egwjREStjp9hJY1dn14hCA/SoLbJjDFpkXjjhgzE68+9aiJQe3LovsVi5fHxXkaSBB5avgfl9Ub0iwnBN3PH89/QBXhFiYha5bXuMZIWdXoYUSoVeO+WUThe2Yg/jkqCWtW5/v8A9ckw0mxiGPE2//k1FxuOVECnVuKdm0by389FuJqGiKiVY1lvBxtWnd8nCjeMSel0EAFsIUantj2/mU2sHmlNdhkuWLQWDy3fg5zyesf9u/Jr8PKPhwEAT80YioFxZ25Spp5jxCMiwumn9TpToFYFo0VCC8OIxzle2YgHP8tCg9GCL3cV4stdhfjDkFjMviAVj3yxFxZJYPqIeNw4JlnuUn0awwgREYCC6iZYJYFAjQqxYTqnfu9AjQq1MKPZJDn1+1LPGC1WzP10FxqMFmQkhyMmVIefssscNwBIiQzComuGQ6Fg47ErMYwQkU9rNlkx/7PdqG40oV9MKPrHhGBAbCgGxoUiOvRk6DjR2i/SOyrI6R889hU1nKbxLC98dxD7iwyICNLg3VtGIl4fiJzyerz7Sy6+ziqCUqnA2zdlIixAI3epPo9hhIh82ks/HsKPB2x/5W4/XuO4X6EA7rm4Lx6ZOhBKpeLkmTROnqIB2uw1wjDiMb7fV4L/bT4BAHh11smVUf1iQvHKrHT8bdpAWKyC58y4CcMIEfmsLblVWPzbcQDAw1MHotlkxdHyehwta0BuZSPe23AMuRUNeP2GDMceI644bdW+vJcbn3mG/KomPPLlXgDAPRf3waRBMac9J+aUHXjJtRhGiMhrCSGwK78GSRFBiA1r/+HRaLTg4S9sB5jdOCYFcyb1a/f4yt1FeOSLvfgpuwzXvbcZytapmbRep2941lP2aRo2sMqv0WjB3E93ob7FgpEp4fjr1IFyl0RgGCEiL9VituKprw/gsx0FCNGp8ezMobg6M8nx+IvfH0JBdTMSwwPx+PTBp71+ZmYikiICcc//7cSBYoPj/t6cpvFZTSYL7liyHXsL6xAepMFbN42EpgvLtMl1+K9ARF6npK4Z17+/BZ/tKAAANBgtePCzPZi/bDcMLWb8llOJ/9ti6wd46Y8jEKI7899do1MjsXLOeAyIPXmuSBqnabxSk8mC349V4r0Nx/Dd3hJYJdHu8RazFXf9bwe25lUjVKfGktvHIJH9IB6DIyNE5FW25VXj/k92orLBBH2gBq/fkIG9BXV4c91RrMwqxo4TNY4Poj+d3xvj+5393JDkyCB8ed8F+Me32QjUqhAT6txlvQAQqOGmZ66QX9WEjzYfx/YTNThQVAdLmwDSPyYED146AJcNjYPJKuHuj3bg92NVCNaqsOSOMchIDpevcDoNwwgReY2Vu4vw18/3wCIJDIoLxft/Go2UqCBMGhiDC/tHYd6yLMfGZcmRgXh0Wsen6bYVGqDBy9elu6xu9ow4X0F1E65973dU1Bsd98XrAzAiSY/Nx6pwtLwB93+yC4PjwxAeqMHm3CoEalRYfPsYjOodIWPldCYMI0TkFUwWCc98ewAWSWBGegL+ee3wdueEjOodidXzLsLT3xzA5mNVeG1WBoI7mJ5xtwBO0zhVdaMJsz/chop6IwbGhuL+SX0xOjXSMe1iaDHjg1/z8MGmPBwssfUDBWiU+PC28zAmLVLO0qkDnvH/VCKic1h3qBw1TWbEhOrw+vUZUClP35gsLECDV2dluL+4c+CmZ87TbLLirv9tR25lIxL0AfjfHWMQp2+/kiosQIMHLx2A2y5Ixfu/5uL3nEr87bJBGNc3Sqaq6VwYRojIK3y5qxAAcHVm4hmDiCdjGHEOqyTwwLLd2JVfi7AA9RmDSFsRwVr87bLOTdWRvLiahog8XlWDEesPlQMArh2VdI5nex77ahr2jHSfEAJPfr0fa7LLoFUr8d/Z56F/LE/R9RUcGSEij/fNnmJYJIERSXoM8MIPIMc+I+wZ6ZYyQwse/mIvNh6pgEIBvHF9Bns/fAzDCBF5vC922qZorh3pfaMiAKdpemL1vhI8tmIfapvM0KmVeHbmMEwbHi93WeRkDCNE5NEOlRpwoNgAjUqBK9MT5C6nW06GEUnmSryHocWMp78+gK92FwEAhiWG4bVZGZya8VEMI0Tk0b5sHRWZPCgWEcFamavpHkfPCKdpOqXFbMWN72/BgWIDlArg/on98MDk/tCq2eboqxhGiEh2Qgh8t68EJbUt+NO43o4eC4tVwordxQC8s3HVjmfTdM2TX+/HgWIDooK1eP/WURjVm/0hvo5hhIhkVVFvxMKv9uHng2UAgM92FODVWekYkRSOjUcrUNlgRFSwFhMHRstcafexZ6TzPtuej+U7CqFUAG/dmMkg4icYRohINj/sL8VjK/ahutEErUqJkAA1csobcPW/f8fcSf1wqNS2e+aVGQlefboqp2k6Z39RHZ74+gAA4K9TB+KCc5wrRL6DYYSI3K7JZMHfV+7HV7tszYmD4kLx2vUZiA0LwBMr9+O7fSV4Y+1Rx/P/6MVTNABHRjqjtsmEez/eCZNFwpTBMbj34r5yl0Ru5L1/ahCR13ph9UF8tasISgUwZ1JffDP3QgyOD0NksBZv35SJN27IgD5QA8AWVIYm6GWuuGfsYcQiCZitXFFzKkkSWLB8DwprmpESGYRXrsuA0st22aWe4cgIEblVdrEBS7fmAwA+mH0eJg2Kafe4QqHAVRmJOL9PFJZtK8C04XFylOlU9mkawDY64s1TTq6w+PfjWHeoHDq1Eu/eMhL6II3cJZGbMYwQkdsIIfCPVQcgCWD6iPjTgkhbsWEBmDelvxurcx2NSgGVUgGrJNBisiIsgB+2dscrG/Hyj4cAAE9cMcTrR8GoexjPichtfthfii251dCplVg4zX8OMFMoFOwbOQNJEnjki71oMUsY3y8KN49NkbskkgnDCBG5RYvZiue+OwgAuGdCXyRFBMlckXtxr5HTfbT5OLYdr0aQVoUXrxkBhYJ9Iv6KYYSI3OI/G3NRVNuMeH0A7pvgfyslArW2X7dNXN4LAMivasI/fzgMAFg4bRCSI/0rnFJ7DCNE5HIldc349y/HAAALLx/crqHTX9inabjXSOv0zJd70Gy24vw+kbh5bG+5SyKZsYGViFyqwWjBk18fQLPZivNSIzBjhH+euMqeEcAqCZyoasTK3UXYkluNQI0K/7x2BJfxEsMIEblGfYsZ//v9OP67KQ+1TWYoFMBTM4b6bV+Av/aM1DaZ8PrPR7E7vwaHy+rR0ubk4kcuG4jeUcEyVkeegmGEiJyq2WTFf37NxX9/zYWhxQIA6BMdjEcvG4Rhif67bNM+NdXsZ9M0j63Yh9X7Sh1fB2iUGBgbismDYzF7XKp8hZFH6XIY2bhxI15++WXs3LkTJSUlWLFiBWbOnHnW13zyySd46aWXcPToUej1ekybNg0vv/wyoqKiuls3EXmgFrMVsxdvw7a8agBA3+hgPDC5P64YkQCVnw/FO3pG/GhkJKugFqv3lUKhAF66dgRG9o5AalSw3/8s0Om63MDa2NiI9PR0vPPOO516/m+//YZbb70Vd955Jw4cOIDPP/8c27Ztw913393lYonIc5ksEu77eCe25VUjVKfGGzdk4KcHJ+CqjER++MD/ekaEEHjxe9tS7mtHJuG60cnoGx3CnwU6oy6PjEybNg3Tpk3r9PM3b96M1NRUPPDAAwCAtLQ03HPPPfjnP//Z1bcmIg9llQQWLM/C+sMVCNAo8eHt5+G8VB793laAY5rGP86m2XCkAltyq6FVK/HgpQPkLoc8nMuX9o4bNw4FBQVYvXo1hBAoKyvDF198gcsvv7zD1xiNRhgMhnY3IvJMQgg8vmIfVu0tgUalwP/702gGkTPwp5ERSRKOPURmj+uNxPBAmSsiT+fyMDJ+/Hh88sknuP7666HVahEXFwe9Xn/WaZ5FixZBr9c7bsnJya4uk4i6QQiBF1YfxLLtBVAqgDduyMSEAdFyl+WR/Kln5Js9xThYYkBogBr3T+wndznkBVweRrKzszFv3jw8+eST2LlzJ3744QccP34c9957b4evWbhwIerq6hy3goICV5dJRF0kSQLPfJuN//yaBwB48doRuHy4f+4h0hn+sprGaLHiXz/ZRkXum9gXEcFamSsib+Dypb2LFi3C+PHj8fDDDwMARowYgeDgYFx00UV47rnnEB9/+i8vnU4HnU7n6tKIqJssVgmPfLEXX+0uAgA8O3MYZo3mCObZ+Ms+I59syUdhTTNiw3S4/YI0ucshL+HyMNLU1AS1uv3bqFS2/1MKIVz99kTkZC1mK/7y6W6syS6DSqnAK9elY2ZmotxleTx/6Bn5YX8JXltzBAAwf8oAv9z2n7qny2GkoaEBOTk5jq/z8vKQlZWFyMhIpKSkYOHChSgqKsJHH30EAJgxYwbuvvtuvPvuu5g6dSpKSkowf/58jBkzBgkJCc77LyEil2swWvDnj3bg92NV0KqV+PdNIzFlSKzcZXkF+0F5vtgzYmgx45lvsvHlrkIAwKjeEbhuVJLMVZE36XIY2bFjByZNmuT4esGCBQCA2bNnY8mSJSgpKUF+fr7j8dtuuw319fV4++238dBDDyE8PByXXHIJl/YSeRmzVXIEkWCtCv+dfR7G9eXGhZ3lGBnxsZ6RLblVeGj5HhTVNkOpAO6d0BfzpwyAWsVzWKnzFMIL5koMBgP0ej3q6uoQFhYmdzlEfkcIgcdW7Men2/IRrFVh6d3nIz05XO6yvMovh8tx2+LtGJoQhu8euEjucnrMKgm8tuYI3vklB0IAKZFBeHVWOkZzWTe10dnPb55NQ0TntPi34/h0Wz4UCuDNGzMZRLrBl3pGKhuMmLdsN37LqQIAXD86GU/MGIIQHT9SqHv4k0NEZ7X+cDme+y4bAPD45YMxeTB7RLrD3szZ4uXTNDtPVGPOJ7tRamhBkFaFF68dgSvT2f9HPcMwQkQdOlJWj78s3Q1J2P76vfNCLtXsLm8fGSkztODLXYV49acjsEgCfaOD8d4to9A/NlTu0sgHMIwQ0RllFxtwz8c70GC0YExaJJ6dOQwKBQ856y5v22dECIEdJ2qw/lA5fjlcgeySk8dyXDEiHi9eO4LTMuQ0/EkionZazFa8vS4H7204BoskkBIZhPduGQWtmqsjesIxTWOWIEkCSg8/vdbesGynUAAjksJxw3nJuOG8ZAZTciqGESIvt7+oDlZJOKWpdMfxavzty704VtEIAJg2LA7PXDUUkdzSu8fs0zQAYLRIHr0h2La8akfD8hUjEnDJoGhc3D8aUSHcGZtcg2GEyIu1mK248f0taLFYsXbBRKREBXX5ezSbrFh/uByr9hbj+/2lEALoFaLDczOH4rJhPGvGWQLahJFms9Vjw4hVEnjqmwMAgBvOS8Gia4bLXBH5A4YRIi+WXWJAvdECAFj8ex6emjG0U68TQmDtwXKsyCrCuoPl7foYrhuVhL9PHwJ9kMYlNfsrlVIBrVoJk0Xy6L6RpdvycbDEgLAANR6eOlDucshP+HUYqWsyo6CmCbFhAYgO5fAjeZ99hXWO/718ewHmTxkAfeC5Q8Tb63LwSusZIgCQFBGI6cPjMSM9AcMS9S6plWxTNSaL5LG7sNY0mvBK64m7D/1hIKfnyG38OozM/2w31h+uwAtXD8dNY1PkLoeoy/a2CSONJiuWbcvHPRP6nvU1P+wvdQSRP53fG38clYQRSXo2JLpBoEaFumazx55P88qaw6htMmNQXChu5u9EciO/bo9PirDNrxfVNslcCVH37CuqBQBMHWrbiGzJ78dhtkodPv9QqQELlmcBAG67IBXPzhyG9ORwBhE3sfeJeOI0zYHiOizdals98/SVQ3m2DLmVX/+0JUYEAgAKa5plroSo6xqNFuSUNwAA/j59CHqFaFFS14LV+0rO+PzqRhPu+t8ONJmsGN8vCn+fPtid5RLa7DXiYdM0Qgg88002JGHbQ+T8PjwAkdzLr8NIUmsYKWIYIS+UXWKAJIC4sAAkRwbh1nGpAID//pqHU8+/NFsl3PfxThTWNKN3VBDevnEk//KVQZCHjoy88tMRbDtejQCNEo9dzpBK7ufXv40SwzkyQt7L3i8yPMnWcHrz2BTo1ErsK6rDtrxqx/OqG01YsHwPtuZVI0Snxn9uHY0INibKwr7XiCf1jPxnYy7eXp8DAHhqxlAktP5eJHInvw4j9p6RsvoWmCwdz7MTeaL9RbYwMqJ19UtUiA7XjkoCAPx3Ux5azFa8t+EYJry0Ht/uKYZCAbx+fQYG8CwR2XjaNM1n2/Px/OqDAICHpw7EjWPYtEry8Osw0itEC51aCSGAkjqOjpB32VtYCwAYlnRyKe4d420H2f18sAyTX9mAF78/hHqjBUPiw7D0rvMxZQhP3JWTJzWwrt5XgoVf7QMA3HNxH9w/8eyrsIhcya/DiEKhcDSxsm+EvEl9ixm5lbYt24e32RekX0wIJg+KgRBAUW0z4sIC8Mp16Vj1lwsxri+bEuUWqLH9ypU7jGw8UoF5y2ynMd84JhmPThvEFVUkK7/eZwSw9Y3kVjSyb4S8yoFiA4Sw/fz2OuW8kIWXD0Kz2YpxfaJw10V9PHbbcX8U6AHTNL/lVOLuj3bAbBWYPiIez80cziBCsvP7MGLvGymsZRgh72HfeXX4GXZL7RcTiqV3n+/ukqgTArTyhpHfcypx5/+2w2iRMHlQDF6blQGVh58eTP7Br6dpgJPLewtruPEZeY+9Re1X0pB3cIyMyDBN83tOJe7433a0mCVcMigG/75lJLRqv/8IIA/h9z+J3GuEvNG+1ubVEQwjXkWuMPL7sfZB5N1bRkKn5vQdeQ6/DyP2vUaKOE1DXqKu2YzjVbaRvDNN05DnsvfvuHOfkexiA+5YYgsikwZGM4iQR/L7MGLvGSmpa4HlLGd6EHmKA61TNCmRQQgP4uZl3sTd+4wIIfD0NwfQYpZwUf9eePeWUQwi5JH8PozEhOqgUSlglQTK6o1yl0N0To5+EY6KeB13T9P8sL/Usc37S38c4QhDRJ7G78OIUqlwbH9cWM0mVvJ8+wrZvOqtToYR14/CGi1WLPr+EADgzxf3Rbye27yT5/L7MAKwb4S8y96iWgAnt4En7+HoGXHDNM2S344jv7oJMaE63Duhj8vfj6gnGEbQdnkvwwh5tppGEwqqbT+nQxlGvE6Am6ZpqhqMeHud7fC7h6cORJDW77eUIg/HMAIgMdzWxMrlvSQnk0XCiarGsz5nX2u/SFqvYOgDNe4oi5zIXT0jr/18BPVGC4YlhuHakUkufS8iZ2AYQZuRkVr2jJB8nvn2ACa8/As+2Xqiw+fsPFEDgM2r3sod0zRHyuqxdGs+AODv04dAyR1WyQswjAA8LI9k12yyYsXuIgDAc6sO4njl6SMkuRUNeH9jLgDgwn693FofOYerR0aEEHh2VTYkAUwdGovz+/BwRPIODCM4OTJSXNsCSRIyV0P+aO2hMjS1/rXcbLbir5/vgbXNz6LZKmH+Z1loNltxQd8o/HEUh969kT2MWCQBswv2NVq1twS/Hq2EVqXEwmmDnf79iVyFYQRAXFgAVEoFTFYJFQ3ca4Tc75usYgDAzIwEhOjU2HGiBh9synU8/vrPR7C3sA76QA1emZXOoXcvFaA9+SvX2aMjdU1mPPPtAQDAnEn9kNor2Knfn8iVGEYAqFVKxIUFAOCBeeR+dc1m/HK4AgBwz4S+eOIK21+0//rxCI6U1WNrbhX+/csxAMCL1wznfhFeTKtSwp4jnd03suj7g6hsMKFvdDDuncilvORduN6rVWJEIIpqm1FY04xRveWuhvzJjwdKYbJK6B8TgkFxoRgUF4of9pdi/eEKPPhZFmoaTRACmDU6CdOGx8tdLvWAQqFAoEaFRpPVqSMjW3KrsGx7AQBg0TUjuOU7eR2OjLTiXiMkl2/32KZorkxPgEKhgEKhwIvXjoA+UIMDxQYU17UgNSoIT80YKnOl5Az2FTXOCiMtZiseW7EPAHDjmBSMSYt0yvclcieGkVZJ3IWVZFDZYMTvx6oAADPSExz3x4YF4B9X2cKHSqnA6zdkIljHgUxf4OzD8v79yzHkVjQiOlSHR6cNcsr3JHI3/nZrZT+9lyMj5E6r95XAKgmkJ+lPazi8Mj0BkhCIDNYhIzlcngLJ6Zy5vDenvB7v/mLbafXpGUO5ER55LYaRVif3GmEDK7mPfRVN21ERO4VCgaszuYTX1zg2PuthGBFC4KlvDsBsFbhkUAwuHx7njPKIZMFpmlb2npGi2mYIwb1GyPWKapux40QNFIozhxHyTSenaXq2z8ia7DL8llMFrUqJp2cMhULB5d7kvRhGWsXrA6FQAC1mCVWNJrnLIR/zj2+zMfypH/G3L/biQLHtfBl74+rYtEjEti4tJ9/njGkao8WK51cfBADcdVEaUqKCnFIbkVw4TdNKq1YiNjQApYYWFNY0o1eITu6SyEfUNZnx8ZYTMFklfLajAJ/tKMDo3hEor7dtsMdREf/ijDCy+LfjOFHVhJhQHe6f1M9ZpRHJhiMjbfCMGnKF7/eXwGSVkNYrGDPSE6BWKrDjRA3yq5ugVipw+TDuHeJPenpYXnl9C95aexQA8MhlgxDCVVbkA/hT3EZSRCB2nqjhLqzkVCuzbAfgzRqdjPsm9kX59MFYui0f3+wpxh+GxCEiWCtzheROAT0cGXn5h8NoNFmRnqTHNZmJziyNSDYMI20kcq8RcrKSumZszasGAMxIt42AxIQFYP6UAZg/ZYCcpZFMejJNs7ewFl/sKgQAPDljKM8oIp/BaZo2uNcIOdu3e4ohBDAmNdLx80X+LbD1sLyubnomhMA/vs2GEMDVmYkY1TvCFeURyYJhpA32jJCzrdxtWzFzVSabVMkmSGsbkO7qPiMHig3YcaIGWrUSf7uMO62Sb2EYaePk+TTsGaGeO1pWj+wSAzQqNqnSSd3tGVmx29Z7dOmQWMTpuRScfAvDSBvRobblvI0ma493RySyN65OGBDNJlVyCOzG2TQWq4RvWveluTqDTavkexhG2gjVqaFqbQirazbLXA15MyEEvm7d6v0qfnhQG46ekS78wfP7sSpU1BsREaTBxQOiXVUakWwYRtpQKBQIC7DN5zKMUE/syq9BYU0zgrUqTBkcK3c55EG6MzKysnWK5ooRCdCq+WubfA9/qk8RHmQbTq9tYhih7rM3rk4dFufY5IoI6HrPSJPJgh8OlAIAZnJfEfJRDCOnsB/BzZER6i6zVcJ3+0oAADM5RUOn6Oo+I2uyy9BksiIlMggjU8JdWBmRfLjp2SnsYaS2iYflUdeZLBKe+y4b1Y0m9ArR4oK+UXKXRB6mq9vB26doZmYk8GRe8lkMI6cID+LICHVPQXUT5i7dhT2FtlN5H5jcH2oVBx+pva6MjFQ2GLHxaCUA4CpO0ZAPYxg5BadpqDt+PFCKhz/fA0OLBfpADV6dlY7JbFylM+hKz8iqPcWwSgLpSXr0jQ5xdWlEsmEYOUW4Y5qGYYQ65821R/HqmiMAgMyUcLx900jHOUdEp3JM05glSJI46/kyK1qXh7NxlXwdw8gpwjgyQl1wrKIBr/9sCyJ3XZiGRy4bxKWXdFb2aRoAMFqkDldbHatowJ6CWqiUClwxgscJkG/r8m/NjRs3YsaMGUhIsDVTrVy58pyvMRqNePzxx9G7d2/odDqkpqbiww8/7E69LudY2ssw4vM2H6vC/qK6Hn2Pt9flQBLAlMEx+PsVQxhE6JwC2oQR+1SN2SrhhdUHcdU7v+Hil9Zj+FM/YvIrGwAAF/br5dgdmshXdXlkpLGxEenp6bjjjjtwzTXXdOo1s2bNQllZGT744AP069cPJSUlkCSpy8W6A3tG/MPxykbc8sFWBGtV2P73KdCpu74XSG5FA75u3fJ93uQBzi6RfJRKqYBWrYTJIjnCyDvrc/D+xtzTnhuoUeGui9LcXSKR23U5jEybNg3Tpk3r9PN/+OEHbNiwAbm5uYiMjAQApKamdvVt3caxmoZLe33aT9mlsEoChhYLtufV4ML+vbr8Pd5ebxsVuWRQDIYn6V1QJfmqQI3KFkZMVhworsPb63IAAH+7bBDGpEUgPEiLyCAtwgI1jiMqiHyZy8eUv/nmG4wePRovvfQSEhMTMWDAAPz1r39Fc3Nzh68xGo0wGAztbu7CkRH/sCa7zPG/Nxwp7/Lrj1c2Os6emTe5v9PqIv9g7xsxtJjx0PI9sEgC04bF4d4JfTCqdyT6RocgIljLIEJ+w+VhJDc3F5s2bcL+/fuxYsUKvP766/jiiy9w//33d/iaRYsWQa/XO27JycmuLtMhvE0YkSThtvcl96lqMGLniRrH1xuOVHT5e7y9PgdWSWDiwGikJ4c7sTryB/am1X/9eBiHSusRGazFszOHcVMz8lsuDyOSJEGhUOCTTz7BmDFjcPnll+PVV1/F//73vw5HRxYuXIi6ujrHraCgwNVlOthX00gCqDda3Pa+5D7rDpVDEkBqVBCUCuBIWQOKazseqTvViapGrNht7xXhqAh1nb2J9fdjVQCAZ68ahl4hbFIl/+XyMBIfH4/ExETo9Sfn1AcPHgwhBAoLC8/4Gp1Oh7CwsHY3dwnQqBCgsV0WA6dqfJJ9iuaqjERktI5qdGV05J3WUZGLB0QjMyXCFSWSjwvUnPzVe8WIeEwfES9jNUTyc3kYGT9+PIqLi9HQ0OC478iRI1AqlUhKSnL123dLeCBP7vVVLWYrfm3dXvvSIbGYMCAGALDhcMdhxGKVcLyyEesPleM/G3Px1S6OilDP2KdpeoVo8Y+rhslcDZH8uryapqGhATk5OY6v8/LykJWVhcjISKSkpGDhwoUoKirCRx99BAC46aab8Oyzz+L222/HM888g8rKSjz88MO44447EBjombtU6gM1KDW0sInVB/2WU4lmsxUJ+gAMTQiDVRJ47ecj+C2nEmarBE2bs2QkSWDO0l1Yk10Gyyn9Qxf174VRvTkqQt0zKiUC24/X4MVrRiAyWCt3OUSy63IY2bFjByZNmuT4esGCBQCA2bNnY8mSJSgpKUF+fr7j8ZCQEKxZswZ/+ctfMHr0aERFRWHWrFl47rnnnFC+a+hbl/fWNnN5r6+xT9FMGRILhUKB4Yl6RAZrUd1owq4TNRjb5+Qpu6v2leD7/aUAgACNEqlRwegTHYy+0SG4eWxvWeon37DgDwNx78S+CNJyE2wioBthZOLEiRCi41UmS5YsOe2+QYMGYc2aNV19K9lwea9vkiSBnw/alvFOaT3ETqlU4KL+vfB1VjE2HKlwhBGzVcIrPx0GYJuOmTe5/1nPECHqKgYRopO4d/UZ8LA835RVWIvKBiNCdWqc32YEZOLAaADtm1iXbS/Aiaom9ArR4c8X92EQISJyIYaRM7CPjHA1jW+xT9FMGBjd7gyZi/rbwsiBYgPK61vQZLLgzbVHAQAPTO6HYB3/giUiciX+lj0D+5bwHBnxLT+3hpFLh8S2u79XiA4jkvTYW1iHjUcqUWZoQUW9EcmRgbjhvBQ5SiUi8isMI2dgHxlhA6vvOF7ZiKPlDVArFZjYupy3rQkDorG3sA5fZxUhq6AWAPDQpQN5Ci8RkRvwN+0Z6INsS+3YwOo77FM0Y9IiHaul2powwDZV8+vRStS3WDAoLhRXpie4tUYiIn/FMHIGbGD1Pb8ds210Nnlw7Bkfz0gOR1jAyYHCRy4byKZVIiI3YRg5Azaw+p7sYtvJzxnJ+jM+rlYpHY2s56VGYNLA06dyiIjINdgzcgaOBlaGEZ9Q2WBEeb0RCgUwMK7jc47mTekPlVKBeVP68/RUIiI3Yhg5A/vISJPJCpNFYhOjlztYYhsVSY0KRshZlukOiA3FmzdmuqssIiJqxU/ZMwgN0MD+hzGbWL2fPYwMjg+VuRIiIjoThpEzUCkVCG39C5phxPvZ+0WGxHc8RUNERPJhGOlAuGN5L/ca8XYHS+oBAIMZRoiIPBLDSAf0XN7rE1rMVuRUNAAAhiQwjBAReSKGkQ7YV9Rwmsa75ZQ3wCoJhAdpEBcWIHc5RER0BgwjHQjjyIhPaNsvwuW6RESeiWGkA/ZdWDky4t2yHStpOEVDROSpGEY6wGka32API1xJQ0TkuRhGOqDnyIjXE0I49hhh8yoRkediGOlAeKBtaW9tE5f2eqvCmmbUt1igUSnQNzpE7nKIiKgDDCMdCOPIiNezj4r0jwnllv5ERB6Mv6E7wMPyvB+bV4mIvAPDSAccPSNc2uu12C9CROQdGEY60HY1jRBC5mqoO7J5QB4RkVdgGOmAfWTEIgk0mqwyV0NdZWgxo6C6GQCX9RIReTqGkQ4EalTQqmyXh02s3udQ6+F4CfoAx6GHRETkmRhGOqBQKNpsCc/lvd6G/SJERN6DYeQsuAur92p7Jg0REXk2hpGzCOeKGq/FZb1ERN6DYeQsuCW8d7JYJRwus/WMcJqGiMjzMYychZ4bn3mlnIoGmCwSgrUqJEcEyV0OERGdA8PIWegdDawMI97kx/1lAIBRqZFQKhUyV0NEROfCMHIW9sPyOE3jPYQQ+DqrCABwVXqCzNUQEVFnMIychT5QDQCoa+bSXm+xv8iA3MpG6NRKTB0WJ3c5RETUCQwjZ2HfLIsjI95jZeuoyJQhsQjRqWWuhoiIOoNh5CzYM+JdrJLAt3uKAQAzMxJlroaIiDqLYeQs9Nz0zKtsya1Ceb0R+kANJgyIlrscIiLqJIaRs9Bz0zOvsnK3bYrm8uHx0Kr5o01E5C34G/ss7Duw1hstsFglmauhs2kxW/HD/lIAwMwMrqIhIvImDCNnYR8ZAQBDi0XGSuhc1h8qR73RggR9AM5LjZS7HCIi6gKGkbNQq5SOFRk8udez2VfRXJmRyI3OiIi8DMPIOfB8Gs9X12TG+kMVAICrOEVDROR1GEbOwbG8l2HEY/1woAQmq4SBsaE8pZeIyAsxjJxDeOvyXgPDiMdatbcEAHBVJkdFiIi8EcPIOXDjM89mtFixLa8aAPCHIbEyV0NERN3BMHIO4dz4zKPtLayD0SKhV4gWfaND5C6HiIi6gWHkHGLDAgAAB4rrZK7EfwkhOnxsa24VAGBMWiQUCq6iISLyRgwj53BZ68mv6w9VcCdWNyuubcas/7cZF/5zfYdLq7e2TtGM4d4iRERei2HkHAbFhWFQXChMVgmr95fIXY7f+C2nEle8tQnb8qpRVNuM71t3V23LbJWw80QNAGBsnyh3l0hERE7CMNIJV2faToBd0Xr2CbmOJAm8sz4Hf/pgK6obTQjSqgAAPx44PYzsL6pDk8mK8CANBsaGurtUIiJyEoaRTrgyIwEKBbAtrxqFNU1yl+OzWsxW3PPxTrz842FIArhuVBKW3zMOAPB7ThXqW9pPk9mnaM5LjeSuq0REXoxhpBPi9YEY1zoN8HVWsczV+K7PdxZiTXYZtColFl0zHC/9cQSGJerRNzoYJquE9Ycr2j3fvqR3bBr7RYiIvBnDSCfNbDNVc7bVHdR9OWX1AIDbL0zFjWNSHKtjpg61NRG3naqxSgLbHWGE/SJERN6MYaSTLhsWB51aiZzyBhwoNshdjk8qqm0GAKREBrW73x5GfjlUjhazFQBwsMSAeqMFoTo1hiRwC3giIm/GMNJJYQEaTGnd4XMlG1ldorDGFkaSItqHkRFJesTrA9BosuK3nEoAwJbW/UVGp0ZAxX4RIiKvxjDSBVdn2KZqvtlTDKvEqRpnEkK0CSOB7R5TKBSOrd7tUzX2fpExnKIhIvJ6DCNdcPGAaIQHaVBeb8TmY1Vyl+NT6prNaDBaAACJ4YGnPW6fqvn5YDnMVgnbjrf2i/Rh8yoRkbfrchjZuHEjZsyYgYSEBCgUCqxcubLTr/3tt9+gVquRkZHR1bf1CFq1EleMiAfAPUeczT4q0itEhwCN6rTHx6RFIjxIg+pGEz7dlo/aJjOCtCoMT9S7u1QiInKyLoeRxsZGpKen45133unS62pra3Hrrbdi8uTJXX1Lj2LfAO2H/SWOZkrquY6maOzUKiUmD7JN1by25ggAYFTvCGhUHNwjIvJ26q6+YNq0aZg2bVqX3+jee+/FTTfdBJVK1aXRFE8zMiUCvUJ0qGww4kCxAaN6R8hdkk+wbyaX2EEYAYCpQ2Px5a5C1LSeEcTzaIiIfINb/qxcvHgxcnNz8dRTT3Xq+UajEQaDod3NUygUCoxIsk0N7C/iSb7OYl/W29HICGDr2QlsM4XD82iIiHyDy8PI0aNH8eijj+Ljjz+GWt25gZhFixZBr9c7bsnJyS6usmuGtfYp7C1kGHGWjpb1thWgUWHCgGgAgE6tRHoy+0WIiHyBS8OI1WrFTTfdhGeeeQYDBgzo9OsWLlyIuro6x62goMCFVXbdiESOjDjbuXpG7GakJwAAxvWNgk59eqMrERF5ny73jHRFfX09duzYgd27d2Pu3LkAAEmSIISAWq3GTz/9hEsuueS01+l0Ouh0OleW1iPDW6dpjpbXo9lkRaCWH4o9Ze8ZSTrDst62Lh8eh8W3nYeh3HWViMhnuDSMhIWFYd++fe3u+/e//41169bhiy++QFpamivf3mViwwIQHapDRb0R2SV1GNWbjZQ9UddsRn1L6x4j5xgZUSgUmDQoxh1lERGRm3Q5jDQ0NCAnJ8fxdV5eHrKyshAZGYmUlBQsXLgQRUVF+Oijj6BUKjFs2LB2r4+JiUFAQMBp93ubEYl6rD1Ujn2FDCM9VdQ6RRMVrEWQ1qX5mIiIPFCXe0Z27NiBzMxMZGZmAgAWLFiAzMxMPPnkkwCAkpIS5OfnO7dKD+RoYmXfSI91ZlkvERH5ri7/GTpx4kQI0fG5LEuWLDnr659++mk8/fTTXX1bjzOcTaxO09nmVSIi8k3cvrKb7E2sOeUNaDJZZK7Gu53cY6TjZb1EROS7GEa6KTYsADGhOkgCyC72nE3ZvJFjJQ1HRoiI/BLDSA/Yd2Ldx6maHrFP05zptF4iIvJ9DCM9YG9iZRjpmc7svkpERL6LYaQH7E2s+7gtfLfVt5hR12w7+I6raYiI/BPDSA/Yw8ixCjaxdpe9eTU8SIMQHfcYISLyRwwjPRATFoDYMDax9kRhNZf1EhH5O4aRHhrOE3x7xLGsN5z9IkRE/ophpIeGJ4YD4OZn3cVlvURExDDSQ8OTbKfHclv47nEs62UYISLyWwwjPTSsTRNro5FNrF3FZb1ERMQw0kMxoQGICwuAEEB2CZtYu+rkVvAcGSEi8lcMI04wjE2s3dJotKC60QSA0zRERP6MYcQJ7NvCL9uW7/hwpXOzj4qEBagRFqCRuRoiIpILw4gTXDc6CTGhOhwtb8At/92K2iYGks44uZKG/SJERP6MYcQJ4vWBWHr3WPQK0SK7xIBbP9wGQ4tZ7rI8XlEN+0WIiIhhxGn6xYTik7vOR0SQBnsL63Dbh9vQwNU1Z8VlvUREBDCMONXAuFD8351jERagxq78WtyxZDuskpC7LI/FZb1ERAQwjDjdsEQ9/u/OsQjSqrAtrxo7T9TIXZLH4u6rREQEMIy4RHpyOMakRQIAcsobZK7GM1U3mnCimmGEiIgYRlymb3QIANvOrHSSySLhv7/mYsLL61HbZEZYgBq9o4LlLouIiGSklrsAX8Uw0p4QAj8fLMcLqw8ir7IRADAkPgzPzhyKEB1/DImI/Bk/BVykb7Ttr32GEZsvdxXhr5/vAQD0CtHh4akD8MdRyVApFTJXRkREcmMYcZG+MbaRkcKaZrSYrQjQqGSuSF6r95UAAK7JTMQ/Zg7jaAgRETmwZ8RFooK10AdqIASQW9EodzmyEkJgb2EtAOCWcb0ZRIiIqB2GERdRKBScqmlVXNeCygYT1EoFhsSHyV0OERF5GIYRF+oXwyZWANhbUAsAGBAb6vfTVUREdDqGERc6uaLGv6dp9hbVAQDSk/UyV0JERJ6IYcSFHGHEzzc+s/eLjEgKl7UOIiLyTAwjLmRfUZNb2QDJT8+okSSBvYW2kZHhiRwZISKi0zGMuFByRCA0KgVazBKK65rlLkcWx6saUd9igU6txMC4ULnLISIiD8Qw4kJqlRKpUfYVNf7ZN7KvtV9kSEIYNCr+uBER0en46eBi/t43sqegtXmV/SJERNQBhhEX6xvj33uN2JtX2S9CREQdYRhxMfvISI4fjoxYrBL2F3NZLxERnR3DiIv5814jORUNaDFLCNGp0adXiNzlEBGRh2IYcbE+rVvCVzYYUddklrka99rb2i8yLDEMSp7OS0REHWAYcbHQAA3iwgIAAMcq/WuqZg83OyMiok5gGHEDRxOrn/WN2Dc7G5HEfhEiIuoYw4gb+GPfiNFixaFSAwAu6yUiorNjGHGDk2HEf0ZGDpXUw2wViAjSICkiUO5yiIjIgzGMuIE/hhHH/iJJ4VAo2LxKREQdYxhxA3vPSH5VE8xWSeZq3GNPoX3nVfaLEBHR2TGMuEFcWACCtCpYJIETVU1yl+MW+xzNq+HyFkJERB6PYcQNFAqFX03VNBotOFpeD4AraYiI6NwYRtykb+vmZ/6wLXxWQS0kAcTrAxDbuscKERFRRxhG3MSfRka2H68GAJyXGilzJURE5A0YRtxkYFwoAGDtwXKUGVpkrsa1HGEkjWGEiIjOjWHETSYNisGwxDDUNZvxty/3Qgghd0kuYbZK2HWiFgAwhiMjRETUCQwjbqJRKfHarAxo1Ur8crgCS7fly12SS2QXG9BstkIfqEH/GJ7US0RE58Yw4kb9Y0PxyNSBAIDnVh3E8Urf2x7ePkUzuncET+olIqJOYRhxszvGp+H8PpFoNlvx0Od7YJV8a7pmWx77RYiIqGsYRtxMqVTgX9elI0Snxs4TNXh/Y67cJTmNEAI7TtQAAM5LjZC5GiIi8hYMIzJIigjCUzOGAABeXXMYK3YXek1Da4PRgqve3oRnV2Wf9tixikZUN5qgUysxPDHc/cUREZFXYhiRyR9HJeHy4XEwWwUe/GwP5n66G7VNJrnLOqetuVXYU1iHDzblIad1l1U7e79IRnI4tGr+aBERUefwE0MmCoUCb96QiQWXDoBKqcB3e0tw2eu/YtPRSrlLO6u8Nk23//01r91j2/O42RkREXUdw4iM1ColHpjcH1/ddwH69ApGqaEFt3ywFf/91XP7SNqGka92F6Gi3uj4evsJNq8SEVHXdTmMbNy4ETNmzEBCQgIUCgVWrlx51ud/9dVXuPTSSxEdHY2wsDCMGzcOP/74Y3fr9UnpyeFY9cCFuHFMCgDgrXU5svSQ1DSacP8nO/HD/tIOn2MPIyqlAiaLhP/bcgIAUFrXgoLqZigVwMiUcHeUS0REPqLLYaSxsRHp6el45513OvX8jRs34tJLL8Xq1auxc+dOTJo0CTNmzMDu3bu7XKwvC9Kq8dSMIVAqgLpmc7sRB3f5aPMJrN5XijfWHu3wOfYwMntcKgDg4y0n0GK2Yltrv8iQhDCEBmhcXisREfkOdVdfMG3aNEybNq3Tz3/99dfbff3CCy/g66+/xrfffovMzMyuvr1PC9Co0DsqGHmVjThS1oAYN554K4TA13uKANgO87NYJahV7bNqs8mKkjrbuTr3TeyLHw+Uoqi2GV/tKsKhUgMAYHRvTtEQEVHXuL1nRJIk1NfXIzKy4w8to9EIg8HQ7uYv7FuoHymrP8cznWt/kQG5FbZRD5NFwonqptOec7zK9rg+UIPoUB3uuDANAPDfTbmOzc7GsF+EiIi6yO1h5F//+hcaGhowa9asDp+zaNEi6PV6xy05OdmNFcprQKztdN+j5e4NI19nFbX7+ugZwpB9+/q0XsEAgOvPS0ZogBq5FY04VGp7/mhudkZERF3k1jCydOlSPPPMM1i+fDliYmI6fN7ChQtRV1fnuBUUFLixSnn1j7WPjDS47T2tksA3e4oBALFhOgDA4dLT3z/3lDASolPjptamWwBIjQpCTKj7ppaIiMg3uC2MLFu2DHfddReWL1+OKVOmnPW5Op0OYWFh7W7+wj4ycqSs3m0rarbmVqG83gh9oAa3tjamHjnDyEzeKWEEAG4bnwp164F43F+EiIi6wy1h5NNPP8Xtt9+OTz/9FNOnT3fHW3qtPtHBUCkVqG+xoMzgnhU1K1unaC4fHo8hCbbg15lpGgCI1wfiutG2abQ/DI1zdalEROSDuryapqGhATk5OY6v8/LykJWVhcjISKSkpGDhwoUoKirCRx99BMA2NTN79my88cYbGDt2LEpLbXtYBAYGQq/XO+k/w3fo1Cr0jgpCbkUjjpTVI07v2mmPFrMV3++z/ZtclZGA5MggAEBuRSNMFqndtu5nGhkBgGevGoq7LkpD3+gQl9ZKRES+qcsjIzt27EBmZqZjWe6CBQuQmZmJJ598EgBQUlKC/Px8x/Pff/99WCwWzJkzB/Hx8Y7bvHnznPSf4HsGtpmqcbVfDpej3mhBvD4AY1IjkaAPQIhODYskHKtnANveJ1WNtrNzUk8JI2qVkkGEiIi6rcsjIxMnTjxrL8OSJUvaff3LL7909S38Xv/YUHy/vxRH3dDEunK3rXH1yvQEKFt7P/rHhmB3fi2OlNU7eljsUzTRoTqE6Lr8Y0NERNQhnk3jgQbYV9S4eHlvXbMZ6w6XAwCuykg8+f4xrSMzpSff3z5KcuoUDRERUU8xjHgg+2hETlmDS1fU/Li/FCaLhP4xIRgcH3ry/ePs00QnR2bsG6KlRTGMEBGRczGMeKDUqGColQrUGy2O7dddwb63yMzMRCgUCsf9jpGZNj0rjubVaIYRIiJyLoYRD6RVKx3TIa5sYt1TWAsAmDy4/QZ09gba41WNaDFbHf8b4DQNERE5H8OIh3JsC++iJlZDixn1LRYAQErrcl676FAd9IEaSMI2PSOEQF4FwwgREbkGw4iHsm8Lf9hFIyNFNc0AgIggDYK07VfHKBSKdlM1VY0m1BstUChODy5EREQ9xTDioU6OjLgmjBTX2sJIYkTgWd//SFm9o18kQR+IAI3KJfUQEZH/YhjxUPaRiaPlDZAk56+oKWoNIwn6zoeRPmxeJSIiF2AY8VC9o4KhUSnQZLI6goMzFZ1jZKTt6cH2MJLKZb1EROQCDCMeSqNSok8v++iI86dq7D0jieFnDiP2FTUFNU3ILjYAYPMqERG5BsOIB2s7OuFsjp6RDsJIVIgOUcFaCAFsPlYFgHuMEBGRazCMeLABLjwwz9Ez0kEYafv+JqsEgLuvEhGRazCMeDBHE6uTR0ZMFgnl9UYAHfeMtH1/AFArFUg6y3OJiIi6i2HEg/W3n1Hj5BU1pXUtEALQqZWICtae8/0B2/4iahV/XIiIyPn46eLBekcGQatSotlsRWGN81bUFLXpF2l7Js2pBsadDCNsXiUiIldhGPFgapXSsbeHM/tGOtMvAgADYk6GkVSGESIichGGEQ83NEEPAPjlSLnTvue5VtLY6YM0iAnVAeDICBERuQ7DiIe7dmQiAGDl7mI0Gi1O+Z72PUbONTICAJMGxkCjUmBsWqRT3puIiOhUDCMeblzfKPTpFYwGowVfZxU75XsW151999W2Xrx2OHY9cWm7ZlYiIiJnYhjxcAqFAjeNTQEAfLzlBITo+aqakyMjAZ16/9AATY/fk4iIqCMMI17gj6OSoFMrkV1iwO6C2h59LyGEo4E1KTzICdURERH1DMOIFwgP0uKKEQkAgE+25Pfoe1U1mmC0SFAogDj9uUdGiIiIXI1hxEvccr5tqmbV3mLUNpm6/X3sK2liQnXQqvnPT0RE8uOnkZfISA7HkPgwGC0SvthZ2O3vc67TeomIiNyNYcRLKBQK3HJ+bwDA0q353W5k7eyGZ0RERO7CMOJFrspIQIhOjdzKRvx+rKpb38OxFTwPvSMiIg/BMOJFgnVqXJ1p2wTtg015MFulLn+Pzu6+SkRE5C4MI17GPlWz7lA5pr6+EWuyy7o0ZVPEMEJERB6GYcTLDIwLxSvXpSMyWIvcikbc/dEO3PD+FuwtrO3U64trWwCwZ4SIiDwHw4gXunZUEn55eCLun9gXOrUSW/OqceXbv2FNdtlZX9dksqC60bYsmD0jRETkKRhGvFRYgAaPXDYI6/46EZMGRgMAPttecNbX2PtFQnVqhHGLdyIi8hAMI14uMTwQ86YMAABsP14NSeq4f6SodYqGoyJERORJGEZ8wLCEMARrVahrNuNQaX2Hzzt5QB7DCBEReQ6GER+gVikxKjUSALA1r+P9R7isl4iIPBHDiI8Ym9YaRnKrO3wOd18lIiJPxDDiI87vYwsj245Xd7jvCHdfJSIiT8Qw4iOGJ4YjQKNEdaMJR8sbzvick4fkBbizNCIiorNiGPERWrUSo3pHAAC25p7eN2KVBEoNratpwoPcWhsREdHZMIz4kLFpUQCALXmn942UGVpglQTUSgWiQ3XuLo2IiKhDDCM+5GQTa9VpfSP2lTTx4QFQKRVur42IiKgjDCM+JD05HFq1EpUNJhyraGz3mGMljZ7Nq0RE5FkYRnxIgEaFzORwAO33GxFC4NNt+QCAvjEhcpRGRETUIYYRHzO2j61vpO1+I8t3FGBLbjUCNSrcN6GvXKURERGdEcOIjzk/7eROrEIIlNe34PnvDgIAFlw6AMmRXElDRESeRS13AeRcmSkR0KgUKDMYcaKqCS//dBiGFguGJ+px+/hUucsjIiI6DUdGfEygVoWM1r6RF78/hO/2lkClVGDRNcOhVvGfm4iIPA8/nXyQfb+RHw6UAgDuujANwxL1cpZERETUIYYRHzS29ZwaAEiJDML8KQNkrIaIiOjsGEZ80KjeEQjQ2P5pn796GAK1KpkrIiIi6hgbWH1QkFaNxbeNQYPRgov6R8tdDhER0VkxjPiocX2j5C6BiIioUzhNQ0RERLJiGCEiIiJZMYwQERGRrBhGiIiISFYMI0RERCSrLoeRjRs3YsaMGUhISIBCocDKlSvP+ZpffvkFI0eOhE6nQ79+/bBkyZJulEpERES+qMthpLGxEenp6XjnnXc69fy8vDxMnz4dkyZNQlZWFubPn4+77roLP/74Y5eLJSIiIt/T5X1Gpk2bhmnTpnX6+e+99x7S0tLwyiuvAAAGDx6MTZs24bXXXsPUqVO7+vZERETkY1zeM7J582ZMmTKl3X1Tp07F5s2bXf3WRERE5AVcvgNraWkpYmNj290XGxsLg8GA5uZmBAYGnvYao9EIo9Ho+NpgMLi6TCIiIpKJR66mWbRoEfR6veOWnJwsd0lERETkIi4PI3FxcSgrK2t3X1lZGcLCws44KgIACxcuRF1dneNWUFDg6jKJiIhIJi6fphk3bhxWr17d7r41a9Zg3LhxHb5Gp9NBp9O5ujQiIiLyAF0OIw0NDcjJyXF8nZeXh6ysLERGRiIlJQULFy5EUVERPvroIwDAvffei7fffhuPPPII7rjjDqxbtw7Lly/Hd9991+n3FEIAYO8IERGRN7F/bts/xzskumj9+vUCwGm32bNnCyGEmD17tpgwYcJpr8nIyBBarVb06dNHLF68uEvvWVBQcMb35I033njjjTfePP9WUFBw1s95hRDniivykyQJxcXFCA0NhUKhcNr3NRgMSE5ORkFBAcLCwpz2fel0vNbuxevtPrzW7sNr7T7OutZCCNTX1yMhIQFKZcdtqi7vGXEGpVKJpKQkl33/sLAw/mC7Ca+1e/F6uw+vtfvwWruPM661Xq8/53M8cmkvERER+Q+GESIiIpKVX4cRnU6Hp556isuI3YDX2r14vd2H19p9eK3dx93X2isaWImIiMh3+fXICBEREcmPYYSIiIhkxTBCREREsmIYISIiIln5dRh55513kJqaioCAAIwdOxbbtm2TuySvt2jRIpx33nkIDQ1FTEwMZs6cicOHD7d7TktLC+bMmYOoqCiEhITg2muvPe1kZ+q6F198EQqFAvPnz3fcx2vtPEVFRbjlllsQFRWFwMBADB8+HDt27HA8LoTAk08+ifj4eAQGBmLKlCk4evSojBV7J6vViieeeAJpaWkIDAxE37598eyzz7Y724TXuns2btyIGTNmICEhAQqFAitXrmz3eGeua3V1NW6++WaEhYUhPDwcd955JxoaGnpeXJcOifEhy5YtE1qtVnz44YfiwIED4u677xbh4eGirKxM7tK82tSpU8XixYvF/v37RVZWlrj88stFSkqKaGhocDzn3nvvFcnJyWLt2rVix44d4vzzzxcXXHCBjFV7v23btonU1FQxYsQIMW/ePMf9vNbOUV1dLXr37i1uu+02sXXrVpGbmyt+/PFHkZOT43jOiy++KPR6vVi5cqXYs2ePuPLKK0VaWppobm6WsXLv8/zzz4uoqCixatUqkZeXJz7//HMREhIi3njjDcdzeK27Z/Xq1eLxxx8XX331lQAgVqxY0e7xzlzXyy67TKSnp4stW7aIX3/9VfTr10/ceOONPa7Nb8PImDFjxJw5cxxfW61WkZCQIBYtWiRjVb6nvLxcABAbNmwQQghRW1srNBqN+Pzzzx3POXjwoAAgNm/eLFeZXq2+vl70799frFmzRkyYMMERRnitnedvf/ubuPDCCzt8XJIkERcXJ15++WXHfbW1tUKn04lPP/3UHSX6jOnTp4s77rij3X3XXHONuPnmm4UQvNbOcmoY6cx1zc7OFgDE9u3bHc/5/vvvhUKhEEVFRT2qxy+naUwmE3bu3IkpU6Y47lMqlZgyZQo2b94sY2W+p66uDgAQGRkJANi5cyfMZnO7az9o0CCkpKTw2nfTnDlzMH369HbXFOC1dqZvvvkGo0ePxnXXXYeYmBhkZmbiP//5j+PxvLw8lJaWtrvWer0eY8eO5bXuogsuuABr167FkSNHAAB79uzBpk2bMG3aNAC81q7Smeu6efNmhIeHY/To0Y7nTJkyBUqlElu3bu3R+3vFQXnOVllZCavVitjY2Hb3x8bG4tChQzJV5XskScL8+fMxfvx4DBs2DABQWloKrVaL8PDwds+NjY1FaWmpDFV6t2XLlmHXrl3Yvn37aY/xWjtPbm4u3n33XSxYsACPPfYYtm/fjgceeABarRazZ892XM8z/U7hte6aRx99FAaDAYMGDYJKpYLVasXzzz+Pm2++GQB4rV2kM9e1tLQUMTEx7R5Xq9WIjIzs8bX3yzBC7jFnzhzs378fmzZtkrsUn1RQUIB58+ZhzZo1CAgIkLscnyZJEkaPHo0XXngBAJCZmYn9+/fjvffew+zZs2WuzrcsX74cn3zyCZYuXYqhQ4ciKysL8+fPR0JCAq+1D/PLaZpevXpBpVKdtqqgrKwMcXFxMlXlW+bOnYtVq1Zh/fr1SEpKctwfFxcHk8mE2trads/nte+6nTt3ory8HCNHjoRarYZarcaGDRvw5ptvQq1WIzY2ltfaSeLj4zFkyJB29w0ePBj5+fkA4Lie/J3Scw8//DAeffRR3HDDDRg+fDj+9Kc/4cEHH8SiRYsA8Fq7Smeua1xcHMrLy9s9brFYUF1d3eNr75dhRKvVYtSoUVi7dq3jPkmSsHbtWowbN07GyryfEAJz587FihUrsG7dOqSlpbV7fNSoUdBoNO2u/eHDh5Gfn89r30WTJ0/Gvn37kJWV5biNHj0aN998s+N/81o7x/jx409bon7kyBH07t0bAJCWloa4uLh219pgMGDr1q281l3U1NQEpbL9R5NKpYIkSQB4rV2lM9d13LhxqK2txc6dOx3PWbduHSRJwtixY3tWQI/aX73YsmXLhE6nE0uWLBHZ2dniz3/+swgPDxelpaVyl+bV7rvvPqHX68Uvv/wiSkpKHLempibHc+69916RkpIi1q1bJ3bs2CHGjRsnxo0bJ2PVvqPtahoheK2dZdu2bUKtVovnn39eHD16VHzyySciKChIfPzxx47nvPjiiyI8PFx8/fXXYu/eveKqq67ictNumD17tkhMTHQs7f3qq69Er169xCOPPOJ4Dq9199TX14vdu3eL3bt3CwDi1VdfFbt37xYnTpwQQnTuul522WUiMzNTbN26VWzatEn079+fS3t76q233hIpKSlCq9WKMWPGiC1btshdktcDcMbb4sWLHc9pbm4W999/v4iIiBBBQUHi6quvFiUlJfIV7UNODSO81s7z7bffimHDhgmdTicGDRok3n///XaPS5IknnjiCREbGyt0Op2YPHmyOHz4sEzVei+DwSDmzZsnUlJSREBAgOjTp494/PHHhdFodDyH17p71q9ff8bfz7NnzxZCdO66VlVViRtvvFGEhISIsLAwcfvtt4v6+voe16YQos22dkRERERu5pc9I0REROQ5GEaIiIhIVgwjREREJCuGESIiIpIVwwgRERHJimGEiIiIZMUwQkRERLJiGCEiIiJZMYwQERGRrBhGiIiISFYMI0RERCQrhhEiIiKS1f8HPUhSOEL4djcAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a820d289-befb-4cd4-8462-f6d62527a197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff35d465710>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(all_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "46da2d99-81a4-4004-b321-0448334edbfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff35d2d1f10>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f683e964-2d5c-417c-9323-ab8c94a80215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff35d31b950>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epoch_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9e28a8c8-3530-4c90-9cc8-56658ba2e048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99f3e523982e4223afeecc1f6a846fb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.014 MB of 0.014 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>TrainAccuracy</td><td></td></tr><tr><td>TrainLoss</td><td></td></tr><tr><td>ValidationAccuracy</td><td></td></tr><tr><td>ValidationLoss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>TrainAccuracy</td><td>0.93151</td></tr><tr><td>TrainLoss</td><td>0.0002</td></tr><tr><td>ValidationAccuracy</td><td>0.62462</td></tr><tr><td>ValidationLoss</td><td>324.88838</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">copper-star-79</strong> at: <a href='https://wandb.ai/jawardell/dcgan-project/runs/1z9k16h8' target=\"_blank\">https://wandb.ai/jawardell/dcgan-project/runs/1z9k16h8</a><br/> View job at <a href='https://wandb.ai/jawardell/dcgan-project/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjExODk0MzY3Mg==/version_details/v1' target=\"_blank\">https://wandb.ai/jawardell/dcgan-project/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjExODk0MzY3Mg==/version_details/v1</a><br/>Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231126_224643-1z9k16h8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fc350c-42b1-4749-8614-af72ed0e9aad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
