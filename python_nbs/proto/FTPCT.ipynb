{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a83f965d-b493-4e1a-90d5-25c4cd4d6541",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "5000\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "8000\n",
      "32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:go1jcox0) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fearless-snowflake-211</strong> at: <a href='https://wandb.ai/jawardell/dcgan-project/runs/go1jcox0' target=\"_blank\">https://wandb.ai/jawardell/dcgan-project/runs/go1jcox0</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231128_215455-go1jcox0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:go1jcox0). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e7f99b535fa4a3cb908f0ba691bdaaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011111988654981057, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/users2/jwardell1/dcgan-project/python_nbs/wandb/run-20231128_215852-u17w6oyc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jawardell/dcgan-project/runs/u17w6oyc' target=\"_blank\">wobbly-durian-212</a></strong> to <a href='https://wandb.ai/jawardell/dcgan-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jawardell/dcgan-project' target=\"_blank\">https://wandb.ai/jawardell/dcgan-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jawardell/dcgan-project/runs/u17w6oyc' target=\"_blank\">https://wandb.ai/jawardell/dcgan-project/runs/u17w6oyc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percent 10\n",
      "Starting Training Loop...\n",
      "iteration 0 current loss: 2.313260316848755 current acc: 0.0048\n",
      "iteration 1 current loss: 2.3071842193603516 current acc: 0.0098\n",
      "iteration 2 current loss: 2.297994375228882 current acc: 0.0158\n",
      "iteration 3 current loss: 2.2982096672058105 current acc: 0.0214\n",
      "iteration 4 current loss: 2.3054964542388916 current acc: 0.0274\n",
      "iteration 5 current loss: 2.2956817150115967 current acc: 0.0354\n",
      "iteration 6 current loss: 2.3144121170043945 current acc: 0.039\n",
      "iteration 7 current loss: 2.2972333431243896 current acc: 0.047\n",
      "iteration 8 current loss: 2.2996745109558105 current acc: 0.0552\n",
      "iteration 9 current loss: 2.2920374870300293 current acc: 0.0632\n",
      "iteration 10 current loss: 2.2856314182281494 current acc: 0.0712\n",
      "iteration 11 current loss: 2.2930538654327393 current acc: 0.0794\n",
      "iteration 12 current loss: 2.291191577911377 current acc: 0.0874\n",
      "iteration 13 current loss: 2.292447805404663 current acc: 0.0942\n",
      "iteration 14 current loss: 2.274488925933838 current acc: 0.1024\n",
      "iteration 15 current loss: 2.2823543548583984 current acc: 0.1098\n",
      "iteration 16 current loss: 2.2704505920410156 current acc: 0.1208\n",
      "iteration 17 current loss: 2.2758140563964844 current acc: 0.1294\n",
      "iteration 18 current loss: 2.2648308277130127 current acc: 0.1396\n",
      "iteration 19 current loss: 2.2634782791137695 current acc: 0.1448\n",
      "\t\tTrain Epoch 0/100,Train Accuracy: 0.1448, Train Loss: 2.290746295452118.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 0/100, Validation Accuracy: 0.206875, Validation Loss: 2.2541990652680397\n",
      "best loss 2.290746295452118\n",
      "iteration 0 current loss: 2.2640745639801025 current acc: 0.0084\n",
      "iteration 1 current loss: 2.263018846511841 current acc: 0.0196\n",
      "iteration 2 current loss: 2.254365921020508 current acc: 0.0316\n",
      "iteration 3 current loss: 2.248542308807373 current acc: 0.0422\n",
      "iteration 4 current loss: 2.242746591567993 current acc: 0.0532\n",
      "iteration 5 current loss: 2.2430121898651123 current acc: 0.065\n",
      "iteration 6 current loss: 2.2325663566589355 current acc: 0.0774\n",
      "iteration 7 current loss: 2.2270431518554688 current acc: 0.0894\n",
      "iteration 8 current loss: 2.213216781616211 current acc: 0.1022\n",
      "iteration 9 current loss: 2.212810516357422 current acc: 0.115\n",
      "iteration 10 current loss: 2.212721586227417 current acc: 0.1302\n",
      "iteration 11 current loss: 2.21028733253479 current acc: 0.1444\n",
      "iteration 12 current loss: 2.2098426818847656 current acc: 0.1548\n",
      "iteration 13 current loss: 2.215566635131836 current acc: 0.1642\n",
      "iteration 14 current loss: 2.192014455795288 current acc: 0.177\n",
      "iteration 15 current loss: 2.170386791229248 current acc: 0.1898\n",
      "iteration 16 current loss: 2.215759754180908 current acc: 0.2\n",
      "iteration 17 current loss: 2.174677610397339 current acc: 0.2146\n",
      "iteration 18 current loss: 2.1726975440979004 current acc: 0.2266\n",
      "iteration 19 current loss: 2.181957960128784 current acc: 0.2342\n",
      "\t\tTrain Epoch 1/100,Train Accuracy: 0.2342, Train Loss: 2.2178654789924623.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 1/100, Validation Accuracy: 0.271, Validation Loss: 2.155557282269001\n",
      "best loss 2.2178654789924623\n",
      "iteration 0 current loss: 2.161151885986328 current acc: 0.0126\n",
      "iteration 1 current loss: 2.147707939147949 current acc: 0.0258\n",
      "iteration 2 current loss: 2.1247568130493164 current acc: 0.0404\n",
      "iteration 3 current loss: 2.1197731494903564 current acc: 0.054\n",
      "iteration 4 current loss: 2.1254608631134033 current acc: 0.068\n",
      "iteration 5 current loss: 2.1070327758789062 current acc: 0.0826\n",
      "iteration 6 current loss: 2.120126962661743 current acc: 0.0966\n",
      "iteration 7 current loss: 2.129046678543091 current acc: 0.1076\n",
      "iteration 8 current loss: 2.091843605041504 current acc: 0.1218\n",
      "iteration 9 current loss: 2.0805914402008057 current acc: 0.1358\n",
      "iteration 10 current loss: 2.10482120513916 current acc: 0.1496\n",
      "iteration 11 current loss: 2.072688579559326 current acc: 0.1658\n",
      "iteration 12 current loss: 2.0748777389526367 current acc: 0.18\n",
      "iteration 13 current loss: 2.094757556915283 current acc: 0.192\n",
      "iteration 14 current loss: 2.0283801555633545 current acc: 0.2072\n",
      "iteration 15 current loss: 2.0659596920013428 current acc: 0.2216\n",
      "iteration 16 current loss: 2.043698310852051 current acc: 0.2338\n",
      "iteration 17 current loss: 2.063765525817871 current acc: 0.2458\n",
      "iteration 18 current loss: 2.0401694774627686 current acc: 0.258\n",
      "iteration 19 current loss: 2.006077766418457 current acc: 0.2666\n",
      "\t\tTrain Epoch 2/100,Train Accuracy: 0.2666, Train Loss: 2.0901344060897826.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 2/100, Validation Accuracy: 0.2755, Validation Loss: 2.0170046649873257\n",
      "best loss 2.0901344060897826\n",
      "iteration 0 current loss: 2.0324976444244385 current acc: 0.0164\n",
      "iteration 1 current loss: 2.025007486343384 current acc: 0.0296\n",
      "iteration 2 current loss: 2.03139591217041 current acc: 0.0422\n",
      "iteration 3 current loss: 1.9876917600631714 current acc: 0.0568\n",
      "iteration 4 current loss: 1.9788744449615479 current acc: 0.0714\n",
      "iteration 5 current loss: 1.9783210754394531 current acc: 0.0888\n",
      "iteration 6 current loss: 2.0034074783325195 current acc: 0.1018\n",
      "iteration 7 current loss: 2.0061564445495605 current acc: 0.1132\n",
      "iteration 8 current loss: 1.9789966344833374 current acc: 0.1286\n",
      "iteration 9 current loss: 1.9832507371902466 current acc: 0.1426\n",
      "iteration 10 current loss: 1.9493308067321777 current acc: 0.1574\n",
      "iteration 11 current loss: 1.9565860033035278 current acc: 0.1718\n",
      "iteration 12 current loss: 1.9180623292922974 current acc: 0.1882\n",
      "iteration 13 current loss: 1.9100552797317505 current acc: 0.2046\n",
      "iteration 14 current loss: 1.910156011581421 current acc: 0.2226\n",
      "iteration 15 current loss: 1.97063148021698 current acc: 0.2364\n",
      "iteration 16 current loss: 1.9306325912475586 current acc: 0.2498\n",
      "iteration 17 current loss: 1.9396413564682007 current acc: 0.2642\n",
      "iteration 18 current loss: 1.9366636276245117 current acc: 0.2776\n",
      "iteration 19 current loss: 1.943230390548706 current acc: 0.2848\n",
      "\t\tTrain Epoch 3/100,Train Accuracy: 0.2848, Train Loss: 1.96852947473526.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 3/100, Validation Accuracy: 0.2265, Validation Loss: 1.9868210144340992\n",
      "best loss 1.96852947473526\n",
      "iteration 0 current loss: 1.895261287689209 current acc: 0.0162\n",
      "iteration 1 current loss: 1.9069575071334839 current acc: 0.0304\n",
      "iteration 2 current loss: 1.948095440864563 current acc: 0.043\n",
      "iteration 3 current loss: 1.9120757579803467 current acc: 0.0578\n",
      "iteration 4 current loss: 1.9082618951797485 current acc: 0.0724\n",
      "iteration 5 current loss: 1.9348583221435547 current acc: 0.0878\n",
      "iteration 6 current loss: 1.8825324773788452 current acc: 0.1018\n",
      "iteration 7 current loss: 1.866232991218567 current acc: 0.1188\n",
      "iteration 8 current loss: 1.8644763231277466 current acc: 0.136\n",
      "iteration 9 current loss: 1.8927019834518433 current acc: 0.1512\n",
      "iteration 10 current loss: 1.8596851825714111 current acc: 0.1672\n",
      "iteration 11 current loss: 1.8541386127471924 current acc: 0.1824\n",
      "iteration 12 current loss: 1.8468108177185059 current acc: 0.198\n",
      "iteration 13 current loss: 1.8762308359146118 current acc: 0.2136\n",
      "iteration 14 current loss: 1.823342204093933 current acc: 0.23\n",
      "iteration 15 current loss: 1.824742078781128 current acc: 0.2472\n",
      "iteration 16 current loss: 1.8264098167419434 current acc: 0.2614\n",
      "iteration 17 current loss: 1.8308411836624146 current acc: 0.2762\n",
      "iteration 18 current loss: 1.811835765838623 current acc: 0.2916\n",
      "iteration 19 current loss: 1.7941985130310059 current acc: 0.301\n",
      "\t\tTrain Epoch 4/100,Train Accuracy: 0.301, Train Loss: 1.867984449863434.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 4/100, Validation Accuracy: 0.297875, Validation Loss: 1.8462828174233437\n",
      "best loss 1.867984449863434\n",
      "iteration 0 current loss: 1.8505475521087646 current acc: 0.0168\n",
      "iteration 1 current loss: 1.8342194557189941 current acc: 0.0328\n",
      "iteration 2 current loss: 1.809500813484192 current acc: 0.0496\n",
      "iteration 3 current loss: 1.8303799629211426 current acc: 0.0654\n",
      "iteration 4 current loss: 1.8193352222442627 current acc: 0.0818\n",
      "iteration 5 current loss: 1.832606315612793 current acc: 0.099\n",
      "iteration 6 current loss: 1.7980822324752808 current acc: 0.1172\n",
      "iteration 7 current loss: 1.8258721828460693 current acc: 0.1318\n",
      "iteration 8 current loss: 1.725874662399292 current acc: 0.1518\n",
      "iteration 9 current loss: 1.806976556777954 current acc: 0.1676\n",
      "iteration 10 current loss: 1.815181016921997 current acc: 0.1844\n",
      "iteration 11 current loss: 1.7685359716415405 current acc: 0.202\n",
      "iteration 12 current loss: 1.793992042541504 current acc: 0.2174\n",
      "iteration 13 current loss: 1.8266284465789795 current acc: 0.2328\n",
      "iteration 14 current loss: 1.7900872230529785 current acc: 0.2512\n",
      "iteration 15 current loss: 1.8194087743759155 current acc: 0.2674\n",
      "iteration 16 current loss: 1.786062240600586 current acc: 0.284\n",
      "iteration 17 current loss: 1.7742321491241455 current acc: 0.3008\n",
      "iteration 18 current loss: 1.7788172960281372 current acc: 0.3194\n",
      "iteration 19 current loss: 1.8958444595336914 current acc: 0.326\n",
      "\t\tTrain Epoch 5/100,Train Accuracy: 0.326, Train Loss: 1.809109228849411.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 5/100, Validation Accuracy: 0.335, Validation Loss: 1.7585666924715042\n",
      "best loss 1.809109228849411\n",
      "iteration 0 current loss: 1.8220573663711548 current acc: 0.0148\n",
      "iteration 1 current loss: 1.8327109813690186 current acc: 0.0308\n",
      "iteration 2 current loss: 1.7567260265350342 current acc: 0.0494\n",
      "iteration 3 current loss: 1.7957326173782349 current acc: 0.0652\n",
      "iteration 4 current loss: 1.7790464162826538 current acc: 0.0812\n",
      "iteration 5 current loss: 1.7905166149139404 current acc: 0.0994\n",
      "iteration 6 current loss: 1.7502793073654175 current acc: 0.1162\n",
      "iteration 7 current loss: 1.7383137941360474 current acc: 0.1326\n",
      "iteration 8 current loss: 1.7598856687545776 current acc: 0.1502\n",
      "iteration 9 current loss: 1.7433228492736816 current acc: 0.1692\n",
      "iteration 10 current loss: 1.7812479734420776 current acc: 0.185\n",
      "iteration 11 current loss: 1.747092843055725 current acc: 0.2036\n",
      "iteration 12 current loss: 1.7181673049926758 current acc: 0.2226\n",
      "iteration 13 current loss: 1.7434051036834717 current acc: 0.2414\n",
      "iteration 14 current loss: 1.7340710163116455 current acc: 0.2624\n",
      "iteration 15 current loss: 1.7313143014907837 current acc: 0.2792\n",
      "iteration 16 current loss: 1.7038081884384155 current acc: 0.2986\n",
      "iteration 17 current loss: 1.733497977256775 current acc: 0.3164\n",
      "iteration 18 current loss: 1.6991605758666992 current acc: 0.3348\n",
      "iteration 19 current loss: 1.6907496452331543 current acc: 0.343\n",
      "\t\tTrain Epoch 6/100,Train Accuracy: 0.343, Train Loss: 1.7525553286075592.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 6/100, Validation Accuracy: 0.318875, Validation Loss: 1.7759764827787876\n",
      "best loss 1.7525553286075592\n",
      "iteration 0 current loss: 1.7045197486877441 current acc: 0.0184\n",
      "iteration 1 current loss: 1.7322551012039185 current acc: 0.036\n",
      "iteration 2 current loss: 1.6872068643569946 current acc: 0.0558\n",
      "iteration 3 current loss: 1.7215148210525513 current acc: 0.0736\n",
      "iteration 4 current loss: 1.755661964416504 current acc: 0.0908\n",
      "iteration 5 current loss: 1.7560641765594482 current acc: 0.1056\n",
      "iteration 6 current loss: 1.7245720624923706 current acc: 0.1244\n",
      "iteration 7 current loss: 1.71632719039917 current acc: 0.143\n",
      "iteration 8 current loss: 1.7061793804168701 current acc: 0.1624\n",
      "iteration 9 current loss: 1.7460918426513672 current acc: 0.1804\n",
      "iteration 10 current loss: 1.6811336278915405 current acc: 0.1984\n",
      "iteration 11 current loss: 1.8150020837783813 current acc: 0.2116\n",
      "iteration 12 current loss: 1.7330659627914429 current acc: 0.2318\n",
      "iteration 13 current loss: 1.7679588794708252 current acc: 0.2496\n",
      "iteration 14 current loss: 1.7214971780776978 current acc: 0.267\n",
      "iteration 15 current loss: 1.689879298210144 current acc: 0.2864\n",
      "iteration 16 current loss: 1.708574652671814 current acc: 0.3046\n",
      "iteration 17 current loss: 1.7314143180847168 current acc: 0.3204\n",
      "iteration 18 current loss: 1.689491868019104 current acc: 0.3386\n",
      "iteration 19 current loss: 1.6583858728408813 current acc: 0.3494\n",
      "\t\tTrain Epoch 7/100,Train Accuracy: 0.3494, Train Loss: 1.7223398447036744.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 7/100, Validation Accuracy: 0.3, Validation Loss: 1.7548131756484509\n",
      "best loss 1.7223398447036744\n",
      "iteration 0 current loss: 1.6929194927215576 current acc: 0.018\n",
      "iteration 1 current loss: 1.698276400566101 current acc: 0.0348\n",
      "iteration 2 current loss: 1.7350784540176392 current acc: 0.0546\n",
      "iteration 3 current loss: 1.7503001689910889 current acc: 0.0714\n",
      "iteration 4 current loss: 1.6842420101165771 current acc: 0.0912\n",
      "iteration 5 current loss: 1.74575936794281 current acc: 0.1082\n",
      "iteration 6 current loss: 1.704677939414978 current acc: 0.1278\n",
      "iteration 7 current loss: 1.730367660522461 current acc: 0.1452\n",
      "iteration 8 current loss: 1.6451280117034912 current acc: 0.1648\n",
      "iteration 9 current loss: 1.7102222442626953 current acc: 0.1852\n",
      "iteration 10 current loss: 1.7102620601654053 current acc: 0.2016\n",
      "iteration 11 current loss: 1.7141019105911255 current acc: 0.2196\n",
      "iteration 12 current loss: 1.6443352699279785 current acc: 0.239\n",
      "iteration 13 current loss: 1.6796363592147827 current acc: 0.2594\n",
      "iteration 14 current loss: 1.7304842472076416 current acc: 0.2772\n",
      "iteration 15 current loss: 1.6923493146896362 current acc: 0.2938\n",
      "iteration 16 current loss: 1.7437262535095215 current acc: 0.3096\n",
      "iteration 17 current loss: 1.7151821851730347 current acc: 0.328\n",
      "iteration 18 current loss: 1.6662665605545044 current acc: 0.345\n",
      "iteration 19 current loss: 1.693965196609497 current acc: 0.3546\n",
      "\t\tTrain Epoch 8/100,Train Accuracy: 0.3546, Train Loss: 1.7043640553951263.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 8/100, Validation Accuracy: 0.355, Validation Loss: 1.7128534577786922\n",
      "best loss 1.7043640553951263\n",
      "iteration 0 current loss: 1.6436638832092285 current acc: 0.0208\n",
      "iteration 1 current loss: 1.6581354141235352 current acc: 0.0406\n",
      "iteration 2 current loss: 1.680876612663269 current acc: 0.0594\n",
      "iteration 3 current loss: 1.6521937847137451 current acc: 0.08\n",
      "iteration 4 current loss: 1.6508846282958984 current acc: 0.099\n",
      "iteration 5 current loss: 1.674670934677124 current acc: 0.114\n",
      "iteration 6 current loss: 1.6890827417373657 current acc: 0.1302\n",
      "iteration 7 current loss: 1.6910516023635864 current acc: 0.1478\n",
      "iteration 8 current loss: 1.650886058807373 current acc: 0.1668\n",
      "iteration 9 current loss: 1.664681315422058 current acc: 0.186\n",
      "iteration 10 current loss: 1.6750030517578125 current acc: 0.2036\n",
      "iteration 11 current loss: 1.6232109069824219 current acc: 0.2232\n",
      "iteration 12 current loss: 1.6854063272476196 current acc: 0.242\n",
      "iteration 13 current loss: 1.7397561073303223 current acc: 0.2602\n",
      "iteration 14 current loss: 1.6639548540115356 current acc: 0.279\n",
      "iteration 15 current loss: 1.6594204902648926 current acc: 0.298\n",
      "iteration 16 current loss: 1.7148765325546265 current acc: 0.3136\n",
      "iteration 17 current loss: 1.706772804260254 current acc: 0.3304\n",
      "iteration 18 current loss: 1.7132904529571533 current acc: 0.3476\n",
      "iteration 19 current loss: 1.685151219367981 current acc: 0.358\n",
      "\t\tTrain Epoch 9/100,Train Accuracy: 0.358, Train Loss: 1.67614848613739.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 9/100, Validation Accuracy: 0.34025, Validation Loss: 1.7634018883109093\n",
      "best loss 1.67614848613739\n",
      "iteration 0 current loss: 1.6951897144317627 current acc: 0.018\n",
      "iteration 1 current loss: 1.7137771844863892 current acc: 0.036\n",
      "iteration 2 current loss: 1.6786623001098633 current acc: 0.0554\n",
      "iteration 3 current loss: 1.697265863418579 current acc: 0.0738\n",
      "iteration 4 current loss: 1.656524896621704 current acc: 0.094\n",
      "iteration 5 current loss: 1.6720716953277588 current acc: 0.1134\n",
      "iteration 6 current loss: 1.6223078966140747 current acc: 0.1344\n",
      "iteration 7 current loss: 1.6491793394088745 current acc: 0.1538\n",
      "iteration 8 current loss: 1.7254953384399414 current acc: 0.1714\n",
      "iteration 9 current loss: 1.6942371129989624 current acc: 0.1894\n",
      "iteration 10 current loss: 1.699489712715149 current acc: 0.2076\n",
      "iteration 11 current loss: 1.671060562133789 current acc: 0.2252\n",
      "iteration 12 current loss: 1.6722009181976318 current acc: 0.2424\n",
      "iteration 13 current loss: 1.6888551712036133 current acc: 0.2598\n",
      "iteration 14 current loss: 1.6441034078598022 current acc: 0.28\n",
      "iteration 15 current loss: 1.6485750675201416 current acc: 0.2984\n",
      "iteration 16 current loss: 1.671398639678955 current acc: 0.318\n",
      "iteration 17 current loss: 1.6198761463165283 current acc: 0.3386\n",
      "iteration 18 current loss: 1.599481225013733 current acc: 0.3586\n",
      "iteration 19 current loss: 1.666662573814392 current acc: 0.369\n",
      "\t\tTrain Epoch 10/100,Train Accuracy: 0.369, Train Loss: 1.6693207383155824.\n",
      "Starting Validation Loop...\n",
      "\t\tValidation Epoch 10/100, Validation Accuracy: 0.35325, Validation Loss: 1.6566788479685783\n",
      "best loss 1.6693207383155824\n",
      "iteration 0 current loss: 1.678360104560852 current acc: 0.0182\n",
      "iteration 1 current loss: 1.6823292970657349 current acc: 0.0338\n",
      "iteration 2 current loss: 1.6263360977172852 current acc: 0.054\n",
      "iteration 3 current loss: 1.6754109859466553 current acc: 0.0704\n",
      "iteration 4 current loss: 1.6589758396148682 current acc: 0.09\n",
      "iteration 5 current loss: 1.7220311164855957 current acc: 0.107\n",
      "iteration 6 current loss: 1.6359416246414185 current acc: 0.1276\n",
      "iteration 7 current loss: 1.6627193689346313 current acc: 0.1464\n",
      "iteration 8 current loss: 1.6433873176574707 current acc: 0.166\n",
      "iteration 9 current loss: 1.667482852935791 current acc: 0.1848\n",
      "iteration 10 current loss: 1.6447997093200684 current acc: 0.2072\n",
      "iteration 11 current loss: 1.641538143157959 current acc: 0.2286\n",
      "iteration 12 current loss: 1.6322520971298218 current acc: 0.2504\n",
      "iteration 13 current loss: 1.6549514532089233 current acc: 0.2714\n",
      "iteration 14 current loss: 1.6441869735717773 current acc: 0.2918\n",
      "iteration 15 current loss: 1.548558235168457 current acc: 0.3122\n",
      "iteration 16 current loss: 1.6082885265350342 current acc: 0.3334\n",
      "iteration 17 current loss: 1.6299840211868286 current acc: 0.3522\n",
      "iteration 18 current loss: 1.6061488389968872 current acc: 0.372\n",
      "iteration 19 current loss: 1.7153899669647217 current acc: 0.3804\n",
      "\t\tTrain Epoch 11/100,Train Accuracy: 0.3804, Train Loss: 1.648953628540039.\n",
      "Starting Validation Loop...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 251\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# Iterate over the validation dataset in batches\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 251\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Put val data to device (CPU, GPU, or TPU)\u001b[39;49;00m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_real\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/users2/jwardell1/miniconda3/envs/dcgan/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/data/users2/jwardell1/miniconda3/envs/dcgan/lib/python3.11/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/data/users2/jwardell1/miniconda3/envs/dcgan/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/data/users2/jwardell1/miniconda3/envs/dcgan/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/data/users2/jwardell1/miniconda3/envs/dcgan/lib/python3.11/site-packages/torchvision/datasets/stl10.py:118\u001b[0m, in \u001b[0;36mSTL10.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    114\u001b[0m     img, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[index], \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# doing this so that it is consistent with all other datasets\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# to return a PIL Image\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(img)\n",
      "File \u001b[0;32m/data/users2/jwardell1/miniconda3/envs/dcgan/lib/python3.11/site-packages/PIL/Image.py:3114\u001b[0m, in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   3112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m strides \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtobytes\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 3114\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtobytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3116\u001b[0m         obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mtostring()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Error while calling W&B API: An internal error occurred. Please contact support. (<Response [500]>)\n",
      "wandb: ERROR Error while calling W&B API: An internal error occurred. Please contact support. (<Response [500]>)\n",
      "wandb: ERROR Error while calling W&B API: An internal error occurred. Please contact support. (<Response [500]>)\n",
      "wandb: ERROR Error while calling W&B API: An internal error occurred. Please contact support. (<Response [500]>)\n",
      "wandb: ERROR Error while calling W&B API: An internal error occurred. Please contact support. (<Response [500]>)\n",
      "wandb: ERROR Error while calling W&B API: An internal error occurred. Please contact support. (<Response [500]>)\n",
      "wandb: ERROR Error while calling W&B API: An internal error occurred. Please contact support. (<Response [500]>)\n",
      "wandb: ERROR Error while calling W&B API: An internal error occurred. Please contact support. (<Response [500]>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import STL10\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "import sys\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "learning_rate = 0.0001\n",
    "batch_size = 256\n",
    "weight_decay = 0.04\n",
    "\n",
    "\n",
    "# Create a new transformation that resizes the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    # Convert the PIL Image to Torch Tensor before RandomErasing\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomErasing(p=0.5, scale=(0.2, 0.33), ratio=(0.3, 0.3), value='random'),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "\n",
    "# Load STL-10 dataset\n",
    "train_dataset = STL10(root='./data', split='train', transform=transform, download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "print(len(train_dataset))\n",
    "print(len(train_loader))\n",
    "\n",
    "test_dataset = STL10(root='./data', split='test', transform=transform, download=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "print(len(test_dataset))\n",
    "print(len(test_loader))\n",
    "\n",
    "num_train = len(train_dataset)\n",
    "nc = 3\n",
    "ndf = 96\n",
    "num_epochs = 100\n",
    "lr=learning_rate\n",
    "beta1 = 0.5\n",
    "ngpu = 1\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, ngpu, dim_z, num_classes):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        nc = 3  # Number of input channels for the 96x96x3 image\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 96 x 96\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size: (ndf) x 48 x 48\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size: (ndf*2) x 24 x 24\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size: (ndf*4) x 12 x 12\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size: (ndf*8) x 6 x 6\n",
    "            nn.Conv2d(ndf * 8, dim_z, 6, 1, 0, bias=False)\n",
    "        )\n",
    "        self.fc = nn.Linear(dim_z, num_classes)\n",
    "\n",
    "    def forward(self, input):\n",
    "        z = self.main(input)\n",
    "        z = z.view(input.size(0), -1)  # Flatten z to (batch_size, dim_z)\n",
    "        c = self.fc(z)\n",
    "        return c\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        # Use Kaiming initialization for Conv layers\n",
    "        nn.init.kaiming_normal_(m.weight.data, mode='fan_out', nonlinearity='relu')\n",
    "        if hasattr(m, 'bias') and m.bias is not None:\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "\n",
    "def generate_n_samples_per_class(dataset, labels, num_samples_per_class):\n",
    "    # generate fake class distribution\n",
    "    print(f'percent {percent}')\n",
    "    num_samples = len(dataset)\n",
    "    num_classes = len(np.unique(labels))\n",
    "    indexes = range(len(labels))\n",
    "    #print (np.histogram(labels, bins=range(num_classes+1)))\n",
    "\n",
    "    # parameters\n",
    "    unique_classes = set(labels)\n",
    "    selected_indexes = []\n",
    "    #num_samples_per_class = 10\n",
    "\n",
    "    # random sampling\n",
    "    random_indexes = list(range(len(labels)))\n",
    "    random.shuffle(random_indexes)\n",
    "\n",
    "    # counters and flag to stop\n",
    "    counters = {cls: [] for cls in unique_classes}\n",
    "    is_full = [False for _ in range(len(unique_classes))]\n",
    "\n",
    "    # sample until class full-filled\n",
    "    for idx in random_indexes:\n",
    "        cls = labels[idx]\n",
    "        if len(counters[cls]) < num_samples_per_class:\n",
    "            counters[cls].append(idx)\n",
    "            if len(counters[cls]) == num_samples_per_class:\n",
    "                is_full[cls] = True\n",
    "        if all(is_full):\n",
    "            break\n",
    "    #print (counters)\n",
    "\n",
    "    # combine all indexes by class from the dictionary\n",
    "    all_indexes = []\n",
    "    for k in counters.keys():\n",
    "        all_indexes += counters[k]\n",
    "    #print (all_indexes)\n",
    "\n",
    "    # check final results\n",
    "    all_classes_selected = []\n",
    "    for k in counters.keys():\n",
    "        for idx in counters[k]:\n",
    "            all_classes_selected.append(labels[idx])\n",
    "    #print (np.histogram(all_classes_selected, bins=range(num_classes+1)))\n",
    "\n",
    "    return all_indexes\n",
    "\n",
    "# set up wandb\n",
    "wandb.login()\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"dcgan-project\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": lr,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"weight_decay\": weight_decay,\n",
    "    \"epochs\": num_epochs,\n",
    "    \"architecture\": \"FT-PCT\",\n",
    "    \"dataset\": \"STL-10\",\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Lists to store data for plotting\n",
    "percentages = []\n",
    "avg_val_accs = []\n",
    "\n",
    "\n",
    "\n",
    "# Training loop for different percentages of labeled data\n",
    "for percent in range(10, 101, 10):  # Train on 10%, 20%, ..., 100% of the labeled data\n",
    "    # Calculate the number of samples to use\n",
    "    #num_samples = int(num_train * percent / 100)\n",
    "    \n",
    "    labels = train_dataset.labels\n",
    "    num_classes = len(np.unique(labels))\n",
    "    samples_per_class = (len(train_dataset) * percent) / num_classes\n",
    "    all_indexes = generate_n_samples_per_class(train_dataset, labels, samples_per_class)\n",
    "\n",
    "    \n",
    "    # Create a subset of the dataset with the desired percentage\n",
    "    subset_train_dataset = torch.utils.data.Subset(train_dataset, indices=all_indexes)\n",
    "    subset_train_loader = DataLoader(subset_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "    model = Encoder(ngpu=1, dim_z=64, num_classes=10).to(device)\n",
    "    PATH='/data/users2/jwardell1/dcgan-project/models/ae_pretraining_0.0001_256_0.0004.pth'\n",
    "    model.main.load_state_dict(torch.load(PATH))\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    \n",
    "        \n",
    "    # Training loop\n",
    "    best_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"Starting Training Loop...\")\n",
    "    # For each epoch\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss_value = 0\n",
    "        train_correct = 0\n",
    "        \n",
    "        # Set Network to Train Mode\n",
    "        model.train()\n",
    "    \n",
    "        # For each batch in the dataloader\n",
    "        for i, (data, labels) in enumerate(train_loader, 0): \n",
    "            data_real = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data_real)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            predicted = torch.argmax(output.data, dim=1).to(device)\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            \n",
    "            train_correct += correct\n",
    "            train_loss_value += loss.item()\n",
    "            \n",
    "            train_accuracy = train_correct / len(train_dataset)\n",
    "            print(f'iteration {i} current loss: {loss.item()} current acc: {train_accuracy}')\n",
    "    \n",
    "    \n",
    "        wandb.log({\"TrainAccuracy\": train_accuracy})\n",
    "    \n",
    "        train_loss = train_loss_value / len(train_loader)\n",
    "        wandb.log({\"TrainLoss\": loss.item()})\n",
    "    \n",
    "    \n",
    "        print(f'\\t\\tTrain Epoch {epoch}/{num_epochs},Train Accuracy: {train_accuracy}, Train Loss: {train_loss}.')\n",
    "    \n",
    "    \n",
    "    \n",
    "        # Validation Step\n",
    "        print('Starting Validation Loop...')\n",
    "        val_correct = 0\n",
    "        val_loss_value = 0\n",
    "        val_running_total = 0\n",
    "        val_acc = []\n",
    "    \n",
    "        # Set the model to valuation mode\n",
    "        model.eval()\n",
    "    \n",
    "    \n",
    "        # Iterate over the validation dataset in batches\n",
    "        with torch.no_grad():\n",
    "            for data, labels in test_loader:\n",
    "    \n",
    "                # Put val data to device (CPU, GPU, or TPU)\n",
    "                data_real = data.to(device)\n",
    "                labels = labels.to(device)\n",
    "    \n",
    "    \n",
    "                # Forward pass batch through D\n",
    "                output = model(data_real)\n",
    "    \n",
    "                # Calculate loss on validation batch\n",
    "                v_loss = criterion(output, labels)\n",
    "    \n",
    "                # Compute Predicted Labels for a Batch in Validation Dataset\n",
    "                predicted = torch.argmax(output.data, dim=1).to(device)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "                # Update Val Data\n",
    "                val_loss_value += v_loss.item()\n",
    "    \n",
    "    \n",
    "        val_accuracy = val_correct / len(test_dataset)\n",
    "        val_acc.append(val_accuracy)\n",
    "        wandb.log({\"ValidationAccuracy\": val_accuracy})\n",
    "    \n",
    "        \n",
    "        avg_val_loss = val_loss_value / len(test_loader)\n",
    "        wandb.log({\"ValidationLoss\": val_loss})\n",
    "    \n",
    "        print(f\"\\t\\tValidation Epoch {epoch}/{num_epochs}, Validation Accuracy: {val_accuracy}, Validation Loss: {val_loss}\")\n",
    "    \n",
    "        # Update best model if this epoch had the higest accuracy so far\n",
    "        if train_loss < best_loss:\n",
    "            best_loss = train_loss\n",
    "            print(f'best loss {best_loss}')\n",
    "            best_model_state = model.main.state_dict()\n",
    "    \n",
    "\n",
    "    # Record avg val acc across all epochs for each percent\n",
    "    percentages.append(percent)\n",
    "    avg_val_accs.append(sum(val_acc) / len(val_acc))\n",
    "    print(f'percent {percent}')\n",
    "    print(f'avg val acc {sum(val_acc) / len(val_acc)}')\n",
    "\n",
    "\n",
    "\n",
    "# Save the best model\n",
    "if best_model_state is not None:\n",
    "    PATH = '../models/sbl_da_ki_pct_{}_{}_{}.pth'.format(learning_rate, batch_size, weight_decay)\n",
    "    torch.save(best_model_state, PATH)\n",
    "\n",
    "\n",
    "plt.plot(percentages, avg_val_accs, marker='o')\n",
    "plt.xlabel('Percentage of Labeled Data')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.title('FT Validation Accuracy vs Percentage of Labeled Data')\n",
    "plt.grid(True)\n",
    "plt.savefig('../models/sbl_acc_per.png')\n",
    "np.save('../models/ft_avg_val_accs_{}_{}_{}.npy'.format(learning_rate, batch_size, weight_decay), avg_val_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c13c8e-a07b-4650-8d07-ee86d8586851",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05038782-29db-4b05-bfd0-9ddc225a46b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
