{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee692aa2-4962-4051-b67c-d5f906766a50",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f246f1-5c8d-40c1-956c-c92a091d7cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import STL10\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5cb75a-5df2-418b-bdd1-fed3190f3a5b",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6d114a-b43a-48bc-98a5-f020814a2591",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "image_size = 64\n",
    "\n",
    "# Create a new transformation that resizes the images\n",
    "transform=transforms.Compose([\n",
    "                               transforms.Resize(image_size),\n",
    "                               transforms.CenterCrop(image_size),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "\n",
    "# Load STL-10 dataset\n",
    "train_dataset = STL10(root='./data', split='train', transform=transform, download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "print(len(train_dataset))\n",
    "print(len(train_loader))\n",
    "\n",
    "test_dataset = STL10(root='./data', split='test', transform=transform, download=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "print(len(test_dataset))\n",
    "print(len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d244d7-272d-4e10-b677-ab9199697818",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_labels = next(iter(train_loader))\n",
    "\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "img = np.transpose(img, (1, 2, 0))\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fbd265-736f-43c1-a85b-9a2a746cabe2",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c10e5a0-09e3-4370-8523-b3fda30de789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of channels in the training images. For color images this is 3\n",
    "nc = 3\n",
    "\n",
    "# Size of z latent vector (i.e. size of generator input)\n",
    "nz = 64\n",
    "\n",
    "# Size of feature maps in discriminator\n",
    "ndf = 64\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 100\n",
    "\n",
    "# Learning rate for optimizers\n",
    "lr=0.001\n",
    "\n",
    "# Beta1 hyperparameter for Adam optimizers\n",
    "beta1 = 0.5\n",
    "\n",
    "# Number of GPUs available. Use 0 for CPU mode.\n",
    "ngpu = 1\n",
    "\n",
    "\n",
    "# Decide which device we want to run on\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5ad761-83ac-4767-9c35-677cc7634a66",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c4e9ac-1326-4175-a410-ad415cdc6699",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu, dim_z, num_classes):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        nc = 3  # Number of input channels for the 96x96x3 image\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 96 x 96\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size: (ndf) x 48 x 48\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size: (ndf*2) x 24 x 24\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size: (ndf*4) x 12 x 12\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size: (ndf*8) x 6 x 6\n",
    "            nn.Conv2d(ndf * 8, dim_z, 4, 1, 0, bias=False),  # Adjusted kernel size\n",
    "        )\n",
    "        self.fc = nn.Linear(dim_z, num_classes)\n",
    "\n",
    "    def forward(self, input):\n",
    "        z = self.main(input)\n",
    "        z = z.view(input.size(0), -1)  # Flatten z to (batch_size, dim_z)\n",
    "        c = self.fc(z)\n",
    "        return c\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate the model\n",
    "netD = Discriminator(ngpu=0, dim_z=64, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f9e85e-e7e7-4543-8a93-66e696935de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom weights initialization called on ``netG`` and ``netD``\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "netD.apply(weights_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da68eb03-0d8e-4819-99f5-4faebb27c891",
   "metadata": {},
   "source": [
    "# Criterion / Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab81ecbf-94f4-4f29-85ae-b96810bb91df",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08e7f6c-dea5-4992-94ae-f32a28d7ff5f",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c010e881-8e23-4488-b09c-cba0e8698fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(netD.parameters(), lr=lr, momentum=0.9, weight_decay=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60379802-c268-4c15-a81c-df9568fe2f4a",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cecb43-30cb-4f28-841d-d7b0bbe3ef2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# set up wandb\n",
    "# wandb.login()\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"dcgan-project\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": lr,\n",
    "    \"architecture\": \"DC-GAN\",\n",
    "    \"dataset\": \"STL-10\",\n",
    "    \"epochs\": num_epochs,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b951135c-73d9-40f9-90fc-7f08c21d1555",
   "metadata": {},
   "outputs": [],
   "source": [
    "netD.cuda()\n",
    "\n",
    "# Training loop\n",
    "best_acc = float('-inf')\n",
    "best_model_state = None\n",
    "num_train = len(train_dataset)\n",
    "num_train_batches = len(train_loader)\n",
    "\n",
    "# Set up total loss/acc trackers\n",
    "all_loss = []\n",
    "all_acc = []\n",
    "all_correct = 0\n",
    "train_running_total = 0\n",
    "\n",
    "# Set up epochal loss/acc trackers\n",
    "epoch_loss = []\n",
    "epoch_acc = []\n",
    "\n",
    "# Set up validation loss/acc trackers\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "val_running_total = 0\n",
    "\n",
    "# Set up batch loss/acc trackers\n",
    "batch_loss = []\n",
    "batch_acc = []\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "\n",
    "# For each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    # Refresh Epoch Statistics\n",
    "    print('reset epoch statistics')\n",
    "    epoch_correct = 0\n",
    "    epoch_loss_val = 0\n",
    "\n",
    "    \n",
    "    # Set Network to Train Mode\n",
    "    netD.train()\n",
    "\n",
    "    # For each batch in the dataloader\n",
    "    for i, (data, labels) in enumerate(train_loader, 0):\n",
    "        \n",
    "        # Refresh Batch Statistics\n",
    "        if (i % batch_size) == 0:\n",
    "            print('reset batch statistics')\n",
    "            batch_correct = 0\n",
    "            batch_train_loss = 0\n",
    "            batch_touched = 0\n",
    "\n",
    "        # Put train data to device (CPU, GPU, or TPU)\n",
    "        real_cpu = data.to(device)\n",
    "\n",
    "        #\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass batch through D\n",
    "        output = netD(real_cpu)\n",
    "        \n",
    "        # Calculate loss on batch\n",
    "        loss = criterion(output, labels.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Compute Predicted Labels for a Batch in Training Dataset\n",
    "        predicted = torch.argmax(output.data, dim=1).cpu()\n",
    "        correct = (predicted == labels).sum().item()\n",
    "\n",
    "        # Update All Data\n",
    "        all_loss.append(loss.item())\n",
    "        \n",
    "        all_correct += correct\n",
    "        train_running_total += labels.size(0)\n",
    "\n",
    "        # Compute All Loss/Acc at each datapoint\n",
    "        all_accuracy = all_correct / train_running_total\n",
    "        all_acc.append(all_accuracy)\n",
    "\n",
    "        print(f'iteration {i} current loss: {loss.item()} current acc: {all_accuracy}')\n",
    "        \n",
    "        # Log All metrics to wandb\n",
    "        wandb.log({\"All Loss\": loss.item(), \"All Accuracy\": all_accuracy})\n",
    "\n",
    "        # Update Epoch Data\n",
    "        epoch_correct += correct\n",
    "        epoch_loss_val += loss.item()\n",
    "        \n",
    "        # Update Batch Data\n",
    "        batch_correct += correct\n",
    "        batch_touched += labels.size(0)\n",
    "        batch_train_loss += loss.item()\n",
    "\n",
    "        # Compute Batch Loss/Acc at end of Batch\n",
    "        if (i % batch_size) == (batch_size-1):\n",
    "            \n",
    "            batch_accuracy = batch_correct / batch_touched\n",
    "            batch_acc.append(batch_accuracy)\n",
    "            \n",
    "            \n",
    "            avg_train_batch_loss = batch_train_loss / batch_size\n",
    "            batch_loss.append(avg_train_batch_loss)\n",
    "\n",
    "            print(f'\\tbatch {i % (batch_size-1)} of epoch {epoch} batch loss {avg_train_batch_loss} batch accuracy {batch_accuracy}')\n",
    "            \n",
    "            # Log Batch metrics to wandb\n",
    "            wandb.log({\"Batch Loss\": avg_train_batch_loss, \"Batch Accuracy\": batch_accuracy})\n",
    "\n",
    "    # Compute Epoch Loss/Acc at end of Epoch\n",
    "    epoch_accuracy = epoch_correct / num_train\n",
    "    epoch_acc.append(epoch_accuracy)\n",
    "\n",
    "    avg_epoch_loss = epoch_loss_val / num_train_batches\n",
    "    epoch_loss.append(avg_epoch_loss)\n",
    "\n",
    "    print(f'\\t\\tEpoch {epoch}/{num_epochs} complete. Epoch loss {avg_epoch_loss} Epoch accuracy {epoch_accuracy}')\n",
    "    \n",
    "    # Log Epoch metrics to wandb\n",
    "    wandb.log({\"Epoch Loss\": avg_epoch_loss, \"Epoch Accuracy\": epoch_accuracy})\n",
    "\n",
    "    # Validation Step\n",
    "    print('Starting Validation Loop...')\n",
    "    \n",
    "    # Refresh Validation Statistics\n",
    "    print('reset Validation statistics')\n",
    "    val_correct = 0\n",
    "    val_loss_value = 0\n",
    "\n",
    "    # Set the model to valuation mode\n",
    "    netD.eval()  \n",
    "\n",
    "    # Iterate over the validation dataset in batches\n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_loader:\n",
    "            # Put val data to device (CPU, GPU, or TPU)\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            \n",
    "            # Forward pass batch through D\n",
    "            outputs = netD(data)\n",
    "\n",
    "            # Calculate loss on validation batch\n",
    "            v_loss = criterion(outputs, labels)\n",
    "            wandb.log({\"Epoch val_loss\": v_loss.item()}) \n",
    "\n",
    "            \n",
    "            # Compute Predicted Labels for a Batch in Validation Dataset\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            \n",
    "            # Update Val Data\n",
    "            val_loss_value += v_loss.item()\n",
    "            val_running_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    val_accuracy = val_correct / val_running_total\n",
    "    val_loss_value /= len(test_loader)\n",
    "    \n",
    "    val_acc.append(val_accuracy)\n",
    "    val_loss.append(val_loss_value)\n",
    "    \n",
    "    print(f\"\\t\\tValidation Epoch {epoch}, Validation Accuracy: {val_accuracy}, Validation Loss: {val_loss_value}\")\n",
    "\n",
    "    # Log metrics to wandb\n",
    "    wandb.log({\"Epoch val_accuracy\": val_accuracy})\n",
    "    \n",
    "    # Update best model if this epoch had the higest accuracy so far\n",
    "    if epoch_accuracy > best_acc:\n",
    "        best_acc = epoch_accuracy\n",
    "        #print(f'best accuracy {best_acc}')\n",
    "        best_model_state = netD.state_dict()\n",
    "    \n",
    "    \n",
    "# Load the best model\n",
    "if best_model_state is not None:\n",
    "    netD.load_state_dict(best_model_state)\n",
    "    print(\"Loaded the model with the highest accuracy.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e65a84-31a5-43f5-bb7b-7872ffc7c641",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047ec00a-1924-4615-a96e-f331cc9ba654",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
