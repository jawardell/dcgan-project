{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee692aa2-4962-4051-b67c-d5f906766a50",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39f246f1-5c8d-40c1-956c-c92a091d7cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import STL10\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5cb75a-5df2-418b-bdd1-fed3190f3a5b",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c6d114a-b43a-48bc-98a5-f020814a2591",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "image_size = 96\n",
    "\n",
    "# Create a new transformation that resizes the images\n",
    "transform=transforms.Compose([\n",
    "                               transforms.Resize(image_size),\n",
    "                               transforms.CenterCrop(image_size),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "\n",
    "# Load STL-10 dataset\n",
    "train_dataset = STL10(root='./data', split='train', transform=transform, download=False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "test_dataset = STL10(root='./data', split='test', transform=transform, download=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fbd265-736f-43c1-a85b-9a2a746cabe2",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c10e5a0-09e3-4370-8523-b3fda30de789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of channels in the training images. For color images this is 3\n",
    "nc = 3\n",
    "\n",
    "# Size of z latent vector (i.e. size of generator input)\n",
    "nz = 64\n",
    "\n",
    "# Size of feature maps in encoder\n",
    "ndf = 96\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 100\n",
    "\n",
    "\n",
    "# Learning rate for optimizers\n",
    "lr=0.0001\n",
    "\n",
    "# Beta1 hyperparameter for Adam optimizers\n",
    "beta1 = 0.5\n",
    "\n",
    "# Number of GPUs available. Use 0 for CPU mode.\n",
    "ngpu = 1\n",
    "\n",
    "# Weight  Decay\n",
    "weight_decay = 0.0004\n",
    "\n",
    "\n",
    "# Decide which device we want to run on\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5ad761-83ac-4767-9c35-677cc7634a66",
   "metadata": {},
   "source": [
    "# Supervised Encoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3c4e9ac-1326-4175-a410-ad415cdc6699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (main): Sequential(\n",
       "    (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (2): Conv2d(96, 192, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (3): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (5): Conv2d(192, 384, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (6): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (8): Conv2d(384, 768, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (9): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (11): Conv2d(768, 64, kernel_size=(6, 6), stride=(1, 1), bias=False)\n",
       "  )\n",
       "  (fc): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, ngpu, dim_z, num_classes):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        nc = 3  # Number of input channels for the 96x96x3 image\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 96 x 96\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size: (ndf) x 48 x 48\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size: (ndf*2) x 24 x 24\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size: (ndf*4) x 12 x 12\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size: (ndf*8) x 6 x 6\n",
    "            nn.Conv2d(ndf * 8, dim_z, 6, 1, 0, bias=False)\n",
    "        )\n",
    "        self.fc = nn.Linear(dim_z, num_classes)\n",
    "\n",
    "    def forward(self, input):\n",
    "        z = self.main(input)\n",
    "        z = z.view(input.size(0), -1)  # Flatten z to (batch_size, dim_z)\n",
    "        c = self.fc(z)\n",
    "        return c\n",
    "\n",
    "# Load Pretrained Weights\n",
    "encoder = Encoder(ngpu=0, dim_z=64, num_classes=10).to(device)\n",
    "PATH='/data/users2/jwardell1/dcgan-project/models/ae_pretraining_0.0001_256_0.0004.pth'\n",
    "encoder.main.load_state_dict(torch.load(PATH))\n",
    "encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da68eb03-0d8e-4819-99f5-4faebb27c891",
   "metadata": {},
   "source": [
    "# Criterion / Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab81ecbf-94f4-4f29-85ae-b96810bb91df",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08e7f6c-dea5-4992-94ae-f32a28d7ff5f",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c010e881-8e23-4488-b09c-cba0e8698fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(encoder.parameters(), lr=lr, weight_decay=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60379802-c268-4c15-a81c-df9568fe2f4a",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b951135c-73d9-40f9-90fc-7f08c21d1555",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Loop...\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 2.3007729053497314 current acc: 0.08\n",
      "iteration 1 current loss: 2.2993059158325195 current acc: 0.07\n",
      "iteration 2 current loss: 2.3115367889404297 current acc: 0.07333333333333333\n",
      "iteration 3 current loss: 2.3192031383514404 current acc: 0.065\n",
      "iteration 4 current loss: 2.311204433441162 current acc: 0.08\n",
      "iteration 5 current loss: 2.2937283515930176 current acc: 0.09\n",
      "iteration 6 current loss: 2.300940752029419 current acc: 0.08857142857142856\n",
      "iteration 7 current loss: 2.285935401916504 current acc: 0.1025\n",
      "iteration 8 current loss: 2.297381639480591 current acc: 0.10888888888888888\n",
      "iteration 9 current loss: 2.283501148223877 current acc: 0.116\n",
      "iteration 10 current loss: 2.285637855529785 current acc: 0.11818181818181818\n",
      "iteration 11 current loss: 2.294057846069336 current acc: 0.11833333333333333\n",
      "iteration 12 current loss: 2.285944938659668 current acc: 0.11846153846153847\n",
      "iteration 13 current loss: 2.2749392986297607 current acc: 0.12142857142857143\n",
      "iteration 14 current loss: 2.2740533351898193 current acc: 0.12666666666666668\n",
      "iteration 15 current loss: 2.2637505531311035 current acc: 0.1325\n",
      "iteration 16 current loss: 2.275200605392456 current acc: 0.13411764705882354\n",
      "iteration 17 current loss: 2.267267942428589 current acc: 0.1388888888888889\n",
      "iteration 18 current loss: 2.264005184173584 current acc: 0.14526315789473684\n",
      "iteration 19 current loss: 2.2475502490997314 current acc: 0.152\n",
      "iteration 20 current loss: 2.256817579269409 current acc: 0.15333333333333332\n",
      "iteration 21 current loss: 2.239488124847412 current acc: 0.16\n",
      "iteration 22 current loss: 2.2644541263580322 current acc: 0.1617391304347826\n",
      "iteration 23 current loss: 2.2781054973602295 current acc: 0.16333333333333333\n",
      "iteration 24 current loss: 2.2597482204437256 current acc: 0.164\n",
      "iteration 25 current loss: 2.228407382965088 current acc: 0.1676923076923077\n",
      "iteration 26 current loss: 2.241711378097534 current acc: 0.17037037037037037\n",
      "iteration 27 current loss: 2.2352726459503174 current acc: 0.17214285714285715\n",
      "iteration 28 current loss: 2.223712205886841 current acc: 0.17655172413793102\n",
      "iteration 29 current loss: 2.2526180744171143 current acc: 0.176\n",
      "iteration 30 current loss: 2.2048821449279785 current acc: 0.17870967741935484\n",
      "iteration 31 current loss: 2.176623582839966 current acc: 0.18375\n",
      "iteration 32 current loss: 2.1981008052825928 current acc: 0.18484848484848485\n",
      "iteration 33 current loss: 2.1831912994384766 current acc: 0.1888235294117647\n",
      "iteration 34 current loss: 2.168959617614746 current acc: 0.192\n",
      "iteration 35 current loss: 2.2105987071990967 current acc: 0.1922222222222222\n",
      "iteration 36 current loss: 2.1597728729248047 current acc: 0.1945945945945946\n",
      "iteration 37 current loss: 2.1461801528930664 current acc: 0.1968421052631579\n",
      "iteration 38 current loss: 2.1335506439208984 current acc: 0.19846153846153847\n",
      "iteration 39 current loss: 2.1899046897888184 current acc: 0.198\n",
      "iteration 40 current loss: 2.1324121952056885 current acc: 0.2004878048780488\n",
      "iteration 41 current loss: 2.1272122859954834 current acc: 0.2042857142857143\n",
      "iteration 42 current loss: 2.1120855808258057 current acc: 0.20651162790697675\n",
      "iteration 43 current loss: 2.129348039627075 current acc: 0.20545454545454545\n",
      "iteration 44 current loss: 2.0790696144104004 current acc: 0.208\n",
      "iteration 45 current loss: 2.1152443885803223 current acc: 0.21\n",
      "iteration 46 current loss: 2.1078407764434814 current acc: 0.21148936170212765\n",
      "iteration 47 current loss: 2.12707781791687 current acc: 0.2125\n",
      "iteration 48 current loss: 2.0997092723846436 current acc: 0.2146938775510204\n",
      "iteration 49 current loss: 2.0370965003967285 current acc: 0.2176\n",
      "iteration 50 current loss: 2.1224944591522217 current acc: 0.2207843137254902\n",
      "iteration 51 current loss: 2.1169464588165283 current acc: 0.22115384615384615\n",
      "iteration 52 current loss: 2.100379228591919 current acc: 0.2230188679245283\n",
      "iteration 53 current loss: 2.0564842224121094 current acc: 0.2225925925925926\n",
      "iteration 54 current loss: 2.064851760864258 current acc: 0.22254545454545455\n",
      "iteration 55 current loss: 2.0560624599456787 current acc: 0.22214285714285714\n",
      "iteration 56 current loss: 2.0290815830230713 current acc: 0.2217543859649123\n",
      "iteration 57 current loss: 1.9192838668823242 current acc: 0.22344827586206897\n",
      "iteration 58 current loss: 2.005890369415283 current acc: 0.22576271186440677\n",
      "iteration 59 current loss: 2.0498664379119873 current acc: 0.226\n",
      "iteration 60 current loss: 2.0745041370391846 current acc: 0.2259016393442623\n",
      "iteration 61 current loss: 2.019829034805298 current acc: 0.2270967741935484\n",
      "iteration 62 current loss: 2.0297086238861084 current acc: 0.22825396825396826\n",
      "iteration 63 current loss: 2.005286455154419 current acc: 0.228125\n",
      "iteration 64 current loss: 1.9939805269241333 current acc: 0.22923076923076924\n",
      "iteration 65 current loss: 1.9628404378890991 current acc: 0.2296969696969697\n",
      "iteration 66 current loss: 1.916615605354309 current acc: 0.23253731343283582\n",
      "iteration 67 current loss: 1.9278182983398438 current acc: 0.23323529411764707\n",
      "iteration 68 current loss: 1.9979816675186157 current acc: 0.23333333333333334\n",
      "iteration 69 current loss: 1.8701132535934448 current acc: 0.23514285714285715\n",
      "iteration 70 current loss: 1.89473295211792 current acc: 0.23633802816901409\n",
      "iteration 71 current loss: 1.8521724939346313 current acc: 0.23833333333333334\n",
      "iteration 72 current loss: 1.8576773405075073 current acc: 0.24054794520547945\n",
      "iteration 73 current loss: 1.863235592842102 current acc: 0.24027027027027026\n",
      "iteration 74 current loss: 1.9171584844589233 current acc: 0.24106666666666668\n",
      "iteration 75 current loss: 1.8902682065963745 current acc: 0.24263157894736842\n",
      "iteration 76 current loss: 1.915408968925476 current acc: 0.24103896103896105\n",
      "iteration 77 current loss: 1.887946605682373 current acc: 0.24102564102564103\n",
      "iteration 78 current loss: 1.7844383716583252 current acc: 0.2427848101265823\n",
      "iteration 79 current loss: 1.814866065979004 current acc: 0.244\n",
      "iteration 80 current loss: 1.8656806945800781 current acc: 0.24493827160493828\n",
      "iteration 81 current loss: 1.7431868314743042 current acc: 0.2475609756097561\n",
      "iteration 82 current loss: 1.7779638767242432 current acc: 0.2493975903614458\n",
      "iteration 83 current loss: 1.9544788599014282 current acc: 0.2488095238095238\n",
      "iteration 84 current loss: 1.8380781412124634 current acc: 0.2503529411764706\n",
      "iteration 85 current loss: 1.9481641054153442 current acc: 0.24976744186046512\n",
      "iteration 86 current loss: 1.6960272789001465 current acc: 0.2519540229885057\n",
      "iteration 87 current loss: 1.8022805452346802 current acc: 0.2525\n",
      "iteration 88 current loss: 1.8546215295791626 current acc: 0.25280898876404495\n",
      "iteration 89 current loss: 1.7821563482284546 current acc: 0.25355555555555553\n",
      "iteration 90 current loss: 1.8985382318496704 current acc: 0.252967032967033\n",
      "iteration 91 current loss: 1.7912323474884033 current acc: 0.2539130434782609\n",
      "iteration 92 current loss: 1.783218502998352 current acc: 0.25526881720430106\n",
      "iteration 93 current loss: 1.7491025924682617 current acc: 0.25638297872340426\n",
      "iteration 94 current loss: 1.828825831413269 current acc: 0.25663157894736843\n",
      "iteration 95 current loss: 1.7206648588180542 current acc: 0.25604166666666667\n",
      "iteration 96 current loss: 1.7220271825790405 current acc: 0.25649484536082473\n",
      "iteration 97 current loss: 1.7338780164718628 current acc: 0.256530612244898\n",
      "iteration 98 current loss: 1.5903187990188599 current acc: 0.2587878787878788\n",
      "iteration 99 current loss: 1.822633981704712 current acc: 0.2584\n",
      "\t\tEpoch 0/100 complete. Epoch loss 2.059561170339584 Epoch accuracy 0.2584\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 0, Validation Accuracy: 0.29, Validation Loss: 1.7276457644999028\n",
      "best loss 2.059561170339584\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 1.7911196947097778 current acc: 0.25762376237623763\n",
      "iteration 1 current loss: 1.7604916095733643 current acc: 0.25862745098039214\n",
      "iteration 2 current loss: 1.833949089050293 current acc: 0.25844660194174757\n",
      "iteration 3 current loss: 1.740327000617981 current acc: 0.25884615384615384\n",
      "iteration 4 current loss: 1.757612705230713 current acc: 0.2598095238095238\n",
      "iteration 5 current loss: 1.687143087387085 current acc: 0.2611320754716981\n",
      "iteration 6 current loss: 1.643538475036621 current acc: 0.2626168224299065\n",
      "iteration 7 current loss: 1.693737506866455 current acc: 0.26425925925925925\n",
      "iteration 8 current loss: 1.7310304641723633 current acc: 0.26458715596330273\n",
      "iteration 9 current loss: 1.7646592855453491 current acc: 0.26581818181818184\n",
      "iteration 10 current loss: 1.8257606029510498 current acc: 0.26594594594594595\n",
      "iteration 11 current loss: 1.6266850233078003 current acc: 0.2673214285714286\n",
      "iteration 12 current loss: 1.6831481456756592 current acc: 0.26849557522123896\n",
      "iteration 13 current loss: 1.6858543157577515 current acc: 0.2692982456140351\n",
      "iteration 14 current loss: 1.8718522787094116 current acc: 0.26956521739130435\n",
      "iteration 15 current loss: 1.6138156652450562 current acc: 0.27\n",
      "iteration 16 current loss: 1.597062587738037 current acc: 0.2716239316239316\n",
      "iteration 17 current loss: 1.763074517250061 current acc: 0.27152542372881355\n",
      "iteration 18 current loss: 1.679275393486023 current acc: 0.27260504201680674\n",
      "iteration 19 current loss: 1.717995285987854 current acc: 0.273\n",
      "iteration 20 current loss: 1.628267526626587 current acc: 0.2740495867768595\n",
      "iteration 21 current loss: 1.5385220050811768 current acc: 0.27475409836065573\n",
      "iteration 22 current loss: 1.6346811056137085 current acc: 0.2757723577235772\n",
      "iteration 23 current loss: 1.6696418523788452 current acc: 0.27564516129032257\n",
      "iteration 24 current loss: 1.6763769388198853 current acc: 0.2768\n",
      "iteration 25 current loss: 1.683132290840149 current acc: 0.2765079365079365\n",
      "iteration 26 current loss: 1.5234261751174927 current acc: 0.27811023622047243\n",
      "iteration 27 current loss: 1.5955065488815308 current acc: 0.27890625\n",
      "iteration 28 current loss: 1.6545490026474 current acc: 0.2792248062015504\n",
      "iteration 29 current loss: 1.6827099323272705 current acc: 0.2792307692307692\n",
      "iteration 30 current loss: 1.5001929998397827 current acc: 0.28045801526717556\n",
      "iteration 31 current loss: 1.7238059043884277 current acc: 0.28015151515151515\n",
      "iteration 32 current loss: 1.7019799947738647 current acc: 0.28075187969924814\n",
      "iteration 33 current loss: 1.5642503499984741 current acc: 0.2822388059701493\n",
      "iteration 34 current loss: 1.7066349983215332 current acc: 0.2825185185185185\n",
      "iteration 35 current loss: 1.662564992904663 current acc: 0.2822058823529412\n",
      "iteration 36 current loss: 1.646694302558899 current acc: 0.28306569343065696\n",
      "iteration 37 current loss: 1.4434466361999512 current acc: 0.2843478260869565\n",
      "iteration 38 current loss: 1.6663224697113037 current acc: 0.2846043165467626\n",
      "iteration 39 current loss: 1.6371150016784668 current acc: 0.2855714285714286\n",
      "iteration 40 current loss: 1.56926691532135 current acc: 0.2865248226950355\n",
      "iteration 41 current loss: 1.530492901802063 current acc: 0.2880281690140845\n",
      "iteration 42 current loss: 1.5787256956100464 current acc: 0.28923076923076924\n",
      "iteration 43 current loss: 1.5829905271530151 current acc: 0.2902777777777778\n",
      "iteration 44 current loss: 1.5776090621948242 current acc: 0.2906206896551724\n",
      "iteration 45 current loss: 1.5658296346664429 current acc: 0.29041095890410956\n",
      "iteration 46 current loss: 1.6264853477478027 current acc: 0.2914285714285714\n",
      "iteration 47 current loss: 1.6421418190002441 current acc: 0.2922972972972973\n",
      "iteration 48 current loss: 1.5830817222595215 current acc: 0.29288590604026843\n",
      "iteration 49 current loss: 1.5192275047302246 current acc: 0.29346666666666665\n",
      "iteration 50 current loss: 1.485532283782959 current acc: 0.29443708609271524\n",
      "iteration 51 current loss: 1.4922807216644287 current acc: 0.29526315789473684\n",
      "iteration 52 current loss: 1.4778953790664673 current acc: 0.2962091503267974\n",
      "iteration 53 current loss: 1.5031569004058838 current acc: 0.29714285714285715\n",
      "iteration 54 current loss: 1.6051384210586548 current acc: 0.29754838709677417\n",
      "iteration 55 current loss: 1.622068166732788 current acc: 0.2982051282051282\n",
      "iteration 56 current loss: 1.564795732498169 current acc: 0.29821656050955414\n",
      "iteration 57 current loss: 1.5108259916305542 current acc: 0.299746835443038\n",
      "iteration 58 current loss: 1.5732944011688232 current acc: 0.30050314465408806\n",
      "iteration 59 current loss: 1.4877820014953613 current acc: 0.30175\n",
      "iteration 60 current loss: 1.7097947597503662 current acc: 0.302111801242236\n",
      "iteration 61 current loss: 1.5288794040679932 current acc: 0.3022222222222222\n",
      "iteration 62 current loss: 1.638804316520691 current acc: 0.3030674846625767\n",
      "iteration 63 current loss: 1.5832877159118652 current acc: 0.30390243902439024\n",
      "iteration 64 current loss: 1.572239875793457 current acc: 0.30436363636363634\n",
      "iteration 65 current loss: 1.7417560815811157 current acc: 0.30481927710843376\n",
      "iteration 66 current loss: 1.523881435394287 current acc: 0.30574850299401196\n",
      "iteration 67 current loss: 1.6891189813613892 current acc: 0.30583333333333335\n",
      "iteration 68 current loss: 1.547970175743103 current acc: 0.30662721893491124\n",
      "iteration 69 current loss: 1.5267010927200317 current acc: 0.3071764705882353\n",
      "iteration 70 current loss: 1.4523630142211914 current acc: 0.307953216374269\n",
      "iteration 71 current loss: 1.6911098957061768 current acc: 0.3082558139534884\n",
      "iteration 72 current loss: 1.6378867626190186 current acc: 0.3084393063583815\n",
      "iteration 73 current loss: 1.6288508176803589 current acc: 0.30908045977011495\n",
      "iteration 74 current loss: 1.6489500999450684 current acc: 0.30925714285714284\n",
      "iteration 75 current loss: 1.5216375589370728 current acc: 0.3104545454545454\n",
      "iteration 76 current loss: 1.5393604040145874 current acc: 0.31096045197740113\n",
      "iteration 77 current loss: 1.6520048379898071 current acc: 0.3112359550561798\n",
      "iteration 78 current loss: 1.534022569656372 current acc: 0.311731843575419\n",
      "iteration 79 current loss: 1.6785527467727661 current acc: 0.3121111111111111\n",
      "iteration 80 current loss: 1.5075854063034058 current acc: 0.31226519337016573\n",
      "iteration 81 current loss: 1.6107580661773682 current acc: 0.3121978021978022\n",
      "iteration 82 current loss: 1.5776238441467285 current acc: 0.31333333333333335\n",
      "iteration 83 current loss: 1.5377051830291748 current acc: 0.31380434782608696\n",
      "iteration 84 current loss: 1.393825650215149 current acc: 0.3145945945945946\n",
      "iteration 85 current loss: 1.654334306716919 current acc: 0.31483870967741934\n",
      "iteration 86 current loss: 1.6554087400436401 current acc: 0.31475935828877005\n",
      "iteration 87 current loss: 1.6010860204696655 current acc: 0.3147872340425532\n",
      "iteration 88 current loss: 1.5539387464523315 current acc: 0.31513227513227515\n",
      "iteration 89 current loss: 1.6824357509613037 current acc: 0.31484210526315787\n",
      "iteration 90 current loss: 1.6746383905410767 current acc: 0.3150785340314136\n",
      "iteration 91 current loss: 1.5471763610839844 current acc: 0.3154166666666667\n",
      "iteration 92 current loss: 1.4971462488174438 current acc: 0.31626943005181346\n",
      "iteration 93 current loss: 1.5758366584777832 current acc: 0.316701030927835\n",
      "iteration 94 current loss: 1.4903477430343628 current acc: 0.317025641025641\n",
      "iteration 95 current loss: 1.4604215621948242 current acc: 0.31755102040816324\n",
      "iteration 96 current loss: 1.453397512435913 current acc: 0.3180710659898477\n",
      "iteration 97 current loss: 1.4320042133331299 current acc: 0.3190909090909091\n",
      "iteration 98 current loss: 1.3747241497039795 current acc: 0.31959798994974875\n",
      "iteration 99 current loss: 1.4485230445861816 current acc: 0.3197\n",
      "\t\tEpoch 1/100 complete. Epoch loss 1.6088266503810882 Epoch accuracy 0.381\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n",
      "\t\tValidation Epoch 1, Validation Accuracy: 0.4085, Validation Loss: 1.5229741774499417\n",
      "best loss 1.6088266503810882\n",
      "reset epoch statistics\n",
      "iteration 0 current loss: 1.5362577438354492 current acc: 0.31960199004975126\n",
      "iteration 1 current loss: 1.3860690593719482 current acc: 0.3200990099009901\n",
      "iteration 2 current loss: 1.4146610498428345 current acc: 0.3208866995073892\n",
      "iteration 3 current loss: 1.5705385208129883 current acc: 0.32176470588235295\n",
      "iteration 4 current loss: 1.5808258056640625 current acc: 0.3222439024390244\n",
      "iteration 5 current loss: 1.4785487651824951 current acc: 0.32233009708737864\n",
      "iteration 6 current loss: 1.6384769678115845 current acc: 0.322512077294686\n",
      "iteration 7 current loss: 1.6290502548217773 current acc: 0.32269230769230767\n",
      "iteration 8 current loss: 1.2858341932296753 current acc: 0.3236363636363636\n",
      "iteration 9 current loss: 1.4596076011657715 current acc: 0.3238095238095238\n",
      "iteration 10 current loss: 1.3943904638290405 current acc: 0.32407582938388624\n",
      "iteration 11 current loss: 1.4129618406295776 current acc: 0.324622641509434\n",
      "iteration 12 current loss: 1.4291250705718994 current acc: 0.3254460093896714\n",
      "iteration 13 current loss: 1.4313158988952637 current acc: 0.325981308411215\n",
      "iteration 14 current loss: 1.436524510383606 current acc: 0.32632558139534884\n",
      "iteration 15 current loss: 1.5300616025924683 current acc: 0.32657407407407407\n",
      "iteration 16 current loss: 1.4423786401748657 current acc: 0.32746543778801845\n",
      "iteration 17 current loss: 1.558617353439331 current acc: 0.32724770642201834\n",
      "iteration 18 current loss: 1.3598710298538208 current acc: 0.3280365296803653\n",
      "iteration 19 current loss: 1.625555157661438 current acc: 0.32827272727272727\n",
      "iteration 20 current loss: 1.5124517679214478 current acc: 0.3290497737556561\n",
      "iteration 21 current loss: 1.3427629470825195 current acc: 0.3299099099099099\n",
      "iteration 22 current loss: 1.4919443130493164 current acc: 0.33004484304932735\n",
      "iteration 23 current loss: 1.4774067401885986 current acc: 0.3304464285714286\n",
      "iteration 24 current loss: 1.4951969385147095 current acc: 0.33084444444444444\n",
      "iteration 25 current loss: 1.6327687501907349 current acc: 0.3310619469026549\n",
      "iteration 26 current loss: 1.2983033657073975 current acc: 0.3317180616740088\n",
      "iteration 27 current loss: 1.4803681373596191 current acc: 0.3323684210526316\n",
      "iteration 28 current loss: 1.5221272706985474 current acc: 0.3324017467248908\n",
      "iteration 29 current loss: 1.352504014968872 current acc: 0.3331304347826087\n",
      "iteration 30 current loss: 1.532183051109314 current acc: 0.332987012987013\n",
      "iteration 31 current loss: 1.4080919027328491 current acc: 0.333448275862069\n",
      "iteration 32 current loss: 1.812596082687378 current acc: 0.3332188841201717\n",
      "iteration 33 current loss: 1.5165640115737915 current acc: 0.33350427350427353\n",
      "iteration 34 current loss: 1.5546940565109253 current acc: 0.33387234042553193\n",
      "iteration 35 current loss: 1.4889682531356812 current acc: 0.33440677966101695\n",
      "iteration 36 current loss: 1.4955822229385376 current acc: 0.33502109704641353\n",
      "iteration 37 current loss: 1.4793064594268799 current acc: 0.3354621848739496\n",
      "iteration 38 current loss: 1.3079990148544312 current acc: 0.336234309623431\n",
      "iteration 39 current loss: 1.4796702861785889 current acc: 0.3368333333333333\n",
      "iteration 40 current loss: 1.462691307067871 current acc: 0.3378423236514523\n",
      "iteration 41 current loss: 1.5588210821151733 current acc: 0.33801652892561984\n",
      "iteration 42 current loss: 1.4151158332824707 current acc: 0.3386008230452675\n",
      "iteration 43 current loss: 1.4833275079727173 current acc: 0.33942622950819673\n",
      "iteration 44 current loss: 1.4087092876434326 current acc: 0.34008163265306124\n",
      "iteration 45 current loss: 1.5213125944137573 current acc: 0.34\n",
      "iteration 46 current loss: 1.404323697090149 current acc: 0.3408097165991903\n",
      "iteration 47 current loss: 1.5715597867965698 current acc: 0.3412903225806452\n",
      "iteration 48 current loss: 1.5910789966583252 current acc: 0.3415261044176707\n",
      "iteration 49 current loss: 1.6669738292694092 current acc: 0.3412\n",
      "iteration 50 current loss: 1.4612185955047607 current acc: 0.34127490039840636\n",
      "iteration 51 current loss: 1.4585113525390625 current acc: 0.3418253968253968\n",
      "iteration 52 current loss: 1.3746336698532104 current acc: 0.3422924901185771\n",
      "iteration 53 current loss: 1.3301259279251099 current acc: 0.342755905511811\n",
      "iteration 54 current loss: 1.3619450330734253 current acc: 0.34305882352941175\n",
      "iteration 55 current loss: 1.6275711059570312 current acc: 0.3428125\n",
      "iteration 56 current loss: 1.445317029953003 current acc: 0.34326848249027236\n",
      "iteration 57 current loss: 1.5222543478012085 current acc: 0.3437209302325581\n",
      "iteration 58 current loss: 1.4052740335464478 current acc: 0.3444015444015444\n",
      "iteration 59 current loss: 1.5167241096496582 current acc: 0.344\n",
      "iteration 60 current loss: 1.3629504442214966 current acc: 0.34436781609195405\n",
      "iteration 61 current loss: 1.5296176671981812 current acc: 0.34442748091603054\n",
      "iteration 62 current loss: 1.3513596057891846 current acc: 0.3449429657794677\n",
      "iteration 63 current loss: 1.4747748374938965 current acc: 0.34515151515151515\n",
      "iteration 64 current loss: 1.2424860000610352 current acc: 0.345811320754717\n",
      "iteration 65 current loss: 1.4529718160629272 current acc: 0.3460902255639098\n",
      "iteration 66 current loss: 1.5903325080871582 current acc: 0.3461423220973783\n",
      "iteration 67 current loss: 1.363296627998352 current acc: 0.34694029850746266\n",
      "iteration 68 current loss: 1.3412479162216187 current acc: 0.3475836431226766\n",
      "iteration 69 current loss: 1.3531323671340942 current acc: 0.3479259259259259\n",
      "iteration 70 current loss: 1.3883802890777588 current acc: 0.34848708487084873\n",
      "iteration 71 current loss: 1.4971363544464111 current acc: 0.34875\n",
      "iteration 72 current loss: 1.4252046346664429 current acc: 0.34915750915750915\n",
      "iteration 73 current loss: 1.5081970691680908 current acc: 0.34934306569343065\n",
      "iteration 74 current loss: 1.5015865564346313 current acc: 0.3496\n",
      "iteration 75 current loss: 1.307102918624878 current acc: 0.3501449275362319\n",
      "iteration 76 current loss: 1.245773196220398 current acc: 0.3508303249097473\n",
      "iteration 77 current loss: 1.565923810005188 current acc: 0.3510791366906475\n",
      "iteration 78 current loss: 1.5728344917297363 current acc: 0.3511111111111111\n",
      "iteration 79 current loss: 1.3098292350769043 current acc: 0.35185714285714287\n",
      "iteration 80 current loss: 1.4424229860305786 current acc: 0.3519572953736655\n",
      "iteration 81 current loss: 1.2799257040023804 current acc: 0.3526241134751773\n",
      "iteration 82 current loss: 1.4856648445129395 current acc: 0.3525088339222615\n",
      "iteration 83 current loss: 1.328424334526062 current acc: 0.3530985915492958\n",
      "iteration 84 current loss: 1.5632259845733643 current acc: 0.35347368421052633\n",
      "iteration 85 current loss: 1.2715246677398682 current acc: 0.35405594405594404\n",
      "iteration 86 current loss: 1.4265432357788086 current acc: 0.35449477351916375\n",
      "iteration 87 current loss: 1.5153579711914062 current acc: 0.354375\n",
      "iteration 88 current loss: 1.3630338907241821 current acc: 0.354878892733564\n",
      "iteration 89 current loss: 1.3396081924438477 current acc: 0.35544827586206895\n",
      "iteration 90 current loss: 1.4540228843688965 current acc: 0.35594501718213056\n",
      "iteration 91 current loss: 1.5143687725067139 current acc: 0.3562328767123288\n",
      "iteration 92 current loss: 1.4508846998214722 current acc: 0.3563139931740614\n",
      "iteration 93 current loss: 1.3329861164093018 current acc: 0.3566666666666667\n",
      "iteration 94 current loss: 1.5210264921188354 current acc: 0.3567457627118644\n",
      "iteration 95 current loss: 1.4643970727920532 current acc: 0.3568918918918919\n",
      "iteration 96 current loss: 1.4877150058746338 current acc: 0.3571043771043771\n",
      "iteration 97 current loss: 1.5383151769638062 current acc: 0.3573154362416107\n",
      "iteration 98 current loss: 1.4359172582626343 current acc: 0.35759197324414715\n",
      "iteration 99 current loss: 1.3245501518249512 current acc: 0.35806666666666664\n",
      "\t\tEpoch 2/100 complete. Epoch loss 1.4579370403289795 Epoch accuracy 0.4348\n",
      "Starting Validation Loop...\n",
      "reset Validation statistics\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 123\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# Iterate over the validation dataset in batches\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 123\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    124\u001b[0m \n\u001b[1;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Put val data to device (CPU, GPU, or TPU)\u001b[39;49;00m\n\u001b[1;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_real\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/users2/jwardell1/miniconda3/envs/dcgan/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/data/users2/jwardell1/miniconda3/envs/dcgan/lib/python3.11/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/data/users2/jwardell1/miniconda3/envs/dcgan/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/data/users2/jwardell1/miniconda3/envs/dcgan/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/data/users2/jwardell1/miniconda3/envs/dcgan/lib/python3.11/site-packages/torchvision/datasets/stl10.py:121\u001b[0m, in \u001b[0;36mSTL10.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    118\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(np\u001b[38;5;241m.\u001b[39mtranspose(img, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m)))\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 121\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[0;32m/data/users2/jwardell1/miniconda3/envs/dcgan/lib/python3.11/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m/data/users2/jwardell1/miniconda3/envs/dcgan/lib/python3.11/site-packages/torchvision/transforms/transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/users2/jwardell1/miniconda3/envs/dcgan/lib/python3.11/site-packages/torchvision/transforms/functional.py:174\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    172\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[0;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_float_dtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;241m255\u001b[39m)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "best_loss = float('inf')\n",
    "best_model_state = None\n",
    "num_train = len(train_dataset)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Set up total loss/acc trackers\n",
    "all_loss = []\n",
    "all_acc = []\n",
    "all_correct = 0\n",
    "train_running_total = 0\n",
    "\n",
    "\n",
    "\n",
    "# Set up epochal loss/acc trackers\n",
    "epoch_loss = []\n",
    "epoch_acc = []\n",
    "\n",
    "\n",
    "# Set up validation loss/acc trackers\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "val_running_total = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "\n",
    "# For each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    # Refresh Epoch Statistics\n",
    "    print('reset epoch statistics')\n",
    "    epoch_correct = 0\n",
    "    epoch_loss_val = 0\n",
    "\n",
    "\n",
    "    # Set Network to Train Mode\n",
    "    encoder.train()\n",
    "\n",
    "\n",
    "    # For each batch in the dataloader\n",
    "    for i, (data, labels) in enumerate(train_loader, 0):\n",
    "        # Put train data to device (CPU, GPU, or TPU)\n",
    "        data_real = data.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        #  what does this do? why is this needed here?\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass batch through D\n",
    "        output = encoder(data_real)\n",
    "\n",
    "\n",
    "        # Calculate loss on batch\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "        # Compute Predicted Labels for a Batch in Training Dataset\n",
    "        predicted = torch.argmax(output.data, dim=1).to(device)\n",
    "\n",
    "\n",
    "\n",
    "        correct = (predicted == labels).sum().item()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Update All Data\n",
    "        all_loss.append(loss.item())\n",
    "\n",
    "        all_correct += correct\n",
    "        train_running_total += labels.size(0)\n",
    "\n",
    "\n",
    "        # Compute All Loss/Acc at each datapoint\n",
    "        all_accuracy = all_correct / train_running_total\n",
    "        all_acc.append(all_accuracy)\n",
    "\n",
    "        print(f'iteration {i} current loss: {loss.item()} current acc: {all_accuracy}')\n",
    "\n",
    "\n",
    "        # Update Epoch Data\n",
    "        epoch_correct += correct\n",
    "        epoch_loss_val += loss.item()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Compute Epoch Loss/Acc at end of Epoch\n",
    "    epoch_accuracy = epoch_correct / num_train\n",
    "    epoch_acc.append(epoch_accuracy)\n",
    "\n",
    "    avg_epoch_loss = epoch_loss_val / len(train_loader)\n",
    "    epoch_loss.append(avg_epoch_loss)\n",
    "\n",
    "    print(f'\\t\\tEpoch {epoch}/{num_epochs} complete. Epoch loss {avg_epoch_loss} Epoch accuracy {epoch_accuracy}')\n",
    "\n",
    "    # Validation Step\n",
    "    print('Starting Validation Loop...')\n",
    "\n",
    "\n",
    "\n",
    "    # Refresh Validation Statistics\n",
    "    print('reset Validation statistics')\n",
    "    val_correct = 0\n",
    "    val_loss_value = 0\n",
    "\n",
    "\n",
    "    # Set the model to valuation mode\n",
    "    encoder.eval()\n",
    "\n",
    "\n",
    "    # Iterate over the validation dataset in batches\n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_loader:\n",
    "\n",
    "            # Put val data to device (CPU, GPU, or TPU)\n",
    "            data_real = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "\n",
    "            # Forward pass batch through D\n",
    "            output = encoder(data_real)\n",
    "\n",
    "            # Calculate loss on validation batch\n",
    "            v_loss = criterion(output, labels)\n",
    "\n",
    "            # Compute Predicted Labels for a Batch in Validation Dataset\n",
    "            predicted = torch.argmax(output.data, dim=1).to(device)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Update Val Data\n",
    "            val_loss_value += v_loss.item()\n",
    "\n",
    "\n",
    "\n",
    "    val_accuracy = val_correct / len(test_dataset)\n",
    "    val_acc.append(val_accuracy)\n",
    "\n",
    "    avg_val_loss = val_loss_value / len(test_loader)\n",
    "    val_loss.append(avg_val_loss)\n",
    "\n",
    "    print(f\"\\t\\tValidation Epoch {epoch}, Validation Accuracy: {val_accuracy}, Validation Loss: {avg_val_loss}\")\n",
    "\n",
    "\n",
    "    # Update best model if this epoch had the higest accuracy so far\n",
    "    if avg_epoch_loss < best_loss:\n",
    "        best_loss = avg_epoch_loss\n",
    "        print(f'best loss {best_loss}')\n",
    "        best_model_state = encoder.main.state_dict()\n",
    "\n",
    "\n",
    "\n",
    "# Save the best model\n",
    "if best_model_state is not None:\n",
    "    PATH = '../models/finetuned_encoder_weights_{}_{}_{}.pth'.format(learning_rate, batch_size, weight_decay)\n",
    "    torch.save(best_model_state, PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490f4bda-2de3-42ea-a3ae-416c1768473e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(all_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8286e5bb-b167-42e0-b223-24f907f17eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3796927b-585b-4ab1-880e-a0a68d50dff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a820d289-befb-4cd4-8462-f6d62527a197",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(all_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46da2d99-81a4-4004-b321-0448334edbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f683e964-2d5c-417c-9323-ab8c94a80215",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epoch_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e28a8c8-3530-4c90-9cc8-56658ba2e048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0059601eb5947a9a95392d99a05a4b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.014 MB of 0.022 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.615795…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">summer-butterfly-78</strong> at: <a href='https://wandb.ai/jawardell/dcgan-project/runs/aldvssi5' target=\"_blank\">https://wandb.ai/jawardell/dcgan-project/runs/aldvssi5</a><br/> View job at <a href='https://wandb.ai/jawardell/dcgan-project/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjExODk0MzY3Mg==/version_details/v0' target=\"_blank\">https://wandb.ai/jawardell/dcgan-project/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjExODk0MzY3Mg==/version_details/v0</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231126_223154-aldvssi5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fc350c-42b1-4749-8614-af72ed0e9aad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
